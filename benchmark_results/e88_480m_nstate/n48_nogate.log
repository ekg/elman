Using device: cuda
Output directory: benchmark_results/e88_480m_nstate/n48_nogate/levelE88_100m_20260125_155824
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,569,184 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3466 | lr 3.00e-04 | grad 10.56 | tok/s 5042
step     20 | loss 3.3456 | lr 3.00e-04 | grad 6.03 | tok/s 6503
step     30 | loss 3.1388 | lr 3.00e-04 | grad 3.47 | tok/s 6400
step     40 | loss 3.0361 | lr 3.00e-04 | grad 19.25 | tok/s 6573
step     50 | loss 4.1139 | lr 3.00e-04 | grad 8.38 | tok/s 6795
step     60 | loss 3.2830 | lr 3.00e-04 | grad 6.69 | tok/s 6772
step     70 | loss 2.8256 | lr 3.00e-04 | grad 1.79 | tok/s 6721
step     80 | loss 2.5420 | lr 3.00e-04 | grad 3.47 | tok/s 6683
step     90 | loss 2.3324 | lr 3.00e-04 | grad 2.39 | tok/s 6651
step    100 | loss 2.2515 | lr 3.00e-04 | grad 3.45 | tok/s 6634
step    110 | loss 2.0560 | lr 3.00e-04 | grad 1.52 | tok/s 6592
step    120 | loss 2.6335 | lr 3.00e-04 | grad 1.90 | tok/s 6380
step    130 | loss 2.3342 | lr 3.00e-04 | grad 2.41 | tok/s 6164
step    140 | loss 2.2372 | lr 3.00e-04 | grad 2.27 | tok/s 6258
step    150 | loss 1.9546 | lr 3.00e-04 | grad 2.12 | tok/s 6460
step    160 | loss 2.0938 | lr 3.00e-04 | grad 2.50 | tok/s 6494
step    170 | loss 2.3464 | lr 3.00e-04 | grad 3.47 | tok/s 6161
step    180 | loss 2.1824 | lr 3.00e-04 | grad 3.05 | tok/s 6364
step    190 | loss 1.9805 | lr 3.00e-04 | grad 2.16 | tok/s 6095
step    200 | loss 1.9195 | lr 3.00e-04 | grad 2.47 | tok/s 6527
step    210 | loss 1.8212 | lr 3.00e-04 | grad 1.72 | tok/s 6338
step    220 | loss 2.2419 | lr 3.00e-04 | grad 3.27 | tok/s 6114
step    230 | loss 2.0863 | lr 3.00e-04 | grad 1.41 | tok/s 6136
step    240 | loss 1.9976 | lr 3.00e-04 | grad 2.38 | tok/s 6155
step    250 | loss 2.1199 | lr 3.00e-04 | grad 1.59 | tok/s 6183
step    260 | loss 1.8833 | lr 3.00e-04 | grad 1.35 | tok/s 6386
step    270 | loss 1.9616 | lr 3.00e-04 | grad 1.34 | tok/s 6396
step    280 | loss 1.8091 | lr 3.00e-04 | grad 1.56 | tok/s 6203
step    290 | loss 1.7387 | lr 3.00e-04 | grad 2.02 | tok/s 5958
step    300 | loss 1.8084 | lr 3.00e-04 | grad 1.47 | tok/s 6062
step    310 | loss 1.8316 | lr 3.00e-04 | grad 1.35 | tok/s 6193
step    320 | loss 1.6726 | lr 3.00e-04 | grad 2.33 | tok/s 5935
step    330 | loss 1.8472 | lr 3.00e-04 | grad 1.52 | tok/s 6214
step    340 | loss 1.9024 | lr 3.00e-04 | grad 3.88 | tok/s 6346
step    350 | loss 1.8514 | lr 3.00e-04 | grad 1.90 | tok/s 6227
step    360 | loss 1.6449 | lr 3.00e-04 | grad 1.38 | tok/s 6392
step    370 | loss 1.4914 | lr 3.00e-04 | grad 1.50 | tok/s 6267
step    380 | loss 1.5069 | lr 3.00e-04 | grad 1.34 | tok/s 6546
step    390 | loss 1.2147 | lr 3.00e-04 | grad 1.41 | tok/s 6597
step    400 | loss 1.1385 | lr 3.00e-04 | grad 1.38 | tok/s 6497
step    410 | loss 1.8142 | lr 3.00e-04 | grad 1.43 | tok/s 6276
step    420 | loss 1.6752 | lr 3.00e-04 | grad 1.32 | tok/s 6272
step    430 | loss 1.6310 | lr 3.00e-04 | grad 3.11 | tok/s 6583
step    440 | loss 1.5457 | lr 3.00e-04 | grad 1.57 | tok/s 6375
step    450 | loss 1.7076 | lr 3.00e-04 | grad 1.34 | tok/s 6286
step    460 | loss 1.5376 | lr 3.00e-04 | grad 3.48 | tok/s 6235
step    470 | loss 1.5871 | lr 3.00e-04 | grad 1.73 | tok/s 6238
step    480 | loss 1.5430 | lr 3.00e-04 | grad 1.42 | tok/s 6531

Training complete! Final step: 482
