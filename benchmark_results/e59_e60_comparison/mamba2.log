Using device: cuda
Output directory: output/levelmamba2_100m_20260113_222749
Model: Level mamba2, 94,350,344 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.6201 | lr 9.00e-07 | grad 12.74 | tok/s 5003
step     20 | loss 5.3456 | lr 1.90e-06 | grad 7.26 | tok/s 23642
step     30 | loss 5.0661 | lr 2.90e-06 | grad 6.47 | tok/s 27417
step     40 | loss 4.8900 | lr 3.90e-06 | grad 8.05 | tok/s 28362
step     50 | loss 5.3207 | lr 4.90e-06 | grad 7.72 | tok/s 29259
step     60 | loss 4.9834 | lr 5.90e-06 | grad 3.97 | tok/s 29317
step     70 | loss 4.5839 | lr 6.90e-06 | grad 7.19 | tok/s 28728
step     80 | loss 4.3481 | lr 7.90e-06 | grad 3.63 | tok/s 28282
step     90 | loss 3.8895 | lr 8.90e-06 | grad 3.59 | tok/s 28565
step    100 | loss 3.6313 | lr 9.90e-06 | grad 3.91 | tok/s 28842
step    110 | loss 3.3034 | lr 1.09e-05 | grad 2.68 | tok/s 28362
step    120 | loss 3.6950 | lr 1.19e-05 | grad 2.45 | tok/s 27775
step    130 | loss 3.0151 | lr 1.29e-05 | grad 2.26 | tok/s 27215
step    140 | loss 2.8203 | lr 1.39e-05 | grad 3.19 | tok/s 26745
step    150 | loss 3.2121 | lr 1.49e-05 | grad 4.96 | tok/s 28070
step    160 | loss 3.0717 | lr 1.59e-05 | grad 4.30 | tok/s 28296
step    170 | loss 2.9429 | lr 1.69e-05 | grad 3.99 | tok/s 26768
step    180 | loss 2.9818 | lr 1.79e-05 | grad 5.21 | tok/s 27508
step    190 | loss 2.7813 | lr 1.89e-05 | grad 2.16 | tok/s 26433
step    200 | loss 2.4053 | lr 1.99e-05 | grad 1.89 | tok/s 28240
step    210 | loss 2.2972 | lr 2.09e-05 | grad 2.41 | tok/s 26929
step    220 | loss 2.6085 | lr 2.19e-05 | grad 3.74 | tok/s 25862
step    230 | loss 2.7275 | lr 2.29e-05 | grad 1.81 | tok/s 26227
step    240 | loss 2.3722 | lr 2.39e-05 | grad 2.41 | tok/s 26425
step    250 | loss 2.6101 | lr 2.49e-05 | grad 2.30 | tok/s 26044
step    260 | loss 2.1921 | lr 2.59e-05 | grad 2.06 | tok/s 27026
step    270 | loss 2.3442 | lr 2.69e-05 | grad 2.17 | tok/s 27281
step    280 | loss 2.0246 | lr 2.79e-05 | grad 2.78 | tok/s 26231
step    290 | loss 2.0161 | lr 2.89e-05 | grad 4.04 | tok/s 25632
step    300 | loss 2.0930 | lr 2.99e-05 | grad 3.27 | tok/s 25569
step    310 | loss 2.0613 | lr 3.09e-05 | grad 2.20 | tok/s 26290
step    320 | loss 1.8574 | lr 3.19e-05 | grad 3.27 | tok/s 25037
step    330 | loss 2.1149 | lr 3.29e-05 | grad 1.81 | tok/s 25989
step    340 | loss 2.1638 | lr 3.39e-05 | grad 5.87 | tok/s 26826
step    350 | loss 2.1609 | lr 3.49e-05 | grad 2.96 | tok/s 26498
step    360 | loss 2.1222 | lr 3.59e-05 | grad 2.71 | tok/s 27151
step    370 | loss 1.8093 | lr 3.69e-05 | grad 2.09 | tok/s 26532
step    380 | loss 1.8645 | lr 3.79e-05 | grad 2.37 | tok/s 27933
step    390 | loss 1.5190 | lr 3.89e-05 | grad 1.97 | tok/s 28138
step    400 | loss 1.3851 | lr 3.99e-05 | grad 1.97 | tok/s 27827
step    410 | loss 2.1947 | lr 4.09e-05 | grad 2.47 | tok/s 26820
step    420 | loss 2.0382 | lr 4.19e-05 | grad 2.74 | tok/s 26843
step    430 | loss 2.0025 | lr 4.29e-05 | grad 3.19 | tok/s 28155
step    440 | loss 1.8587 | lr 4.39e-05 | grad 2.52 | tok/s 27317
step    450 | loss 1.9735 | lr 4.49e-05 | grad 1.69 | tok/s 26825
step    460 | loss 1.7280 | lr 4.59e-05 | grad 4.36 | tok/s 26572
step    470 | loss 1.8402 | lr 4.69e-05 | grad 2.09 | tok/s 26478
step    480 | loss 1.8265 | lr 4.79e-05 | grad 2.82 | tok/s 27963
step    490 | loss 1.8545 | lr 4.89e-05 | grad 2.03 | tok/s 26838
step    500 | loss 1.8561 | lr 4.99e-05 | grad 1.94 | tok/s 26592
step    510 | loss 2.0768 | lr 5.09e-05 | grad 6.33 | tok/s 26064
step    520 | loss 1.7939 | lr 5.19e-05 | grad 2.01 | tok/s 25018
step    530 | loss 1.6721 | lr 5.29e-05 | grad 2.22 | tok/s 26518
step    540 | loss 1.9017 | lr 5.39e-05 | grad 1.93 | tok/s 26354
step    550 | loss 1.8100 | lr 5.49e-05 | grad 1.79 | tok/s 25905
step    560 | loss 1.5456 | lr 5.59e-05 | grad 2.31 | tok/s 27097
step    570 | loss 1.6083 | lr 5.69e-05 | grad 1.94 | tok/s 27942
step    580 | loss 1.4516 | lr 5.79e-05 | grad 1.50 | tok/s 28070
step    590 | loss 1.3957 | lr 5.89e-05 | grad 1.25 | tok/s 28053
step    600 | loss 1.4882 | lr 5.99e-05 | grad 1.80 | tok/s 28080
step    610 | loss 1.3956 | lr 6.09e-05 | grad 1.36 | tok/s 28060
step    620 | loss 1.4053 | lr 6.19e-05 | grad 1.14 | tok/s 27896
step    630 | loss 1.4711 | lr 6.29e-05 | grad 4.79 | tok/s 27643
step    640 | loss 1.8832 | lr 6.39e-05 | grad 2.75 | tok/s 26515
step    650 | loss 1.8674 | lr 6.49e-05 | grad 2.38 | tok/s 26294
step    660 | loss 1.7047 | lr 6.59e-05 | grad 2.35 | tok/s 26622
step    670 | loss 1.7721 | lr 6.69e-05 | grad 1.92 | tok/s 27503
step    680 | loss 1.8301 | lr 6.79e-05 | grad 2.34 | tok/s 26490
step    690 | loss 1.8137 | lr 6.89e-05 | grad 2.22 | tok/s 26283
step    700 | loss 1.7800 | lr 6.99e-05 | grad 1.90 | tok/s 26133
step    710 | loss 1.6492 | lr 7.09e-05 | grad 1.92 | tok/s 26725
step    720 | loss 1.8589 | lr 7.19e-05 | grad 3.13 | tok/s 26305
step    730 | loss 1.4792 | lr 7.29e-05 | grad 1.45 | tok/s 27494
step    740 | loss 1.5970 | lr 7.39e-05 | grad 1.46 | tok/s 26763
step    750 | loss 2.1708 | lr 7.49e-05 | grad 3.40 | tok/s 27762
step    760 | loss 1.8996 | lr 7.59e-05 | grad 1.47 | tok/s 27861
step    770 | loss 1.6890 | lr 7.69e-05 | grad 2.32 | tok/s 27113
step    780 | loss 1.7430 | lr 7.79e-05 | grad 1.92 | tok/s 26333
step    790 | loss 1.6999 | lr 7.89e-05 | grad 1.77 | tok/s 27046
step    800 | loss 1.9263 | lr 7.99e-05 | grad 3.72 | tok/s 27803
step    810 | loss 1.6914 | lr 8.09e-05 | grad 1.38 | tok/s 27082
step    820 | loss 1.4360 | lr 8.19e-05 | grad 3.85 | tok/s 26356
step    830 | loss 1.6825 | lr 8.29e-05 | grad 1.95 | tok/s 26787
step    840 | loss 1.7154 | lr 8.39e-05 | grad 1.34 | tok/s 25947
step    850 | loss 1.8174 | lr 8.49e-05 | grad 1.39 | tok/s 26074
step    860 | loss 1.8068 | lr 8.59e-05 | grad 1.46 | tok/s 26282
step    870 | loss 1.7370 | lr 8.69e-05 | grad 3.04 | tok/s 26724
step    880 | loss 1.7789 | lr 8.79e-05 | grad 2.20 | tok/s 27711
step    890 | loss 1.7690 | lr 8.89e-05 | grad 1.79 | tok/s 26577
step    900 | loss 1.6348 | lr 8.99e-05 | grad 1.30 | tok/s 26360
step    910 | loss 1.5998 | lr 9.09e-05 | grad 1.52 | tok/s 26662
step    920 | loss 1.7107 | lr 9.19e-05 | grad 1.22 | tok/s 26454
step    930 | loss 1.6845 | lr 9.29e-05 | grad 1.29 | tok/s 26501
step    940 | loss 1.5806 | lr 9.39e-05 | grad 1.58 | tok/s 27305
step    950 | loss 1.5192 | lr 9.49e-05 | grad 1.17 | tok/s 26384
step    960 | loss 1.6178 | lr 9.59e-05 | grad 1.01 | tok/s 25942
step    970 | loss 1.5325 | lr 9.69e-05 | grad 1.03 | tok/s 26269
step    980 | loss 1.5354 | lr 9.79e-05 | grad 1.16 | tok/s 26904
step    990 | loss 2.2546 | lr 9.89e-05 | grad 1.81 | tok/s 27955
step   1000 | loss 1.8922 | lr 9.99e-05 | grad 1.22 | tok/s 26711
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8922.pt
step   1010 | loss 1.7880 | lr 1.02e-06 | grad 1.59 | tok/s 13868
step   1020 | loss 1.4702 | lr 1.09e-06 | grad 1.47 | tok/s 23816
step   1030 | loss 1.4640 | lr 1.21e-06 | grad 2.52 | tok/s 24600
step   1040 | loss 1.8921 | lr 1.37e-06 | grad 1.35 | tok/s 24664
step   1050 | loss 1.9483 | lr 1.59e-06 | grad 4.18 | tok/s 22822
step   1060 | loss 2.4951 | lr 1.85e-06 | grad 1.05 | tok/s 25001
step   1070 | loss 1.8033 | lr 2.16e-06 | grad 6.09 | tok/s 23900
step   1080 | loss 1.3134 | lr 2.52e-06 | grad 1.69 | tok/s 21777
step   1090 | loss 1.5392 | lr 2.92e-06 | grad 1.37 | tok/s 24611
step   1100 | loss 1.4528 | lr 3.37e-06 | grad 1.05 | tok/s 24744
step   1110 | loss 1.4211 | lr 3.87e-06 | grad 0.89 | tok/s 25289
step   1120 | loss 1.3906 | lr 4.42e-06 | grad 0.98 | tok/s 24955
step   1130 | loss 1.4025 | lr 5.01e-06 | grad 1.25 | tok/s 22078
step   1140 | loss 1.9019 | lr 5.65e-06 | grad 3.25 | tok/s 24551
step   1150 | loss 2.1357 | lr 6.32e-06 | grad 2.54 | tok/s 24717
step   1160 | loss 1.6749 | lr 7.05e-06 | grad 1.55 | tok/s 24447
step   1170 | loss 2.2469 | lr 7.81e-06 | grad 2.36 | tok/s 21091
step   1180 | loss 1.9109 | lr 8.62e-06 | grad 1.29 | tok/s 23339
step   1190 | loss 1.5817 | lr 9.47e-06 | grad 1.08 | tok/s 23626
step   1200 | loss 1.6323 | lr 1.04e-05 | grad 2.09 | tok/s 24447
step   1210 | loss 2.0691 | lr 1.13e-05 | grad 1.26 | tok/s 24283
step   1220 | loss 1.3493 | lr 1.23e-05 | grad 1.18 | tok/s 24919
step   1230 | loss 1.5517 | lr 1.33e-05 | grad 1.72 | tok/s 22325
step   1240 | loss 1.4843 | lr 1.43e-05 | grad 1.28 | tok/s 22396
step   1250 | loss 1.5283 | lr 1.54e-05 | grad 1.01 | tok/s 23336
step   1260 | loss 1.4522 | lr 1.65e-05 | grad 1.12 | tok/s 24672
step   1270 | loss 1.8249 | lr 1.76e-05 | grad 3.37 | tok/s 23192
step   1280 | loss 1.6982 | lr 1.88e-05 | grad 1.18 | tok/s 25013
step   1290 | loss 1.4675 | lr 2.00e-05 | grad 1.24 | tok/s 24057
step   1300 | loss 1.5701 | lr 2.13e-05 | grad 1.58 | tok/s 23409
step   1310 | loss 1.5334 | lr 2.25e-05 | grad 1.12 | tok/s 21963
step   1320 | loss 2.0062 | lr 2.38e-05 | grad 6.37 | tok/s 23593
step   1330 | loss 1.5943 | lr 2.52e-05 | grad 1.36 | tok/s 22923
step   1340 | loss 1.7167 | lr 2.65e-05 | grad 1.86 | tok/s 23903
step   1350 | loss 1.5420 | lr 2.79e-05 | grad 1.23 | tok/s 23186
step   1360 | loss 1.6707 | lr 2.93e-05 | grad 1.48 | tok/s 23587
step   1370 | loss 1.6434 | lr 3.07e-05 | grad 3.28 | tok/s 22756
step   1380 | loss 1.6266 | lr 3.21e-05 | grad 1.88 | tok/s 23124
step   1390 | loss 1.9764 | lr 3.36e-05 | grad 1.80 | tok/s 24901
step   1400 | loss 1.4989 | lr 3.51e-05 | grad 3.39 | tok/s 23619
step   1410 | loss 1.5613 | lr 3.65e-05 | grad 1.09 | tok/s 25188
step   1420 | loss 1.6935 | lr 3.80e-05 | grad 1.53 | tok/s 24357
step   1430 | loss 1.5070 | lr 3.96e-05 | grad 2.75 | tok/s 23002
step   1440 | loss 1.6085 | lr 4.11e-05 | grad 5.37 | tok/s 25331
step   1450 | loss 1.8188 | lr 4.26e-05 | grad 1.43 | tok/s 23206
step   1460 | loss 1.6577 | lr 4.41e-05 | grad 2.00 | tok/s 24916
step   1470 | loss 1.6367 | lr 4.57e-05 | grad 1.97 | tok/s 24616
step   1480 | loss 1.6483 | lr 4.72e-05 | grad 2.94 | tok/s 23300
step   1490 | loss 1.4157 | lr 4.88e-05 | grad 1.11 | tok/s 22464
step   1500 | loss 1.5418 | lr 5.03e-05 | grad 1.53 | tok/s 24195
step   1510 | loss 2.0323 | lr 5.19e-05 | grad 6.15 | tok/s 24325
step   1520 | loss 1.5621 | lr 5.35e-05 | grad 1.23 | tok/s 24233
step   1530 | loss 1.4352 | lr 5.50e-05 | grad 1.06 | tok/s 24587
step   1540 | loss 1.5942 | lr 5.65e-05 | grad 1.29 | tok/s 24489
step   1550 | loss 1.4768 | lr 5.81e-05 | grad 1.18 | tok/s 23916
step   1560 | loss 1.6508 | lr 5.96e-05 | grad 0.96 | tok/s 24495
step   1570 | loss 1.5786 | lr 6.11e-05 | grad 2.39 | tok/s 24346
step   1580 | loss 1.3594 | lr 6.27e-05 | grad 0.86 | tok/s 24882
step   1590 | loss 1.5436 | lr 6.42e-05 | grad 1.35 | tok/s 24355
step   1600 | loss 1.3767 | lr 6.56e-05 | grad 1.46 | tok/s 23637
step   1610 | loss 1.5867 | lr 6.71e-05 | grad 2.89 | tok/s 22981
step   1620 | loss 1.5039 | lr 6.86e-05 | grad 3.37 | tok/s 24648
step   1630 | loss 2.1121 | lr 7.00e-05 | grad 3.05 | tok/s 24280
step   1640 | loss 2.3674 | lr 7.14e-05 | grad 1.46 | tok/s 25277
step   1650 | loss 1.8011 | lr 7.28e-05 | grad 1.19 | tok/s 25248
step   1660 | loss 1.5470 | lr 7.42e-05 | grad 1.33 | tok/s 24973
step   1670 | loss 1.4040 | lr 7.56e-05 | grad 0.90 | tok/s 24911
step   1680 | loss 1.3459 | lr 7.69e-05 | grad 1.14 | tok/s 25139
step   1690 | loss 1.7332 | lr 7.82e-05 | grad 3.09 | tok/s 24025
step   1700 | loss 1.7115 | lr 7.95e-05 | grad 1.19 | tok/s 24042
step   1710 | loss 1.5744 | lr 8.07e-05 | grad 1.72 | tok/s 23871
step   1720 | loss 1.4974 | lr 8.19e-05 | grad 1.18 | tok/s 24158
step   1730 | loss 1.3631 | lr 8.31e-05 | grad 1.98 | tok/s 24561
step   1740 | loss 1.6136 | lr 8.43e-05 | grad 1.51 | tok/s 23952
step   1750 | loss 1.5745 | lr 8.54e-05 | grad 1.29 | tok/s 23007
step   1760 | loss 1.5560 | lr 8.65e-05 | grad 1.28 | tok/s 23826
step   1770 | loss 1.4888 | lr 8.75e-05 | grad 1.22 | tok/s 24679
step   1780 | loss 1.4679 | lr 8.85e-05 | grad 1.34 | tok/s 25352
step   1790 | loss 1.8854 | lr 8.95e-05 | grad 1.34 | tok/s 25518
step   1800 | loss 1.8867 | lr 9.05e-05 | grad 1.15 | tok/s 24483
step   1810 | loss 1.5247 | lr 9.14e-05 | grad 1.46 | tok/s 25059
step   1820 | loss 1.4604 | lr 9.22e-05 | grad 1.22 | tok/s 25420
step   1830 | loss 1.5453 | lr 9.30e-05 | grad 1.03 | tok/s 24927
step   1840 | loss 1.5410 | lr 9.38e-05 | grad 1.46 | tok/s 25506
step   1850 | loss 1.5712 | lr 9.45e-05 | grad 1.42 | tok/s 24834
step   1860 | loss 1.6021 | lr 9.52e-05 | grad 1.67 | tok/s 25322
step   1870 | loss 1.4516 | lr 9.59e-05 | grad 3.05 | tok/s 24786
step   1880 | loss 1.5774 | lr 9.65e-05 | grad 2.00 | tok/s 25161
step   1890 | loss 1.5960 | lr 9.70e-05 | grad 1.01 | tok/s 24993

Training complete! Final step: 1899
