Using device: cuda
Output directory: output/level42_100m_20260113_221734
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 37,387,328 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.4567 | lr 9.00e-07 | grad 48.96 | tok/s 6595
step     20 | loss 4.8009 | lr 1.90e-06 | grad 24.93 | tok/s 9709
step     30 | loss 4.3100 | lr 2.90e-06 | grad 21.23 | tok/s 10187
step     40 | loss 3.9556 | lr 3.90e-06 | grad 30.20 | tok/s 10773
step     50 | loss 5.1198 | lr 4.90e-06 | grad 25.64 | tok/s 11989
step     60 | loss 4.6096 | lr 5.90e-06 | grad 16.12 | tok/s 12651
step     70 | loss 4.2447 | lr 6.90e-06 | grad 30.48 | tok/s 12453
step     80 | loss 3.9774 | lr 7.90e-06 | grad 14.98 | tok/s 10003
step     90 | loss 3.4582 | lr 8.90e-06 | grad 13.80 | tok/s 11284
step    100 | loss 3.2061 | lr 9.90e-06 | grad 14.77 | tok/s 12036
step    110 | loss 2.9115 | lr 1.09e-05 | grad 20.13 | tok/s 11046
step    120 | loss 3.5456 | lr 1.19e-05 | grad 12.78 | tok/s 10647
step    130 | loss 2.8665 | lr 1.29e-05 | grad 7.67 | tok/s 10290
step    140 | loss 2.6613 | lr 1.39e-05 | grad 19.25 | tok/s 10695
step    150 | loss 2.9661 | lr 1.49e-05 | grad 15.15 | tok/s 15245
step    160 | loss 2.9174 | lr 1.59e-05 | grad 12.63 | tok/s 15794
step    170 | loss 2.9715 | lr 1.69e-05 | grad 18.00 | tok/s 12508
step    180 | loss 2.9117 | lr 1.79e-05 | grad 15.97 | tok/s 12899
step    190 | loss 2.6953 | lr 1.89e-05 | grad 10.03 | tok/s 13509
step    200 | loss 2.3332 | lr 1.99e-05 | grad 6.94 | tok/s 11736
step    210 | loss 2.2340 | lr 2.09e-05 | grad 9.85 | tok/s 10612
step    220 | loss 2.5688 | lr 2.19e-05 | grad 11.62 | tok/s 10422
step    230 | loss 2.8004 | lr 2.29e-05 | grad 8.27 | tok/s 13107
step    240 | loss 2.4057 | lr 2.39e-05 | grad 9.60 | tok/s 13155
step    250 | loss 2.5773 | lr 2.49e-05 | grad 8.42 | tok/s 13368
step    260 | loss 2.1939 | lr 2.59e-05 | grad 6.77 | tok/s 14037
step    270 | loss 2.3378 | lr 2.69e-05 | grad 9.12 | tok/s 12860
step    280 | loss 2.0378 | lr 2.79e-05 | grad 7.59 | tok/s 10886
step    290 | loss 2.0257 | lr 2.89e-05 | grad 13.00 | tok/s 11264
step    300 | loss 2.0824 | lr 2.99e-05 | grad 9.14 | tok/s 11479
step    310 | loss 2.0746 | lr 3.09e-05 | grad 6.23 | tok/s 11796
step    320 | loss 1.8740 | lr 3.19e-05 | grad 10.16 | tok/s 11948
step    330 | loss 2.1376 | lr 3.29e-05 | grad 6.79 | tok/s 12850
step    340 | loss 2.1898 | lr 3.39e-05 | grad 21.60 | tok/s 14936
step    350 | loss 2.1768 | lr 3.49e-05 | grad 9.33 | tok/s 16083
step    360 | loss 2.0858 | lr 3.59e-05 | grad 9.19 | tok/s 14145
step    370 | loss 1.8269 | lr 3.69e-05 | grad 6.26 | tok/s 11044
step    380 | loss 1.8445 | lr 3.79e-05 | grad 7.35 | tok/s 11455
step    390 | loss 1.4766 | lr 3.89e-05 | grad 6.21 | tok/s 11434
step    400 | loss 1.3734 | lr 3.99e-05 | grad 7.43 | tok/s 11360
step    410 | loss 2.2195 | lr 4.09e-05 | grad 6.44 | tok/s 11223
step    420 | loss 2.0782 | lr 4.19e-05 | grad 8.07 | tok/s 11264
step    430 | loss 2.0210 | lr 4.29e-05 | grad 11.46 | tok/s 12639
step    440 | loss 1.9017 | lr 4.39e-05 | grad 8.64 | tok/s 15147
step    450 | loss 2.0171 | lr 4.49e-05 | grad 5.45 | tok/s 12528
step    460 | loss 1.7606 | lr 4.59e-05 | grad 11.54 | tok/s 11670
step    470 | loss 1.8722 | lr 4.69e-05 | grad 6.34 | tok/s 11616
step    480 | loss 1.8771 | lr 4.79e-05 | grad 7.97 | tok/s 12030
step    490 | loss 1.8484 | lr 4.89e-05 | grad 6.00 | tok/s 12925
step    500 | loss 1.8951 | lr 4.99e-05 | grad 5.38 | tok/s 13001
step    510 | loss 2.1221 | lr 5.09e-05 | grad 21.03 | tok/s 15780
step    520 | loss 1.8580 | lr 5.19e-05 | grad 5.64 | tok/s 17098
step    530 | loss 1.7170 | lr 5.29e-05 | grad 5.58 | tok/s 16635
step    540 | loss 1.9495 | lr 5.39e-05 | grad 5.51 | tok/s 12977
step    550 | loss 1.8488 | lr 5.49e-05 | grad 5.00 | tok/s 13166
step    560 | loss 1.5842 | lr 5.59e-05 | grad 5.70 | tok/s 16651
step    570 | loss 1.6041 | lr 5.69e-05 | grad 5.07 | tok/s 14911
step    580 | loss 1.4477 | lr 5.79e-05 | grad 4.03 | tok/s 12801
step    590 | loss 1.3981 | lr 5.89e-05 | grad 3.59 | tok/s 11821
step    600 | loss 1.5024 | lr 5.99e-05 | grad 5.16 | tok/s 12869
step    610 | loss 1.3967 | lr 6.09e-05 | grad 3.93 | tok/s 13259
step    620 | loss 1.4198 | lr 6.19e-05 | grad 2.91 | tok/s 13265
step    630 | loss 1.5180 | lr 6.29e-05 | grad 22.66 | tok/s 13756
step    640 | loss 1.9417 | lr 6.39e-05 | grad 7.64 | tok/s 13375
step    650 | loss 1.8918 | lr 6.49e-05 | grad 5.87 | tok/s 13748
step    660 | loss 1.7740 | lr 6.59e-05 | grad 6.15 | tok/s 11422
step    670 | loss 1.8018 | lr 6.69e-05 | grad 4.36 | tok/s 10514
step    680 | loss 1.8726 | lr 6.79e-05 | grad 5.47 | tok/s 10280
step    690 | loss 1.8635 | lr 6.89e-05 | grad 4.96 | tok/s 11070
step    700 | loss 1.8346 | lr 6.99e-05 | grad 5.11 | tok/s 11207
step    710 | loss 1.7024 | lr 7.09e-05 | grad 4.54 | tok/s 11597
step    720 | loss 1.8978 | lr 7.19e-05 | grad 7.86 | tok/s 11914
step    730 | loss 1.5310 | lr 7.29e-05 | grad 3.57 | tok/s 12276
step    740 | loss 1.6367 | lr 7.39e-05 | grad 3.80 | tok/s 12556
step    750 | loss 2.2147 | lr 7.49e-05 | grad 10.30 | tok/s 11309
step    760 | loss 1.9121 | lr 7.59e-05 | grad 3.69 | tok/s 11607
step    770 | loss 1.7587 | lr 7.69e-05 | grad 4.43 | tok/s 10811
step    780 | loss 1.7617 | lr 7.79e-05 | grad 4.19 | tok/s 10387
step    790 | loss 1.7322 | lr 7.89e-05 | grad 3.72 | tok/s 10301
step    800 | loss 1.9674 | lr 7.99e-05 | grad 8.44 | tok/s 11223
step    810 | loss 1.7513 | lr 8.09e-05 | grad 4.44 | tok/s 13450
step    820 | loss 1.4760 | lr 8.19e-05 | grad 7.46 | tok/s 11007
step    830 | loss 1.7218 | lr 8.29e-05 | grad 4.51 | tok/s 10293
step    840 | loss 1.7730 | lr 8.39e-05 | grad 2.91 | tok/s 11059
step    850 | loss 1.9005 | lr 8.49e-05 | grad 4.23 | tok/s 11744
step    860 | loss 1.8852 | lr 8.59e-05 | grad 3.22 | tok/s 11381
step    870 | loss 1.8282 | lr 8.69e-05 | grad 6.30 | tok/s 9759
step    880 | loss 1.7886 | lr 8.79e-05 | grad 4.93 | tok/s 10337
step    890 | loss 1.8457 | lr 8.89e-05 | grad 3.80 | tok/s 10130
step    900 | loss 1.6863 | lr 8.99e-05 | grad 3.11 | tok/s 10137

Training complete! Final step: 907
