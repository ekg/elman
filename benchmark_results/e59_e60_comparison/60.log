Using device: cuda
Output directory: output/level60_100m_20260113_222747
Auto r_h_mode: spectral_norm (level 60 has full W_h)
Model: Level 60, 33,354,457 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5784 | lr 9.00e-07 | grad 672.40 | tok/s 7820
step     20 | loss 5.5412 | lr 1.90e-06 | grad 236.41 | tok/s 12136
step     30 | loss 5.5299 | lr 2.90e-06 | grad 203.92 | tok/s 12072
step     40 | loss 5.5136 | lr 3.90e-06 | grad 241.67 | tok/s 12617
step     50 | loss 5.5866 | lr 4.90e-06 | grad 207.59 | tok/s 12682
step     60 | loss 5.5634 | lr 5.90e-06 | grad 261.63 | tok/s 14215
step     70 | loss 5.5123 | lr 6.90e-06 | grad 239.04 | tok/s 15941
step     80 | loss 5.4696 | lr 7.90e-06 | grad 196.04 | tok/s 15934
step     90 | loss 5.4432 | lr 8.90e-06 | grad 151.36 | tok/s 16026
step    100 | loss 5.3958 | lr 9.90e-06 | grad 194.11 | tok/s 15793
step    110 | loss 5.3542 | lr 1.09e-05 | grad 254.51 | tok/s 15752
step    120 | loss 5.1659 | lr 1.19e-05 | grad 208.95 | tok/s 15343
step    130 | loss 5.0135 | lr 1.29e-05 | grad 115.84 | tok/s 15061
step    140 | loss 4.8331 | lr 1.39e-05 | grad 140.59 | tok/s 15539
step    150 | loss 4.5765 | lr 1.49e-05 | grad 159.53 | tok/s 16092
step    160 | loss 4.4975 | lr 1.59e-05 | grad 82.14 | tok/s 16143
step    170 | loss 4.5325 | lr 1.69e-05 | grad 170.05 | tok/s 15231
step    180 | loss 4.4508 | lr 1.79e-05 | grad 61.45 | tok/s 15056
step    190 | loss 4.4000 | lr 1.89e-05 | grad 99.91 | tok/s 12912
step    200 | loss 4.0448 | lr 1.99e-05 | grad 47.99 | tok/s 14547
step    210 | loss 4.0091 | lr 2.09e-05 | grad 128.20 | tok/s 16380
step    220 | loss 4.0312 | lr 2.19e-05 | grad 107.31 | tok/s 14850
step    230 | loss 4.2317 | lr 2.29e-05 | grad 109.10 | tok/s 15278
step    240 | loss 3.9317 | lr 2.39e-05 | grad 140.49 | tok/s 15561
step    250 | loss 4.0944 | lr 2.49e-05 | grad 26.65 | tok/s 15667
step    260 | loss 3.8169 | lr 2.59e-05 | grad 28.19 | tok/s 15308
step    270 | loss 3.7887 | lr 2.69e-05 | grad 47.63 | tok/s 15116
step    280 | loss 3.6564 | lr 2.79e-05 | grad 52.49 | tok/s 14522
step    290 | loss 3.7018 | lr 2.89e-05 | grad 16.54 | tok/s 13962
step    300 | loss 3.7092 | lr 2.99e-05 | grad 45.75 | tok/s 14252
step    310 | loss 3.6395 | lr 3.09e-05 | grad 20.22 | tok/s 14923
step    320 | loss 3.5098 | lr 3.19e-05 | grad 18.88 | tok/s 14433
step    330 | loss 3.6043 | lr 3.29e-05 | grad 115.96 | tok/s 15118
step    340 | loss 3.6022 | lr 3.39e-05 | grad 29.54 | tok/s 15805
step    350 | loss 3.6668 | lr 3.49e-05 | grad 8.47 | tok/s 15381
step    360 | loss 3.6612 | lr 3.59e-05 | grad 13.82 | tok/s 15756
step    370 | loss 3.5338 | lr 3.69e-05 | grad 7.29 | tok/s 13313
step    380 | loss 3.4242 | lr 3.79e-05 | grad 6.47 | tok/s 15354
step    390 | loss 3.3446 | lr 3.89e-05 | grad 6.70 | tok/s 16567
step    400 | loss 3.3738 | lr 3.99e-05 | grad 20.34 | tok/s 16331
step    410 | loss 3.6472 | lr 4.09e-05 | grad 4.03 | tok/s 15812
step    420 | loss 3.4754 | lr 4.19e-05 | grad 6.23 | tok/s 15803
step    430 | loss 3.5766 | lr 4.29e-05 | grad 3.85 | tok/s 16415
step    440 | loss 3.4381 | lr 4.39e-05 | grad 4.88 | tok/s 15980
step    450 | loss 3.4903 | lr 4.49e-05 | grad 6.83 | tok/s 13187
step    460 | loss 3.3083 | lr 4.59e-05 | grad 4.04 | tok/s 12934
step    470 | loss 3.5158 | lr 4.69e-05 | grad 8.54 | tok/s 13669
step    480 | loss 3.5098 | lr 4.79e-05 | grad 1.35 | tok/s 13969
step    490 | loss 3.4604 | lr 4.89e-05 | grad 6.78 | tok/s 13935
step    500 | loss 3.4079 | lr 4.99e-05 | grad 1.95 | tok/s 13559
step    510 | loss 3.5622 | lr 5.09e-05 | grad 4.28 | tok/s 14741
step    520 | loss 3.3724 | lr 5.19e-05 | grad 2.87 | tok/s 13389
step    530 | loss 3.3358 | lr 5.29e-05 | grad 6.12 | tok/s 13900
step    540 | loss 3.4733 | lr 5.39e-05 | grad 3.17 | tok/s 14014
step    550 | loss 3.5449 | lr 5.49e-05 | grad 1.44 | tok/s 13446
step    560 | loss 3.3160 | lr 5.59e-05 | grad 3.32 | tok/s 13785
step    570 | loss 3.2410 | lr 5.69e-05 | grad 0.82 | tok/s 13686
step    580 | loss 3.1446 | lr 5.79e-05 | grad 0.73 | tok/s 13877
step    590 | loss 3.1131 | lr 5.89e-05 | grad 0.84 | tok/s 13553
step    600 | loss 3.1769 | lr 5.99e-05 | grad 0.72 | tok/s 13848
step    610 | loss 3.1979 | lr 6.09e-05 | grad 0.63 | tok/s 13810
step    620 | loss 3.1019 | lr 6.19e-05 | grad 0.71 | tok/s 13790
step    630 | loss 3.3359 | lr 6.29e-05 | grad 5.68 | tok/s 13828
step    640 | loss 3.6320 | lr 6.39e-05 | grad 2.15 | tok/s 12932
step    650 | loss 3.5276 | lr 6.49e-05 | grad 2.14 | tok/s 12863
step    660 | loss 3.4020 | lr 6.59e-05 | grad 1.02 | tok/s 12386
step    670 | loss 3.3858 | lr 6.69e-05 | grad 1.27 | tok/s 12710
step    680 | loss 3.4461 | lr 6.79e-05 | grad 3.36 | tok/s 12239
step    690 | loss 3.4610 | lr 6.89e-05 | grad 2.58 | tok/s 13290
step    700 | loss 3.4562 | lr 6.99e-05 | grad 1.23 | tok/s 12253
step    710 | loss 3.3907 | lr 7.09e-05 | grad 1.80 | tok/s 13128
step    720 | loss 3.4964 | lr 7.19e-05 | grad 2.38 | tok/s 12048
step    730 | loss 3.3039 | lr 7.29e-05 | grad 1.28 | tok/s 13626
step    740 | loss 3.4584 | lr 7.39e-05 | grad 4.67 | tok/s 12555
step    750 | loss 3.7450 | lr 7.49e-05 | grad 2.43 | tok/s 13914
step    760 | loss 3.6656 | lr 7.59e-05 | grad 1.33 | tok/s 13356
step    770 | loss 3.3372 | lr 7.69e-05 | grad 2.92 | tok/s 13285
step    780 | loss 3.3850 | lr 7.79e-05 | grad 1.30 | tok/s 12216
step    790 | loss 3.3277 | lr 7.89e-05 | grad 1.18 | tok/s 13046
step    800 | loss 3.5859 | lr 7.99e-05 | grad 3.21 | tok/s 13062
step    810 | loss 3.5751 | lr 8.09e-05 | grad 9.38 | tok/s 12041
step    820 | loss 3.1762 | lr 8.19e-05 | grad 2.03 | tok/s 12267
step    830 | loss 3.3644 | lr 8.29e-05 | grad 2.20 | tok/s 12948
step    840 | loss 3.3497 | lr 8.39e-05 | grad 1.95 | tok/s 12054
step    850 | loss 3.5651 | lr 8.49e-05 | grad 2.17 | tok/s 11977
step    860 | loss 3.4868 | lr 8.59e-05 | grad 0.87 | tok/s 12004
step    870 | loss 3.4414 | lr 8.69e-05 | grad 2.46 | tok/s 12644
step    880 | loss 3.8370 | lr 8.79e-05 | grad 1.42 | tok/s 13419
step    890 | loss 3.3829 | lr 8.89e-05 | grad 3.07 | tok/s 12513
step    900 | loss 3.3404 | lr 8.99e-05 | grad 0.76 | tok/s 12788
step    910 | loss 3.2915 | lr 9.09e-05 | grad 0.96 | tok/s 13065
step    920 | loss 3.5077 | lr 9.19e-05 | grad 1.07 | tok/s 12419
step    930 | loss 3.4487 | lr 9.29e-05 | grad 2.37 | tok/s 13172
step    940 | loss 3.3480 | lr 9.39e-05 | grad 1.41 | tok/s 13022
step    950 | loss 3.3404 | lr 9.49e-05 | grad 2.44 | tok/s 12771
step    960 | loss 3.3697 | lr 9.59e-05 | grad 3.19 | tok/s 14597
step    970 | loss 3.2694 | lr 9.69e-05 | grad 1.67 | tok/s 14436
step    980 | loss 3.2692 | lr 9.79e-05 | grad 0.59 | tok/s 14265
step    990 | loss 4.1719 | lr 9.89e-05 | grad 2.58 | tok/s 14804
step   1000 | loss 3.8570 | lr 9.99e-05 | grad 3.39 | tok/s 14462
  >>> saved checkpoint: checkpoint_step_001000_loss_3.8570.pt
step   1010 | loss 3.6831 | lr 1.02e-06 | grad 2.47 | tok/s 12267
step   1020 | loss 3.3070 | lr 1.09e-06 | grad 2.55 | tok/s 13797
step   1030 | loss 3.2963 | lr 1.21e-06 | grad 9.45 | tok/s 14725
step   1040 | loss 3.5658 | lr 1.37e-06 | grad 1.98 | tok/s 14746
step   1050 | loss 3.6047 | lr 1.59e-06 | grad 4.02 | tok/s 13463

Training complete! Final step: 1054
