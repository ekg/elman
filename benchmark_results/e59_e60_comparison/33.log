Using device: cuda
Output directory: output/level33_100m_20260113_221736
Auto r_h_mode: spectral_norm (level 33 has full W_h)
Model: Level 33, 33,354,432 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5235 | lr 9.00e-07 | grad 14.87 | tok/s 7957
step     20 | loss 5.1905 | lr 1.90e-06 | grad 11.58 | tok/s 11882
step     30 | loss 4.8413 | lr 2.90e-06 | grad 10.24 | tok/s 11287
step     40 | loss 4.5344 | lr 3.90e-06 | grad 11.08 | tok/s 13107
step     50 | loss 5.2497 | lr 4.90e-06 | grad 9.88 | tok/s 14862
step     60 | loss 4.9769 | lr 5.90e-06 | grad 5.96 | tok/s 13448
step     70 | loss 4.6875 | lr 6.90e-06 | grad 12.23 | tok/s 13878
step     80 | loss 4.5182 | lr 7.90e-06 | grad 5.33 | tok/s 14203
step     90 | loss 4.1812 | lr 8.90e-06 | grad 6.17 | tok/s 13489
step    100 | loss 3.9611 | lr 9.90e-06 | grad 7.39 | tok/s 13802
step    110 | loss 3.6711 | lr 1.09e-05 | grad 8.12 | tok/s 13052
step    120 | loss 3.9235 | lr 1.19e-05 | grad 4.59 | tok/s 12234
step    130 | loss 3.2435 | lr 1.29e-05 | grad 3.72 | tok/s 12327
step    140 | loss 3.0358 | lr 1.39e-05 | grad 4.78 | tok/s 13170
step    150 | loss 3.3217 | lr 1.49e-05 | grad 7.11 | tok/s 17441
step    160 | loss 3.2421 | lr 1.59e-05 | grad 5.45 | tok/s 14990
step    170 | loss 3.1793 | lr 1.69e-05 | grad 6.59 | tok/s 12974
step    180 | loss 3.1403 | lr 1.79e-05 | grad 6.01 | tok/s 12058
step    190 | loss 2.9705 | lr 1.89e-05 | grad 3.65 | tok/s 15412
step    200 | loss 2.6067 | lr 1.99e-05 | grad 3.45 | tok/s 13442
step    210 | loss 2.4989 | lr 2.09e-05 | grad 3.52 | tok/s 13162
step    220 | loss 2.7322 | lr 2.19e-05 | grad 5.35 | tok/s 16135
step    230 | loss 2.9859 | lr 2.29e-05 | grad 5.54 | tok/s 16013
step    240 | loss 2.6036 | lr 2.39e-05 | grad 4.11 | tok/s 16740
step    250 | loss 2.7947 | lr 2.49e-05 | grad 5.52 | tok/s 16692
step    260 | loss 2.4175 | lr 2.59e-05 | grad 4.45 | tok/s 15040
step    270 | loss 2.5325 | lr 2.69e-05 | grad 4.28 | tok/s 15297
step    280 | loss 2.2135 | lr 2.79e-05 | grad 3.99 | tok/s 14851
step    290 | loss 2.1959 | lr 2.89e-05 | grad 6.43 | tok/s 14396
step    300 | loss 2.2629 | lr 2.99e-05 | grad 4.25 | tok/s 14304
step    310 | loss 2.2503 | lr 3.09e-05 | grad 3.38 | tok/s 14549
step    320 | loss 2.0243 | lr 3.19e-05 | grad 5.94 | tok/s 14170
step    330 | loss 2.2938 | lr 3.29e-05 | grad 5.07 | tok/s 14920
step    340 | loss 2.3403 | lr 3.39e-05 | grad 10.76 | tok/s 15110
step    350 | loss 2.3294 | lr 3.49e-05 | grad 5.08 | tok/s 14228
step    360 | loss 2.3058 | lr 3.59e-05 | grad 5.85 | tok/s 9699
step    370 | loss 2.0378 | lr 3.69e-05 | grad 4.68 | tok/s 9308
step    380 | loss 2.0544 | lr 3.79e-05 | grad 4.25 | tok/s 10564
step    390 | loss 1.7564 | lr 3.89e-05 | grad 3.81 | tok/s 10867
step    400 | loss 1.6323 | lr 3.99e-05 | grad 3.67 | tok/s 11034
step    410 | loss 2.3375 | lr 4.09e-05 | grad 4.22 | tok/s 10479
step    420 | loss 2.2037 | lr 4.19e-05 | grad 5.01 | tok/s 11213
step    430 | loss 2.2254 | lr 4.29e-05 | grad 5.77 | tok/s 11358
step    440 | loss 2.0521 | lr 4.39e-05 | grad 6.09 | tok/s 10479
step    450 | loss 2.1453 | lr 4.49e-05 | grad 3.02 | tok/s 9468
step    460 | loss 1.8903 | lr 4.59e-05 | grad 7.37 | tok/s 10197
step    470 | loss 2.0191 | lr 4.69e-05 | grad 3.73 | tok/s 11834
step    480 | loss 2.0336 | lr 4.79e-05 | grad 4.96 | tok/s 14281
step    490 | loss 1.9994 | lr 4.89e-05 | grad 3.45 | tok/s 13993
step    500 | loss 2.0130 | lr 4.99e-05 | grad 3.88 | tok/s 10272
step    510 | loss 2.2180 | lr 5.09e-05 | grad 11.49 | tok/s 10229
step    520 | loss 1.9727 | lr 5.19e-05 | grad 3.79 | tok/s 10891
step    530 | loss 1.8418 | lr 5.29e-05 | grad 3.38 | tok/s 12237
step    540 | loss 2.0473 | lr 5.39e-05 | grad 3.73 | tok/s 12000
step    550 | loss 1.9821 | lr 5.49e-05 | grad 3.89 | tok/s 11095
step    560 | loss 1.7052 | lr 5.59e-05 | grad 4.78 | tok/s 11697
step    570 | loss 1.7673 | lr 5.69e-05 | grad 3.89 | tok/s 12453
step    580 | loss 1.6013 | lr 5.79e-05 | grad 3.55 | tok/s 12694
step    590 | loss 1.5300 | lr 5.89e-05 | grad 3.11 | tok/s 13341
step    600 | loss 1.6175 | lr 5.99e-05 | grad 4.09 | tok/s 14066
step    610 | loss 1.5233 | lr 6.09e-05 | grad 3.01 | tok/s 14696
step    620 | loss 1.5205 | lr 6.19e-05 | grad 2.55 | tok/s 12060
step    630 | loss 1.6227 | lr 6.29e-05 | grad 7.75 | tok/s 11946
step    640 | loss 2.0000 | lr 6.39e-05 | grad 5.66 | tok/s 12259
step    650 | loss 2.0144 | lr 6.49e-05 | grad 4.08 | tok/s 12173
step    660 | loss 1.8572 | lr 6.59e-05 | grad 4.50 | tok/s 13563
step    670 | loss 1.9134 | lr 6.69e-05 | grad 3.73 | tok/s 13949
step    680 | loss 1.9974 | lr 6.79e-05 | grad 4.73 | tok/s 14040
step    690 | loss 1.9695 | lr 6.89e-05 | grad 4.44 | tok/s 13358
step    700 | loss 1.9254 | lr 6.99e-05 | grad 3.98 | tok/s 12337
step    710 | loss 1.8045 | lr 7.09e-05 | grad 4.09 | tok/s 11734
step    720 | loss 2.0136 | lr 7.19e-05 | grad 6.66 | tok/s 11526
step    730 | loss 1.6742 | lr 7.29e-05 | grad 3.08 | tok/s 13151
step    740 | loss 1.7439 | lr 7.39e-05 | grad 2.90 | tok/s 12596
step    750 | loss 2.3683 | lr 7.49e-05 | grad 7.15 | tok/s 12853
step    760 | loss 2.1118 | lr 7.59e-05 | grad 3.32 | tok/s 12894
step    770 | loss 1.8409 | lr 7.69e-05 | grad 3.73 | tok/s 12410
step    780 | loss 1.8663 | lr 7.79e-05 | grad 3.24 | tok/s 11794
step    790 | loss 1.8364 | lr 7.89e-05 | grad 3.40 | tok/s 12001
step    800 | loss 2.0942 | lr 7.99e-05 | grad 7.15 | tok/s 12519
step    810 | loss 1.9102 | lr 8.09e-05 | grad 4.05 | tok/s 12161
step    820 | loss 1.5819 | lr 8.19e-05 | grad 6.62 | tok/s 11431
step    830 | loss 1.8213 | lr 8.29e-05 | grad 3.40 | tok/s 11646
step    840 | loss 1.8698 | lr 8.39e-05 | grad 3.15 | tok/s 11253
step    850 | loss 2.0074 | lr 8.49e-05 | grad 3.53 | tok/s 11227
step    860 | loss 1.9752 | lr 8.59e-05 | grad 2.99 | tok/s 11573
step    870 | loss 1.9131 | lr 8.69e-05 | grad 5.41 | tok/s 13005
step    880 | loss 1.9756 | lr 8.79e-05 | grad 3.64 | tok/s 16249
step    890 | loss 1.9188 | lr 8.89e-05 | grad 3.47 | tok/s 15910
step    900 | loss 1.7720 | lr 8.99e-05 | grad 2.63 | tok/s 16392
step    910 | loss 1.7435 | lr 9.09e-05 | grad 2.90 | tok/s 16719
step    920 | loss 1.8503 | lr 9.19e-05 | grad 2.60 | tok/s 15874
step    930 | loss 1.8435 | lr 9.29e-05 | grad 2.72 | tok/s 16166
step    940 | loss 1.7450 | lr 9.39e-05 | grad 3.19 | tok/s 16613
step    950 | loss 1.6646 | lr 9.49e-05 | grad 2.04 | tok/s 13567
step    960 | loss 1.7809 | lr 9.59e-05 | grad 2.08 | tok/s 11284
step    970 | loss 1.6750 | lr 9.69e-05 | grad 2.17 | tok/s 11353

Training complete! Final step: 972
