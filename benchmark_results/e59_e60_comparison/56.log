Using device: cuda
Output directory: output/level56_100m_20260113_221736
Auto r_h_mode: spectral_norm (level 56 has full W_h)
Model: Level 56, 36,076,552 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.6451 | lr 9.00e-07 | grad 13.14 | tok/s 5725
step     20 | loss 5.5145 | lr 1.90e-06 | grad 5.95 | tok/s 8394
step     30 | loss 5.3925 | lr 2.90e-06 | grad 4.34 | tok/s 9755
step     40 | loss 5.4092 | lr 3.90e-06 | grad 4.31 | tok/s 9099
step     50 | loss 5.6183 | lr 4.90e-06 | grad 4.31 | tok/s 9686
step     60 | loss 5.5548 | lr 5.90e-06 | grad 3.37 | tok/s 9660
step     70 | loss 5.4654 | lr 6.90e-06 | grad 5.31 | tok/s 11369
step     80 | loss 5.4114 | lr 7.90e-06 | grad 3.52 | tok/s 11019
step     90 | loss 5.2545 | lr 8.90e-06 | grad 3.14 | tok/s 9771
step    100 | loss 5.1364 | lr 9.90e-06 | grad 3.31 | tok/s 10097
step    110 | loss 4.9241 | lr 1.09e-05 | grad 8.71 | tok/s 8814
step    120 | loss 4.8550 | lr 1.19e-05 | grad 3.98 | tok/s 8796
step    130 | loss 4.3213 | lr 1.29e-05 | grad 4.05 | tok/s 8460
step    140 | loss 3.8832 | lr 1.39e-05 | grad 3.72 | tok/s 9754
step    150 | loss 3.7799 | lr 1.49e-05 | grad 4.98 | tok/s 8830
step    160 | loss 3.6602 | lr 1.59e-05 | grad 3.32 | tok/s 8919
step    170 | loss 3.5227 | lr 1.69e-05 | grad 2.37 | tok/s 8066
step    180 | loss 3.5113 | lr 1.79e-05 | grad 3.17 | tok/s 7912
step    190 | loss 3.2682 | lr 1.89e-05 | grad 1.97 | tok/s 9316
step    200 | loss 2.9398 | lr 1.99e-05 | grad 1.95 | tok/s 9673
step    210 | loss 2.7749 | lr 2.09e-05 | grad 2.12 | tok/s 7861
step    220 | loss 2.9460 | lr 2.19e-05 | grad 2.44 | tok/s 7275
step    230 | loss 3.2241 | lr 2.29e-05 | grad 1.55 | tok/s 6414
step    240 | loss 2.7284 | lr 2.39e-05 | grad 2.12 | tok/s 6691
step    250 | loss 2.9774 | lr 2.49e-05 | grad 1.89 | tok/s 6954
step    260 | loss 2.5513 | lr 2.59e-05 | grad 1.67 | tok/s 7259
step    270 | loss 2.6395 | lr 2.69e-05 | grad 1.55 | tok/s 7287
step    280 | loss 2.3087 | lr 2.79e-05 | grad 1.85 | tok/s 6394
step    290 | loss 2.2927 | lr 2.89e-05 | grad 3.18 | tok/s 5758
step    300 | loss 2.3697 | lr 2.99e-05 | grad 2.11 | tok/s 6543
step    310 | loss 2.3284 | lr 3.09e-05 | grad 1.57 | tok/s 9096
step    320 | loss 2.0852 | lr 3.19e-05 | grad 2.36 | tok/s 8890
step    330 | loss 2.3564 | lr 3.29e-05 | grad 1.73 | tok/s 8798
step    340 | loss 2.3872 | lr 3.39e-05 | grad 7.95 | tok/s 8292
step    350 | loss 2.3783 | lr 3.49e-05 | grad 2.41 | tok/s 8962
step    360 | loss 2.3841 | lr 3.59e-05 | grad 2.34 | tok/s 8877
step    370 | loss 2.0673 | lr 3.69e-05 | grad 1.89 | tok/s 10210
step    380 | loss 2.0900 | lr 3.79e-05 | grad 2.40 | tok/s 11113
step    390 | loss 1.8112 | lr 3.89e-05 | grad 1.88 | tok/s 11251
step    400 | loss 1.6912 | lr 3.99e-05 | grad 1.95 | tok/s 9309
step    410 | loss 2.3484 | lr 4.09e-05 | grad 2.21 | tok/s 7589
step    420 | loss 2.1897 | lr 4.19e-05 | grad 2.59 | tok/s 7357
step    430 | loss 2.2408 | lr 4.29e-05 | grad 2.67 | tok/s 9054
step    440 | loss 2.0481 | lr 4.39e-05 | grad 2.25 | tok/s 8566
step    450 | loss 2.1353 | lr 4.49e-05 | grad 1.72 | tok/s 7771
step    460 | loss 1.8732 | lr 4.59e-05 | grad 3.94 | tok/s 10423
step    470 | loss 2.0174 | lr 4.69e-05 | grad 2.04 | tok/s 10380
step    480 | loss 2.0357 | lr 4.79e-05 | grad 2.78 | tok/s 10529
step    490 | loss 2.0183 | lr 4.89e-05 | grad 2.04 | tok/s 9334
step    500 | loss 1.9743 | lr 4.99e-05 | grad 2.08 | tok/s 9434
step    510 | loss 2.2057 | lr 5.09e-05 | grad 7.07 | tok/s 10265
step    520 | loss 1.9282 | lr 5.19e-05 | grad 2.02 | tok/s 9636
step    530 | loss 1.8095 | lr 5.29e-05 | grad 2.26 | tok/s 8339
step    540 | loss 2.0293 | lr 5.39e-05 | grad 2.03 | tok/s 7667
step    550 | loss 1.9530 | lr 5.49e-05 | grad 1.98 | tok/s 7436
step    560 | loss 1.6873 | lr 5.59e-05 | grad 2.53 | tok/s 8864
step    570 | loss 1.7366 | lr 5.69e-05 | grad 1.98 | tok/s 10581
step    580 | loss 1.6043 | lr 5.79e-05 | grad 1.75 | tok/s 10300
step    590 | loss 1.5337 | lr 5.89e-05 | grad 1.56 | tok/s 9974
step    600 | loss 1.6115 | lr 5.99e-05 | grad 2.26 | tok/s 8464
step    610 | loss 1.5350 | lr 6.09e-05 | grad 1.63 | tok/s 8974
step    620 | loss 1.5113 | lr 6.19e-05 | grad 1.41 | tok/s 9544
step    630 | loss 1.6145 | lr 6.29e-05 | grad 4.34 | tok/s 9794
step    640 | loss 1.9633 | lr 6.39e-05 | grad 3.35 | tok/s 9452
step    650 | loss 2.0090 | lr 6.49e-05 | grad 2.55 | tok/s 10186
step    660 | loss 1.7999 | lr 6.59e-05 | grad 2.65 | tok/s 10114

Training complete! Final step: 661
