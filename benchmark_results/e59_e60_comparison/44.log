Using device: cuda
Output directory: output/level44_100m_20260113_222748
Auto r_h_mode: none (level 44 has bounded/no W_h)
Model: Level 44, 50,332,000 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.7044 | lr 9.00e-07 | grad 17.93 | tok/s 10398
step     20 | loss 5.4339 | lr 1.90e-06 | grad 13.51 | tok/s 29852
step     30 | loss 5.0874 | lr 2.90e-06 | grad 9.72 | tok/s 32241
step     40 | loss 4.7439 | lr 3.90e-06 | grad 11.95 | tok/s 28214
step     50 | loss 5.2105 | lr 4.90e-06 | grad 11.61 | tok/s 34662
step     60 | loss 4.8600 | lr 5.90e-06 | grad 5.97 | tok/s 33553
step     70 | loss 4.5140 | lr 6.90e-06 | grad 11.39 | tok/s 35542
step     80 | loss 4.2861 | lr 7.90e-06 | grad 4.34 | tok/s 34426
step     90 | loss 3.9355 | lr 8.90e-06 | grad 3.26 | tok/s 35791
step    100 | loss 3.7806 | lr 9.90e-06 | grad 2.86 | tok/s 35361
step    110 | loss 3.6089 | lr 1.09e-05 | grad 5.83 | tok/s 31270
step    120 | loss 3.8732 | lr 1.19e-05 | grad 3.21 | tok/s 26121
step    130 | loss 3.3125 | lr 1.29e-05 | grad 2.14 | tok/s 31372
step    140 | loss 3.2102 | lr 1.39e-05 | grad 2.41 | tok/s 33733
step    150 | loss 3.5576 | lr 1.49e-05 | grad 5.31 | tok/s 32430
step    160 | loss 3.3043 | lr 1.59e-05 | grad 3.51 | tok/s 26311
step    170 | loss 3.3226 | lr 1.69e-05 | grad 5.71 | tok/s 26742
step    180 | loss 3.4111 | lr 1.79e-05 | grad 4.93 | tok/s 27032
step    190 | loss 3.2525 | lr 1.89e-05 | grad 2.03 | tok/s 24874
step    200 | loss 3.0493 | lr 1.99e-05 | grad 1.82 | tok/s 22499
step    210 | loss 2.9543 | lr 2.09e-05 | grad 4.58 | tok/s 25321
step    220 | loss 3.0738 | lr 2.19e-05 | grad 3.74 | tok/s 25469
step    230 | loss 3.1445 | lr 2.29e-05 | grad 1.69 | tok/s 24150
step    240 | loss 2.9677 | lr 2.39e-05 | grad 2.43 | tok/s 23945
step    250 | loss 3.1517 | lr 2.49e-05 | grad 3.01 | tok/s 24009
step    260 | loss 2.8918 | lr 2.59e-05 | grad 2.27 | tok/s 26895
step    270 | loss 2.9329 | lr 2.69e-05 | grad 3.13 | tok/s 26950
step    280 | loss 2.6709 | lr 2.79e-05 | grad 2.65 | tok/s 26279
step    290 | loss 2.6044 | lr 2.89e-05 | grad 3.94 | tok/s 25056
step    300 | loss 2.6799 | lr 2.99e-05 | grad 4.65 | tok/s 25470
step    310 | loss 2.6505 | lr 3.09e-05 | grad 2.58 | tok/s 25859
step    320 | loss 2.4276 | lr 3.19e-05 | grad 3.17 | tok/s 24603
step    330 | loss 2.6645 | lr 3.29e-05 | grad 1.95 | tok/s 28405
step    340 | loss 2.6914 | lr 3.39e-05 | grad 5.76 | tok/s 27789
step    350 | loss 2.6787 | lr 3.49e-05 | grad 2.75 | tok/s 25623
step    360 | loss 2.6899 | lr 3.59e-05 | grad 2.94 | tok/s 24254
step    370 | loss 2.4041 | lr 3.69e-05 | grad 2.12 | tok/s 24316
step    380 | loss 2.4412 | lr 3.79e-05 | grad 2.25 | tok/s 25273
step    390 | loss 2.2066 | lr 3.89e-05 | grad 2.31 | tok/s 24252
step    400 | loss 2.0499 | lr 3.99e-05 | grad 1.73 | tok/s 25807
step    410 | loss 2.5961 | lr 4.09e-05 | grad 2.69 | tok/s 24877
step    420 | loss 2.4822 | lr 4.19e-05 | grad 2.85 | tok/s 21741
step    430 | loss 2.5415 | lr 4.29e-05 | grad 3.78 | tok/s 16592
step    440 | loss 2.3515 | lr 4.39e-05 | grad 2.41 | tok/s 16294
step    450 | loss 2.4358 | lr 4.49e-05 | grad 2.13 | tok/s 22246
step    460 | loss 2.1655 | lr 4.59e-05 | grad 3.28 | tok/s 23514
step    470 | loss 2.2856 | lr 4.69e-05 | grad 2.20 | tok/s 22077
step    480 | loss 2.3633 | lr 4.79e-05 | grad 2.66 | tok/s 21887
step    490 | loss 2.3018 | lr 4.89e-05 | grad 1.66 | tok/s 23215
step    500 | loss 2.2595 | lr 4.99e-05 | grad 2.07 | tok/s 27679
step    510 | loss 2.4516 | lr 5.09e-05 | grad 6.00 | tok/s 33044
step    520 | loss 2.1563 | lr 5.19e-05 | grad 1.60 | tok/s 35137
step    530 | loss 2.0613 | lr 5.29e-05 | grad 1.73 | tok/s 35810
step    540 | loss 2.2413 | lr 5.39e-05 | grad 1.98 | tok/s 35043
step    550 | loss 2.1875 | lr 5.49e-05 | grad 2.14 | tok/s 34264
step    560 | loss 1.9180 | lr 5.59e-05 | grad 2.49 | tok/s 36036
step    570 | loss 2.0276 | lr 5.69e-05 | grad 1.72 | tok/s 37577
step    580 | loss 1.8531 | lr 5.79e-05 | grad 1.84 | tok/s 37955
step    590 | loss 1.7485 | lr 5.89e-05 | grad 1.58 | tok/s 39128
step    600 | loss 1.8167 | lr 5.99e-05 | grad 1.87 | tok/s 39710
step    610 | loss 1.7156 | lr 6.09e-05 | grad 1.71 | tok/s 40533
step    620 | loss 1.6830 | lr 6.19e-05 | grad 1.35 | tok/s 39530
step    630 | loss 1.7840 | lr 6.29e-05 | grad 7.51 | tok/s 39158
step    640 | loss 2.1949 | lr 6.39e-05 | grad 2.94 | tok/s 40768
step    650 | loss 2.1658 | lr 6.49e-05 | grad 2.35 | tok/s 40615
step    660 | loss 2.0004 | lr 6.59e-05 | grad 2.56 | tok/s 40867
step    670 | loss 2.0587 | lr 6.69e-05 | grad 2.01 | tok/s 41596
step    680 | loss 2.1064 | lr 6.79e-05 | grad 2.14 | tok/s 33085
step    690 | loss 2.0971 | lr 6.89e-05 | grad 2.38 | tok/s 21609
step    700 | loss 2.0663 | lr 6.99e-05 | grad 2.27 | tok/s 22448
step    710 | loss 1.9255 | lr 7.09e-05 | grad 2.27 | tok/s 25135
step    720 | loss 2.1366 | lr 7.19e-05 | grad 3.77 | tok/s 28623
step    730 | loss 1.7820 | lr 7.29e-05 | grad 1.90 | tok/s 26127
step    740 | loss 1.8635 | lr 7.39e-05 | grad 1.50 | tok/s 24760
step    750 | loss 2.4141 | lr 7.49e-05 | grad 2.92 | tok/s 32931
step    760 | loss 2.1859 | lr 7.59e-05 | grad 1.81 | tok/s 27719
step    770 | loss 1.9504 | lr 7.69e-05 | grad 2.08 | tok/s 30103
step    780 | loss 1.9819 | lr 7.79e-05 | grad 1.85 | tok/s 32112
step    790 | loss 1.9121 | lr 7.89e-05 | grad 1.82 | tok/s 32421
step    800 | loss 2.1930 | lr 7.99e-05 | grad 3.37 | tok/s 30936
step    810 | loss 1.9780 | lr 8.09e-05 | grad 10.07 | tok/s 30144
step    820 | loss 1.6686 | lr 8.19e-05 | grad 3.54 | tok/s 30948
step    830 | loss 1.9415 | lr 8.29e-05 | grad 1.96 | tok/s 31704
step    840 | loss 1.9698 | lr 8.39e-05 | grad 1.66 | tok/s 30534
step    850 | loss 2.0947 | lr 8.49e-05 | grad 1.63 | tok/s 31568
step    860 | loss 2.0783 | lr 8.59e-05 | grad 1.58 | tok/s 31409
step    870 | loss 1.9734 | lr 8.69e-05 | grad 2.79 | tok/s 32160
step    880 | loss 2.1391 | lr 8.79e-05 | grad 1.95 | tok/s 32487
step    890 | loss 1.9819 | lr 8.89e-05 | grad 1.65 | tok/s 31532
step    900 | loss 1.8407 | lr 8.99e-05 | grad 1.53 | tok/s 34231
step    910 | loss 1.8081 | lr 9.09e-05 | grad 1.38 | tok/s 33148
step    920 | loss 1.9249 | lr 9.19e-05 | grad 1.43 | tok/s 27504
step    930 | loss 1.9155 | lr 9.29e-05 | grad 1.48 | tok/s 33881
step    940 | loss 1.8068 | lr 9.39e-05 | grad 1.86 | tok/s 34329
step    950 | loss 1.7407 | lr 9.49e-05 | grad 1.42 | tok/s 29959
step    960 | loss 1.8489 | lr 9.59e-05 | grad 1.38 | tok/s 30641
step    970 | loss 1.7344 | lr 9.69e-05 | grad 1.20 | tok/s 30434
step    980 | loss 1.7163 | lr 9.79e-05 | grad 1.26 | tok/s 28522
step    990 | loss 2.5859 | lr 9.89e-05 | grad 2.19 | tok/s 36931
step   1000 | loss 2.2823 | lr 9.99e-05 | grad 1.96 | tok/s 35761
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2823.pt
step   1010 | loss 2.0609 | lr 1.02e-06 | grad 2.08 | tok/s 19023
step   1020 | loss 1.7226 | lr 1.09e-06 | grad 1.59 | tok/s 32102
step   1030 | loss 1.6895 | lr 1.21e-06 | grad 3.28 | tok/s 34702
step   1040 | loss 2.1055 | lr 1.37e-06 | grad 1.36 | tok/s 32212
step   1050 | loss 2.1593 | lr 1.59e-06 | grad 5.29 | tok/s 33075
step   1060 | loss 2.7224 | lr 1.85e-06 | grad 1.12 | tok/s 37343
step   1070 | loss 2.0410 | lr 2.16e-06 | grad 4.19 | tok/s 37440
step   1080 | loss 1.5759 | lr 2.52e-06 | grad 1.62 | tok/s 38258
step   1090 | loss 1.7473 | lr 2.92e-06 | grad 1.51 | tok/s 36003
step   1100 | loss 1.6620 | lr 3.37e-06 | grad 1.14 | tok/s 35270
step   1110 | loss 1.6205 | lr 3.87e-06 | grad 0.95 | tok/s 35920
step   1120 | loss 1.5876 | lr 4.42e-06 | grad 1.05 | tok/s 35346
step   1130 | loss 1.5852 | lr 5.01e-06 | grad 1.21 | tok/s 33341
step   1140 | loss 2.1454 | lr 5.65e-06 | grad 3.02 | tok/s 33412
step   1150 | loss 2.3558 | lr 6.32e-06 | grad 3.07 | tok/s 30891
step   1160 | loss 1.8862 | lr 7.05e-06 | grad 1.49 | tok/s 30792
step   1170 | loss 2.4984 | lr 7.81e-06 | grad 2.71 | tok/s 25734
step   1180 | loss 2.0973 | lr 8.62e-06 | grad 1.67 | tok/s 29665
step   1190 | loss 1.8143 | lr 9.47e-06 | grad 1.12 | tok/s 23940
step   1200 | loss 1.8384 | lr 1.04e-05 | grad 2.43 | tok/s 30281
step   1210 | loss 2.2961 | lr 1.13e-05 | grad 1.72 | tok/s 24198
step   1220 | loss 1.5651 | lr 1.23e-05 | grad 1.22 | tok/s 24223
step   1230 | loss 1.7422 | lr 1.33e-05 | grad 1.52 | tok/s 25548
step   1240 | loss 1.6954 | lr 1.43e-05 | grad 1.35 | tok/s 22997
step   1250 | loss 1.7192 | lr 1.54e-05 | grad 1.09 | tok/s 27855
step   1260 | loss 1.6457 | lr 1.65e-05 | grad 1.15 | tok/s 23121
step   1270 | loss 2.1009 | lr 1.76e-05 | grad 3.15 | tok/s 25620
step   1280 | loss 1.9824 | lr 1.88e-05 | grad 1.41 | tok/s 25610
step   1290 | loss 1.6683 | lr 2.00e-05 | grad 1.40 | tok/s 23638
step   1300 | loss 1.7885 | lr 2.13e-05 | grad 1.73 | tok/s 23052
step   1310 | loss 1.7350 | lr 2.25e-05 | grad 1.12 | tok/s 20397
step   1320 | loss 2.3319 | lr 2.38e-05 | grad 6.56 | tok/s 27111
step   1330 | loss 1.7969 | lr 2.52e-05 | grad 1.48 | tok/s 24218
step   1340 | loss 1.9449 | lr 2.65e-05 | grad 2.27 | tok/s 23190
step   1350 | loss 1.7352 | lr 2.79e-05 | grad 1.29 | tok/s 23294
step   1360 | loss 1.9326 | lr 2.93e-05 | grad 1.76 | tok/s 25428
step   1370 | loss 1.8766 | lr 3.07e-05 | grad 3.17 | tok/s 26006
step   1380 | loss 1.8503 | lr 3.21e-05 | grad 1.93 | tok/s 23242
step   1390 | loss 2.1574 | lr 3.36e-05 | grad 2.08 | tok/s 24534
step   1400 | loss 1.7062 | lr 3.51e-05 | grad 2.91 | tok/s 23225
step   1410 | loss 1.7593 | lr 3.65e-05 | grad 1.55 | tok/s 22147
step   1420 | loss 1.9119 | lr 3.80e-05 | grad 1.63 | tok/s 23997
step   1430 | loss 1.7326 | lr 3.96e-05 | grad 3.23 | tok/s 23532
step   1440 | loss 2.0285 | lr 4.11e-05 | grad 6.58 | tok/s 30977
step   1450 | loss 2.0785 | lr 4.26e-05 | grad 2.08 | tok/s 29378
step   1460 | loss 1.8592 | lr 4.41e-05 | grad 1.84 | tok/s 31196
step   1470 | loss 1.8909 | lr 4.57e-05 | grad 2.89 | tok/s 31737
step   1480 | loss 1.8597 | lr 4.72e-05 | grad 3.21 | tok/s 30174
step   1490 | loss 1.6138 | lr 4.88e-05 | grad 1.33 | tok/s 27164
step   1500 | loss 1.7687 | lr 5.03e-05 | grad 1.81 | tok/s 31321
step   1510 | loss 2.2347 | lr 5.19e-05 | grad 4.56 | tok/s 31689
step   1520 | loss 1.7635 | lr 5.35e-05 | grad 1.31 | tok/s 32115
step   1530 | loss 1.6632 | lr 5.50e-05 | grad 1.37 | tok/s 31139
step   1540 | loss 1.7946 | lr 5.65e-05 | grad 1.63 | tok/s 32775
step   1550 | loss 1.6689 | lr 5.81e-05 | grad 1.47 | tok/s 34150
step   1560 | loss 1.8620 | lr 5.96e-05 | grad 1.13 | tok/s 31752
step   1570 | loss 1.7884 | lr 6.11e-05 | grad 2.26 | tok/s 32094
step   1580 | loss 1.5711 | lr 6.27e-05 | grad 1.14 | tok/s 33145
step   1590 | loss 1.7461 | lr 6.42e-05 | grad 1.50 | tok/s 34753
step   1600 | loss 1.5550 | lr 6.56e-05 | grad 1.44 | tok/s 30349
step   1610 | loss 1.8087 | lr 6.71e-05 | grad 3.47 | tok/s 25618
step   1620 | loss 1.7298 | lr 6.86e-05 | grad 3.51 | tok/s 26265
step   1630 | loss 2.3925 | lr 7.00e-05 | grad 2.97 | tok/s 24454
step   1640 | loss 2.5458 | lr 7.14e-05 | grad 1.33 | tok/s 31299
step   1650 | loss 2.0385 | lr 7.28e-05 | grad 1.17 | tok/s 31640
step   1660 | loss 1.7938 | lr 7.42e-05 | grad 1.12 | tok/s 32801
step   1670 | loss 1.6403 | lr 7.56e-05 | grad 1.03 | tok/s 28253
step   1680 | loss 1.5721 | lr 7.69e-05 | grad 1.14 | tok/s 30992
step   1690 | loss 1.9504 | lr 7.82e-05 | grad 3.66 | tok/s 27945
step   1700 | loss 1.9816 | lr 7.95e-05 | grad 1.43 | tok/s 29503
step   1710 | loss 1.7759 | lr 8.07e-05 | grad 1.44 | tok/s 29829
step   1720 | loss 1.6837 | lr 8.19e-05 | grad 1.65 | tok/s 29928
step   1730 | loss 1.6089 | lr 8.31e-05 | grad 1.85 | tok/s 30386
step   1740 | loss 1.7859 | lr 8.43e-05 | grad 1.61 | tok/s 30030
step   1750 | loss 1.7883 | lr 8.54e-05 | grad 1.67 | tok/s 30402
step   1760 | loss 1.8113 | lr 8.65e-05 | grad 1.56 | tok/s 30878
step   1770 | loss 1.6889 | lr 8.75e-05 | grad 1.49 | tok/s 30862
step   1780 | loss 1.6532 | lr 8.85e-05 | grad 1.73 | tok/s 30271
step   1790 | loss 2.1506 | lr 8.95e-05 | grad 1.75 | tok/s 30526
step   1800 | loss 2.0725 | lr 9.05e-05 | grad 1.36 | tok/s 29204
step   1810 | loss 1.7313 | lr 9.14e-05 | grad 2.69 | tok/s 30593
step   1820 | loss 1.7066 | lr 9.22e-05 | grad 1.26 | tok/s 31764
step   1830 | loss 1.7299 | lr 9.30e-05 | grad 1.20 | tok/s 29700
step   1840 | loss 1.7114 | lr 9.38e-05 | grad 1.54 | tok/s 28630
step   1850 | loss 1.7580 | lr 9.45e-05 | grad 1.50 | tok/s 26276
step   1860 | loss 1.8216 | lr 9.52e-05 | grad 1.75 | tok/s 23768
step   1870 | loss 1.6620 | lr 9.59e-05 | grad 3.06 | tok/s 29644
step   1880 | loss 1.8408 | lr 9.65e-05 | grad 2.28 | tok/s 31739
step   1890 | loss 1.8126 | lr 9.70e-05 | grad 1.34 | tok/s 32225
step   1900 | loss 1.8365 | lr 9.75e-05 | grad 1.37 | tok/s 28388
step   1910 | loss 1.5507 | lr 9.80e-05 | grad 0.96 | tok/s 27970
step   1920 | loss 1.4074 | lr 9.84e-05 | grad 0.82 | tok/s 27739
step   1930 | loss 1.3697 | lr 9.88e-05 | grad 0.96 | tok/s 28046
step   1940 | loss 1.3216 | lr 9.91e-05 | grad 0.81 | tok/s 28969
step   1950 | loss 1.2620 | lr 9.94e-05 | grad 0.84 | tok/s 28609
step   1960 | loss 1.8035 | lr 9.96e-05 | grad 4.01 | tok/s 26686
step   1970 | loss 1.8887 | lr 9.98e-05 | grad 1.76 | tok/s 26759
step   1980 | loss 1.9177 | lr 9.99e-05 | grad 1.38 | tok/s 28219
step   1990 | loss 1.8703 | lr 1.00e-04 | grad 2.10 | tok/s 26944
step   2000 | loss 1.6695 | lr 1.00e-04 | grad 1.13 | tok/s 24169
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6695.pt
step   2010 | loss 2.0960 | lr 1.00e-04 | grad 1.43 | tok/s 17668
step   2020 | loss 1.7976 | lr 9.99e-05 | grad 1.15 | tok/s 31407
step   2030 | loss 1.4660 | lr 9.98e-05 | grad 1.25 | tok/s 35379
step   2040 | loss 1.7037 | lr 9.96e-05 | grad 1.17 | tok/s 32555
step   2050 | loss 1.7487 | lr 9.94e-05 | grad 1.25 | tok/s 33542
step   2060 | loss 2.2683 | lr 9.92e-05 | grad 1.22 | tok/s 32225
step   2070 | loss 1.6415 | lr 9.88e-05 | grad 1.00 | tok/s 33041
step   2080 | loss 1.6906 | lr 9.85e-05 | grad 1.69 | tok/s 29933
step   2090 | loss 1.9585 | lr 9.81e-05 | grad 1.59 | tok/s 29956
step   2100 | loss 1.5694 | lr 9.76e-05 | grad 1.54 | tok/s 30849
step   2110 | loss 1.5958 | lr 9.71e-05 | grad 1.14 | tok/s 31266
step   2120 | loss 2.1137 | lr 9.66e-05 | grad 2.17 | tok/s 32714
step   2130 | loss 2.1888 | lr 9.60e-05 | grad 1.49 | tok/s 32581
step   2140 | loss 1.7229 | lr 9.54e-05 | grad 1.37 | tok/s 30478
step   2150 | loss 2.8110 | lr 9.47e-05 | grad 2.84 | tok/s 31689
step   2160 | loss 1.9277 | lr 9.40e-05 | grad 1.89 | tok/s 30248
step   2170 | loss 1.9639 | lr 9.32e-05 | grad 1.17 | tok/s 30552

Training complete! Final step: 2176
