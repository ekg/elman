Using device: cuda
Output directory: /home/erikg/elman/benchmark_results/100m_systematic/Mamba2_ssm/levelmamba2_100m_20260113_170959
Created Mamba2 model: dim=736, depth=30, expand=2, params=101,359,638
Model: Level mamba2, 101,359,638 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.6345 | lr 4.90e-06 | grad 7.12 | tok/s 16731
step    100 | loss 5.5830 | lr 9.90e-06 | grad 4.41 | tok/s 41247
step    150 | loss 5.0297 | lr 1.49e-05 | grad 14.88 | tok/s 41343
step    200 | loss 3.9600 | lr 1.99e-05 | grad 3.06 | tok/s 41313
step    250 | loss 3.2409 | lr 2.49e-05 | grad 2.50 | tok/s 38056
step    300 | loss 2.7166 | lr 2.99e-05 | grad 2.45 | tok/s 39513
step    350 | loss 2.5621 | lr 3.49e-05 | grad 3.12 | tok/s 42118
step    400 | loss 2.2548 | lr 3.99e-05 | grad 2.19 | tok/s 48547
step    450 | loss 2.3446 | lr 4.49e-05 | grad 2.11 | tok/s 45665
step    500 | loss 2.1157 | lr 4.99e-05 | grad 2.52 | tok/s 46972
step    550 | loss 2.0879 | lr 5.49e-05 | grad 2.36 | tok/s 44751
step    600 | loss 1.7163 | lr 5.99e-05 | grad 2.61 | tok/s 46942
step    650 | loss 1.7767 | lr 6.49e-05 | grad 3.12 | tok/s 45296
step    700 | loss 1.9239 | lr 6.99e-05 | grad 2.67 | tok/s 48417
step    750 | loss 1.9066 | lr 7.49e-05 | grad 4.91 | tok/s 42479
step    800 | loss 1.9364 | lr 7.99e-05 | grad 5.28 | tok/s 41974
step    850 | loss 1.8003 | lr 8.49e-05 | grad 2.14 | tok/s 41860
step    900 | loss 1.8685 | lr 8.99e-05 | grad 1.85 | tok/s 42033
step    950 | loss 1.7151 | lr 9.49e-05 | grad 1.72 | tok/s 40116
step   1000 | loss 1.9208 | lr 9.99e-05 | grad 1.76 | tok/s 38007
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9208.pt
step   1050 | loss 1.8566 | lr 1.59e-06 | grad 5.44 | tok/s 38006
step   1100 | loss 1.9165 | lr 3.37e-06 | grad 1.88 | tok/s 43379
step   1150 | loss 1.8699 | lr 6.32e-06 | grad 3.67 | tok/s 43659
step   1200 | loss 2.0300 | lr 1.04e-05 | grad 2.94 | tok/s 42010
step   1250 | loss 1.8198 | lr 1.54e-05 | grad 1.48 | tok/s 43053
step   1300 | loss 1.8772 | lr 2.13e-05 | grad 2.39 | tok/s 42795
step   1350 | loss 1.8637 | lr 2.79e-05 | grad 1.72 | tok/s 42439
step   1400 | loss 1.8486 | lr 3.51e-05 | grad 4.44 | tok/s 41982
step   1450 | loss 1.8112 | lr 4.26e-05 | grad 2.06 | tok/s 42510
step   1500 | loss 1.7025 | lr 5.03e-05 | grad 2.38 | tok/s 42472
step   1550 | loss 1.7650 | lr 5.81e-05 | grad 1.74 | tok/s 42401
step   1600 | loss 1.6200 | lr 6.56e-05 | grad 2.11 | tok/s 42989
step   1650 | loss 2.0186 | lr 7.28e-05 | grad 1.57 | tok/s 43494
step   1700 | loss 1.6728 | lr 7.95e-05 | grad 1.77 | tok/s 43589
step   1750 | loss 1.6280 | lr 8.54e-05 | grad 1.91 | tok/s 42180
step   1800 | loss 1.7896 | lr 9.05e-05 | grad 1.61 | tok/s 41878
step   1850 | loss 1.6426 | lr 9.45e-05 | grad 2.06 | tok/s 41959
step   1900 | loss 1.6786 | lr 9.75e-05 | grad 1.61 | tok/s 42043
step   1950 | loss 1.3038 | lr 9.94e-05 | grad 0.99 | tok/s 44531
step   2000 | loss 1.7208 | lr 1.00e-04 | grad 1.56 | tok/s 41663
  >>> saved checkpoint: checkpoint_step_002000_loss_1.7208.pt
step   2050 | loss 1.6605 | lr 9.94e-05 | grad 1.84 | tok/s 38773
step   2100 | loss 1.7092 | lr 9.76e-05 | grad 1.53 | tok/s 41879
step   2150 | loss 1.9757 | lr 9.47e-05 | grad 2.41 | tok/s 42549
step   2200 | loss 1.8032 | lr 9.06e-05 | grad 1.45 | tok/s 41980
step   2250 | loss 1.6510 | lr 8.56e-05 | grad 1.55 | tok/s 42587
step   2300 | loss 1.6748 | lr 7.97e-05 | grad 2.02 | tok/s 42026
step   2350 | loss 1.6208 | lr 7.31e-05 | grad 1.07 | tok/s 42402
step   2400 | loss 1.3630 | lr 6.59e-05 | grad 0.95 | tok/s 44335
step   2450 | loss 1.3790 | lr 5.84e-05 | grad 1.26 | tok/s 43921
step   2500 | loss 1.5139 | lr 5.07e-05 | grad 8.50 | tok/s 42152
step   2550 | loss 1.6641 | lr 4.29e-05 | grad 1.80 | tok/s 42425
step   2600 | loss 1.5872 | lr 3.54e-05 | grad 1.21 | tok/s 41671
step   2650 | loss 1.6503 | lr 2.82e-05 | grad 2.73 | tok/s 41788
step   2700 | loss 1.4888 | lr 2.15e-05 | grad 2.91 | tok/s 42739
step   2750 | loss 1.5669 | lr 1.56e-05 | grad 1.55 | tok/s 41715
step   2800 | loss 1.5436 | lr 1.05e-05 | grad 5.91 | tok/s 41911
step   2850 | loss 1.5966 | lr 6.47e-06 | grad 1.69 | tok/s 41880
step   2900 | loss 1.4078 | lr 3.47e-06 | grad 2.00 | tok/s 42677
step   2950 | loss 1.5617 | lr 1.63e-06 | grad 1.23 | tok/s 41548
step   3000 | loss 1.6535 | lr 1.00e-06 | grad 4.31 | tok/s 42020
  >>> saved checkpoint: checkpoint_step_003000_loss_1.6535.pt
step   3050 | loss 1.6213 | lr 1.59e-06 | grad 1.28 | tok/s 39803
step   3100 | loss 1.5390 | lr 3.37e-06 | grad 6.91 | tok/s 42679
step   3150 | loss 1.8427 | lr 6.32e-06 | grad 5.56 | tok/s 42557

Training complete! Final step: 3161
