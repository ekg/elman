Using device: cuda
Output directory: /home/erikg/elman/benchmark_results/100m_systematic/E1_gated_elman/level1_100m_20260113_170959
Created Level 1 model: dim=768, depth=19, params=101,093,760
Model: Level 1, 101,093,760 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.6208 | lr 4.90e-06 | grad 22.50 | tok/s 17444
step    100 | loss 5.3310 | lr 9.90e-06 | grad 12.56 | tok/s 22923
step    150 | loss 4.2283 | lr 1.49e-05 | grad 15.44 | tok/s 22078
step    200 | loss 3.3306 | lr 1.99e-05 | grad 5.91 | tok/s 21499
step    250 | loss 2.8399 | lr 2.49e-05 | grad 6.16 | tok/s 29445
step    300 | loss 2.4579 | lr 2.99e-05 | grad 7.06 | tok/s 30802
step    350 | loss 2.3881 | lr 3.49e-05 | grad 7.31 | tok/s 30624
step    400 | loss 2.1493 | lr 3.99e-05 | grad 4.94 | tok/s 32007
step    450 | loss 2.3362 | lr 4.49e-05 | grad 3.34 | tok/s 31641
step    500 | loss 2.1415 | lr 4.99e-05 | grad 3.53 | tok/s 31368
step    550 | loss 2.1440 | lr 5.49e-05 | grad 3.39 | tok/s 30440
step    600 | loss 1.8074 | lr 5.99e-05 | grad 3.47 | tok/s 32511
step    650 | loss 1.8755 | lr 6.49e-05 | grad 4.62 | tok/s 31775
step    700 | loss 2.0375 | lr 6.99e-05 | grad 3.08 | tok/s 30890
step    750 | loss 2.0401 | lr 7.49e-05 | grad 5.69 | tok/s 31336
step    800 | loss 2.0597 | lr 7.99e-05 | grad 6.16 | tok/s 31439
step    850 | loss 1.9313 | lr 8.49e-05 | grad 1.84 | tok/s 30633
step    900 | loss 2.0066 | lr 8.99e-05 | grad 1.57 | tok/s 31202
step    950 | loss 1.8592 | lr 9.49e-05 | grad 1.56 | tok/s 31051
step   1000 | loss 2.0170 | lr 9.99e-05 | grad 2.02 | tok/s 31029
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0170.pt
step   1050 | loss 1.9828 | lr 1.59e-06 | grad 5.34 | tok/s 28316
step   1100 | loss 2.0626 | lr 3.37e-06 | grad 2.06 | tok/s 30668
step   1150 | loss 2.0225 | lr 6.32e-06 | grad 3.55 | tok/s 30884
step   1200 | loss 2.1241 | lr 1.04e-05 | grad 3.17 | tok/s 29776
step   1250 | loss 1.9256 | lr 1.54e-05 | grad 1.10 | tok/s 30481
step   1300 | loss 1.9735 | lr 2.13e-05 | grad 1.88 | tok/s 30336
step   1350 | loss 1.9565 | lr 2.79e-05 | grad 1.50 | tok/s 30072
step   1400 | loss 1.9323 | lr 3.51e-05 | grad 4.00 | tok/s 29805
step   1450 | loss 1.9024 | lr 4.26e-05 | grad 2.22 | tok/s 30017
step   1500 | loss 1.8139 | lr 5.03e-05 | grad 2.02 | tok/s 30076
step   1550 | loss 1.8673 | lr 5.81e-05 | grad 1.47 | tok/s 29989
step   1600 | loss 1.7277 | lr 6.56e-05 | grad 1.82 | tok/s 30442
step   1650 | loss 2.0539 | lr 7.28e-05 | grad 1.52 | tok/s 30755
step   1700 | loss 1.7217 | lr 7.95e-05 | grad 1.52 | tok/s 30850
step   1750 | loss 1.7246 | lr 8.54e-05 | grad 1.81 | tok/s 29875
step   1800 | loss 1.8888 | lr 9.05e-05 | grad 1.20 | tok/s 29642
step   1850 | loss 1.7274 | lr 9.45e-05 | grad 1.62 | tok/s 29599
step   1900 | loss 1.7727 | lr 9.75e-05 | grad 1.48 | tok/s 29661
step   1950 | loss 1.3782 | lr 9.94e-05 | grad 0.83 | tok/s 31465
step   2000 | loss 1.7958 | lr 1.00e-04 | grad 1.23 | tok/s 29408
  >>> saved checkpoint: checkpoint_step_002000_loss_1.7958.pt
step   2050 | loss 1.7408 | lr 9.94e-05 | grad 1.44 | tok/s 28551
step   2100 | loss 1.7804 | lr 9.76e-05 | grad 1.27 | tok/s 29771
step   2150 | loss 2.0452 | lr 9.47e-05 | grad 2.67 | tok/s 30299
step   2200 | loss 1.8663 | lr 9.06e-05 | grad 1.29 | tok/s 29842

Training complete! Final step: 2228
