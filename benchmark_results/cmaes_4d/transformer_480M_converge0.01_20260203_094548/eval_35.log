Using device: cuda
Output directory: benchmark_results/cmaes_4d/transformer_480M_converge0.01_20260203_094548/eval_35/levelllama_100m_20260203_114734
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 264,585,600 parameters
Using schedule-free AdamW (lr=0.00012114146383860301)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 7.2934 | lr 1.21e-04 | grad 6.06 | tok/s 12740
step     20 | loss 5.7059 | lr 1.21e-04 | grad 10.00 | tok/s 21009
step     30 | loss 6.1279 | lr 1.21e-04 | grad 12.31 | tok/s 21294
step     40 | loss 4.9180 | lr 1.21e-04 | grad 3.56 | tok/s 21285
step     50 | loss 4.4223 | lr 1.21e-04 | grad 2.70 | tok/s 21277
step     60 | loss 4.0945 | lr 1.21e-04 | grad 4.38 | tok/s 20430
step     70 | loss 3.8139 | lr 1.21e-04 | grad 4.50 | tok/s 20756
step     80 | loss 3.4886 | lr 1.21e-04 | grad 1.62 | tok/s 20600
step     90 | loss 3.5401 | lr 1.21e-04 | grad 1.13 | tok/s 20091
step    100 | loss 3.0491 | lr 1.21e-04 | grad 0.96 | tok/s 20830
step    110 | loss 3.3736 | lr 1.21e-04 | grad 1.02 | tok/s 20124
step    120 | loss 3.1526 | lr 1.21e-04 | grad 1.21 | tok/s 20247
step    130 | loss 2.9583 | lr 1.21e-04 | grad 1.61 | tok/s 20743
step    140 | loss 2.7241 | lr 1.21e-04 | grad 1.20 | tok/s 19761
step    150 | loss 2.8472 | lr 1.21e-04 | grad 0.86 | tok/s 19937
step    160 | loss 2.7864 | lr 1.21e-04 | grad 1.12 | tok/s 19985
step    170 | loss 2.9877 | lr 1.21e-04 | grad 1.71 | tok/s 20455
step    180 | loss 2.8173 | lr 1.21e-04 | grad 0.97 | tok/s 20387
step    190 | loss 2.7947 | lr 1.21e-04 | grad 0.98 | tok/s 21098
step    200 | loss 2.8035 | lr 1.21e-04 | grad 0.64 | tok/s 20468
step    210 | loss 2.8521 | lr 1.21e-04 | grad 0.93 | tok/s 20767
step    220 | loss 2.7439 | lr 1.21e-04 | grad 0.85 | tok/s 20355
step    230 | loss 2.6859 | lr 1.21e-04 | grad 0.89 | tok/s 20303
step    240 | loss 2.8490 | lr 1.21e-04 | grad 0.71 | tok/s 20752
step    250 | loss 2.7865 | lr 1.21e-04 | grad 0.76 | tok/s 20251
step    260 | loss 2.5539 | lr 1.21e-04 | grad 0.58 | tok/s 19816
step    270 | loss 2.6421 | lr 1.21e-04 | grad 1.46 | tok/s 20259
step    280 | loss 2.5247 | lr 1.21e-04 | grad 0.81 | tok/s 20885
step    290 | loss 2.5289 | lr 1.21e-04 | grad 0.85 | tok/s 21154
step    300 | loss 2.5550 | lr 1.21e-04 | grad 0.68 | tok/s 21151
step    310 | loss 2.5543 | lr 1.21e-04 | grad 1.42 | tok/s 20933
step    320 | loss 2.5078 | lr 1.21e-04 | grad 0.68 | tok/s 20001
step    330 | loss 2.5898 | lr 1.21e-04 | grad 1.88 | tok/s 20631
step    340 | loss 2.5414 | lr 1.21e-04 | grad 2.05 | tok/s 19967
step    350 | loss 2.4929 | lr 1.21e-04 | grad 0.73 | tok/s 19841
step    360 | loss 2.5202 | lr 1.21e-04 | grad 0.70 | tok/s 20523
step    370 | loss 2.8744 | lr 1.21e-04 | grad 0.82 | tok/s 20574
step    380 | loss 2.5332 | lr 1.21e-04 | grad 1.55 | tok/s 20890
step    390 | loss 2.4400 | lr 1.21e-04 | grad 0.83 | tok/s 20366
step    400 | loss 2.5017 | lr 1.21e-04 | grad 0.67 | tok/s 20523
step    410 | loss 2.4146 | lr 1.21e-04 | grad 0.79 | tok/s 19895
step    420 | loss 2.5090 | lr 1.21e-04 | grad 0.85 | tok/s 20033
step    430 | loss 2.7380 | lr 1.21e-04 | grad 2.59 | tok/s 20648
step    440 | loss 2.4974 | lr 1.21e-04 | grad 0.74 | tok/s 20293
step    450 | loss 2.3992 | lr 1.21e-04 | grad 0.62 | tok/s 20286
step    460 | loss 2.4627 | lr 1.21e-04 | grad 1.05 | tok/s 20323
step    470 | loss 2.3391 | lr 1.21e-04 | grad 0.86 | tok/s 19727
step    480 | loss 2.2885 | lr 1.21e-04 | grad 0.75 | tok/s 20180
step    490 | loss 3.0134 | lr 1.21e-04 | grad 1.20 | tok/s 20796
step    500 | loss 2.4161 | lr 1.21e-04 | grad 2.19 | tok/s 20299
step    510 | loss 2.2392 | lr 1.21e-04 | grad 0.85 | tok/s 20894
step    520 | loss 2.6733 | lr 1.21e-04 | grad 1.68 | tok/s 20355
step    530 | loss 2.2114 | lr 1.21e-04 | grad 0.89 | tok/s 20600
step    540 | loss 2.2779 | lr 1.21e-04 | grad 0.59 | tok/s 20779
step    550 | loss 2.1972 | lr 1.21e-04 | grad 1.20 | tok/s 21134
step    560 | loss 2.2953 | lr 1.21e-04 | grad 1.91 | tok/s 20843
step    570 | loss 2.6573 | lr 1.21e-04 | grad 0.97 | tok/s 20893
step    580 | loss 2.7454 | lr 1.21e-04 | grad 1.92 | tok/s 20129
step    590 | loss 2.3551 | lr 1.21e-04 | grad 1.02 | tok/s 20338
step    600 | loss 2.4609 | lr 1.21e-04 | grad 0.98 | tok/s 21125
step    610 | loss 2.2427 | lr 1.21e-04 | grad 0.60 | tok/s 20062
step    620 | loss 2.3021 | lr 1.21e-04 | grad 1.38 | tok/s 20803
step    630 | loss 2.6567 | lr 1.21e-04 | grad 0.91 | tok/s 20780
step    640 | loss 2.3009 | lr 1.21e-04 | grad 0.95 | tok/s 20293
step    650 | loss 2.5396 | lr 1.21e-04 | grad 3.72 | tok/s 20024
step    660 | loss 2.4303 | lr 1.21e-04 | grad 1.42 | tok/s 20663
step    670 | loss 2.3967 | lr 1.21e-04 | grad 1.43 | tok/s 20255
step    680 | loss 2.3779 | lr 1.21e-04 | grad 0.96 | tok/s 20100
step    690 | loss 2.5119 | lr 1.21e-04 | grad 1.10 | tok/s 20286
step    700 | loss 2.4439 | lr 1.21e-04 | grad 1.26 | tok/s 20490
step    710 | loss 2.5072 | lr 1.21e-04 | grad 2.11 | tok/s 20289
step    720 | loss 2.4027 | lr 1.21e-04 | grad 1.00 | tok/s 20428
step    730 | loss 2.3531 | lr 1.21e-04 | grad 2.02 | tok/s 20361
step    740 | loss 2.2091 | lr 1.21e-04 | grad 0.73 | tok/s 20137
step    750 | loss 2.6524 | lr 1.21e-04 | grad 0.67 | tok/s 20390
step    760 | loss 2.1971 | lr 1.21e-04 | grad 1.10 | tok/s 20345
step    770 | loss 2.3097 | lr 1.21e-04 | grad 0.92 | tok/s 20669
step    780 | loss 2.3187 | lr 1.21e-04 | grad 0.99 | tok/s 20745
step    790 | loss 2.3707 | lr 1.21e-04 | grad 2.30 | tok/s 20484
step    800 | loss 2.2059 | lr 1.21e-04 | grad 1.08 | tok/s 20332
step    810 | loss 2.8677 | lr 1.21e-04 | grad 1.96 | tok/s 20988
step    820 | loss 2.8514 | lr 1.21e-04 | grad 1.50 | tok/s 21136
step    830 | loss 2.7618 | lr 1.21e-04 | grad 1.20 | tok/s 21132
step    840 | loss 2.3469 | lr 1.21e-04 | grad 1.06 | tok/s 20158
step    850 | loss 2.2142 | lr 1.21e-04 | grad 0.98 | tok/s 20407
step    860 | loss 2.2165 | lr 1.21e-04 | grad 1.17 | tok/s 20339
step    870 | loss 2.3181 | lr 1.21e-04 | grad 1.20 | tok/s 20710
step    880 | loss 2.2128 | lr 1.21e-04 | grad 1.42 | tok/s 20111
step    890 | loss 2.5204 | lr 1.21e-04 | grad 1.00 | tok/s 19994
step    900 | loss 2.1856 | lr 1.21e-04 | grad 0.80 | tok/s 20007
step    910 | loss 2.2334 | lr 1.21e-04 | grad 0.68 | tok/s 20195
step    920 | loss 2.2409 | lr 1.21e-04 | grad 0.74 | tok/s 20091
step    930 | loss 2.2336 | lr 1.21e-04 | grad 0.98 | tok/s 20078
step    940 | loss 2.2368 | lr 1.21e-04 | grad 1.18 | tok/s 20549
step    950 | loss 2.1444 | lr 1.21e-04 | grad 0.94 | tok/s 21125
step    960 | loss 2.0942 | lr 1.21e-04 | grad 0.74 | tok/s 21121
step    970 | loss 2.1956 | lr 1.21e-04 | grad 0.97 | tok/s 20381
step    980 | loss 2.3499 | lr 1.21e-04 | grad 1.59 | tok/s 20013
step    990 | loss 2.3718 | lr 1.21e-04 | grad 1.38 | tok/s 20313
step   1000 | loss 2.2551 | lr 1.21e-04 | grad 0.91 | tok/s 20775
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2551.pt
step   1010 | loss 2.1785 | lr 1.21e-04 | grad 0.61 | tok/s 15571
step   1020 | loss 2.3208 | lr 1.21e-04 | grad 0.73 | tok/s 20325
step   1030 | loss 2.3150 | lr 1.21e-04 | grad 0.74 | tok/s 20305
step   1040 | loss 2.0003 | lr 1.21e-04 | grad 0.68 | tok/s 20234
step   1050 | loss 2.8238 | lr 1.21e-04 | grad 1.14 | tok/s 20815
step   1060 | loss 2.6028 | lr 1.21e-04 | grad 0.88 | tok/s 20155
step   1070 | loss 2.3350 | lr 1.21e-04 | grad 1.68 | tok/s 20161
step   1080 | loss 2.3330 | lr 1.21e-04 | grad 0.66 | tok/s 20503
step   1090 | loss 2.1629 | lr 1.21e-04 | grad 0.68 | tok/s 20712
step   1100 | loss 2.5341 | lr 1.21e-04 | grad 0.88 | tok/s 20896
step   1110 | loss 2.1565 | lr 1.21e-04 | grad 0.54 | tok/s 19746
step   1120 | loss 2.2298 | lr 1.21e-04 | grad 1.06 | tok/s 20335
step   1130 | loss 2.2791 | lr 1.21e-04 | grad 0.80 | tok/s 20393
step   1140 | loss 2.1593 | lr 1.21e-04 | grad 1.05 | tok/s 20203
step   1150 | loss 2.3103 | lr 1.21e-04 | grad 0.75 | tok/s 19738
step   1160 | loss 2.0415 | lr 1.21e-04 | grad 0.73 | tok/s 21055
step   1170 | loss 2.0604 | lr 1.21e-04 | grad 1.29 | tok/s 21123
step   1180 | loss 2.0233 | lr 1.21e-04 | grad 0.55 | tok/s 21128
step   1190 | loss 2.0250 | lr 1.21e-04 | grad 0.78 | tok/s 21123
step   1200 | loss 2.0444 | lr 1.21e-04 | grad 0.64 | tok/s 21126
step   1210 | loss 1.9905 | lr 1.21e-04 | grad 0.89 | tok/s 20652
step   1220 | loss 2.1252 | lr 1.21e-04 | grad 1.11 | tok/s 19793
step   1230 | loss 2.1890 | lr 1.21e-04 | grad 0.80 | tok/s 20716
step   1240 | loss 2.2106 | lr 1.21e-04 | grad 1.32 | tok/s 20433
step   1250 | loss 2.3505 | lr 1.21e-04 | grad 0.73 | tok/s 20246
step   1260 | loss 2.2833 | lr 1.21e-04 | grad 1.88 | tok/s 20364
step   1270 | loss 2.1489 | lr 1.21e-04 | grad 0.82 | tok/s 19949
step   1280 | loss 2.1468 | lr 1.21e-04 | grad 0.95 | tok/s 20059
step   1290 | loss 2.3089 | lr 1.21e-04 | grad 0.89 | tok/s 20119
step   1300 | loss 2.1788 | lr 1.21e-04 | grad 1.12 | tok/s 20159
step   1310 | loss 2.2463 | lr 1.21e-04 | grad 1.00 | tok/s 20544
step   1320 | loss 2.0992 | lr 1.21e-04 | grad 0.77 | tok/s 20119
step   1330 | loss 2.1920 | lr 1.21e-04 | grad 2.22 | tok/s 20735
step   1340 | loss 2.1743 | lr 1.21e-04 | grad 0.75 | tok/s 20074
step   1350 | loss 2.2049 | lr 1.21e-04 | grad 1.52 | tok/s 20227
step   1360 | loss 2.2729 | lr 1.21e-04 | grad 1.60 | tok/s 20059
step   1370 | loss 2.2428 | lr 1.21e-04 | grad 1.90 | tok/s 20254
step   1380 | loss 2.1834 | lr 1.21e-04 | grad 2.78 | tok/s 20665
step   1390 | loss 2.2353 | lr 1.21e-04 | grad 1.05 | tok/s 20711
step   1400 | loss 2.2354 | lr 1.21e-04 | grad 0.93 | tok/s 19867
step   1410 | loss 2.0976 | lr 1.21e-04 | grad 0.84 | tok/s 19530
step   1420 | loss 1.9702 | lr 1.21e-04 | grad 0.77 | tok/s 20271
step   1430 | loss 2.1048 | lr 1.21e-04 | grad 2.02 | tok/s 20773
step   1440 | loss 2.1155 | lr 1.21e-04 | grad 1.58 | tok/s 19796
step   1450 | loss 2.1948 | lr 1.21e-04 | grad 1.51 | tok/s 20301
step   1460 | loss 2.1366 | lr 1.21e-04 | grad 1.22 | tok/s 20310
step   1470 | loss 2.2791 | lr 1.21e-04 | grad 4.41 | tok/s 20396
step   1480 | loss 2.2597 | lr 1.21e-04 | grad 1.32 | tok/s 19914
step   1490 | loss 2.2828 | lr 1.21e-04 | grad 1.84 | tok/s 20624
step   1500 | loss 2.2014 | lr 1.21e-04 | grad 0.90 | tok/s 20639
step   1510 | loss 2.1477 | lr 1.21e-04 | grad 0.78 | tok/s 20238
step   1520 | loss 2.1660 | lr 1.21e-04 | grad 0.94 | tok/s 20183
step   1530 | loss 2.1860 | lr 1.21e-04 | grad 1.89 | tok/s 20749
step   1540 | loss 2.5646 | lr 1.21e-04 | grad 1.27 | tok/s 20301
step   1550 | loss 2.1252 | lr 1.21e-04 | grad 0.97 | tok/s 20156
step   1560 | loss 2.2956 | lr 1.21e-04 | grad 0.62 | tok/s 20471
step   1570 | loss 2.0426 | lr 1.21e-04 | grad 0.84 | tok/s 19955
step   1580 | loss 2.0949 | lr 1.21e-04 | grad 0.97 | tok/s 20171
step   1590 | loss 1.9816 | lr 1.21e-04 | grad 1.53 | tok/s 20778
step   1600 | loss 2.0659 | lr 1.21e-04 | grad 0.80 | tok/s 20432
step   1610 | loss 2.1572 | lr 1.21e-04 | grad 0.92 | tok/s 20779
step   1620 | loss 2.0521 | lr 1.21e-04 | grad 0.84 | tok/s 19649
step   1630 | loss 2.0729 | lr 1.21e-04 | grad 0.65 | tok/s 20039
step   1640 | loss 2.1610 | lr 1.21e-04 | grad 1.00 | tok/s 20144
step   1650 | loss 2.3088 | lr 1.21e-04 | grad 1.83 | tok/s 20713
step   1660 | loss 2.5725 | lr 1.21e-04 | grad 0.71 | tok/s 20327
step   1670 | loss 2.0732 | lr 1.21e-04 | grad 0.85 | tok/s 20361
step   1680 | loss 2.4646 | lr 1.21e-04 | grad 0.71 | tok/s 20375
step   1690 | loss 2.1880 | lr 1.21e-04 | grad 0.69 | tok/s 20134
step   1700 | loss 2.1680 | lr 1.21e-04 | grad 0.75 | tok/s 20290
step   1710 | loss 2.2719 | lr 1.21e-04 | grad 0.78 | tok/s 20215
step   1720 | loss 2.1573 | lr 1.21e-04 | grad 0.88 | tok/s 20063
step   1730 | loss 2.1246 | lr 1.21e-04 | grad 1.43 | tok/s 20151
step   1740 | loss 2.3162 | lr 1.21e-04 | grad 1.05 | tok/s 20171
step   1750 | loss 2.2334 | lr 1.21e-04 | grad 0.70 | tok/s 20009
step   1760 | loss 2.1474 | lr 1.21e-04 | grad 2.16 | tok/s 20147
step   1770 | loss 2.1698 | lr 1.21e-04 | grad 1.04 | tok/s 19894
step   1780 | loss 2.1219 | lr 1.21e-04 | grad 1.25 | tok/s 20548
step   1790 | loss 1.9523 | lr 1.21e-04 | grad 0.82 | tok/s 20546
step   1800 | loss 2.0396 | lr 1.21e-04 | grad 0.96 | tok/s 19981
step   1810 | loss 2.0986 | lr 1.21e-04 | grad 1.20 | tok/s 19969
step   1820 | loss 2.1799 | lr 1.21e-04 | grad 0.91 | tok/s 20154
step   1830 | loss 2.1967 | lr 1.21e-04 | grad 1.23 | tok/s 19972
step   1840 | loss 2.0885 | lr 1.21e-04 | grad 0.82 | tok/s 20162
step   1850 | loss 2.1334 | lr 1.21e-04 | grad 0.92 | tok/s 20730
step   1860 | loss 2.0934 | lr 1.21e-04 | grad 1.11 | tok/s 20610
step   1870 | loss 2.3427 | lr 1.21e-04 | grad 0.95 | tok/s 20169
step   1880 | loss 2.0564 | lr 1.21e-04 | grad 0.77 | tok/s 20455
step   1890 | loss 2.1756 | lr 1.21e-04 | grad 0.80 | tok/s 20310
step   1900 | loss 2.0175 | lr 1.21e-04 | grad 0.84 | tok/s 20402
step   1910 | loss 2.0372 | lr 1.21e-04 | grad 0.79 | tok/s 20672
step   1920 | loss 2.0241 | lr 1.21e-04 | grad 1.06 | tok/s 20798
step   1930 | loss 2.1434 | lr 1.21e-04 | grad 1.95 | tok/s 20539
step   1940 | loss 2.5179 | lr 1.21e-04 | grad 1.18 | tok/s 20519
step   1950 | loss 1.9759 | lr 1.21e-04 | grad 0.87 | tok/s 20184
step   1960 | loss 2.3553 | lr 1.21e-04 | grad 0.87 | tok/s 20456
step   1970 | loss 2.0415 | lr 1.21e-04 | grad 2.52 | tok/s 20309
step   1980 | loss 2.1007 | lr 1.21e-04 | grad 1.02 | tok/s 20755
step   1990 | loss 1.8901 | lr 1.21e-04 | grad 0.90 | tok/s 21129
step   2000 | loss 2.0306 | lr 1.21e-04 | grad 2.53 | tok/s 20986
  >>> saved checkpoint: checkpoint_step_002000_loss_2.0306.pt
step   2010 | loss 2.0066 | lr 1.21e-04 | grad 1.09 | tok/s 16258
step   2020 | loss 1.9202 | lr 1.21e-04 | grad 0.90 | tok/s 21158
step   2030 | loss 2.0534 | lr 1.21e-04 | grad 0.96 | tok/s 19963
step   2040 | loss 1.9942 | lr 1.21e-04 | grad 1.45 | tok/s 20803
step   2050 | loss 2.1662 | lr 1.21e-04 | grad 0.79 | tok/s 20011
step   2060 | loss 2.1669 | lr 1.21e-04 | grad 1.30 | tok/s 20543
step   2070 | loss 2.0675 | lr 1.21e-04 | grad 0.66 | tok/s 20151
step   2080 | loss 1.9773 | lr 1.21e-04 | grad 1.00 | tok/s 20749
step   2090 | loss 1.8938 | lr 1.21e-04 | grad 0.80 | tok/s 20204
step   2100 | loss 2.0744 | lr 1.21e-04 | grad 0.82 | tok/s 20321
step   2110 | loss 2.0862 | lr 1.21e-04 | grad 1.23 | tok/s 19993
step   2120 | loss 2.2013 | lr 1.21e-04 | grad 0.66 | tok/s 20272
step   2130 | loss 2.0930 | lr 1.21e-04 | grad 0.88 | tok/s 20115
step   2140 | loss 2.2187 | lr 1.21e-04 | grad 0.84 | tok/s 20445
step   2150 | loss 2.0656 | lr 1.21e-04 | grad 0.66 | tok/s 20505
step   2160 | loss 2.4520 | lr 1.21e-04 | grad 0.84 | tok/s 20607
step   2170 | loss 1.9490 | lr 1.21e-04 | grad 0.64 | tok/s 21124
step   2180 | loss 1.9749 | lr 1.21e-04 | grad 0.66 | tok/s 21127
step   2190 | loss 1.9707 | lr 1.21e-04 | grad 0.93 | tok/s 21124
step   2200 | loss 2.0594 | lr 1.21e-04 | grad 1.03 | tok/s 20588
step   2210 | loss 2.3861 | lr 1.21e-04 | grad 1.43 | tok/s 20865
step   2220 | loss 2.3643 | lr 1.21e-04 | grad 1.20 | tok/s 20793
step   2230 | loss 2.2133 | lr 1.21e-04 | grad 1.13 | tok/s 20601
step   2240 | loss 2.1043 | lr 1.21e-04 | grad 1.05 | tok/s 20402
step   2250 | loss 2.1717 | lr 1.21e-04 | grad 1.18 | tok/s 20064
step   2260 | loss 2.0072 | lr 1.21e-04 | grad 1.16 | tok/s 20296
step   2270 | loss 1.9707 | lr 1.21e-04 | grad 1.09 | tok/s 19962
step   2280 | loss 1.9872 | lr 1.21e-04 | grad 1.09 | tok/s 20942
step   2290 | loss 1.9978 | lr 1.21e-04 | grad 0.69 | tok/s 21128
step   2300 | loss 1.9686 | lr 1.21e-04 | grad 0.77 | tok/s 21127

Training complete! Final step: 2303
