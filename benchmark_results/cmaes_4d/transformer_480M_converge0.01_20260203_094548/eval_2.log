Using device: cuda
Output directory: benchmark_results/cmaes_4d/transformer_480M_converge0.01_20260203_094548/eval_2/levelllama_100m_20260203_094555
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 718,934,016 parameters
Using schedule-free AdamW (lr=8.314573080373795e-05)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 10.1237 | lr 8.31e-05 | grad 3.77 | tok/s 5472
step     20 | loss 3.1587 | lr 8.31e-05 | grad 1.80 | tok/s 12131
step     30 | loss 3.0618 | lr 8.31e-05 | grad 0.89 | tok/s 12263
step     40 | loss 3.2597 | lr 8.31e-05 | grad 1.45 | tok/s 11730
step     50 | loss 4.1962 | lr 8.31e-05 | grad 9.69 | tok/s 11900
step     60 | loss 3.0325 | lr 8.31e-05 | grad 15.44 | tok/s 12282
step     70 | loss 2.7860 | lr 8.31e-05 | grad 1.93 | tok/s 12415
step     80 | loss 11.4646 | lr 8.31e-05 | grad 80.50 | tok/s 12424
step     90 | loss 10.8605 | lr 8.31e-05 | grad 2.70 | tok/s 12637
step    100 | loss 7.9858 | lr 8.31e-05 | grad 12.62 | tok/s 12610
step    110 | loss 7.7928 | lr 8.31e-05 | grad 12.00 | tok/s 12599
step    120 | loss 6.8593 | lr 8.31e-05 | grad 10.31 | tok/s 12584
step    130 | loss 6.6114 | lr 8.31e-05 | grad 51.25 | tok/s 12565
step    140 | loss 5.1939 | lr 8.31e-05 | grad 33.75 | tok/s 12574
step    150 | loss 5.7796 | lr 8.31e-05 | grad 8.06 | tok/s 12554
step    160 | loss 4.5868 | lr 8.31e-05 | grad 6.84 | tok/s 12540
step    170 | loss 4.5015 | lr 8.31e-05 | grad 7.09 | tok/s 12523
step    180 | loss 4.4190 | lr 8.31e-05 | grad 4.03 | tok/s 12529
step    190 | loss 4.4093 | lr 8.31e-05 | grad 4.94 | tok/s 12519
step    200 | loss 4.1340 | lr 8.31e-05 | grad 4.72 | tok/s 12521
step    210 | loss 4.0965 | lr 8.31e-05 | grad 5.28 | tok/s 12508
step    220 | loss 3.6705 | lr 8.31e-05 | grad 3.41 | tok/s 12332
step    230 | loss 4.0532 | lr 8.31e-05 | grad 3.36 | tok/s 12190
step    240 | loss 3.0563 | lr 8.31e-05 | grad 2.86 | tok/s 11568
step    250 | loss 2.8600 | lr 8.31e-05 | grad 1.23 | tok/s 11880
step    260 | loss 2.7252 | lr 8.31e-05 | grad 1.38 | tok/s 12257
step    270 | loss 2.9935 | lr 8.31e-05 | grad 1.04 | tok/s 12108
step    280 | loss 3.1258 | lr 8.31e-05 | grad 2.39 | tok/s 11888
step    290 | loss 4.2382 | lr 8.31e-05 | grad 6.34 | tok/s 12514
step    300 | loss 3.7128 | lr 8.31e-05 | grad 3.81 | tok/s 12522
step    310 | loss 3.2749 | lr 8.31e-05 | grad 4.16 | tok/s 12281
step    320 | loss 3.1351 | lr 8.31e-05 | grad 2.95 | tok/s 12035
step    330 | loss 2.7173 | lr 8.31e-05 | grad 1.77 | tok/s 11627
step    340 | loss 3.0714 | lr 8.31e-05 | grad 1.23 | tok/s 11808
step    350 | loss 2.9158 | lr 8.31e-05 | grad 3.30 | tok/s 12112
step    360 | loss 3.6917 | lr 8.31e-05 | grad 2.09 | tok/s 12377
step    370 | loss 2.6451 | lr 8.31e-05 | grad 1.18 | tok/s 11206
step    380 | loss 2.7110 | lr 8.31e-05 | grad 1.52 | tok/s 11953
step    390 | loss 2.6005 | lr 8.31e-05 | grad 0.89 | tok/s 12477
step    400 | loss 2.6058 | lr 8.31e-05 | grad 1.52 | tok/s 12359
step    410 | loss 2.5924 | lr 8.31e-05 | grad 0.62 | tok/s 12086
step    420 | loss 2.6558 | lr 8.31e-05 | grad 3.25 | tok/s 11545
step    430 | loss 3.1074 | lr 8.31e-05 | grad 1.30 | tok/s 12294
step    440 | loss 2.8939 | lr 8.31e-05 | grad 2.72 | tok/s 11612
step    450 | loss 3.4815 | lr 8.31e-05 | grad 0.80 | tok/s 12025
step    460 | loss 2.7052 | lr 8.31e-05 | grad 2.16 | tok/s 11763
step    470 | loss 2.8199 | lr 8.31e-05 | grad 1.04 | tok/s 12130
step    480 | loss 3.3281 | lr 8.31e-05 | grad 5.03 | tok/s 12137
step    490 | loss 2.7529 | lr 8.31e-05 | grad 1.11 | tok/s 11467
step    500 | loss 2.6447 | lr 8.31e-05 | grad 1.32 | tok/s 12238
step    510 | loss 2.6548 | lr 8.31e-05 | grad 1.00 | tok/s 12390
step    520 | loss 2.6784 | lr 8.31e-05 | grad 1.03 | tok/s 12374
step    530 | loss 2.9237 | lr 8.31e-05 | grad 1.57 | tok/s 11897
step    540 | loss 2.5091 | lr 8.31e-05 | grad 0.91 | tok/s 11887
step    550 | loss 2.3697 | lr 8.31e-05 | grad 1.01 | tok/s 11645
step    560 | loss 2.5384 | lr 8.31e-05 | grad 1.31 | tok/s 11356
step    570 | loss 2.5791 | lr 8.31e-05 | grad 1.46 | tok/s 11669
step    580 | loss 2.3978 | lr 8.31e-05 | grad 1.25 | tok/s 11620
step    590 | loss 2.8515 | lr 8.31e-05 | grad 1.96 | tok/s 11909
step    600 | loss 2.6186 | lr 8.31e-05 | grad 1.30 | tok/s 11501
step    610 | loss 2.5257 | lr 8.31e-05 | grad 1.02 | tok/s 12093
step    620 | loss 2.3103 | lr 8.31e-05 | grad 0.76 | tok/s 11454
step    630 | loss 2.5020 | lr 8.31e-05 | grad 1.95 | tok/s 11555
step    640 | loss 2.6666 | lr 8.31e-05 | grad 0.99 | tok/s 11849
step    650 | loss 2.6088 | lr 8.31e-05 | grad 1.00 | tok/s 11911
step    660 | loss 2.5118 | lr 8.31e-05 | grad 1.48 | tok/s 11969
step    670 | loss 2.8674 | lr 8.31e-05 | grad 5.75 | tok/s 12057
step    680 | loss 2.6034 | lr 8.31e-05 | grad 1.10 | tok/s 11810
step    690 | loss 2.8966 | lr 8.31e-05 | grad 2.05 | tok/s 12217
step    700 | loss 2.9547 | lr 8.31e-05 | grad 2.86 | tok/s 12457
step    710 | loss 2.4924 | lr 8.31e-05 | grad 0.79 | tok/s 11624
step    720 | loss 2.2732 | lr 8.31e-05 | grad 1.37 | tok/s 11469
step    730 | loss 2.6271 | lr 8.31e-05 | grad 1.57 | tok/s 12417
step    740 | loss 2.4789 | lr 8.31e-05 | grad 1.47 | tok/s 12266
step    750 | loss 2.5163 | lr 8.31e-05 | grad 1.43 | tok/s 12453
step    760 | loss 2.4089 | lr 8.31e-05 | grad 1.24 | tok/s 12447
step    770 | loss 2.4240 | lr 8.31e-05 | grad 1.09 | tok/s 12446
step    780 | loss 2.4134 | lr 8.31e-05 | grad 1.18 | tok/s 12446
step    790 | loss 2.3359 | lr 8.31e-05 | grad 1.20 | tok/s 12063
step    800 | loss 2.8280 | lr 8.31e-05 | grad 3.06 | tok/s 12013
step    810 | loss 2.4308 | lr 8.31e-05 | grad 1.28 | tok/s 11939
step    820 | loss 2.3896 | lr 8.31e-05 | grad 1.66 | tok/s 11475
step    830 | loss 2.6571 | lr 8.31e-05 | grad 1.73 | tok/s 12323
step    840 | loss 2.6824 | lr 8.31e-05 | grad 1.19 | tok/s 12434
step    850 | loss 2.6080 | lr 8.31e-05 | grad 1.21 | tok/s 12390
step    860 | loss 2.5928 | lr 8.31e-05 | grad 2.12 | tok/s 12243
step    870 | loss 2.3366 | lr 8.31e-05 | grad 1.59 | tok/s 11781
step    880 | loss 2.5334 | lr 8.31e-05 | grad 1.12 | tok/s 11852
step    890 | loss 2.4452 | lr 8.31e-05 | grad 1.30 | tok/s 12017
step    900 | loss 2.3745 | lr 8.31e-05 | grad 1.11 | tok/s 12037
step    910 | loss 2.3172 | lr 8.31e-05 | grad 1.81 | tok/s 11785
step    920 | loss 2.5859 | lr 8.31e-05 | grad 1.54 | tok/s 12251
step    930 | loss 2.3705 | lr 8.31e-05 | grad 1.73 | tok/s 11686
step    940 | loss 2.5518 | lr 8.31e-05 | grad 1.20 | tok/s 12335
step    950 | loss 2.6646 | lr 8.31e-05 | grad 1.62 | tok/s 12382
step    960 | loss 2.6565 | lr 8.31e-05 | grad 1.23 | tok/s 12399
step    970 | loss 2.4233 | lr 8.31e-05 | grad 1.61 | tok/s 11652
step    980 | loss 2.4669 | lr 8.31e-05 | grad 1.30 | tok/s 11981
step    990 | loss 2.3532 | lr 8.31e-05 | grad 1.11 | tok/s 12185
step   1000 | loss 2.6096 | lr 8.31e-05 | grad 6.72 | tok/s 11694
  >>> saved checkpoint: checkpoint_step_001000_loss_2.6096.pt
step   1010 | loss 2.5128 | lr 8.31e-05 | grad 1.55 | tok/s 4742
step   1020 | loss 2.3127 | lr 8.31e-05 | grad 1.40 | tok/s 11402
step   1030 | loss 2.2701 | lr 8.31e-05 | grad 0.95 | tok/s 12070
step   1040 | loss 2.2826 | lr 8.31e-05 | grad 1.09 | tok/s 11996
step   1050 | loss 2.3187 | lr 8.31e-05 | grad 1.55 | tok/s 11665
step   1060 | loss 2.5418 | lr 8.31e-05 | grad 1.10 | tok/s 12177
step   1070 | loss 2.7027 | lr 8.31e-05 | grad 1.00 | tok/s 11938
step   1080 | loss 2.1472 | lr 8.31e-05 | grad 0.97 | tok/s 11210
step   1090 | loss 1.8530 | lr 8.31e-05 | grad 2.02 | tok/s 12444
step   1100 | loss 2.3277 | lr 8.31e-05 | grad 2.39 | tok/s 11944
step   1110 | loss 2.3371 | lr 8.31e-05 | grad 0.91 | tok/s 12478
step   1120 | loss 2.3206 | lr 8.31e-05 | grad 1.16 | tok/s 12464
step   1130 | loss 2.2594 | lr 8.31e-05 | grad 1.14 | tok/s 12468
step   1140 | loss 2.2367 | lr 8.31e-05 | grad 1.16 | tok/s 12463
step   1150 | loss 2.2341 | lr 8.31e-05 | grad 1.11 | tok/s 12453
step   1160 | loss 2.1437 | lr 8.31e-05 | grad 1.21 | tok/s 12457
step   1170 | loss 2.2268 | lr 8.31e-05 | grad 1.04 | tok/s 12450
step   1180 | loss 2.2873 | lr 8.31e-05 | grad 1.47 | tok/s 12448
step   1190 | loss 2.2601 | lr 8.31e-05 | grad 1.15 | tok/s 12456
step   1200 | loss 2.2394 | lr 8.31e-05 | grad 1.42 | tok/s 12456
step   1210 | loss 2.1823 | lr 8.31e-05 | grad 1.16 | tok/s 12440
step   1220 | loss 2.1485 | lr 8.31e-05 | grad 1.00 | tok/s 12432
step   1230 | loss 2.1770 | lr 8.31e-05 | grad 1.21 | tok/s 12437
step   1240 | loss 2.3048 | lr 8.31e-05 | grad 4.38 | tok/s 12178
step   1250 | loss 2.4842 | lr 8.31e-05 | grad 0.96 | tok/s 11858
step   1260 | loss 1.9267 | lr 8.31e-05 | grad 1.12 | tok/s 11673
step   1270 | loss 2.3649 | lr 8.31e-05 | grad 1.23 | tok/s 11522
step   1280 | loss 2.6221 | lr 8.31e-05 | grad 2.61 | tok/s 12204
step   1290 | loss 2.2647 | lr 8.31e-05 | grad 2.17 | tok/s 12014
step   1300 | loss 2.2422 | lr 8.31e-05 | grad 2.06 | tok/s 11813
step   1310 | loss 2.2521 | lr 8.31e-05 | grad 2.33 | tok/s 12378
step   1320 | loss 2.4719 | lr 8.31e-05 | grad 2.41 | tok/s 12250
step   1330 | loss 2.4138 | lr 8.31e-05 | grad 1.91 | tok/s 12243
step   1340 | loss 2.3552 | lr 8.31e-05 | grad 1.09 | tok/s 11342
step   1350 | loss 2.3846 | lr 8.31e-05 | grad 1.38 | tok/s 11583
step   1360 | loss 2.3494 | lr 8.31e-05 | grad 1.32 | tok/s 12084
step   1370 | loss 2.5136 | lr 8.31e-05 | grad 4.31 | tok/s 11856
step   1380 | loss 2.2722 | lr 8.31e-05 | grad 1.88 | tok/s 11398
step   1390 | loss 2.1178 | lr 8.31e-05 | grad 1.33 | tok/s 11828
step   1400 | loss 2.3765 | lr 8.31e-05 | grad 1.34 | tok/s 11937
step   1410 | loss 2.3967 | lr 8.31e-05 | grad 1.89 | tok/s 11700
step   1420 | loss 2.3864 | lr 8.31e-05 | grad 1.48 | tok/s 11694
step   1430 | loss 2.2296 | lr 8.31e-05 | grad 2.06 | tok/s 11848
step   1440 | loss 2.1910 | lr 8.31e-05 | grad 1.16 | tok/s 12451
step   1450 | loss 2.2465 | lr 8.31e-05 | grad 4.94 | tok/s 12289
step   1460 | loss 2.4390 | lr 8.31e-05 | grad 3.83 | tok/s 11623
step   1470 | loss 2.2787 | lr 8.31e-05 | grad 1.03 | tok/s 12341
step   1480 | loss 3.2404 | lr 8.31e-05 | grad 3.36 | tok/s 12222
step   1490 | loss 2.8128 | lr 8.31e-05 | grad 3.09 | tok/s 12407
step   1500 | loss 2.2779 | lr 8.31e-05 | grad 1.39 | tok/s 12441
step   1510 | loss 2.3870 | lr 8.31e-05 | grad 1.25 | tok/s 12275
step   1520 | loss 2.2129 | lr 8.31e-05 | grad 2.09 | tok/s 12037
step   1530 | loss 2.2561 | lr 8.31e-05 | grad 1.06 | tok/s 12066
step   1540 | loss 2.3844 | lr 8.31e-05 | grad 1.49 | tok/s 11834
step   1550 | loss 2.1821 | lr 8.31e-05 | grad 1.52 | tok/s 12353
step   1560 | loss 2.2666 | lr 8.31e-05 | grad 1.31 | tok/s 11714
step   1570 | loss 2.2630 | lr 8.31e-05 | grad 2.16 | tok/s 12317
step   1580 | loss 3.2015 | lr 8.31e-05 | grad 3.27 | tok/s 12284
step   1590 | loss 2.3975 | lr 8.31e-05 | grad 1.52 | tok/s 11680
step   1600 | loss 1.6666 | lr 8.31e-05 | grad 1.03 | tok/s 12506
step   1610 | loss 1.8586 | lr 8.31e-05 | grad 1.08 | tok/s 11537
step   1620 | loss 2.3927 | lr 8.31e-05 | grad 2.52 | tok/s 11809
step   1630 | loss 2.3050 | lr 8.31e-05 | grad 2.25 | tok/s 12091
step   1640 | loss 2.1682 | lr 8.31e-05 | grad 2.33 | tok/s 11704
step   1650 | loss 2.2229 | lr 8.31e-05 | grad 2.42 | tok/s 11080
step   1660 | loss 2.1927 | lr 8.31e-05 | grad 1.03 | tok/s 12443
step   1670 | loss 2.6522 | lr 8.31e-05 | grad 3.83 | tok/s 11836
step   1680 | loss 2.1844 | lr 8.31e-05 | grad 1.16 | tok/s 11572
step   1690 | loss 2.4708 | lr 8.31e-05 | grad 3.70 | tok/s 12016
step   1700 | loss 2.3730 | lr 8.31e-05 | grad 1.02 | tok/s 11674
step   1710 | loss 2.3244 | lr 8.31e-05 | grad 1.30 | tok/s 11403
step   1720 | loss 2.6652 | lr 8.31e-05 | grad 2.16 | tok/s 12428
step   1730 | loss 2.9522 | lr 8.31e-05 | grad 2.31 | tok/s 12441
step   1740 | loss 2.4058 | lr 8.31e-05 | grad 2.09 | tok/s 11910
step   1750 | loss 2.2831 | lr 8.31e-05 | grad 1.49 | tok/s 12146
step   1760 | loss 2.2499 | lr 8.31e-05 | grad 1.43 | tok/s 11964
step   1770 | loss 2.1328 | lr 8.31e-05 | grad 1.26 | tok/s 11747
step   1780 | loss 2.1691 | lr 8.31e-05 | grad 1.48 | tok/s 12047
step   1790 | loss 2.1642 | lr 8.31e-05 | grad 1.02 | tok/s 11949
step   1800 | loss 2.2374 | lr 8.31e-05 | grad 1.34 | tok/s 11686
step   1810 | loss 2.3767 | lr 8.31e-05 | grad 4.22 | tok/s 11839
step   1820 | loss 2.3905 | lr 8.31e-05 | grad 2.77 | tok/s 11939
step   1830 | loss 2.1616 | lr 8.31e-05 | grad 1.44 | tok/s 12137
step   1840 | loss 2.1887 | lr 8.31e-05 | grad 1.02 | tok/s 11584
step   1850 | loss 2.3925 | lr 8.31e-05 | grad 1.34 | tok/s 12385
step   1860 | loss 1.9822 | lr 8.31e-05 | grad 1.34 | tok/s 11584
step   1870 | loss 2.2638 | lr 8.31e-05 | grad 1.23 | tok/s 12211
step   1880 | loss 2.0805 | lr 8.31e-05 | grad 1.38 | tok/s 11053
step   1890 | loss 2.3553 | lr 8.31e-05 | grad 1.01 | tok/s 11664
step   1900 | loss 2.0594 | lr 8.31e-05 | grad 1.30 | tok/s 11824
step   1910 | loss 2.1719 | lr 8.31e-05 | grad 1.03 | tok/s 11518
step   1920 | loss 2.1369 | lr 8.31e-05 | grad 1.16 | tok/s 12307
step   1930 | loss 2.1365 | lr 8.31e-05 | grad 1.33 | tok/s 11574
step   1940 | loss 2.1337 | lr 8.31e-05 | grad 1.51 | tok/s 12222
step   1950 | loss 3.5578 | lr 8.31e-05 | grad 3.36 | tok/s 12456
step   1960 | loss 3.3713 | lr 8.31e-05 | grad 3.05 | tok/s 12464
step   1970 | loss 2.4949 | lr 8.31e-05 | grad 2.17 | tok/s 12034
step   1980 | loss 2.2327 | lr 8.31e-05 | grad 1.29 | tok/s 11714
step   1990 | loss 2.5829 | lr 8.31e-05 | grad 1.20 | tok/s 11826
step   2000 | loss 2.1418 | lr 8.31e-05 | grad 1.23 | tok/s 11994
  >>> saved checkpoint: checkpoint_step_002000_loss_2.1418.pt
step   2010 | loss 1.9644 | lr 8.31e-05 | grad 1.55 | tok/s 4964
step   2020 | loss 2.1676 | lr 8.31e-05 | grad 1.91 | tok/s 12184
step   2030 | loss 1.7234 | lr 8.31e-05 | grad 1.55 | tok/s 12539
step   2040 | loss 2.4914 | lr 8.31e-05 | grad 1.17 | tok/s 12489
step   2050 | loss 2.2588 | lr 8.31e-05 | grad 1.70 | tok/s 11972
step   2060 | loss 2.3227 | lr 8.31e-05 | grad 1.51 | tok/s 11785
step   2070 | loss 2.5289 | lr 8.31e-05 | grad 3.84 | tok/s 11842
step   2080 | loss 3.0436 | lr 8.31e-05 | grad 3.70 | tok/s 12469
step   2090 | loss 2.2551 | lr 8.31e-05 | grad 1.90 | tok/s 12152
step   2100 | loss 2.5373 | lr 8.31e-05 | grad 1.89 | tok/s 12329
step   2110 | loss 2.2580 | lr 8.31e-05 | grad 1.12 | tok/s 11639
step   2120 | loss 1.3734 | lr 8.31e-05 | grad 0.87 | tok/s 12553
step   2130 | loss 2.2268 | lr 8.31e-05 | grad 1.24 | tok/s 11775
step   2140 | loss 2.1385 | lr 8.31e-05 | grad 1.40 | tok/s 12311
step   2150 | loss 2.0402 | lr 8.31e-05 | grad 1.27 | tok/s 12448
step   2160 | loss 2.0246 | lr 8.31e-05 | grad 1.50 | tok/s 12461
step   2170 | loss 2.0142 | lr 8.31e-05 | grad 1.23 | tok/s 12453
step   2180 | loss 2.0215 | lr 8.31e-05 | grad 1.06 | tok/s 12440
step   2190 | loss 2.0102 | lr 8.31e-05 | grad 1.16 | tok/s 12439
step   2200 | loss 1.9901 | lr 8.31e-05 | grad 1.04 | tok/s 12440
step   2210 | loss 1.9631 | lr 8.31e-05 | grad 1.41 | tok/s 12445
step   2220 | loss 1.9747 | lr 8.31e-05 | grad 1.02 | tok/s 12443
step   2230 | loss 2.1508 | lr 8.31e-05 | grad 1.23 | tok/s 12215
step   2240 | loss 2.1847 | lr 8.31e-05 | grad 2.05 | tok/s 12015
step   2250 | loss 2.9877 | lr 8.31e-05 | grad 3.56 | tok/s 12463
step   2260 | loss 2.3933 | lr 8.31e-05 | grad 1.23 | tok/s 12031
step   2270 | loss 2.6644 | lr 8.31e-05 | grad 1.76 | tok/s 12324
step   2280 | loss 2.2703 | lr 8.31e-05 | grad 1.48 | tok/s 12449
step   2290 | loss 2.4413 | lr 8.31e-05 | grad 1.39 | tok/s 11886
step   2300 | loss 3.1301 | lr 8.31e-05 | grad 1.59 | tok/s 12137
step   2310 | loss 2.4123 | lr 8.31e-05 | grad 2.73 | tok/s 11846
step   2320 | loss 2.5260 | lr 8.31e-05 | grad 2.25 | tok/s 11798
step   2330 | loss 2.0739 | lr 8.31e-05 | grad 1.39 | tok/s 11513
step   2340 | loss 2.4111 | lr 8.31e-05 | grad 1.57 | tok/s 11867
step   2350 | loss 2.1787 | lr 8.31e-05 | grad 1.52 | tok/s 12186
step   2360 | loss 2.1788 | lr 8.31e-05 | grad 2.02 | tok/s 12343
step   2370 | loss 2.5688 | lr 8.31e-05 | grad 2.56 | tok/s 12301
step   2380 | loss 2.7260 | lr 8.31e-05 | grad 2.14 | tok/s 12461
step   2390 | loss 2.0599 | lr 8.31e-05 | grad 1.15 | tok/s 12427
step   2400 | loss 2.0547 | lr 8.31e-05 | grad 2.02 | tok/s 12444
step   2410 | loss 1.8902 | lr 8.31e-05 | grad 2.02 | tok/s 11888
step   2420 | loss 2.1396 | lr 8.31e-05 | grad 1.40 | tok/s 11305
step   2430 | loss 2.1682 | lr 8.31e-05 | grad 2.09 | tok/s 12375
step   2440 | loss 2.2291 | lr 8.31e-05 | grad 1.31 | tok/s 11961
step   2450 | loss 2.1064 | lr 8.31e-05 | grad 1.28 | tok/s 11935
step   2460 | loss 2.1308 | lr 8.31e-05 | grad 1.24 | tok/s 12474
step   2470 | loss 2.1233 | lr 8.31e-05 | grad 2.05 | tok/s 12277
step   2480 | loss 2.2708 | lr 8.31e-05 | grad 1.61 | tok/s 12246
step   2490 | loss 2.1158 | lr 8.31e-05 | grad 1.48 | tok/s 11862
step   2500 | loss 2.6640 | lr 8.31e-05 | grad 2.09 | tok/s 12294
step   2510 | loss 2.8512 | lr 8.31e-05 | grad 3.67 | tok/s 12450
step   2520 | loss 2.5068 | lr 8.31e-05 | grad 2.31 | tok/s 12232
step   2530 | loss 2.0097 | lr 8.31e-05 | grad 1.37 | tok/s 11892
step   2540 | loss 2.2077 | lr 8.31e-05 | grad 1.55 | tok/s 12069
step   2550 | loss 2.0332 | lr 8.31e-05 | grad 1.79 | tok/s 12303
step   2560 | loss 2.3350 | lr 8.31e-05 | grad 1.38 | tok/s 11391
step   2570 | loss 2.0772 | lr 8.31e-05 | grad 1.21 | tok/s 11673
step   2580 | loss 2.2886 | lr 8.31e-05 | grad 1.99 | tok/s 12066
step   2590 | loss 2.1489 | lr 8.31e-05 | grad 0.86 | tok/s 11352
step   2600 | loss 2.9419 | lr 8.31e-05 | grad 4.44 | tok/s 12030
step   2610 | loss 2.6311 | lr 8.31e-05 | grad 1.53 | tok/s 12125
step   2620 | loss 2.1563 | lr 8.31e-05 | grad 1.84 | tok/s 12175
step   2630 | loss 2.1876 | lr 8.31e-05 | grad 2.20 | tok/s 12341
step   2640 | loss 2.3521 | lr 8.31e-05 | grad 1.55 | tok/s 11988
step   2650 | loss 2.5734 | lr 8.31e-05 | grad 1.70 | tok/s 12158
step   2660 | loss 2.0335 | lr 8.31e-05 | grad 1.30 | tok/s 11925
step   2670 | loss 2.1480 | lr 8.31e-05 | grad 1.72 | tok/s 11637
step   2680 | loss 2.5214 | lr 8.31e-05 | grad 1.35 | tok/s 11960
step   2690 | loss 2.1923 | lr 8.31e-05 | grad 2.17 | tok/s 12347
step   2700 | loss 2.2174 | lr 8.31e-05 | grad 1.25 | tok/s 11739
step   2710 | loss 2.5631 | lr 8.31e-05 | grad 2.53 | tok/s 11692
step   2720 | loss 2.0709 | lr 8.31e-05 | grad 1.55 | tok/s 11525
step   2730 | loss 2.2430 | lr 8.31e-05 | grad 2.27 | tok/s 12311
step   2740 | loss 2.8668 | lr 8.31e-05 | grad 1.97 | tok/s 12136
step   2750 | loss 2.3671 | lr 8.31e-05 | grad 2.33 | tok/s 12137
step   2760 | loss 1.9717 | lr 8.31e-05 | grad 1.44 | tok/s 11242
step   2770 | loss 2.4461 | lr 8.31e-05 | grad 1.71 | tok/s 12226
step   2780 | loss 2.2727 | lr 8.31e-05 | grad 1.91 | tok/s 12282
step   2790 | loss 2.5687 | lr 8.31e-05 | grad 1.66 | tok/s 11229
step   2800 | loss 2.0280 | lr 8.31e-05 | grad 1.39 | tok/s 12429
step   2810 | loss 2.0597 | lr 8.31e-05 | grad 1.06 | tok/s 11462
step   2820 | loss 2.1730 | lr 8.31e-05 | grad 2.64 | tok/s 11640
step   2830 | loss 2.6356 | lr 8.31e-05 | grad 2.59 | tok/s 12448
step   2840 | loss 2.4239 | lr 8.31e-05 | grad 3.38 | tok/s 12188
step   2850 | loss 2.6306 | lr 8.31e-05 | grad 4.34 | tok/s 11932
step   2860 | loss 2.3017 | lr 8.31e-05 | grad 2.36 | tok/s 11809
step   2870 | loss 2.0880 | lr 8.31e-05 | grad 2.48 | tok/s 12063
step   2880 | loss 2.3255 | lr 8.31e-05 | grad 1.65 | tok/s 12313
step   2890 | loss 2.3586 | lr 8.31e-05 | grad 1.55 | tok/s 12159
step   2900 | loss 2.1534 | lr 8.31e-05 | grad 4.25 | tok/s 12167
step   2910 | loss 2.0558 | lr 8.31e-05 | grad 1.56 | tok/s 11660
step   2920 | loss 2.3777 | lr 8.31e-05 | grad 2.67 | tok/s 11982
step   2930 | loss 2.0931 | lr 8.31e-05 | grad 1.10 | tok/s 11748
step   2940 | loss 1.9619 | lr 8.31e-05 | grad 1.30 | tok/s 11328
step   2950 | loss 2.2473 | lr 8.31e-05 | grad 1.46 | tok/s 12195
step   2960 | loss 2.1338 | lr 8.31e-05 | grad 1.77 | tok/s 12163
step   2970 | loss 2.2353 | lr 8.31e-05 | grad 3.23 | tok/s 11784
step   2980 | loss 3.3137 | lr 8.31e-05 | grad 8.06 | tok/s 12206
step   2990 | loss 2.3667 | lr 8.31e-05 | grad 1.48 | tok/s 12184
step   3000 | loss 2.1539 | lr 8.31e-05 | grad 1.46 | tok/s 11857
  >>> saved checkpoint: checkpoint_step_003000_loss_2.1539.pt
step   3010 | loss 2.0559 | lr 8.31e-05 | grad 1.58 | tok/s 4875
step   3020 | loss 2.0966 | lr 8.31e-05 | grad 1.54 | tok/s 12128
step   3030 | loss 2.1244 | lr 8.31e-05 | grad 1.40 | tok/s 11645
step   3040 | loss 2.0754 | lr 8.31e-05 | grad 1.33 | tok/s 12391
step   3050 | loss 2.1387 | lr 8.31e-05 | grad 1.77 | tok/s 12075
step   3060 | loss 2.1415 | lr 8.31e-05 | grad 1.56 | tok/s 12276
step   3070 | loss 2.3105 | lr 8.31e-05 | grad 2.05 | tok/s 12435
step   3080 | loss 2.1414 | lr 8.31e-05 | grad 1.49 | tok/s 11999
step   3090 | loss 2.0437 | lr 8.31e-05 | grad 1.87 | tok/s 11919
step   3100 | loss 2.3725 | lr 8.31e-05 | grad 1.86 | tok/s 12182
step   3110 | loss 2.2001 | lr 8.31e-05 | grad 1.37 | tok/s 12467
step   3120 | loss 2.0561 | lr 8.31e-05 | grad 1.05 | tok/s 12117
step   3130 | loss 2.3473 | lr 8.31e-05 | grad 1.66 | tok/s 12090
step   3140 | loss 2.2706 | lr 8.31e-05 | grad 1.43 | tok/s 12451
step   3150 | loss 2.0657 | lr 8.31e-05 | grad 1.95 | tok/s 12193
step   3160 | loss 2.3346 | lr 8.31e-05 | grad 1.72 | tok/s 11647
step   3170 | loss 1.9965 | lr 8.31e-05 | grad 1.72 | tok/s 11652
step   3180 | loss 1.9959 | lr 8.31e-05 | grad 1.47 | tok/s 12025
step   3190 | loss 2.0384 | lr 8.31e-05 | grad 2.61 | tok/s 12498
step   3200 | loss 2.4621 | lr 8.31e-05 | grad 1.60 | tok/s 12008
step   3210 | loss 2.5258 | lr 8.31e-05 | grad 3.98 | tok/s 12095
step   3220 | loss 2.9790 | lr 8.31e-05 | grad 2.64 | tok/s 12429
step   3230 | loss 2.7895 | lr 8.31e-05 | grad 2.08 | tok/s 12443
step   3240 | loss 2.7099 | lr 8.31e-05 | grad 2.16 | tok/s 12450
step   3250 | loss 2.6573 | lr 8.31e-05 | grad 1.84 | tok/s 12443
step   3260 | loss 2.5637 | lr 8.31e-05 | grad 1.59 | tok/s 12455
step   3270 | loss 2.5073 | lr 8.31e-05 | grad 1.55 | tok/s 12435
step   3280 | loss 2.4584 | lr 8.31e-05 | grad 1.36 | tok/s 12439
step   3290 | loss 2.4419 | lr 8.31e-05 | grad 1.50 | tok/s 12443
step   3300 | loss 2.3996 | lr 8.31e-05 | grad 1.38 | tok/s 12447
step   3310 | loss 2.3691 | lr 8.31e-05 | grad 1.41 | tok/s 12443
step   3320 | loss 2.3499 | lr 8.31e-05 | grad 1.45 | tok/s 12448
step   3330 | loss 2.3624 | lr 8.31e-05 | grad 2.14 | tok/s 11672
step   3340 | loss 2.3638 | lr 8.31e-05 | grad 1.90 | tok/s 12269
step   3350 | loss 2.1076 | lr 8.31e-05 | grad 1.94 | tok/s 11743
step   3360 | loss 2.1280 | lr 8.31e-05 | grad 1.89 | tok/s 11540
step   3370 | loss 2.1596 | lr 8.31e-05 | grad 1.41 | tok/s 11844
step   3380 | loss 2.1232 | lr 8.31e-05 | grad 1.37 | tok/s 11949
step   3390 | loss 2.0602 | lr 8.31e-05 | grad 1.39 | tok/s 12081
step   3400 | loss 2.0437 | lr 8.31e-05 | grad 1.93 | tok/s 12436
step   3410 | loss 2.0823 | lr 8.31e-05 | grad 1.66 | tok/s 12297
step   3420 | loss 2.1256 | lr 8.31e-05 | grad 1.89 | tok/s 11488
step   3430 | loss 2.0708 | lr 8.31e-05 | grad 1.52 | tok/s 11945
step   3440 | loss 2.1424 | lr 8.31e-05 | grad 1.36 | tok/s 12079
step   3450 | loss 2.2034 | lr 8.31e-05 | grad 2.66 | tok/s 12013
step   3460 | loss 2.3694 | lr 8.31e-05 | grad 1.41 | tok/s 12103
step   3470 | loss 2.0965 | lr 8.31e-05 | grad 1.36 | tok/s 12181
step   3480 | loss 2.0658 | lr 8.31e-05 | grad 1.95 | tok/s 12202
step   3490 | loss 2.0829 | lr 8.31e-05 | grad 1.74 | tok/s 11698
step   3500 | loss 2.1080 | lr 8.31e-05 | grad 1.38 | tok/s 11914
step   3510 | loss 1.9905 | lr 8.31e-05 | grad 1.29 | tok/s 12053
step   3520 | loss 2.5271 | lr 8.31e-05 | grad 2.66 | tok/s 11876
step   3530 | loss 2.6280 | lr 8.31e-05 | grad 2.94 | tok/s 12239
step   3540 | loss 2.0313 | lr 8.31e-05 | grad 1.40 | tok/s 11568
step   3550 | loss 2.7110 | lr 8.31e-05 | grad 2.64 | tok/s 11375
step   3560 | loss 2.1536 | lr 8.31e-05 | grad 1.57 | tok/s 11528
step   3570 | loss 1.9327 | lr 8.31e-05 | grad 1.36 | tok/s 12135
step   3580 | loss 2.1336 | lr 8.31e-05 | grad 1.32 | tok/s 11817
step   3590 | loss 2.1217 | lr 8.31e-05 | grad 1.32 | tok/s 11802
step   3600 | loss 2.0828 | lr 8.31e-05 | grad 1.59 | tok/s 11285
step   3610 | loss 2.1167 | lr 8.31e-05 | grad 2.17 | tok/s 12310
step   3620 | loss 2.0205 | lr 8.31e-05 | grad 1.34 | tok/s 11974
step   3630 | loss 2.1634 | lr 8.31e-05 | grad 1.72 | tok/s 12026
step   3640 | loss 2.1685 | lr 8.31e-05 | grad 1.40 | tok/s 11521
step   3650 | loss 1.9602 | lr 8.31e-05 | grad 1.34 | tok/s 11602
step   3660 | loss 2.1255 | lr 8.31e-05 | grad 1.94 | tok/s 11526
step   3670 | loss 2.4392 | lr 8.31e-05 | grad 1.16 | tok/s 12352
step   3680 | loss 1.9172 | lr 8.31e-05 | grad 1.27 | tok/s 11468
step   3690 | loss 2.2848 | lr 8.31e-05 | grad 2.88 | tok/s 12094
step   3700 | loss 2.0545 | lr 8.31e-05 | grad 1.50 | tok/s 11324
step   3710 | loss 2.1903 | lr 8.31e-05 | grad 1.66 | tok/s 12270
step   3720 | loss 2.1270 | lr 8.31e-05 | grad 1.98 | tok/s 11902
step   3730 | loss 2.0788 | lr 8.31e-05 | grad 2.08 | tok/s 11846
step   3740 | loss 2.0193 | lr 8.31e-05 | grad 5.09 | tok/s 11404
step   3750 | loss 2.3490 | lr 8.31e-05 | grad 2.03 | tok/s 12277
step   3760 | loss 2.0316 | lr 8.31e-05 | grad 1.61 | tok/s 12429
step   3770 | loss 1.9869 | lr 8.31e-05 | grad 1.23 | tok/s 12445
step   3780 | loss 1.9664 | lr 8.31e-05 | grad 1.28 | tok/s 12437
step   3790 | loss 1.9452 | lr 8.31e-05 | grad 1.22 | tok/s 12442
step   3800 | loss 1.9046 | lr 8.31e-05 | grad 1.50 | tok/s 12436
step   3810 | loss 1.8974 | lr 8.31e-05 | grad 1.68 | tok/s 12438
step   3820 | loss 1.9185 | lr 8.31e-05 | grad 1.20 | tok/s 12437
step   3830 | loss 1.8977 | lr 8.31e-05 | grad 1.69 | tok/s 12441
step   3840 | loss 1.8882 | lr 8.31e-05 | grad 1.01 | tok/s 12432
step   3850 | loss 1.8553 | lr 8.31e-05 | grad 1.27 | tok/s 12444
step   3860 | loss 2.0903 | lr 8.31e-05 | grad 1.56 | tok/s 11874
step   3870 | loss 2.4208 | lr 8.31e-05 | grad 2.45 | tok/s 11666
step   3880 | loss 2.0308 | lr 8.31e-05 | grad 1.83 | tok/s 11651
step   3890 | loss 2.3019 | lr 8.31e-05 | grad 2.61 | tok/s 11910
step   3900 | loss 2.2616 | lr 8.31e-05 | grad 1.65 | tok/s 12005
step   3910 | loss 2.1338 | lr 8.31e-05 | grad 1.64 | tok/s 11801
step   3920 | loss 2.3227 | lr 8.31e-05 | grad 1.64 | tok/s 11788
step   3930 | loss 2.2875 | lr 8.31e-05 | grad 1.88 | tok/s 12218
step   3940 | loss 2.0724 | lr 8.31e-05 | grad 1.23 | tok/s 11846
step   3950 | loss 2.1413 | lr 8.31e-05 | grad 1.88 | tok/s 11480
step   3960 | loss 2.8608 | lr 8.31e-05 | grad 3.11 | tok/s 12455
step   3970 | loss 2.3086 | lr 8.31e-05 | grad 1.80 | tok/s 11807
step   3980 | loss 2.2643 | lr 8.31e-05 | grad 1.98 | tok/s 12253
step   3990 | loss 1.5376 | lr 8.31e-05 | grad 1.27 | tok/s 12534
step   4000 | loss 2.2434 | lr 8.31e-05 | grad 1.59 | tok/s 12460
  >>> saved checkpoint: checkpoint_step_004000_loss_2.2434.pt
step   4010 | loss 2.1560 | lr 8.31e-05 | grad 1.26 | tok/s 4675
step   4020 | loss 1.9900 | lr 8.31e-05 | grad 2.50 | tok/s 11476
step   4030 | loss 2.1077 | lr 8.31e-05 | grad 1.75 | tok/s 11939
step   4040 | loss 2.0190 | lr 8.31e-05 | grad 1.41 | tok/s 11872
step   4050 | loss 2.4975 | lr 8.31e-05 | grad 10.81 | tok/s 11602
step   4060 | loss 2.3860 | lr 8.31e-05 | grad 2.14 | tok/s 11863
step   4070 | loss 2.0528 | lr 8.31e-05 | grad 1.66 | tok/s 12106
step   4080 | loss 1.9816 | lr 8.31e-05 | grad 1.62 | tok/s 12279
step   4090 | loss 1.9963 | lr 8.31e-05 | grad 1.66 | tok/s 11902
step   4100 | loss 2.2138 | lr 8.31e-05 | grad 1.85 | tok/s 12037
step   4110 | loss 2.3183 | lr 8.31e-05 | grad 1.42 | tok/s 11929
step   4120 | loss 2.2657 | lr 8.31e-05 | grad 1.41 | tok/s 12102
step   4130 | loss 1.5626 | lr 8.31e-05 | grad 1.59 | tok/s 12086
step   4140 | loss 1.9055 | lr 8.31e-05 | grad 1.49 | tok/s 11627
step   4150 | loss 2.0963 | lr 8.31e-05 | grad 1.84 | tok/s 11428
step   4160 | loss 2.0478 | lr 8.31e-05 | grad 1.53 | tok/s 12472
step   4170 | loss 2.3441 | lr 8.31e-05 | grad 2.33 | tok/s 12136
step   4180 | loss 3.0729 | lr 8.31e-05 | grad 2.81 | tok/s 12370
step   4190 | loss 3.0410 | lr 8.31e-05 | grad 2.69 | tok/s 12484
step   4200 | loss 2.3927 | lr 8.31e-05 | grad 2.14 | tok/s 11932
step   4210 | loss 2.1135 | lr 8.31e-05 | grad 2.11 | tok/s 12097
step   4220 | loss 2.2578 | lr 8.31e-05 | grad 6.41 | tok/s 11710
step   4230 | loss 3.3509 | lr 8.31e-05 | grad 4.06 | tok/s 12470
step   4240 | loss 2.2736 | lr 8.31e-05 | grad 1.98 | tok/s 11632
step   4250 | loss 1.9811 | lr 8.31e-05 | grad 2.22 | tok/s 11617
step   4260 | loss 2.2291 | lr 8.31e-05 | grad 1.77 | tok/s 12028
step   4270 | loss 2.5375 | lr 8.31e-05 | grad 3.42 | tok/s 12330
step   4280 | loss 1.9805 | lr 8.31e-05 | grad 3.34 | tok/s 11300
step   4290 | loss 2.6861 | lr 8.31e-05 | grad 1.89 | tok/s 12457
step   4300 | loss 2.1553 | lr 8.31e-05 | grad 1.30 | tok/s 11978
step   4310 | loss 2.2328 | lr 8.31e-05 | grad 2.41 | tok/s 12060
step   4320 | loss 1.9921 | lr 8.31e-05 | grad 1.17 | tok/s 12086
step   4330 | loss 2.0193 | lr 8.31e-05 | grad 2.88 | tok/s 12324
step   4340 | loss 2.1887 | lr 8.31e-05 | grad 1.68 | tok/s 12200
step   4350 | loss 1.5258 | lr 8.31e-05 | grad 1.30 | tok/s 12542
step   4360 | loss 2.6497 | lr 8.31e-05 | grad 4.31 | tok/s 12363
step   4370 | loss 2.1733 | lr 8.31e-05 | grad 1.47 | tok/s 11583
step   4380 | loss 2.2107 | lr 8.31e-05 | grad 1.98 | tok/s 12220
step   4390 | loss 2.4497 | lr 8.31e-05 | grad 2.45 | tok/s 12479
step   4400 | loss 2.5241 | lr 8.31e-05 | grad 1.70 | tok/s 12484
step   4410 | loss 2.4774 | lr 8.31e-05 | grad 1.39 | tok/s 12000
step   4420 | loss 1.9500 | lr 8.31e-05 | grad 1.72 | tok/s 12109
step   4430 | loss 2.0847 | lr 8.31e-05 | grad 1.79 | tok/s 11491
step   4440 | loss 2.0332 | lr 8.31e-05 | grad 1.59 | tok/s 11523
step   4450 | loss 2.2992 | lr 8.31e-05 | grad 5.31 | tok/s 11912
step   4460 | loss 2.0237 | lr 8.31e-05 | grad 1.39 | tok/s 12233
step   4470 | loss 1.9632 | lr 8.31e-05 | grad 1.46 | tok/s 11915
step   4480 | loss 2.1457 | lr 8.31e-05 | grad 1.40 | tok/s 12119
step   4490 | loss 2.0185 | lr 8.31e-05 | grad 1.56 | tok/s 11643
step   4500 | loss 1.9646 | lr 8.31e-05 | grad 1.73 | tok/s 12298
step   4510 | loss 2.3650 | lr 8.31e-05 | grad 1.64 | tok/s 12345
step   4520 | loss 2.3626 | lr 8.31e-05 | grad 1.61 | tok/s 11723
step   4530 | loss 2.1960 | lr 8.31e-05 | grad 3.53 | tok/s 11346
step   4540 | loss 2.2562 | lr 8.31e-05 | grad 1.48 | tok/s 11650
step   4550 | loss 1.9514 | lr 8.31e-05 | grad 2.08 | tok/s 12219
step   4560 | loss 1.8671 | lr 8.31e-05 | grad 1.38 | tok/s 12206
step   4570 | loss 1.9456 | lr 8.31e-05 | grad 1.41 | tok/s 11615
step   4580 | loss 2.4674 | lr 8.31e-05 | grad 3.59 | tok/s 11720
step   4590 | loss 2.2648 | lr 8.31e-05 | grad 1.80 | tok/s 11895
step   4600 | loss 2.2408 | lr 8.31e-05 | grad 1.47 | tok/s 11688
step   4610 | loss 1.7701 | lr 8.31e-05 | grad 1.29 | tok/s 12177
step   4620 | loss 2.0172 | lr 8.31e-05 | grad 1.57 | tok/s 12377
step   4630 | loss 1.9249 | lr 8.31e-05 | grad 1.72 | tok/s 12473
step   4640 | loss 1.9551 | lr 8.31e-05 | grad 1.49 | tok/s 12478
step   4650 | loss 1.9357 | lr 8.31e-05 | grad 1.23 | tok/s 12470
step   4660 | loss 1.9438 | lr 8.31e-05 | grad 1.69 | tok/s 12484
step   4670 | loss 1.9308 | lr 8.31e-05 | grad 1.62 | tok/s 12474
step   4680 | loss 1.9230 | lr 8.31e-05 | grad 1.52 | tok/s 12474
step   4690 | loss 1.8983 | lr 8.31e-05 | grad 1.42 | tok/s 12473
step   4700 | loss 1.9173 | lr 8.31e-05 | grad 1.48 | tok/s 12490
step   4710 | loss 1.8901 | lr 8.31e-05 | grad 1.48 | tok/s 12479
step   4720 | loss 1.8500 | lr 8.31e-05 | grad 0.98 | tok/s 12472
step   4730 | loss 1.8708 | lr 8.31e-05 | grad 1.42 | tok/s 12468
step   4740 | loss 1.8460 | lr 8.31e-05 | grad 1.86 | tok/s 12476
step   4750 | loss 1.8557 | lr 8.31e-05 | grad 1.39 | tok/s 12470
step   4760 | loss 1.8945 | lr 8.31e-05 | grad 1.30 | tok/s 12472
step   4770 | loss 1.8750 | lr 8.31e-05 | grad 1.27 | tok/s 12474
step   4780 | loss 1.8985 | lr 8.31e-05 | grad 1.37 | tok/s 12468
step   4790 | loss 1.8773 | lr 8.31e-05 | grad 2.28 | tok/s 12477
step   4800 | loss 1.8831 | lr 8.31e-05 | grad 1.90 | tok/s 12468
step   4810 | loss 1.9430 | lr 8.31e-05 | grad 1.27 | tok/s 12455
step   4820 | loss 2.4119 | lr 8.31e-05 | grad 1.98 | tok/s 12060
step   4830 | loss 1.9799 | lr 8.31e-05 | grad 1.57 | tok/s 12078
step   4840 | loss 1.2174 | lr 8.31e-05 | grad 1.47 | tok/s 12491
step   4850 | loss 1.9275 | lr 8.31e-05 | grad 1.65 | tok/s 11833
step   4860 | loss 2.0118 | lr 8.31e-05 | grad 1.75 | tok/s 11424
step   4870 | loss 2.0602 | lr 8.31e-05 | grad 1.97 | tok/s 11526
step   4880 | loss 1.9455 | lr 8.31e-05 | grad 1.78 | tok/s 12088
step   4890 | loss 2.0691 | lr 8.31e-05 | grad 1.73 | tok/s 11692
step   4900 | loss 1.9591 | lr 8.31e-05 | grad 1.72 | tok/s 12322
step   4910 | loss 2.4778 | lr 8.31e-05 | grad 1.72 | tok/s 12136
step   4920 | loss 1.9446 | lr 8.31e-05 | grad 1.42 | tok/s 12472
step   4930 | loss 1.9388 | lr 8.31e-05 | grad 1.84 | tok/s 11839
step   4940 | loss 2.0346 | lr 8.31e-05 | grad 1.40 | tok/s 11739
step   4950 | loss 2.2112 | lr 8.31e-05 | grad 2.19 | tok/s 12229
step   4960 | loss 2.2253 | lr 8.31e-05 | grad 1.84 | tok/s 12419
step   4970 | loss 2.1538 | lr 8.31e-05 | grad 4.62 | tok/s 12117
step   4980 | loss 2.3437 | lr 8.31e-05 | grad 1.87 | tok/s 11588
step   4990 | loss 2.4234 | lr 8.31e-05 | grad 1.65 | tok/s 12501
step   5000 | loss 2.1235 | lr 8.31e-05 | grad 1.95 | tok/s 11794
  >>> saved checkpoint: checkpoint_step_005000_loss_2.1235.pt
step   5010 | loss 2.2575 | lr 8.31e-05 | grad 1.16 | tok/s 5011
step   5020 | loss 1.9842 | lr 8.31e-05 | grad 1.66 | tok/s 12074
step   5030 | loss 2.1058 | lr 8.31e-05 | grad 1.68 | tok/s 11812
step   5040 | loss 2.4063 | lr 8.31e-05 | grad 2.52 | tok/s 12220
step   5050 | loss 2.0948 | lr 8.31e-05 | grad 2.73 | tok/s 12111
step   5060 | loss 1.9973 | lr 8.31e-05 | grad 1.70 | tok/s 11428
step   5070 | loss 1.9306 | lr 8.31e-05 | grad 1.57 | tok/s 11738
step   5080 | loss 2.1450 | lr 8.31e-05 | grad 1.91 | tok/s 12060
step   5090 | loss 1.9565 | lr 8.31e-05 | grad 2.02 | tok/s 12007
step   5100 | loss 2.2006 | lr 8.31e-05 | grad 1.73 | tok/s 11895
step   5110 | loss 2.1867 | lr 8.31e-05 | grad 1.98 | tok/s 11837
step   5120 | loss 1.8722 | lr 8.31e-05 | grad 2.17 | tok/s 11754
step   5130 | loss 2.0607 | lr 8.31e-05 | grad 2.09 | tok/s 12042
step   5140 | loss 2.6294 | lr 8.31e-05 | grad 3.12 | tok/s 12399
step   5150 | loss 2.2242 | lr 8.31e-05 | grad 2.02 | tok/s 11516
step   5160 | loss 1.9168 | lr 8.31e-05 | grad 1.86 | tok/s 11606
step   5170 | loss 1.9842 | lr 8.31e-05 | grad 1.87 | tok/s 11761
step   5180 | loss 2.0111 | lr 8.31e-05 | grad 2.33 | tok/s 11301
step   5190 | loss 2.1545 | lr 8.31e-05 | grad 1.34 | tok/s 12434
step   5200 | loss 2.1684 | lr 8.31e-05 | grad 1.56 | tok/s 12019
step   5210 | loss 2.1848 | lr 8.31e-05 | grad 1.26 | tok/s 12490
step   5220 | loss 2.1715 | lr 8.31e-05 | grad 4.25 | tok/s 11621
step   5230 | loss 2.1886 | lr 8.31e-05 | grad 1.89 | tok/s 12099
step   5240 | loss 2.0440 | lr 8.31e-05 | grad 1.38 | tok/s 12521
step   5250 | loss 1.8486 | lr 8.31e-05 | grad 1.63 | tok/s 12041
step   5260 | loss 2.0454 | lr 8.31e-05 | grad 5.09 | tok/s 12296
step   5270 | loss 2.1740 | lr 8.31e-05 | grad 2.03 | tok/s 11527
step   5280 | loss 1.8911 | lr 8.31e-05 | grad 1.45 | tok/s 11725
step   5290 | loss 2.0375 | lr 8.31e-05 | grad 1.81 | tok/s 12391
step   5300 | loss 2.0738 | lr 8.31e-05 | grad 2.12 | tok/s 11710
step   5310 | loss 2.0426 | lr 8.31e-05 | grad 1.69 | tok/s 12473

Training complete! Final step: 5311
