Using device: cuda
Output directory: benchmark_results/cmaes_4d/transformer_480M_converge0.01_20260203_094548/eval_17/levelllama_100m_20260203_104656
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 264,578,048 parameters
Using schedule-free AdamW (lr=0.0003208454589324323)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 5.2570 | lr 3.21e-04 | grad 5.72 | tok/s 14771
step     20 | loss 5.2703 | lr 3.21e-04 | grad 4.72 | tok/s 26946
step     30 | loss 4.6669 | lr 3.21e-04 | grad 2.30 | tok/s 27258
step     40 | loss 3.5721 | lr 3.21e-04 | grad 1.91 | tok/s 27162
step     50 | loss 3.0105 | lr 3.21e-04 | grad 1.09 | tok/s 27060
step     60 | loss 3.2538 | lr 3.21e-04 | grad 2.06 | tok/s 25950
step     70 | loss 3.1284 | lr 3.21e-04 | grad 3.41 | tok/s 26328
step     80 | loss 3.0044 | lr 3.21e-04 | grad 1.22 | tok/s 26138
step     90 | loss 2.9826 | lr 3.21e-04 | grad 1.20 | tok/s 25471
step    100 | loss 2.5884 | lr 3.21e-04 | grad 0.88 | tok/s 26408
step    110 | loss 2.7774 | lr 3.21e-04 | grad 0.82 | tok/s 25489
step    120 | loss 2.7622 | lr 3.21e-04 | grad 1.19 | tok/s 25647
step    130 | loss 2.5941 | lr 3.21e-04 | grad 1.62 | tok/s 26250
step    140 | loss 2.3933 | lr 3.21e-04 | grad 1.27 | tok/s 25027
step    150 | loss 2.4542 | lr 3.21e-04 | grad 0.87 | tok/s 25241
step    160 | loss 2.4378 | lr 3.21e-04 | grad 1.04 | tok/s 25303
step    170 | loss 2.5894 | lr 3.21e-04 | grad 1.76 | tok/s 25895
step    180 | loss 2.3989 | lr 3.21e-04 | grad 1.55 | tok/s 25820
step    190 | loss 2.2570 | lr 3.21e-04 | grad 1.39 | tok/s 26700
step    200 | loss 2.3216 | lr 3.21e-04 | grad 0.66 | tok/s 25914
step    210 | loss 2.4326 | lr 3.21e-04 | grad 0.75 | tok/s 26234
step    220 | loss 2.3248 | lr 3.21e-04 | grad 0.74 | tok/s 25703
step    230 | loss 2.2382 | lr 3.21e-04 | grad 0.77 | tok/s 25663
step    240 | loss 2.3613 | lr 3.21e-04 | grad 1.46 | tok/s 26211
step    250 | loss 2.3215 | lr 3.21e-04 | grad 0.82 | tok/s 25600
step    260 | loss 2.1454 | lr 3.21e-04 | grad 0.64 | tok/s 25052
step    270 | loss 2.2058 | lr 3.21e-04 | grad 0.84 | tok/s 25645
step    280 | loss 2.0813 | lr 3.21e-04 | grad 0.80 | tok/s 26440
step    290 | loss 2.0047 | lr 3.21e-04 | grad 1.29 | tok/s 26748
step    300 | loss 2.0283 | lr 3.21e-04 | grad 0.93 | tok/s 26725
step    310 | loss 2.0353 | lr 3.21e-04 | grad 0.88 | tok/s 26473
step    320 | loss 2.1143 | lr 3.21e-04 | grad 0.70 | tok/s 25308
step    330 | loss 2.1741 | lr 3.21e-04 | grad 1.37 | tok/s 26103
step    340 | loss 2.1339 | lr 3.21e-04 | grad 1.70 | tok/s 25251
step    350 | loss 2.0938 | lr 3.21e-04 | grad 0.71 | tok/s 25096
step    360 | loss 2.0693 | lr 3.21e-04 | grad 1.22 | tok/s 25934
step    370 | loss 2.4229 | lr 3.21e-04 | grad 0.97 | tok/s 26039
step    380 | loss 2.0797 | lr 3.21e-04 | grad 1.13 | tok/s 26458
step    390 | loss 2.0342 | lr 3.21e-04 | grad 0.85 | tok/s 25813
step    400 | loss 2.0515 | lr 3.21e-04 | grad 0.73 | tok/s 26013
step    410 | loss 2.0251 | lr 3.21e-04 | grad 0.78 | tok/s 25227
step    420 | loss 2.0868 | lr 3.21e-04 | grad 0.84 | tok/s 25400
step    430 | loss 2.2487 | lr 3.21e-04 | grad 1.23 | tok/s 26207
step    440 | loss 2.0671 | lr 3.21e-04 | grad 0.81 | tok/s 25740
step    450 | loss 1.9948 | lr 3.21e-04 | grad 0.90 | tok/s 25724
step    460 | loss 2.0369 | lr 3.21e-04 | grad 0.96 | tok/s 25797
step    470 | loss 1.9301 | lr 3.21e-04 | grad 0.74 | tok/s 25023
step    480 | loss 1.8941 | lr 3.21e-04 | grad 0.59 | tok/s 25615
step    490 | loss 2.4274 | lr 3.21e-04 | grad 1.08 | tok/s 26398
step    500 | loss 1.9509 | lr 3.21e-04 | grad 1.92 | tok/s 25756
step    510 | loss 1.8246 | lr 3.21e-04 | grad 0.65 | tok/s 26511
step    520 | loss 2.3205 | lr 3.21e-04 | grad 0.92 | tok/s 25825
step    530 | loss 1.8470 | lr 3.21e-04 | grad 1.05 | tok/s 26161
step    540 | loss 1.8563 | lr 3.21e-04 | grad 1.09 | tok/s 26361
step    550 | loss 1.7315 | lr 3.21e-04 | grad 0.55 | tok/s 26807
step    560 | loss 1.8468 | lr 3.21e-04 | grad 1.73 | tok/s 26442
step    570 | loss 2.2392 | lr 3.21e-04 | grad 0.93 | tok/s 26504
step    580 | loss 2.3392 | lr 3.21e-04 | grad 1.59 | tok/s 25550
step    590 | loss 1.8882 | lr 3.21e-04 | grad 1.00 | tok/s 25789
step    600 | loss 1.9806 | lr 3.21e-04 | grad 0.96 | tok/s 26786
step    610 | loss 1.8527 | lr 3.21e-04 | grad 0.84 | tok/s 25433
step    620 | loss 1.8490 | lr 3.21e-04 | grad 0.88 | tok/s 26384
step    630 | loss 2.1065 | lr 3.21e-04 | grad 0.89 | tok/s 26346
step    640 | loss 1.8939 | lr 3.21e-04 | grad 1.02 | tok/s 25751
step    650 | loss 2.0732 | lr 3.21e-04 | grad 3.31 | tok/s 25434
step    660 | loss 1.9966 | lr 3.21e-04 | grad 1.15 | tok/s 26229
step    670 | loss 1.9728 | lr 3.21e-04 | grad 1.02 | tok/s 25706
step    680 | loss 1.9493 | lr 3.21e-04 | grad 0.94 | tok/s 25509
step    690 | loss 2.0374 | lr 3.21e-04 | grad 1.08 | tok/s 25743
step    700 | loss 1.9383 | lr 3.21e-04 | grad 0.89 | tok/s 25989
step    710 | loss 1.9572 | lr 3.21e-04 | grad 2.08 | tok/s 25750
step    720 | loss 1.9915 | lr 3.21e-04 | grad 0.88 | tok/s 25918
step    730 | loss 1.9676 | lr 3.21e-04 | grad 1.59 | tok/s 25827
step    740 | loss 1.7946 | lr 3.21e-04 | grad 0.93 | tok/s 25559
step    750 | loss 2.0510 | lr 3.21e-04 | grad 0.71 | tok/s 25883
step    760 | loss 1.8086 | lr 3.21e-04 | grad 1.16 | tok/s 25831
step    770 | loss 1.8910 | lr 3.21e-04 | grad 0.78 | tok/s 26253
step    780 | loss 1.8165 | lr 3.21e-04 | grad 0.64 | tok/s 26333
step    790 | loss 1.8485 | lr 3.21e-04 | grad 1.70 | tok/s 26022
step    800 | loss 1.7545 | lr 3.21e-04 | grad 0.89 | tok/s 25808
step    810 | loss 2.4538 | lr 3.21e-04 | grad 1.45 | tok/s 26632
step    820 | loss 2.2039 | lr 3.21e-04 | grad 1.26 | tok/s 26824
step    830 | loss 2.0631 | lr 3.21e-04 | grad 0.53 | tok/s 26824
step    840 | loss 1.9538 | lr 3.21e-04 | grad 0.69 | tok/s 25600
step    850 | loss 1.7821 | lr 3.21e-04 | grad 0.85 | tok/s 25919
step    860 | loss 1.8043 | lr 3.21e-04 | grad 0.94 | tok/s 25839
step    870 | loss 1.8763 | lr 3.21e-04 | grad 0.78 | tok/s 26306
step    880 | loss 1.7956 | lr 3.21e-04 | grad 1.34 | tok/s 25549
step    890 | loss 2.0794 | lr 3.21e-04 | grad 0.84 | tok/s 25394
step    900 | loss 1.7494 | lr 3.21e-04 | grad 0.79 | tok/s 25419
step    910 | loss 1.8290 | lr 3.21e-04 | grad 0.69 | tok/s 25647
step    920 | loss 1.8059 | lr 3.21e-04 | grad 0.80 | tok/s 25512
step    930 | loss 1.8509 | lr 3.21e-04 | grad 0.98 | tok/s 25488
step    940 | loss 1.8100 | lr 3.21e-04 | grad 0.57 | tok/s 26084
step    950 | loss 1.6598 | lr 3.21e-04 | grad 0.96 | tok/s 26809
step    960 | loss 1.6078 | lr 3.21e-04 | grad 0.44 | tok/s 26793
step    970 | loss 1.7670 | lr 3.21e-04 | grad 0.94 | tok/s 25867
step    980 | loss 1.9003 | lr 3.21e-04 | grad 1.12 | tok/s 25410
step    990 | loss 1.9071 | lr 3.21e-04 | grad 1.28 | tok/s 25787
step   1000 | loss 1.7961 | lr 3.21e-04 | grad 0.94 | tok/s 26396
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7961.pt
step   1010 | loss 1.7719 | lr 3.21e-04 | grad 0.89 | tok/s 19113
step   1020 | loss 1.9460 | lr 3.21e-04 | grad 0.64 | tok/s 25743
step   1030 | loss 1.8999 | lr 3.21e-04 | grad 0.93 | tok/s 25833
step   1040 | loss 1.5744 | lr 3.21e-04 | grad 0.88 | tok/s 25603
step   1050 | loss 2.2123 | lr 3.21e-04 | grad 0.64 | tok/s 26333
step   1060 | loss 2.2598 | lr 3.21e-04 | grad 1.16 | tok/s 25785
step   1070 | loss 1.8517 | lr 3.21e-04 | grad 0.80 | tok/s 25431
step   1080 | loss 1.9874 | lr 3.21e-04 | grad 0.59 | tok/s 26159
step   1090 | loss 1.7572 | lr 3.21e-04 | grad 1.39 | tok/s 26545
step   1100 | loss 1.8270 | lr 3.21e-04 | grad 1.05 | tok/s 26212
step   1110 | loss 1.8184 | lr 3.21e-04 | grad 0.62 | tok/s 25333
step   1120 | loss 1.8133 | lr 3.21e-04 | grad 0.98 | tok/s 25878
step   1130 | loss 1.8585 | lr 3.21e-04 | grad 1.47 | tok/s 25780
step   1140 | loss 1.7962 | lr 3.21e-04 | grad 1.12 | tok/s 25461
step   1150 | loss 1.8355 | lr 3.21e-04 | grad 0.91 | tok/s 25217
step   1160 | loss 1.6915 | lr 3.21e-04 | grad 0.69 | tok/s 26587
step   1170 | loss 1.7171 | lr 3.21e-04 | grad 0.62 | tok/s 26815
step   1180 | loss 1.6580 | lr 3.21e-04 | grad 1.01 | tok/s 26815
step   1190 | loss 1.6142 | lr 3.21e-04 | grad 0.61 | tok/s 26812
step   1200 | loss 1.6098 | lr 3.21e-04 | grad 0.68 | tok/s 26799
step   1210 | loss 1.6442 | lr 3.21e-04 | grad 0.61 | tok/s 26361
step   1220 | loss 1.6970 | lr 3.21e-04 | grad 0.82 | tok/s 25179
step   1230 | loss 1.7849 | lr 3.21e-04 | grad 0.79 | tok/s 26142
step   1240 | loss 1.7553 | lr 3.21e-04 | grad 1.17 | tok/s 25894
step   1250 | loss 1.9448 | lr 3.21e-04 | grad 0.73 | tok/s 25762
step   1260 | loss 1.8071 | lr 3.21e-04 | grad 0.98 | tok/s 25743
step   1270 | loss 1.8035 | lr 3.21e-04 | grad 0.75 | tok/s 25405
step   1280 | loss 1.7539 | lr 3.21e-04 | grad 0.72 | tok/s 25535
step   1290 | loss 1.8599 | lr 3.21e-04 | grad 0.67 | tok/s 25387
step   1300 | loss 1.7801 | lr 3.21e-04 | grad 0.73 | tok/s 25590
step   1310 | loss 1.8240 | lr 3.21e-04 | grad 0.84 | tok/s 25987
step   1320 | loss 1.6531 | lr 3.21e-04 | grad 0.60 | tok/s 25607
step   1330 | loss 1.6588 | lr 3.21e-04 | grad 0.87 | tok/s 26218
step   1340 | loss 1.7360 | lr 3.21e-04 | grad 0.64 | tok/s 25656
step   1350 | loss 1.6921 | lr 3.21e-04 | grad 1.05 | tok/s 25524
step   1360 | loss 1.8979 | lr 3.21e-04 | grad 0.80 | tok/s 25539
step   1370 | loss 1.7398 | lr 3.21e-04 | grad 0.64 | tok/s 25572
step   1380 | loss 1.7119 | lr 3.21e-04 | grad 0.64 | tok/s 26264
step   1390 | loss 1.8117 | lr 3.21e-04 | grad 0.98 | tok/s 26248
step   1400 | loss 1.7569 | lr 3.21e-04 | grad 1.27 | tok/s 25202
step   1410 | loss 1.7222 | lr 3.21e-04 | grad 0.67 | tok/s 24807
step   1420 | loss 1.5650 | lr 3.21e-04 | grad 0.83 | tok/s 25696
step   1430 | loss 1.6236 | lr 3.21e-04 | grad 0.70 | tok/s 26373
step   1440 | loss 1.7382 | lr 3.21e-04 | grad 1.09 | tok/s 25172
step   1450 | loss 1.7825 | lr 3.21e-04 | grad 0.84 | tok/s 25658
step   1460 | loss 1.6668 | lr 3.21e-04 | grad 1.84 | tok/s 25886
step   1470 | loss 1.7644 | lr 3.21e-04 | grad 0.92 | tok/s 25737
step   1480 | loss 1.9380 | lr 3.21e-04 | grad 1.55 | tok/s 25341
step   1490 | loss 1.7698 | lr 3.21e-04 | grad 0.89 | tok/s 26186
step   1500 | loss 1.7812 | lr 3.21e-04 | grad 1.02 | tok/s 26015
step   1510 | loss 1.7151 | lr 3.21e-04 | grad 0.66 | tok/s 25791
step   1520 | loss 1.7034 | lr 3.21e-04 | grad 0.81 | tok/s 25450
step   1530 | loss 1.6669 | lr 3.21e-04 | grad 2.45 | tok/s 26438
step   1540 | loss 2.1082 | lr 3.21e-04 | grad 0.82 | tok/s 25716
step   1550 | loss 1.7185 | lr 3.21e-04 | grad 0.81 | tok/s 25402
step   1560 | loss 1.8349 | lr 3.21e-04 | grad 0.88 | tok/s 26073
step   1570 | loss 1.6095 | lr 3.21e-04 | grad 0.93 | tok/s 25511
step   1580 | loss 1.7207 | lr 3.21e-04 | grad 0.66 | tok/s 25304
step   1590 | loss 1.5570 | lr 3.21e-04 | grad 0.74 | tok/s 26399
step   1600 | loss 1.7359 | lr 3.21e-04 | grad 0.76 | tok/s 25889
step   1610 | loss 1.6958 | lr 3.21e-04 | grad 0.59 | tok/s 26199
step   1620 | loss 1.6253 | lr 3.21e-04 | grad 0.62 | tok/s 25107
step   1630 | loss 1.6916 | lr 3.21e-04 | grad 1.84 | tok/s 25332
step   1640 | loss 1.7154 | lr 3.21e-04 | grad 0.99 | tok/s 25400
step   1650 | loss 1.7314 | lr 3.21e-04 | grad 1.43 | tok/s 26256
step   1660 | loss 2.0403 | lr 3.21e-04 | grad 1.30 | tok/s 25952
step   1670 | loss 1.6261 | lr 3.21e-04 | grad 0.86 | tok/s 25662
step   1680 | loss 1.8678 | lr 3.21e-04 | grad 1.08 | tok/s 25980
step   1690 | loss 1.7602 | lr 3.21e-04 | grad 0.83 | tok/s 25726
step   1700 | loss 1.7089 | lr 3.21e-04 | grad 1.23 | tok/s 25541
step   1710 | loss 1.7973 | lr 3.21e-04 | grad 0.74 | tok/s 25586
step   1720 | loss 1.7229 | lr 3.21e-04 | grad 0.73 | tok/s 25551
step   1730 | loss 1.6855 | lr 3.21e-04 | grad 0.80 | tok/s 25262
step   1740 | loss 1.8178 | lr 3.21e-04 | grad 0.98 | tok/s 25613
step   1750 | loss 1.8241 | lr 3.21e-04 | grad 1.20 | tok/s 25475
step   1760 | loss 1.6775 | lr 3.21e-04 | grad 1.07 | tok/s 25381
step   1770 | loss 1.8099 | lr 3.21e-04 | grad 0.62 | tok/s 25312
step   1780 | loss 1.6871 | lr 3.21e-04 | grad 1.38 | tok/s 25981
step   1790 | loss 1.6070 | lr 3.21e-04 | grad 0.65 | tok/s 26141
step   1800 | loss 1.6084 | lr 3.21e-04 | grad 0.61 | tok/s 25287
step   1810 | loss 1.6594 | lr 3.21e-04 | grad 0.73 | tok/s 25388
step   1820 | loss 1.7544 | lr 3.21e-04 | grad 0.80 | tok/s 25480
step   1830 | loss 1.7785 | lr 3.21e-04 | grad 0.79 | tok/s 25307
step   1840 | loss 1.6246 | lr 3.21e-04 | grad 0.84 | tok/s 25656
step   1850 | loss 1.6408 | lr 3.21e-04 | grad 0.83 | tok/s 26271
step   1860 | loss 1.6850 | lr 3.21e-04 | grad 0.72 | tok/s 26043
step   1870 | loss 1.8019 | lr 3.21e-04 | grad 1.80 | tok/s 25807
step   1880 | loss 1.6206 | lr 3.21e-04 | grad 0.95 | tok/s 25887
step   1890 | loss 1.7474 | lr 3.21e-04 | grad 0.62 | tok/s 25672
step   1900 | loss 1.5850 | lr 3.21e-04 | grad 0.97 | tok/s 26042
step   1910 | loss 1.6280 | lr 3.21e-04 | grad 0.71 | tok/s 26017
step   1920 | loss 1.6793 | lr 3.21e-04 | grad 0.81 | tok/s 26500
step   1930 | loss 1.6576 | lr 3.21e-04 | grad 1.09 | tok/s 25917
step   1940 | loss 1.8533 | lr 3.21e-04 | grad 1.03 | tok/s 26094
step   1950 | loss 1.5565 | lr 3.21e-04 | grad 1.00 | tok/s 25620
step   1960 | loss 1.8363 | lr 3.21e-04 | grad 2.03 | tok/s 25969
step   1970 | loss 1.6107 | lr 3.21e-04 | grad 1.02 | tok/s 25554
step   1980 | loss 1.6302 | lr 3.21e-04 | grad 0.83 | tok/s 26322
step   1990 | loss 1.4128 | lr 3.21e-04 | grad 0.91 | tok/s 26785
step   2000 | loss 1.5074 | lr 3.21e-04 | grad 2.17 | tok/s 26782
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5074.pt
step   2010 | loss 1.5637 | lr 3.21e-04 | grad 0.50 | tok/s 19669
step   2020 | loss 1.4773 | lr 3.21e-04 | grad 0.77 | tok/s 26899
step   2030 | loss 1.6662 | lr 3.21e-04 | grad 1.05 | tok/s 25759
step   2040 | loss 1.6619 | lr 3.21e-04 | grad 0.58 | tok/s 26167
step   2050 | loss 1.7178 | lr 3.21e-04 | grad 1.51 | tok/s 25885
step   2060 | loss 1.7106 | lr 3.21e-04 | grad 0.75 | tok/s 25872
step   2070 | loss 1.6279 | lr 3.21e-04 | grad 0.85 | tok/s 25606
step   2080 | loss 1.5144 | lr 3.21e-04 | grad 0.71 | tok/s 26065
step   2090 | loss 1.5782 | lr 3.21e-04 | grad 0.94 | tok/s 25659
step   2100 | loss 1.4672 | lr 3.21e-04 | grad 1.26 | tok/s 26020
step   2110 | loss 1.6926 | lr 3.21e-04 | grad 0.86 | tok/s 25527
step   2120 | loss 1.7577 | lr 3.21e-04 | grad 0.94 | tok/s 25775
step   2130 | loss 1.7091 | lr 3.21e-04 | grad 1.29 | tok/s 25504
step   2140 | loss 1.8016 | lr 3.21e-04 | grad 0.75 | tok/s 25989
step   2150 | loss 1.6395 | lr 3.21e-04 | grad 0.97 | tok/s 25780
step   2160 | loss 1.8795 | lr 3.21e-04 | grad 1.13 | tok/s 26424
step   2170 | loss 1.5014 | lr 3.21e-04 | grad 0.63 | tok/s 26500
step   2180 | loss 1.4715 | lr 3.21e-04 | grad 0.60 | tok/s 26816
step   2190 | loss 1.4762 | lr 3.21e-04 | grad 0.55 | tok/s 26822
step   2200 | loss 1.5798 | lr 3.21e-04 | grad 1.68 | tok/s 26287
step   2210 | loss 1.6872 | lr 3.21e-04 | grad 2.06 | tok/s 26325
step   2220 | loss 1.7599 | lr 3.21e-04 | grad 0.89 | tok/s 26523
step   2230 | loss 1.7336 | lr 3.21e-04 | grad 0.79 | tok/s 26148
step   2240 | loss 1.6226 | lr 3.21e-04 | grad 0.83 | tok/s 25815
step   2250 | loss 1.7203 | lr 3.21e-04 | grad 1.54 | tok/s 25890
step   2260 | loss 1.6071 | lr 3.21e-04 | grad 0.74 | tok/s 25612
step   2270 | loss 1.5831 | lr 3.21e-04 | grad 0.71 | tok/s 25436
step   2280 | loss 1.6333 | lr 3.21e-04 | grad 1.07 | tok/s 26159
step   2290 | loss 1.6390 | lr 3.21e-04 | grad 0.53 | tok/s 26790
step   2300 | loss 1.5786 | lr 3.21e-04 | grad 0.57 | tok/s 26781
step   2310 | loss 1.6331 | lr 3.21e-04 | grad 1.09 | tok/s 26415
step   2320 | loss 1.6826 | lr 3.21e-04 | grad 0.74 | tok/s 25881
step   2330 | loss 1.9880 | lr 3.21e-04 | grad 1.30 | tok/s 25979
step   2340 | loss 1.6983 | lr 3.21e-04 | grad 0.86 | tok/s 26002
step   2350 | loss 1.5870 | lr 3.21e-04 | grad 0.79 | tok/s 25757
step   2360 | loss 1.5639 | lr 3.21e-04 | grad 1.14 | tok/s 25541
step   2370 | loss 1.7047 | lr 3.21e-04 | grad 1.38 | tok/s 25610
step   2380 | loss 1.6202 | lr 3.21e-04 | grad 1.00 | tok/s 25448
step   2390 | loss 1.7279 | lr 3.21e-04 | grad 0.80 | tok/s 25840
step   2400 | loss 1.5780 | lr 3.21e-04 | grad 1.38 | tok/s 25252
step   2410 | loss 1.7389 | lr 3.21e-04 | grad 0.89 | tok/s 25996
step   2420 | loss 1.5016 | lr 3.21e-04 | grad 0.96 | tok/s 25749
step   2430 | loss 1.7112 | lr 3.21e-04 | grad 1.30 | tok/s 26078
step   2440 | loss 1.5941 | lr 3.21e-04 | grad 1.21 | tok/s 26801
step   2450 | loss 1.5695 | lr 3.21e-04 | grad 0.78 | tok/s 25738
step   2460 | loss 1.5498 | lr 3.21e-04 | grad 0.88 | tok/s 25440
step   2470 | loss 1.6307 | lr 3.21e-04 | grad 0.83 | tok/s 26083
step   2480 | loss 1.6744 | lr 3.21e-04 | grad 0.75 | tok/s 26101
step   2490 | loss 1.8124 | lr 3.21e-04 | grad 1.32 | tok/s 25598
step   2500 | loss 1.6692 | lr 3.21e-04 | grad 1.01 | tok/s 25296
step   2510 | loss 1.6817 | lr 3.21e-04 | grad 0.83 | tok/s 25533
step   2520 | loss 1.9554 | lr 3.21e-04 | grad 1.93 | tok/s 25779
step   2530 | loss 1.7349 | lr 3.21e-04 | grad 1.09 | tok/s 26219
step   2540 | loss 1.6463 | lr 3.21e-04 | grad 1.03 | tok/s 25465
step   2550 | loss 1.6738 | lr 3.21e-04 | grad 0.72 | tok/s 25975
step   2560 | loss 1.5236 | lr 3.21e-04 | grad 0.80 | tok/s 25645
step   2570 | loss 1.6761 | lr 3.21e-04 | grad 0.61 | tok/s 26353
step   2580 | loss 1.6028 | lr 3.21e-04 | grad 0.57 | tok/s 26263
step   2590 | loss 1.5728 | lr 3.21e-04 | grad 1.02 | tok/s 26787
step   2600 | loss 1.6301 | lr 3.21e-04 | grad 1.28 | tok/s 26015
step   2610 | loss 1.6047 | lr 3.21e-04 | grad 0.86 | tok/s 25505
step   2620 | loss 1.5926 | lr 3.21e-04 | grad 1.13 | tok/s 24814
step   2630 | loss 1.8989 | lr 3.21e-04 | grad 0.68 | tok/s 26488
step   2640 | loss 1.5339 | lr 3.21e-04 | grad 0.58 | tok/s 26791
step   2650 | loss 1.4992 | lr 3.21e-04 | grad 0.60 | tok/s 26791
step   2660 | loss 1.5077 | lr 3.21e-04 | grad 0.74 | tok/s 26778
step   2670 | loss 1.6151 | lr 3.21e-04 | grad 0.70 | tok/s 25634
step   2680 | loss 1.7785 | lr 3.21e-04 | grad 1.95 | tok/s 25410
step   2690 | loss 1.9819 | lr 3.21e-04 | grad 2.22 | tok/s 26349
step   2700 | loss 1.5736 | lr 3.21e-04 | grad 0.66 | tok/s 25654
step   2710 | loss 1.5235 | lr 3.21e-04 | grad 0.79 | tok/s 26001
step   2720 | loss 1.7009 | lr 3.21e-04 | grad 0.71 | tok/s 25633
step   2730 | loss 1.7469 | lr 3.21e-04 | grad 0.75 | tok/s 25210
step   2740 | loss 1.6210 | lr 3.21e-04 | grad 0.82 | tok/s 26145
step   2750 | loss 1.5653 | lr 3.21e-04 | grad 0.61 | tok/s 25332
step   2760 | loss 1.5049 | lr 3.21e-04 | grad 0.87 | tok/s 25603
step   2770 | loss 1.9710 | lr 3.21e-04 | grad 1.73 | tok/s 25802
step   2780 | loss 1.5196 | lr 3.21e-04 | grad 0.61 | tok/s 26300
step   2790 | loss 1.9081 | lr 3.21e-04 | grad 1.18 | tok/s 26132
step   2800 | loss 1.6680 | lr 3.21e-04 | grad 0.84 | tok/s 26202
step   2810 | loss 1.5960 | lr 3.21e-04 | grad 0.73 | tok/s 25864
step   2820 | loss 1.5316 | lr 3.21e-04 | grad 0.90 | tok/s 26085
step   2830 | loss 1.5446 | lr 3.21e-04 | grad 1.06 | tok/s 25775
step   2840 | loss 1.7763 | lr 3.21e-04 | grad 2.81 | tok/s 26155
step   2850 | loss 1.8332 | lr 3.21e-04 | grad 1.01 | tok/s 25936
step   2860 | loss 1.7673 | lr 3.21e-04 | grad 0.93 | tok/s 26196
step   2870 | loss 1.5350 | lr 3.21e-04 | grad 0.72 | tok/s 25451
step   2880 | loss 1.7117 | lr 3.21e-04 | grad 1.35 | tok/s 25524
step   2890 | loss 1.6085 | lr 3.21e-04 | grad 0.70 | tok/s 26081
step   2900 | loss 1.6045 | lr 3.21e-04 | grad 0.75 | tok/s 25682
step   2910 | loss 1.6395 | lr 3.21e-04 | grad 0.96 | tok/s 25558
step   2920 | loss 1.6065 | lr 3.21e-04 | grad 1.02 | tok/s 25627

Training complete! Final step: 2920
