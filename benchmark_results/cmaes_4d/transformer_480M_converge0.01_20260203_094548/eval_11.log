Using device: cuda
Output directory: benchmark_results/cmaes_4d/transformer_480M_converge0.01_20260203_094548/eval_11/levelllama_100m_20260203_101626
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 693,273,856 parameters
Using schedule-free AdamW (lr=0.0008817377866947871)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 6.4650 | lr 8.82e-04 | grad 5.06 | tok/s 5359
step     20 | loss 3.0942 | lr 8.82e-04 | grad 5.91 | tok/s 11308
step     30 | loss 3.0766 | lr 8.82e-04 | grad 1.57 | tok/s 11372
step     40 | loss 3.0039 | lr 8.82e-04 | grad 2.41 | tok/s 10885
step     50 | loss 3.5579 | lr 8.82e-04 | grad 6.12 | tok/s 11052
step     60 | loss 2.7820 | lr 8.82e-04 | grad 5.97 | tok/s 11371
step     70 | loss 2.7130 | lr 8.82e-04 | grad 3.31 | tok/s 11484
step     80 | loss 5.0693 | lr 8.82e-04 | grad 8.19 | tok/s 11585
step     90 | loss 4.3455 | lr 8.82e-04 | grad 2.61 | tok/s 11781
step    100 | loss 3.8661 | lr 8.82e-04 | grad 2.98 | tok/s 11760
step    110 | loss 3.9036 | lr 8.82e-04 | grad 9.12 | tok/s 11749
step    120 | loss 3.7872 | lr 8.82e-04 | grad 9.00 | tok/s 11718
step    130 | loss 3.8058 | lr 8.82e-04 | grad 4.41 | tok/s 11701
step    140 | loss 3.4050 | lr 8.82e-04 | grad 4.84 | tok/s 11673
step    150 | loss 3.8544 | lr 8.82e-04 | grad 7.31 | tok/s 11660
step    160 | loss 3.1779 | lr 8.82e-04 | grad 7.28 | tok/s 11623
step    170 | loss 3.2094 | lr 8.82e-04 | grad 7.78 | tok/s 11610
step    180 | loss 3.0066 | lr 8.82e-04 | grad 4.88 | tok/s 11593
step    190 | loss 3.1184 | lr 8.82e-04 | grad 5.47 | tok/s 11583
step    200 | loss 2.6931 | lr 8.82e-04 | grad 3.36 | tok/s 11591
step    210 | loss 2.7119 | lr 8.82e-04 | grad 5.56 | tok/s 11557
step    220 | loss 2.9155 | lr 8.82e-04 | grad 3.23 | tok/s 11403
step    230 | loss 3.3028 | lr 8.82e-04 | grad 3.25 | tok/s 11268
step    240 | loss 2.7918 | lr 8.82e-04 | grad 3.91 | tok/s 10707
step    250 | loss 2.5837 | lr 8.82e-04 | grad 2.36 | tok/s 11008
step    260 | loss 2.3934 | lr 8.82e-04 | grad 3.12 | tok/s 11364
step    270 | loss 2.6397 | lr 8.82e-04 | grad 1.63 | tok/s 11206
step    280 | loss 2.7213 | lr 8.82e-04 | grad 2.97 | tok/s 10988
step    290 | loss 2.5277 | lr 8.82e-04 | grad 2.61 | tok/s 11565
step    300 | loss 1.5838 | lr 8.82e-04 | grad 2.08 | tok/s 11544
step    310 | loss 2.8742 | lr 8.82e-04 | grad 2.08 | tok/s 11363
step    320 | loss 2.5983 | lr 8.82e-04 | grad 4.00 | tok/s 11128
step    330 | loss 2.4271 | lr 8.82e-04 | grad 1.80 | tok/s 10736
step    340 | loss 2.7152 | lr 8.82e-04 | grad 1.68 | tok/s 10908
step    350 | loss 2.5312 | lr 8.82e-04 | grad 2.52 | tok/s 11188
step    360 | loss 2.6896 | lr 8.82e-04 | grad 3.70 | tok/s 11445
step    370 | loss 2.4186 | lr 8.82e-04 | grad 2.30 | tok/s 10367
step    380 | loss 2.3221 | lr 8.82e-04 | grad 2.25 | tok/s 11040
step    390 | loss 2.1965 | lr 8.82e-04 | grad 2.64 | tok/s 11517
step    400 | loss 2.1987 | lr 8.82e-04 | grad 1.92 | tok/s 11418
step    410 | loss 2.1180 | lr 8.82e-04 | grad 1.55 | tok/s 11181
step    420 | loss 2.2788 | lr 8.82e-04 | grad 2.00 | tok/s 10669
step    430 | loss 2.5547 | lr 8.82e-04 | grad 2.06 | tok/s 11354
step    440 | loss 2.6056 | lr 8.82e-04 | grad 2.09 | tok/s 10729
step    450 | loss 2.7264 | lr 8.82e-04 | grad 1.70 | tok/s 11092
step    460 | loss 2.2729 | lr 8.82e-04 | grad 2.64 | tok/s 10870
step    470 | loss 2.3843 | lr 8.82e-04 | grad 1.81 | tok/s 11208
step    480 | loss 2.7608 | lr 8.82e-04 | grad 2.91 | tok/s 11220
step    490 | loss 2.2733 | lr 8.82e-04 | grad 1.90 | tok/s 10596
step    500 | loss 2.2428 | lr 8.82e-04 | grad 2.25 | tok/s 11306
step    510 | loss 2.2560 | lr 8.82e-04 | grad 1.34 | tok/s 11448
step    520 | loss 2.2359 | lr 8.82e-04 | grad 1.53 | tok/s 11436
step    530 | loss 2.3770 | lr 8.82e-04 | grad 1.55 | tok/s 10982
step    540 | loss 2.1585 | lr 8.82e-04 | grad 2.33 | tok/s 11002
step    550 | loss 2.0121 | lr 8.82e-04 | grad 1.85 | tok/s 10786
step    560 | loss 2.1929 | lr 8.82e-04 | grad 1.73 | tok/s 10490
step    570 | loss 2.1292 | lr 8.82e-04 | grad 2.36 | tok/s 10787
step    580 | loss 2.0164 | lr 8.82e-04 | grad 1.59 | tok/s 10746
step    590 | loss 2.3490 | lr 8.82e-04 | grad 1.55 | tok/s 11003
step    600 | loss 2.2077 | lr 8.82e-04 | grad 1.09 | tok/s 10626
step    610 | loss 2.0703 | lr 8.82e-04 | grad 1.58 | tok/s 11174
step    620 | loss 1.9427 | lr 8.82e-04 | grad 1.02 | tok/s 10584
step    630 | loss 2.0785 | lr 8.82e-04 | grad 1.82 | tok/s 10689
step    640 | loss 2.2755 | lr 8.82e-04 | grad 1.44 | tok/s 10988
step    650 | loss 2.1005 | lr 8.82e-04 | grad 2.03 | tok/s 11041
step    660 | loss 2.1160 | lr 8.82e-04 | grad 1.09 | tok/s 11095
step    670 | loss 2.3951 | lr 8.82e-04 | grad 5.75 | tok/s 11165
step    680 | loss 2.0971 | lr 8.82e-04 | grad 1.42 | tok/s 10940
step    690 | loss 2.3661 | lr 8.82e-04 | grad 1.70 | tok/s 11321
step    700 | loss 2.1426 | lr 8.82e-04 | grad 1.41 | tok/s 11525
step    710 | loss 2.0128 | lr 8.82e-04 | grad 1.73 | tok/s 10750
step    720 | loss 1.8645 | lr 8.82e-04 | grad 1.65 | tok/s 10597
step    730 | loss 1.9483 | lr 8.82e-04 | grad 1.77 | tok/s 11501
step    740 | loss 1.9853 | lr 8.82e-04 | grad 1.15 | tok/s 11327
step    750 | loss 1.7704 | lr 8.82e-04 | grad 1.09 | tok/s 11522
step    760 | loss 1.6200 | lr 8.82e-04 | grad 1.75 | tok/s 11512
step    770 | loss 1.5721 | lr 8.82e-04 | grad 1.29 | tok/s 11534
step    780 | loss 1.5211 | lr 8.82e-04 | grad 1.51 | tok/s 11505
step    790 | loss 1.5895 | lr 8.82e-04 | grad 1.50 | tok/s 11156
step    800 | loss 2.3429 | lr 8.82e-04 | grad 2.75 | tok/s 11112
step    810 | loss 2.0428 | lr 8.82e-04 | grad 0.99 | tok/s 11057
step    820 | loss 2.0716 | lr 8.82e-04 | grad 1.73 | tok/s 10636
step    830 | loss 2.1062 | lr 8.82e-04 | grad 1.20 | tok/s 11407
step    840 | loss 1.9835 | lr 8.82e-04 | grad 1.16 | tok/s 11512
step    850 | loss 2.0230 | lr 8.82e-04 | grad 1.71 | tok/s 11471
step    860 | loss 1.9784 | lr 8.82e-04 | grad 2.09 | tok/s 11346
step    870 | loss 1.9250 | lr 8.82e-04 | grad 1.45 | tok/s 10913
step    880 | loss 2.0874 | lr 8.82e-04 | grad 1.22 | tok/s 10969
step    890 | loss 2.0589 | lr 8.82e-04 | grad 1.49 | tok/s 11138
step    900 | loss 1.9170 | lr 8.82e-04 | grad 1.06 | tok/s 11150
step    910 | loss 1.7871 | lr 8.82e-04 | grad 1.53 | tok/s 10899
step    920 | loss 1.9608 | lr 8.82e-04 | grad 2.02 | tok/s 11325
step    930 | loss 1.9705 | lr 8.82e-04 | grad 1.70 | tok/s 10825
step    940 | loss 1.8687 | lr 8.82e-04 | grad 1.16 | tok/s 11404
step    950 | loss 1.9486 | lr 8.82e-04 | grad 1.51 | tok/s 11446
step    960 | loss 1.7697 | lr 8.82e-04 | grad 1.30 | tok/s 11494
step    970 | loss 2.0409 | lr 8.82e-04 | grad 1.57 | tok/s 10802
step    980 | loss 1.9645 | lr 8.82e-04 | grad 0.92 | tok/s 11081
step    990 | loss 1.8587 | lr 8.82e-04 | grad 1.20 | tok/s 11276
step   1000 | loss 2.1741 | lr 8.82e-04 | grad 4.75 | tok/s 10834
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1741.pt
step   1010 | loss 2.0344 | lr 8.82e-04 | grad 1.57 | tok/s 4340
step   1020 | loss 1.9337 | lr 8.82e-04 | grad 1.36 | tok/s 10585
step   1030 | loss 1.7531 | lr 8.82e-04 | grad 1.14 | tok/s 11199
step   1040 | loss 1.8408 | lr 8.82e-04 | grad 1.38 | tok/s 11133
step   1050 | loss 1.9194 | lr 8.82e-04 | grad 1.66 | tok/s 10799
step   1060 | loss 2.0614 | lr 8.82e-04 | grad 1.29 | tok/s 11286
step   1070 | loss 2.0873 | lr 8.82e-04 | grad 1.26 | tok/s 11039
step   1080 | loss 1.7172 | lr 8.82e-04 | grad 1.34 | tok/s 10365
step   1090 | loss 1.3138 | lr 8.82e-04 | grad 2.27 | tok/s 11486
step   1100 | loss 1.8557 | lr 8.82e-04 | grad 1.30 | tok/s 11072
step   1110 | loss 1.7367 | lr 8.82e-04 | grad 1.43 | tok/s 11545
step   1120 | loss 1.6694 | lr 8.82e-04 | grad 1.33 | tok/s 11541
step   1130 | loss 1.5935 | lr 8.82e-04 | grad 1.30 | tok/s 11546
step   1140 | loss 1.5904 | lr 8.82e-04 | grad 1.33 | tok/s 11546
step   1150 | loss 1.5886 | lr 8.82e-04 | grad 1.16 | tok/s 11542
step   1160 | loss 1.4808 | lr 8.82e-04 | grad 1.05 | tok/s 11540
step   1170 | loss 1.5622 | lr 8.82e-04 | grad 1.23 | tok/s 11523
step   1180 | loss 1.6176 | lr 8.82e-04 | grad 1.43 | tok/s 11510
step   1190 | loss 1.5052 | lr 8.82e-04 | grad 0.89 | tok/s 11512
step   1200 | loss 1.5244 | lr 8.82e-04 | grad 1.08 | tok/s 11514
step   1210 | loss 1.5410 | lr 8.82e-04 | grad 1.19 | tok/s 11514
step   1220 | loss 1.5253 | lr 8.82e-04 | grad 1.12 | tok/s 11504
step   1230 | loss 1.5172 | lr 8.82e-04 | grad 0.95 | tok/s 11488
step   1240 | loss 1.7484 | lr 8.82e-04 | grad 5.06 | tok/s 11245
step   1250 | loss 2.0372 | lr 8.82e-04 | grad 0.99 | tok/s 10954
step   1260 | loss 1.5855 | lr 8.82e-04 | grad 1.18 | tok/s 10768
step   1270 | loss 1.9879 | lr 8.82e-04 | grad 1.29 | tok/s 10650
step   1280 | loss 1.9106 | lr 8.82e-04 | grad 1.10 | tok/s 11271
step   1290 | loss 1.8079 | lr 8.82e-04 | grad 1.28 | tok/s 11098
step   1300 | loss 1.7935 | lr 8.82e-04 | grad 1.58 | tok/s 10901
step   1310 | loss 1.7556 | lr 8.82e-04 | grad 1.91 | tok/s 11449
step   1320 | loss 1.9026 | lr 8.82e-04 | grad 2.17 | tok/s 11305
step   1330 | loss 1.7073 | lr 8.82e-04 | grad 1.16 | tok/s 11311
step   1340 | loss 1.9227 | lr 8.82e-04 | grad 0.89 | tok/s 10489
step   1350 | loss 1.9938 | lr 8.82e-04 | grad 1.63 | tok/s 10678
step   1360 | loss 1.8032 | lr 8.82e-04 | grad 0.95 | tok/s 11173
step   1370 | loss 1.8380 | lr 8.82e-04 | grad 1.91 | tok/s 10947
step   1380 | loss 1.8617 | lr 8.82e-04 | grad 1.22 | tok/s 10532
step   1390 | loss 1.6824 | lr 8.82e-04 | grad 1.14 | tok/s 10899
step   1400 | loss 1.6842 | lr 8.82e-04 | grad 0.98 | tok/s 11010
step   1410 | loss 1.9433 | lr 8.82e-04 | grad 1.51 | tok/s 10785
step   1420 | loss 1.8492 | lr 8.82e-04 | grad 1.14 | tok/s 10781
step   1430 | loss 1.6233 | lr 8.82e-04 | grad 1.02 | tok/s 10927
step   1440 | loss 1.4301 | lr 8.82e-04 | grad 1.22 | tok/s 11488
step   1450 | loss 1.5394 | lr 8.82e-04 | grad 3.94 | tok/s 11339
step   1460 | loss 1.9326 | lr 8.82e-04 | grad 2.34 | tok/s 10717
step   1470 | loss 1.7318 | lr 8.82e-04 | grad 1.12 | tok/s 11387
step   1480 | loss 2.4581 | lr 8.82e-04 | grad 1.72 | tok/s 11268
step   1490 | loss 2.0113 | lr 8.82e-04 | grad 2.50 | tok/s 11433
step   1500 | loss 1.5517 | lr 8.82e-04 | grad 1.16 | tok/s 11469
step   1510 | loss 1.8520 | lr 8.82e-04 | grad 1.07 | tok/s 11326
step   1520 | loss 1.6997 | lr 8.82e-04 | grad 1.24 | tok/s 11096
step   1530 | loss 1.6044 | lr 8.82e-04 | grad 0.94 | tok/s 11129
step   1540 | loss 1.9131 | lr 8.82e-04 | grad 1.08 | tok/s 10917
step   1550 | loss 1.5311 | lr 8.82e-04 | grad 1.17 | tok/s 11391
step   1560 | loss 1.8126 | lr 8.82e-04 | grad 0.96 | tok/s 10800
step   1570 | loss 1.6429 | lr 8.82e-04 | grad 1.60 | tok/s 11374
step   1580 | loss 2.2452 | lr 8.82e-04 | grad 2.59 | tok/s 11334
step   1590 | loss 1.8434 | lr 8.82e-04 | grad 1.14 | tok/s 10781
step   1600 | loss 1.0731 | lr 8.82e-04 | grad 0.60 | tok/s 11535
step   1610 | loss 1.3866 | lr 8.82e-04 | grad 1.00 | tok/s 10653
step   1620 | loss 1.6990 | lr 8.82e-04 | grad 1.38 | tok/s 10896
step   1630 | loss 1.6508 | lr 8.82e-04 | grad 1.62 | tok/s 11164
step   1640 | loss 1.6639 | lr 8.82e-04 | grad 1.63 | tok/s 10801
step   1650 | loss 1.7897 | lr 8.82e-04 | grad 1.52 | tok/s 10228
step   1660 | loss 1.5431 | lr 8.82e-04 | grad 0.93 | tok/s 11479
step   1670 | loss 1.9375 | lr 8.82e-04 | grad 3.27 | tok/s 10908
step   1680 | loss 1.7625 | lr 8.82e-04 | grad 1.10 | tok/s 10108
step   1690 | loss 1.6840 | lr 8.82e-04 | grad 1.45 | tok/s 11080
step   1700 | loss 1.7721 | lr 8.82e-04 | grad 0.93 | tok/s 10779
step   1710 | loss 1.7387 | lr 8.82e-04 | grad 1.02 | tok/s 11136
step   1720 | loss 1.8523 | lr 8.82e-04 | grad 1.33 | tok/s 11496
step   1730 | loss 1.6561 | lr 8.82e-04 | grad 1.72 | tok/s 11491
step   1740 | loss 1.7161 | lr 8.82e-04 | grad 1.31 | tok/s 11004
step   1750 | loss 1.7297 | lr 8.82e-04 | grad 0.98 | tok/s 11213
step   1760 | loss 1.8033 | lr 8.82e-04 | grad 1.24 | tok/s 11051
step   1770 | loss 1.6402 | lr 8.82e-04 | grad 1.08 | tok/s 10843
step   1780 | loss 1.6830 | lr 8.82e-04 | grad 1.23 | tok/s 11114
step   1790 | loss 1.6416 | lr 8.82e-04 | grad 0.93 | tok/s 11029
step   1800 | loss 1.7694 | lr 8.82e-04 | grad 1.08 | tok/s 10784
step   1810 | loss 1.7332 | lr 8.82e-04 | grad 1.67 | tok/s 10913
step   1820 | loss 1.7130 | lr 8.82e-04 | grad 2.36 | tok/s 11013
step   1830 | loss 1.6525 | lr 8.82e-04 | grad 0.92 | tok/s 11206
step   1840 | loss 1.6652 | lr 8.82e-04 | grad 0.77 | tok/s 10689
step   1850 | loss 1.6098 | lr 8.82e-04 | grad 1.01 | tok/s 11424
step   1860 | loss 1.5447 | lr 8.82e-04 | grad 1.08 | tok/s 10675
step   1870 | loss 1.5923 | lr 8.82e-04 | grad 0.67 | tok/s 11253
step   1880 | loss 1.5158 | lr 8.82e-04 | grad 1.05 | tok/s 10187
step   1890 | loss 1.7815 | lr 8.82e-04 | grad 0.82 | tok/s 10755
step   1900 | loss 1.5574 | lr 8.82e-04 | grad 0.65 | tok/s 10897
step   1910 | loss 1.6654 | lr 8.82e-04 | grad 0.90 | tok/s 10615
step   1920 | loss 1.5780 | lr 8.82e-04 | grad 0.75 | tok/s 11363
step   1930 | loss 1.6726 | lr 8.82e-04 | grad 0.97 | tok/s 10675
step   1940 | loss 1.6537 | lr 8.82e-04 | grad 1.20 | tok/s 11270
step   1950 | loss 2.2588 | lr 8.82e-04 | grad 1.94 | tok/s 11471
step   1960 | loss 1.8834 | lr 8.82e-04 | grad 1.52 | tok/s 11476
step   1970 | loss 1.8617 | lr 8.82e-04 | grad 1.57 | tok/s 11094
step   1980 | loss 1.7139 | lr 8.82e-04 | grad 0.96 | tok/s 10799
step   1990 | loss 1.9132 | lr 8.82e-04 | grad 0.80 | tok/s 10901
step   2000 | loss 1.6947 | lr 8.82e-04 | grad 1.12 | tok/s 11057
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6947.pt
step   2010 | loss 1.4204 | lr 8.82e-04 | grad 1.04 | tok/s 4542
step   2020 | loss 1.5320 | lr 8.82e-04 | grad 1.14 | tok/s 11285
step   2030 | loss 1.2256 | lr 8.82e-04 | grad 1.12 | tok/s 11596
step   2040 | loss 1.5579 | lr 8.82e-04 | grad 0.79 | tok/s 11548
step   2050 | loss 1.5990 | lr 8.82e-04 | grad 1.38 | tok/s 11061
step   2060 | loss 1.8674 | lr 8.82e-04 | grad 0.91 | tok/s 10897
step   2070 | loss 2.0742 | lr 8.82e-04 | grad 4.06 | tok/s 10944
step   2080 | loss 2.2324 | lr 8.82e-04 | grad 2.09 | tok/s 11522
step   2090 | loss 1.7736 | lr 8.82e-04 | grad 1.67 | tok/s 11230
step   2100 | loss 1.6399 | lr 8.82e-04 | grad 1.03 | tok/s 11412
step   2110 | loss 1.6897 | lr 8.82e-04 | grad 0.94 | tok/s 10761
step   2120 | loss 0.8514 | lr 8.82e-04 | grad 0.81 | tok/s 11584
step   2130 | loss 1.6316 | lr 8.82e-04 | grad 1.05 | tok/s 10874
step   2140 | loss 1.5878 | lr 8.82e-04 | grad 1.03 | tok/s 11382
step   2150 | loss 1.4492 | lr 8.82e-04 | grad 0.91 | tok/s 11504
step   2160 | loss 1.3722 | lr 8.82e-04 | grad 0.77 | tok/s 11514
step   2170 | loss 1.3702 | lr 8.82e-04 | grad 1.04 | tok/s 11504
step   2180 | loss 1.3673 | lr 8.82e-04 | grad 0.95 | tok/s 11505
step   2190 | loss 1.3896 | lr 8.82e-04 | grad 0.94 | tok/s 11501
step   2200 | loss 1.3261 | lr 8.82e-04 | grad 0.79 | tok/s 11492
step   2210 | loss 1.3020 | lr 8.82e-04 | grad 0.84 | tok/s 11485
step   2220 | loss 1.2787 | lr 8.82e-04 | grad 0.80 | tok/s 11479
step   2230 | loss 1.6409 | lr 8.82e-04 | grad 0.99 | tok/s 11257
step   2240 | loss 1.5872 | lr 8.82e-04 | grad 1.45 | tok/s 11075
step   2250 | loss 1.8332 | lr 8.82e-04 | grad 1.49 | tok/s 11481
step   2260 | loss 1.8247 | lr 8.82e-04 | grad 0.98 | tok/s 11090
step   2270 | loss 2.1976 | lr 8.82e-04 | grad 1.01 | tok/s 11357
step   2280 | loss 1.6157 | lr 8.82e-04 | grad 0.98 | tok/s 11472
step   2290 | loss 1.7748 | lr 8.82e-04 | grad 1.02 | tok/s 10933
step   2300 | loss 1.9254 | lr 8.82e-04 | grad 1.38 | tok/s 11168
step   2310 | loss 1.7333 | lr 8.82e-04 | grad 2.08 | tok/s 10914
step   2320 | loss 2.0828 | lr 8.82e-04 | grad 1.84 | tok/s 10865
step   2330 | loss 1.6063 | lr 8.82e-04 | grad 1.06 | tok/s 10600
step   2340 | loss 1.7137 | lr 8.82e-04 | grad 1.37 | tok/s 10919
step   2350 | loss 1.5705 | lr 8.82e-04 | grad 1.05 | tok/s 11229
step   2360 | loss 1.4501 | lr 8.82e-04 | grad 1.02 | tok/s 11370
step   2370 | loss 1.8882 | lr 8.82e-04 | grad 1.34 | tok/s 11317
step   2380 | loss 1.7448 | lr 8.82e-04 | grad 1.69 | tok/s 11469
step   2390 | loss 1.2968 | lr 8.82e-04 | grad 0.68 | tok/s 11432
step   2400 | loss 1.1930 | lr 8.82e-04 | grad 1.17 | tok/s 11461
step   2410 | loss 1.4128 | lr 8.82e-04 | grad 1.18 | tok/s 10945
step   2420 | loss 1.7027 | lr 8.82e-04 | grad 1.20 | tok/s 10414
step   2430 | loss 1.4386 | lr 8.82e-04 | grad 0.96 | tok/s 11415
step   2440 | loss 1.6373 | lr 8.82e-04 | grad 0.91 | tok/s 11028
step   2450 | loss 1.6489 | lr 8.82e-04 | grad 1.12 | tok/s 11013
step   2460 | loss 1.2801 | lr 8.82e-04 | grad 0.80 | tok/s 11474
step   2470 | loss 1.4057 | lr 8.82e-04 | grad 0.98 | tok/s 11298
step   2480 | loss 1.4773 | lr 8.82e-04 | grad 1.02 | tok/s 11279
step   2490 | loss 1.6330 | lr 8.82e-04 | grad 0.84 | tok/s 10930
step   2500 | loss 1.6543 | lr 8.82e-04 | grad 1.04 | tok/s 11318
step   2510 | loss 1.3117 | lr 8.82e-04 | grad 1.16 | tok/s 11466
step   2520 | loss 1.8036 | lr 8.82e-04 | grad 1.16 | tok/s 11264
step   2530 | loss 1.4866 | lr 8.82e-04 | grad 0.89 | tok/s 10972
step   2540 | loss 1.5804 | lr 8.82e-04 | grad 0.95 | tok/s 11126
step   2550 | loss 1.3207 | lr 8.82e-04 | grad 0.88 | tok/s 11355
step   2560 | loss 1.7461 | lr 8.82e-04 | grad 0.94 | tok/s 10513
step   2570 | loss 1.4848 | lr 8.82e-04 | grad 0.84 | tok/s 10766
step   2580 | loss 1.5569 | lr 8.82e-04 | grad 1.22 | tok/s 11141
step   2590 | loss 1.6389 | lr 8.82e-04 | grad 0.73 | tok/s 10469
step   2600 | loss 1.9847 | lr 8.82e-04 | grad 1.46 | tok/s 11088
step   2610 | loss 1.5054 | lr 8.82e-04 | grad 1.41 | tok/s 11171
step   2620 | loss 1.7131 | lr 8.82e-04 | grad 1.44 | tok/s 11243
step   2630 | loss 1.5343 | lr 8.82e-04 | grad 1.65 | tok/s 11388
step   2640 | loss 1.7302 | lr 8.82e-04 | grad 0.82 | tok/s 11056
step   2650 | loss 1.7140 | lr 8.82e-04 | grad 0.90 | tok/s 11208
step   2660 | loss 1.4805 | lr 8.82e-04 | grad 1.04 | tok/s 11005
step   2670 | loss 1.6403 | lr 8.82e-04 | grad 0.88 | tok/s 10728
step   2680 | loss 1.7976 | lr 8.82e-04 | grad 0.79 | tok/s 11025
step   2690 | loss 1.4953 | lr 8.82e-04 | grad 1.29 | tok/s 11386
step   2700 | loss 1.6115 | lr 8.82e-04 | grad 1.01 | tok/s 10814
step   2710 | loss 1.7570 | lr 8.82e-04 | grad 1.54 | tok/s 10787
step   2720 | loss 1.5609 | lr 8.82e-04 | grad 1.21 | tok/s 10629
step   2730 | loss 1.3426 | lr 8.82e-04 | grad 1.22 | tok/s 11367
step   2740 | loss 2.0081 | lr 8.82e-04 | grad 1.28 | tok/s 11214
step   2750 | loss 1.7145 | lr 8.82e-04 | grad 2.19 | tok/s 11209
step   2760 | loss 1.4978 | lr 8.82e-04 | grad 1.02 | tok/s 10375
step   2770 | loss 1.5539 | lr 8.82e-04 | grad 1.48 | tok/s 11277
step   2780 | loss 1.3916 | lr 8.82e-04 | grad 1.01 | tok/s 11329
step   2790 | loss 2.1355 | lr 8.82e-04 | grad 1.32 | tok/s 10335
step   2800 | loss 1.2989 | lr 8.82e-04 | grad 0.82 | tok/s 11472
step   2810 | loss 1.5105 | lr 8.82e-04 | grad 0.93 | tok/s 10582
step   2820 | loss 1.6180 | lr 8.82e-04 | grad 1.76 | tok/s 10754
step   2830 | loss 1.1764 | lr 8.82e-04 | grad 2.06 | tok/s 11471
step   2840 | loss 1.2604 | lr 8.82e-04 | grad 2.89 | tok/s 11228
step   2850 | loss 1.9418 | lr 8.82e-04 | grad 2.81 | tok/s 10984
step   2860 | loss 1.7474 | lr 8.82e-04 | grad 1.23 | tok/s 10889
step   2870 | loss 1.6051 | lr 8.82e-04 | grad 1.57 | tok/s 11113
step   2880 | loss 1.5338 | lr 8.82e-04 | grad 0.90 | tok/s 11355
step   2890 | loss 1.6011 | lr 8.82e-04 | grad 1.28 | tok/s 11214
step   2900 | loss 1.6351 | lr 8.82e-04 | grad 2.69 | tok/s 11215
step   2910 | loss 1.5304 | lr 8.82e-04 | grad 0.98 | tok/s 10751
step   2920 | loss 1.8443 | lr 8.82e-04 | grad 1.59 | tok/s 11037
step   2930 | loss 1.5380 | lr 8.82e-04 | grad 0.86 | tok/s 10835
step   2940 | loss 1.4272 | lr 8.82e-04 | grad 0.92 | tok/s 10440
step   2950 | loss 1.4349 | lr 8.82e-04 | grad 1.00 | tok/s 11249
step   2960 | loss 1.4765 | lr 8.82e-04 | grad 0.98 | tok/s 11208
step   2970 | loss 1.7908 | lr 8.82e-04 | grad 2.66 | tok/s 10855
step   2980 | loss 2.3072 | lr 8.82e-04 | grad 3.23 | tok/s 11238
step   2990 | loss 1.7020 | lr 8.82e-04 | grad 0.98 | tok/s 11216
step   3000 | loss 1.5218 | lr 8.82e-04 | grad 0.92 | tok/s 10920
  >>> saved checkpoint: checkpoint_step_003000_loss_1.5218.pt
step   3010 | loss 1.5016 | lr 8.82e-04 | grad 1.00 | tok/s 4391
step   3020 | loss 1.5747 | lr 8.82e-04 | grad 0.98 | tok/s 11223
step   3030 | loss 1.5965 | lr 8.82e-04 | grad 0.79 | tok/s 10765
step   3040 | loss 1.5922 | lr 8.82e-04 | grad 0.73 | tok/s 11444
step   3050 | loss 1.5242 | lr 8.82e-04 | grad 1.43 | tok/s 11148
step   3060 | loss 1.6607 | lr 8.82e-04 | grad 1.27 | tok/s 11350
step   3070 | loss 1.6198 | lr 8.82e-04 | grad 0.72 | tok/s 11508
step   3080 | loss 1.5622 | lr 8.82e-04 | grad 1.06 | tok/s 11065
step   3090 | loss 1.4996 | lr 8.82e-04 | grad 1.23 | tok/s 11017
step   3100 | loss 1.6777 | lr 8.82e-04 | grad 1.13 | tok/s 11241
step   3110 | loss 1.1035 | lr 8.82e-04 | grad 0.68 | tok/s 11507
step   3120 | loss 1.2782 | lr 8.82e-04 | grad 0.80 | tok/s 11189
step   3130 | loss 1.6203 | lr 8.82e-04 | grad 0.71 | tok/s 11174
step   3140 | loss 1.2680 | lr 8.82e-04 | grad 1.07 | tok/s 11510
step   3150 | loss 1.3782 | lr 8.82e-04 | grad 1.45 | tok/s 11255
step   3160 | loss 1.7497 | lr 8.82e-04 | grad 0.86 | tok/s 10766
step   3170 | loss 1.4674 | lr 8.82e-04 | grad 1.22 | tok/s 10757
step   3180 | loss 1.4506 | lr 8.82e-04 | grad 1.36 | tok/s 11111
step   3190 | loss 1.0685 | lr 8.82e-04 | grad 3.03 | tok/s 11535
step   3200 | loss 1.7668 | lr 8.82e-04 | grad 0.91 | tok/s 11070
step   3210 | loss 1.9511 | lr 8.82e-04 | grad 1.77 | tok/s 11167
step   3220 | loss 2.3278 | lr 8.82e-04 | grad 1.73 | tok/s 11503
step   3230 | loss 1.9612 | lr 8.82e-04 | grad 1.21 | tok/s 11485
step   3240 | loss 1.8035 | lr 8.82e-04 | grad 1.36 | tok/s 11473
step   3250 | loss 1.7070 | lr 8.82e-04 | grad 1.52 | tok/s 11489
step   3260 | loss 1.6330 | lr 8.82e-04 | grad 0.98 | tok/s 11473
step   3270 | loss 1.5771 | lr 8.82e-04 | grad 1.49 | tok/s 11471
step   3280 | loss 1.5097 | lr 8.82e-04 | grad 0.88 | tok/s 11469
step   3290 | loss 1.4784 | lr 8.82e-04 | grad 1.14 | tok/s 11467
step   3300 | loss 1.4715 | lr 8.82e-04 | grad 0.99 | tok/s 11474
step   3310 | loss 1.4502 | lr 8.82e-04 | grad 1.38 | tok/s 11470
step   3320 | loss 1.4358 | lr 8.82e-04 | grad 1.25 | tok/s 11481
step   3330 | loss 1.7966 | lr 8.82e-04 | grad 1.12 | tok/s 10763
step   3340 | loss 1.6361 | lr 8.82e-04 | grad 1.14 | tok/s 11291
step   3350 | loss 1.5458 | lr 8.82e-04 | grad 0.97 | tok/s 10830
step   3360 | loss 1.5769 | lr 8.82e-04 | grad 0.90 | tok/s 10647
step   3370 | loss 1.6401 | lr 8.82e-04 | grad 0.92 | tok/s 10925
step   3380 | loss 1.6251 | lr 8.82e-04 | grad 0.86 | tok/s 11037
step   3390 | loss 1.3976 | lr 8.82e-04 | grad 0.91 | tok/s 11143
step   3400 | loss 1.2325 | lr 8.82e-04 | grad 1.23 | tok/s 11466
step   3410 | loss 1.3390 | lr 8.82e-04 | grad 0.93 | tok/s 11350
step   3420 | loss 1.6416 | lr 8.82e-04 | grad 0.86 | tok/s 10604
step   3430 | loss 1.5998 | lr 8.82e-04 | grad 1.00 | tok/s 11030
step   3440 | loss 1.5025 | lr 8.82e-04 | grad 0.79 | tok/s 11166
step   3450 | loss 1.6098 | lr 8.82e-04 | grad 0.98 | tok/s 11090
step   3460 | loss 1.5846 | lr 8.82e-04 | grad 0.80 | tok/s 11157
step   3470 | loss 1.5798 | lr 8.82e-04 | grad 0.73 | tok/s 11234
step   3480 | loss 1.4674 | lr 8.82e-04 | grad 1.02 | tok/s 11264
step   3490 | loss 1.3254 | lr 8.82e-04 | grad 0.72 | tok/s 10775
step   3500 | loss 1.4007 | lr 8.82e-04 | grad 0.84 | tok/s 10972
step   3510 | loss 1.5270 | lr 8.82e-04 | grad 0.65 | tok/s 11117
step   3520 | loss 1.7693 | lr 8.82e-04 | grad 1.80 | tok/s 10941
step   3530 | loss 1.7787 | lr 8.82e-04 | grad 1.06 | tok/s 11282
step   3540 | loss 1.5191 | lr 8.82e-04 | grad 0.65 | tok/s 10651
step   3550 | loss 2.2202 | lr 8.82e-04 | grad 2.20 | tok/s 10480
step   3560 | loss 1.5570 | lr 8.82e-04 | grad 1.15 | tok/s 10615
step   3570 | loss 1.3116 | lr 8.82e-04 | grad 0.70 | tok/s 11171
step   3580 | loss 1.3421 | lr 8.82e-04 | grad 0.76 | tok/s 10885
step   3590 | loss 1.4720 | lr 8.82e-04 | grad 0.86 | tok/s 10887
step   3600 | loss 1.6199 | lr 8.82e-04 | grad 1.14 | tok/s 10395
step   3610 | loss 1.4843 | lr 8.82e-04 | grad 1.34 | tok/s 11347
step   3620 | loss 1.4996 | lr 8.82e-04 | grad 0.81 | tok/s 11037
step   3630 | loss 1.6273 | lr 8.82e-04 | grad 0.97 | tok/s 11101
step   3640 | loss 1.6353 | lr 8.82e-04 | grad 1.09 | tok/s 10624
step   3650 | loss 1.4739 | lr 8.82e-04 | grad 1.00 | tok/s 10704
step   3660 | loss 1.5680 | lr 8.82e-04 | grad 1.59 | tok/s 10636
step   3670 | loss 1.3904 | lr 8.82e-04 | grad 0.74 | tok/s 11384
step   3680 | loss 1.4288 | lr 8.82e-04 | grad 0.80 | tok/s 10559
step   3690 | loss 1.5303 | lr 8.82e-04 | grad 1.53 | tok/s 10758
step   3700 | loss 1.5029 | lr 8.82e-04 | grad 0.87 | tok/s 10417
step   3710 | loss 1.6306 | lr 8.82e-04 | grad 0.92 | tok/s 11291
step   3720 | loss 1.5638 | lr 8.82e-04 | grad 0.93 | tok/s 10960
step   3730 | loss 1.5664 | lr 8.82e-04 | grad 2.92 | tok/s 10898
step   3740 | loss 1.3612 | lr 8.82e-04 | grad 3.41 | tok/s 10997
step   3750 | loss 1.7038 | lr 8.82e-04 | grad 0.90 | tok/s 11293
step   3760 | loss 1.3985 | lr 8.82e-04 | grad 0.74 | tok/s 11440
step   3770 | loss 1.3459 | lr 8.82e-04 | grad 0.99 | tok/s 11448
step   3780 | loss 1.3309 | lr 8.82e-04 | grad 0.85 | tok/s 11439
step   3790 | loss 1.3032 | lr 8.82e-04 | grad 0.84 | tok/s 11443
step   3800 | loss 1.3107 | lr 8.82e-04 | grad 0.85 | tok/s 11445
step   3810 | loss 1.2582 | lr 8.82e-04 | grad 0.97 | tok/s 11440
step   3820 | loss 1.2756 | lr 8.82e-04 | grad 0.79 | tok/s 11428
step   3830 | loss 1.2422 | lr 8.82e-04 | grad 1.02 | tok/s 11440
step   3840 | loss 1.2432 | lr 8.82e-04 | grad 0.88 | tok/s 11443
step   3850 | loss 1.2060 | lr 8.82e-04 | grad 1.05 | tok/s 11432
step   3860 | loss 1.5838 | lr 8.82e-04 | grad 0.93 | tok/s 10919
step   3870 | loss 1.7702 | lr 8.82e-04 | grad 1.07 | tok/s 10723
step   3880 | loss 1.5101 | lr 8.82e-04 | grad 1.14 | tok/s 10702
step   3890 | loss 1.6283 | lr 8.82e-04 | grad 1.02 | tok/s 10946
step   3900 | loss 1.6220 | lr 8.82e-04 | grad 0.92 | tok/s 11039
step   3910 | loss 1.4444 | lr 8.82e-04 | grad 0.98 | tok/s 10848
step   3920 | loss 1.7159 | lr 8.82e-04 | grad 1.01 | tok/s 10852
step   3930 | loss 1.5821 | lr 8.82e-04 | grad 0.82 | tok/s 11240
step   3940 | loss 1.5982 | lr 8.82e-04 | grad 0.76 | tok/s 10895
step   3950 | loss 1.5433 | lr 8.82e-04 | grad 1.71 | tok/s 10571
step   3960 | loss 1.7006 | lr 8.82e-04 | grad 1.76 | tok/s 11436
step   3970 | loss 1.6398 | lr 8.82e-04 | grad 1.65 | tok/s 10846
step   3980 | loss 1.6143 | lr 8.82e-04 | grad 1.02 | tok/s 11250
step   3990 | loss 1.0017 | lr 8.82e-04 | grad 0.75 | tok/s 11488
step   4000 | loss 1.1784 | lr 8.82e-04 | grad 0.65 | tok/s 11441
  >>> saved checkpoint: checkpoint_step_004000_loss_1.1784.pt
step   4010 | loss 1.4395 | lr 8.82e-04 | grad 0.95 | tok/s 4272
step   4020 | loss 1.5692 | lr 8.82e-04 | grad 1.07 | tok/s 10586
step   4030 | loss 1.4415 | lr 8.82e-04 | grad 1.10 | tok/s 11006
step   4040 | loss 1.5266 | lr 8.82e-04 | grad 0.96 | tok/s 10969
step   4050 | loss 2.0689 | lr 8.82e-04 | grad 10.56 | tok/s 10699
step   4060 | loss 1.8431 | lr 8.82e-04 | grad 1.33 | tok/s 10937
step   4070 | loss 1.4835 | lr 8.82e-04 | grad 1.12 | tok/s 11163
step   4080 | loss 1.4481 | lr 8.82e-04 | grad 0.75 | tok/s 11326
step   4090 | loss 1.4856 | lr 8.82e-04 | grad 1.03 | tok/s 10961
step   4100 | loss 1.5236 | lr 8.82e-04 | grad 0.94 | tok/s 11103
step   4110 | loss 1.6007 | lr 8.82e-04 | grad 0.89 | tok/s 10977
step   4120 | loss 1.7754 | lr 8.82e-04 | grad 0.74 | tok/s 11131
step   4130 | loss 1.1187 | lr 8.82e-04 | grad 1.12 | tok/s 11123
step   4140 | loss 1.2826 | lr 8.82e-04 | grad 0.83 | tok/s 10686
step   4150 | loss 1.4872 | lr 8.82e-04 | grad 0.82 | tok/s 10513
step   4160 | loss 1.0587 | lr 8.82e-04 | grad 0.59 | tok/s 11464
step   4170 | loss 1.5822 | lr 8.82e-04 | grad 1.84 | tok/s 11151
step   4180 | loss 2.0041 | lr 8.82e-04 | grad 2.39 | tok/s 11370
step   4190 | loss 1.5487 | lr 8.82e-04 | grad 1.69 | tok/s 11468
step   4200 | loss 1.5818 | lr 8.82e-04 | grad 1.00 | tok/s 10968
step   4210 | loss 1.4763 | lr 8.82e-04 | grad 1.09 | tok/s 11104
step   4220 | loss 1.7870 | lr 8.82e-04 | grad 2.95 | tok/s 10771
step   4230 | loss 2.4059 | lr 8.82e-04 | grad 2.42 | tok/s 11474
step   4240 | loss 1.7108 | lr 8.82e-04 | grad 1.07 | tok/s 10706
step   4250 | loss 1.4680 | lr 8.82e-04 | grad 0.88 | tok/s 10675
step   4260 | loss 1.6543 | lr 8.82e-04 | grad 1.00 | tok/s 11044
step   4270 | loss 1.5933 | lr 8.82e-04 | grad 2.28 | tok/s 11315
step   4280 | loss 1.4894 | lr 8.82e-04 | grad 2.25 | tok/s 10391
step   4290 | loss 1.9509 | lr 8.82e-04 | grad 1.16 | tok/s 11461
step   4300 | loss 1.5886 | lr 8.82e-04 | grad 0.86 | tok/s 10987
step   4310 | loss 1.6117 | lr 8.82e-04 | grad 1.39 | tok/s 11084
step   4320 | loss 1.4402 | lr 8.82e-04 | grad 0.88 | tok/s 11096
step   4330 | loss 1.4830 | lr 8.82e-04 | grad 1.69 | tok/s 11289
step   4340 | loss 1.5567 | lr 8.82e-04 | grad 0.80 | tok/s 11207
step   4350 | loss 0.8330 | lr 8.82e-04 | grad 0.67 | tok/s 11485
step   4360 | loss 1.6830 | lr 8.82e-04 | grad 2.27 | tok/s 11337
step   4370 | loss 1.6085 | lr 8.82e-04 | grad 0.96 | tok/s 10627
step   4380 | loss 1.4846 | lr 8.82e-04 | grad 0.92 | tok/s 11213
step   4390 | loss 1.2017 | lr 8.82e-04 | grad 1.50 | tok/s 11441
step   4400 | loss 1.2467 | lr 8.82e-04 | grad 0.85 | tok/s 11442
step   4410 | loss 1.6872 | lr 8.82e-04 | grad 0.78 | tok/s 11005
step   4420 | loss 1.4076 | lr 8.82e-04 | grad 0.74 | tok/s 11117
step   4430 | loss 1.6195 | lr 8.82e-04 | grad 1.05 | tok/s 10542
step   4440 | loss 1.5306 | lr 8.82e-04 | grad 0.87 | tok/s 10566
step   4450 | loss 1.8585 | lr 8.82e-04 | grad 2.11 | tok/s 10925
step   4460 | loss 1.4703 | lr 8.82e-04 | grad 0.64 | tok/s 11213
step   4470 | loss 1.4093 | lr 8.82e-04 | grad 1.08 | tok/s 10916
step   4480 | loss 1.5243 | lr 8.82e-04 | grad 0.75 | tok/s 11103
step   4490 | loss 1.4568 | lr 8.82e-04 | grad 0.58 | tok/s 10661
step   4500 | loss 1.4959 | lr 8.82e-04 | grad 1.18 | tok/s 11277
step   4510 | loss 1.6020 | lr 8.82e-04 | grad 0.79 | tok/s 11301
step   4520 | loss 1.7640 | lr 8.82e-04 | grad 1.02 | tok/s 10748
step   4530 | loss 1.7051 | lr 8.82e-04 | grad 1.83 | tok/s 10403
step   4540 | loss 1.7029 | lr 8.82e-04 | grad 0.95 | tok/s 10659
step   4550 | loss 1.4780 | lr 8.82e-04 | grad 0.89 | tok/s 11204
step   4560 | loss 1.3901 | lr 8.82e-04 | grad 1.88 | tok/s 11181
step   4570 | loss 1.3148 | lr 8.82e-04 | grad 0.79 | tok/s 10632
step   4580 | loss 2.0589 | lr 8.82e-04 | grad 2.64 | tok/s 10744
step   4590 | loss 1.7181 | lr 8.82e-04 | grad 1.01 | tok/s 10908
step   4600 | loss 1.7261 | lr 8.82e-04 | grad 1.03 | tok/s 10706
step   4610 | loss 1.2003 | lr 8.82e-04 | grad 0.68 | tok/s 11142
step   4620 | loss 1.5853 | lr 8.82e-04 | grad 1.01 | tok/s 11335
step   4630 | loss 1.4420 | lr 8.82e-04 | grad 0.89 | tok/s 11430
step   4640 | loss 1.4154 | lr 8.82e-04 | grad 1.09 | tok/s 11433
step   4650 | loss 1.3668 | lr 8.82e-04 | grad 0.64 | tok/s 11440
step   4660 | loss 1.3768 | lr 8.82e-04 | grad 0.88 | tok/s 11450
step   4670 | loss 1.3436 | lr 8.82e-04 | grad 0.81 | tok/s 11442
step   4680 | loss 1.3177 | lr 8.82e-04 | grad 0.82 | tok/s 11447
step   4690 | loss 1.3185 | lr 8.82e-04 | grad 0.91 | tok/s 11441
step   4700 | loss 1.3465 | lr 8.82e-04 | grad 0.83 | tok/s 11451
step   4710 | loss 1.2750 | lr 8.82e-04 | grad 0.87 | tok/s 11435
step   4720 | loss 1.2226 | lr 8.82e-04 | grad 0.82 | tok/s 11436
step   4730 | loss 1.2771 | lr 8.82e-04 | grad 0.70 | tok/s 11443
step   4740 | loss 1.2858 | lr 8.82e-04 | grad 1.02 | tok/s 11438
step   4750 | loss 1.2404 | lr 8.82e-04 | grad 0.99 | tok/s 11435
step   4760 | loss 1.2596 | lr 8.82e-04 | grad 0.83 | tok/s 11445
step   4770 | loss 1.2463 | lr 8.82e-04 | grad 0.70 | tok/s 11439
step   4780 | loss 1.2733 | lr 8.82e-04 | grad 0.71 | tok/s 11440
step   4790 | loss 1.2079 | lr 8.82e-04 | grad 0.82 | tok/s 11439
step   4800 | loss 1.2116 | lr 8.82e-04 | grad 0.95 | tok/s 11433
step   4810 | loss 1.3511 | lr 8.82e-04 | grad 0.85 | tok/s 11414
step   4820 | loss 1.8204 | lr 8.82e-04 | grad 1.11 | tok/s 11066
step   4830 | loss 1.4283 | lr 8.82e-04 | grad 0.75 | tok/s 11080
step   4840 | loss 0.7609 | lr 8.82e-04 | grad 0.74 | tok/s 11448
step   4850 | loss 1.4517 | lr 8.82e-04 | grad 0.73 | tok/s 10858
step   4860 | loss 1.4995 | lr 8.82e-04 | grad 0.86 | tok/s 10478
step   4870 | loss 1.5466 | lr 8.82e-04 | grad 0.96 | tok/s 10579
step   4880 | loss 1.3105 | lr 8.82e-04 | grad 1.01 | tok/s 11075
step   4890 | loss 1.5130 | lr 8.82e-04 | grad 0.62 | tok/s 10726
step   4900 | loss 1.4272 | lr 8.82e-04 | grad 1.14 | tok/s 11313
step   4910 | loss 1.7524 | lr 8.82e-04 | grad 2.22 | tok/s 11151

Training complete! Final step: 4913
