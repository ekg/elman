Using device: cuda
Output directory: benchmark_results/cmaes_4d/transformer_480M_converge0.01_20260203_094548/eval_37/levelllama_100m_20260203_114734
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 220,510,208 parameters
Using schedule-free AdamW (lr=0.0003272411165680974)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 5.2438 | lr 3.27e-04 | grad 6.31 | tok/s 13903
step     20 | loss 5.1510 | lr 3.27e-04 | grad 4.81 | tok/s 24103
step     30 | loss 4.7151 | lr 3.27e-04 | grad 2.48 | tok/s 24440
step     40 | loss 3.7175 | lr 3.27e-04 | grad 2.02 | tok/s 24407
step     50 | loss 3.2620 | lr 3.27e-04 | grad 1.23 | tok/s 24388
step     60 | loss 3.3218 | lr 3.27e-04 | grad 2.45 | tok/s 23416
step     70 | loss 3.2092 | lr 3.27e-04 | grad 3.45 | tok/s 23798
step     80 | loss 3.0788 | lr 3.27e-04 | grad 0.97 | tok/s 23601
step     90 | loss 3.0241 | lr 3.27e-04 | grad 1.27 | tok/s 23021
step    100 | loss 2.6611 | lr 3.27e-04 | grad 1.19 | tok/s 23867
step    110 | loss 2.8677 | lr 3.27e-04 | grad 0.96 | tok/s 23070
step    120 | loss 2.8166 | lr 3.27e-04 | grad 1.18 | tok/s 23215
step    130 | loss 2.6407 | lr 3.27e-04 | grad 1.50 | tok/s 23775
step    140 | loss 2.4239 | lr 3.27e-04 | grad 1.18 | tok/s 22654
step    150 | loss 2.4831 | lr 3.27e-04 | grad 0.59 | tok/s 22855
step    160 | loss 2.4679 | lr 3.27e-04 | grad 1.22 | tok/s 22912
step    170 | loss 2.6201 | lr 3.27e-04 | grad 1.77 | tok/s 23463
step    180 | loss 2.4290 | lr 3.27e-04 | grad 1.43 | tok/s 23382
step    190 | loss 2.2813 | lr 3.27e-04 | grad 1.09 | tok/s 24197
step    200 | loss 2.3482 | lr 3.27e-04 | grad 0.69 | tok/s 23482
step    210 | loss 2.4554 | lr 3.27e-04 | grad 0.90 | tok/s 23819
step    220 | loss 2.3425 | lr 3.27e-04 | grad 0.78 | tok/s 23351
step    230 | loss 2.2442 | lr 3.27e-04 | grad 0.86 | tok/s 23297
step    240 | loss 2.3680 | lr 3.27e-04 | grad 1.65 | tok/s 23806
step    250 | loss 2.3302 | lr 3.27e-04 | grad 0.93 | tok/s 23227
step    260 | loss 2.1461 | lr 3.27e-04 | grad 0.67 | tok/s 22735
step    270 | loss 2.2053 | lr 3.27e-04 | grad 1.02 | tok/s 23239
step    280 | loss 2.0766 | lr 3.27e-04 | grad 1.10 | tok/s 23970
step    290 | loss 1.9901 | lr 3.27e-04 | grad 1.58 | tok/s 24275
step    300 | loss 2.0104 | lr 3.27e-04 | grad 0.81 | tok/s 24270
step    310 | loss 2.0163 | lr 3.27e-04 | grad 0.96 | tok/s 24027
step    320 | loss 2.1046 | lr 3.27e-04 | grad 0.76 | tok/s 22949
step    330 | loss 2.1605 | lr 3.27e-04 | grad 1.36 | tok/s 23668
step    340 | loss 2.1160 | lr 3.27e-04 | grad 1.72 | tok/s 22913
step    350 | loss 2.0773 | lr 3.27e-04 | grad 0.82 | tok/s 22761
step    360 | loss 2.0462 | lr 3.27e-04 | grad 1.49 | tok/s 23549
step    370 | loss 2.3902 | lr 3.27e-04 | grad 1.16 | tok/s 23609
step    380 | loss 2.0572 | lr 3.27e-04 | grad 1.22 | tok/s 23984
step    390 | loss 2.0103 | lr 3.27e-04 | grad 0.88 | tok/s 23366
step    400 | loss 2.0224 | lr 3.27e-04 | grad 0.76 | tok/s 23548
step    410 | loss 2.0058 | lr 3.27e-04 | grad 0.87 | tok/s 22838
step    420 | loss 2.0721 | lr 3.27e-04 | grad 0.90 | tok/s 22991
step    430 | loss 2.2159 | lr 3.27e-04 | grad 1.45 | tok/s 23693
step    440 | loss 2.0466 | lr 3.27e-04 | grad 1.21 | tok/s 23289
step    450 | loss 1.9633 | lr 3.27e-04 | grad 0.78 | tok/s 23272
step    460 | loss 2.0089 | lr 3.27e-04 | grad 1.01 | tok/s 23321
step    470 | loss 1.8934 | lr 3.27e-04 | grad 0.76 | tok/s 22644
step    480 | loss 1.8572 | lr 3.27e-04 | grad 0.61 | tok/s 23158
step    490 | loss 2.4145 | lr 3.27e-04 | grad 1.18 | tok/s 23860
step    500 | loss 1.9179 | lr 3.27e-04 | grad 1.69 | tok/s 23297
step    510 | loss 1.7840 | lr 3.27e-04 | grad 0.77 | tok/s 23979
step    520 | loss 2.2836 | lr 3.27e-04 | grad 1.05 | tok/s 23348
step    530 | loss 1.8144 | lr 3.27e-04 | grad 1.07 | tok/s 23648
step    540 | loss 1.8102 | lr 3.27e-04 | grad 1.19 | tok/s 23857
step    550 | loss 1.6789 | lr 3.27e-04 | grad 0.67 | tok/s 24261
step    560 | loss 1.8113 | lr 3.27e-04 | grad 2.03 | tok/s 23942
step    570 | loss 2.2041 | lr 3.27e-04 | grad 1.05 | tok/s 23999
step    580 | loss 2.3111 | lr 3.27e-04 | grad 1.74 | tok/s 23121
step    590 | loss 1.8477 | lr 3.27e-04 | grad 1.12 | tok/s 23362
step    600 | loss 1.9386 | lr 3.27e-04 | grad 1.02 | tok/s 24264
step    610 | loss 1.8065 | lr 3.27e-04 | grad 1.16 | tok/s 23047
step    620 | loss 1.7972 | lr 3.27e-04 | grad 1.05 | tok/s 23891
step    630 | loss 2.0702 | lr 3.27e-04 | grad 1.11 | tok/s 23857
step    640 | loss 1.8400 | lr 3.27e-04 | grad 1.13 | tok/s 23316
step    650 | loss 2.0247 | lr 3.27e-04 | grad 3.42 | tok/s 23009
step    660 | loss 1.9598 | lr 3.27e-04 | grad 1.22 | tok/s 23732
step    670 | loss 1.9331 | lr 3.27e-04 | grad 1.12 | tok/s 23274
step    680 | loss 1.9035 | lr 3.27e-04 | grad 1.14 | tok/s 23082
step    690 | loss 1.9988 | lr 3.27e-04 | grad 1.16 | tok/s 23297
step    700 | loss 1.8787 | lr 3.27e-04 | grad 1.26 | tok/s 23542
step    710 | loss 1.9106 | lr 3.27e-04 | grad 2.39 | tok/s 23317
step    720 | loss 1.9519 | lr 3.27e-04 | grad 0.95 | tok/s 23464
step    730 | loss 1.9255 | lr 3.27e-04 | grad 1.68 | tok/s 23383
step    740 | loss 1.7495 | lr 3.27e-04 | grad 1.04 | tok/s 23133
step    750 | loss 2.0214 | lr 3.27e-04 | grad 0.85 | tok/s 23422
step    760 | loss 1.7683 | lr 3.27e-04 | grad 1.19 | tok/s 23380
step    770 | loss 1.8423 | lr 3.27e-04 | grad 0.90 | tok/s 23745
step    780 | loss 1.7558 | lr 3.27e-04 | grad 0.93 | tok/s 23829
step    790 | loss 1.7934 | lr 3.27e-04 | grad 1.79 | tok/s 23542
step    800 | loss 1.6986 | lr 3.27e-04 | grad 0.95 | tok/s 23363
step    810 | loss 2.4315 | lr 3.27e-04 | grad 1.81 | tok/s 24117
step    820 | loss 2.1886 | lr 3.27e-04 | grad 1.44 | tok/s 24287
step    830 | loss 2.0308 | lr 3.27e-04 | grad 1.12 | tok/s 24279
step    840 | loss 1.8901 | lr 3.27e-04 | grad 1.05 | tok/s 23174
step    850 | loss 1.7235 | lr 3.27e-04 | grad 1.02 | tok/s 23451
step    860 | loss 1.7486 | lr 3.27e-04 | grad 1.05 | tok/s 23373
step    870 | loss 1.8276 | lr 3.27e-04 | grad 0.84 | tok/s 23801
step    880 | loss 1.7417 | lr 3.27e-04 | grad 1.59 | tok/s 23122
step    890 | loss 2.0363 | lr 3.27e-04 | grad 0.93 | tok/s 22988
step    900 | loss 1.6973 | lr 3.27e-04 | grad 1.02 | tok/s 23012
step    910 | loss 1.7795 | lr 3.27e-04 | grad 0.79 | tok/s 23224
step    920 | loss 1.7478 | lr 3.27e-04 | grad 1.00 | tok/s 23098
step    930 | loss 1.8011 | lr 3.27e-04 | grad 1.06 | tok/s 23089
step    940 | loss 1.7625 | lr 3.27e-04 | grad 0.62 | tok/s 23623
step    950 | loss 1.6008 | lr 3.27e-04 | grad 1.12 | tok/s 24283
step    960 | loss 1.5424 | lr 3.27e-04 | grad 0.50 | tok/s 24282
step    970 | loss 1.7077 | lr 3.27e-04 | grad 1.09 | tok/s 23435
step    980 | loss 1.8569 | lr 3.27e-04 | grad 1.23 | tok/s 22997
step    990 | loss 1.8527 | lr 3.27e-04 | grad 1.38 | tok/s 23358
step   1000 | loss 1.7472 | lr 3.27e-04 | grad 1.23 | tok/s 23887
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7472.pt
step   1010 | loss 1.7215 | lr 3.27e-04 | grad 1.22 | tok/s 17736
step   1020 | loss 1.8950 | lr 3.27e-04 | grad 0.78 | tok/s 23274
step   1030 | loss 1.8487 | lr 3.27e-04 | grad 1.13 | tok/s 23343
step   1040 | loss 1.5169 | lr 3.27e-04 | grad 1.06 | tok/s 23157
step   1050 | loss 2.1673 | lr 3.27e-04 | grad 0.80 | tok/s 23815
step   1060 | loss 2.1858 | lr 3.27e-04 | grad 1.57 | tok/s 23340
step   1070 | loss 1.8052 | lr 3.27e-04 | grad 0.91 | tok/s 23024
step   1080 | loss 1.9315 | lr 3.27e-04 | grad 0.82 | tok/s 23663
step   1090 | loss 1.7135 | lr 3.27e-04 | grad 1.55 | tok/s 24024
step   1100 | loss 1.7639 | lr 3.27e-04 | grad 1.12 | tok/s 23747
step   1110 | loss 1.7643 | lr 3.27e-04 | grad 0.76 | tok/s 22941
step   1120 | loss 1.7616 | lr 3.27e-04 | grad 1.15 | tok/s 23437
step   1130 | loss 1.8051 | lr 3.27e-04 | grad 1.60 | tok/s 23349
step   1140 | loss 1.7444 | lr 3.27e-04 | grad 1.18 | tok/s 23056
step   1150 | loss 1.7851 | lr 3.27e-04 | grad 1.00 | tok/s 22829
step   1160 | loss 1.6409 | lr 3.27e-04 | grad 1.06 | tok/s 24071
step   1170 | loss 1.6537 | lr 3.27e-04 | grad 0.68 | tok/s 24290
step   1180 | loss 1.5856 | lr 3.27e-04 | grad 1.22 | tok/s 24288
step   1190 | loss 1.5523 | lr 3.27e-04 | grad 1.11 | tok/s 24284
step   1200 | loss 1.5385 | lr 3.27e-04 | grad 0.72 | tok/s 24285
step   1210 | loss 1.5779 | lr 3.27e-04 | grad 0.59 | tok/s 23880
step   1220 | loss 1.6413 | lr 3.27e-04 | grad 0.95 | tok/s 22801
step   1230 | loss 1.7344 | lr 3.27e-04 | grad 0.94 | tok/s 23674
step   1240 | loss 1.7051 | lr 3.27e-04 | grad 1.30 | tok/s 23463
step   1250 | loss 1.8994 | lr 3.27e-04 | grad 0.89 | tok/s 23338
step   1260 | loss 1.7498 | lr 3.27e-04 | grad 1.07 | tok/s 23321
step   1270 | loss 1.7579 | lr 3.27e-04 | grad 0.84 | tok/s 23007
step   1280 | loss 1.6968 | lr 3.27e-04 | grad 0.84 | tok/s 23131
step   1290 | loss 1.8048 | lr 3.27e-04 | grad 0.73 | tok/s 23011
step   1300 | loss 1.7245 | lr 3.27e-04 | grad 0.84 | tok/s 23202
step   1310 | loss 1.7765 | lr 3.27e-04 | grad 0.89 | tok/s 23561
step   1320 | loss 1.6017 | lr 3.27e-04 | grad 0.69 | tok/s 23201
step   1330 | loss 1.5918 | lr 3.27e-04 | grad 1.04 | tok/s 23763
step   1340 | loss 1.6754 | lr 3.27e-04 | grad 0.75 | tok/s 23244
step   1350 | loss 1.6373 | lr 3.27e-04 | grad 1.16 | tok/s 23136
step   1360 | loss 1.8472 | lr 3.27e-04 | grad 0.86 | tok/s 23148
step   1370 | loss 1.6880 | lr 3.27e-04 | grad 0.75 | tok/s 23157
step   1380 | loss 1.6584 | lr 3.27e-04 | grad 0.99 | tok/s 23782
step   1390 | loss 1.7581 | lr 3.27e-04 | grad 1.18 | tok/s 23799
step   1400 | loss 1.7070 | lr 3.27e-04 | grad 1.38 | tok/s 22844
step   1410 | loss 1.6647 | lr 3.27e-04 | grad 0.77 | tok/s 22470
step   1420 | loss 1.5087 | lr 3.27e-04 | grad 0.89 | tok/s 23291
step   1430 | loss 1.5533 | lr 3.27e-04 | grad 0.92 | tok/s 23913
step   1440 | loss 1.6903 | lr 3.27e-04 | grad 1.26 | tok/s 22794
step   1450 | loss 1.7265 | lr 3.27e-04 | grad 0.96 | tok/s 23265
step   1460 | loss 1.6198 | lr 3.27e-04 | grad 1.84 | tok/s 23460
step   1470 | loss 1.7043 | lr 3.27e-04 | grad 1.04 | tok/s 23332
step   1480 | loss 1.8605 | lr 3.27e-04 | grad 1.57 | tok/s 22984
step   1490 | loss 1.7064 | lr 3.27e-04 | grad 0.99 | tok/s 23753
step   1500 | loss 1.7272 | lr 3.27e-04 | grad 1.14 | tok/s 23602
step   1510 | loss 1.6619 | lr 3.27e-04 | grad 0.74 | tok/s 23403
step   1520 | loss 1.6455 | lr 3.27e-04 | grad 0.94 | tok/s 23104
step   1530 | loss 1.6188 | lr 3.27e-04 | grad 2.30 | tok/s 23993
step   1540 | loss 2.0686 | lr 3.27e-04 | grad 0.96 | tok/s 23318
step   1550 | loss 1.6591 | lr 3.27e-04 | grad 0.92 | tok/s 23039
step   1560 | loss 1.7751 | lr 3.27e-04 | grad 1.08 | tok/s 23639
step   1570 | loss 1.5612 | lr 3.27e-04 | grad 1.03 | tok/s 23133
step   1580 | loss 1.6684 | lr 3.27e-04 | grad 0.92 | tok/s 22949
step   1590 | loss 1.5142 | lr 3.27e-04 | grad 0.82 | tok/s 23952
step   1600 | loss 1.6822 | lr 3.27e-04 | grad 1.08 | tok/s 23497
step   1610 | loss 1.6454 | lr 3.27e-04 | grad 0.75 | tok/s 23779
step   1620 | loss 1.5741 | lr 3.27e-04 | grad 0.73 | tok/s 22757
step   1630 | loss 1.6405 | lr 3.27e-04 | grad 1.97 | tok/s 22975
step   1640 | loss 1.6553 | lr 3.27e-04 | grad 1.09 | tok/s 23033
step   1650 | loss 1.6796 | lr 3.27e-04 | grad 1.85 | tok/s 23811
step   1660 | loss 2.0010 | lr 3.27e-04 | grad 1.39 | tok/s 23539
step   1670 | loss 1.5609 | lr 3.27e-04 | grad 0.99 | tok/s 23275
step   1680 | loss 1.8070 | lr 3.27e-04 | grad 1.20 | tok/s 23567
step   1690 | loss 1.7054 | lr 3.27e-04 | grad 0.87 | tok/s 23340
step   1700 | loss 1.6594 | lr 3.27e-04 | grad 1.47 | tok/s 23153
step   1710 | loss 1.7427 | lr 3.27e-04 | grad 0.88 | tok/s 23211
step   1720 | loss 1.6701 | lr 3.27e-04 | grad 0.82 | tok/s 23168
step   1730 | loss 1.6321 | lr 3.27e-04 | grad 0.88 | tok/s 22909
step   1740 | loss 1.7646 | lr 3.27e-04 | grad 1.08 | tok/s 23238
step   1750 | loss 1.7673 | lr 3.27e-04 | grad 1.32 | tok/s 23101
step   1760 | loss 1.6256 | lr 3.27e-04 | grad 1.26 | tok/s 23011
step   1770 | loss 1.7669 | lr 3.27e-04 | grad 0.68 | tok/s 22945
step   1780 | loss 1.6331 | lr 3.27e-04 | grad 1.47 | tok/s 23565
step   1790 | loss 1.5487 | lr 3.27e-04 | grad 0.72 | tok/s 23684
step   1800 | loss 1.5551 | lr 3.27e-04 | grad 0.80 | tok/s 22924
step   1810 | loss 1.6116 | lr 3.27e-04 | grad 0.99 | tok/s 23024
step   1820 | loss 1.6936 | lr 3.27e-04 | grad 0.86 | tok/s 23100
step   1830 | loss 1.7243 | lr 3.27e-04 | grad 0.86 | tok/s 22936
step   1840 | loss 1.5636 | lr 3.27e-04 | grad 0.86 | tok/s 23251
step   1850 | loss 1.5851 | lr 3.27e-04 | grad 0.92 | tok/s 23802
step   1860 | loss 1.6251 | lr 3.27e-04 | grad 0.84 | tok/s 23600
step   1870 | loss 1.7577 | lr 3.27e-04 | grad 2.36 | tok/s 23366
step   1880 | loss 1.5691 | lr 3.27e-04 | grad 0.94 | tok/s 23452
step   1890 | loss 1.7066 | lr 3.27e-04 | grad 0.71 | tok/s 23262
step   1900 | loss 1.5270 | lr 3.27e-04 | grad 1.09 | tok/s 23601
step   1910 | loss 1.5723 | lr 3.27e-04 | grad 0.82 | tok/s 23582
step   1920 | loss 1.6356 | lr 3.27e-04 | grad 0.95 | tok/s 24037
step   1930 | loss 1.6015 | lr 3.27e-04 | grad 1.22 | tok/s 23491
step   1940 | loss 1.7909 | lr 3.27e-04 | grad 1.12 | tok/s 23663
step   1950 | loss 1.5050 | lr 3.27e-04 | grad 1.06 | tok/s 23250
step   1960 | loss 1.7869 | lr 3.27e-04 | grad 2.30 | tok/s 23572
step   1970 | loss 1.5595 | lr 3.27e-04 | grad 1.23 | tok/s 23174
step   1980 | loss 1.5752 | lr 3.27e-04 | grad 1.08 | tok/s 23860
step   1990 | loss 1.3485 | lr 3.27e-04 | grad 0.96 | tok/s 24288
step   2000 | loss 1.4377 | lr 3.27e-04 | grad 2.12 | tok/s 24291
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4377.pt
step   2010 | loss 1.5051 | lr 3.27e-04 | grad 0.62 | tok/s 18697
step   2020 | loss 1.4178 | lr 3.27e-04 | grad 1.00 | tok/s 24308
step   2030 | loss 1.6088 | lr 3.27e-04 | grad 1.03 | tok/s 23292
step   2040 | loss 1.6100 | lr 3.27e-04 | grad 0.70 | tok/s 23653
step   2050 | loss 1.6750 | lr 3.27e-04 | grad 1.41 | tok/s 23415
step   2060 | loss 1.6654 | lr 3.27e-04 | grad 0.89 | tok/s 23404
step   2070 | loss 1.5753 | lr 3.27e-04 | grad 0.89 | tok/s 23154
step   2080 | loss 1.4717 | lr 3.27e-04 | grad 0.79 | tok/s 23592
step   2090 | loss 1.5216 | lr 3.27e-04 | grad 1.02 | tok/s 23219
step   2100 | loss 1.4155 | lr 3.27e-04 | grad 1.39 | tok/s 23569
step   2110 | loss 1.6473 | lr 3.27e-04 | grad 0.91 | tok/s 23116
step   2120 | loss 1.7094 | lr 3.27e-04 | grad 1.08 | tok/s 23348
step   2130 | loss 1.6576 | lr 3.27e-04 | grad 1.56 | tok/s 23095
step   2140 | loss 1.7555 | lr 3.27e-04 | grad 0.89 | tok/s 23512
step   2150 | loss 1.5854 | lr 3.27e-04 | grad 1.03 | tok/s 23332
step   2160 | loss 1.8293 | lr 3.27e-04 | grad 1.34 | tok/s 23917
step   2170 | loss 1.4389 | lr 3.27e-04 | grad 1.09 | tok/s 23991
step   2180 | loss 1.4048 | lr 3.27e-04 | grad 0.66 | tok/s 24289
step   2190 | loss 1.4090 | lr 3.27e-04 | grad 1.09 | tok/s 24292
step   2200 | loss 1.5133 | lr 3.27e-04 | grad 1.88 | tok/s 23793
step   2210 | loss 1.6298 | lr 3.27e-04 | grad 2.30 | tok/s 23856
step   2220 | loss 1.7034 | lr 3.27e-04 | grad 1.07 | tok/s 24043
step   2230 | loss 1.6789 | lr 3.27e-04 | grad 0.95 | tok/s 23678
step   2240 | loss 1.5604 | lr 3.27e-04 | grad 0.81 | tok/s 23389
step   2250 | loss 1.6723 | lr 3.27e-04 | grad 1.74 | tok/s 23438
step   2260 | loss 1.5592 | lr 3.27e-04 | grad 0.83 | tok/s 23183
step   2270 | loss 1.5348 | lr 3.27e-04 | grad 0.75 | tok/s 23049
step   2280 | loss 1.5830 | lr 3.27e-04 | grad 1.16 | tok/s 23714
step   2290 | loss 1.5806 | lr 3.27e-04 | grad 0.60 | tok/s 24292
step   2300 | loss 1.5119 | lr 3.27e-04 | grad 0.67 | tok/s 24287
step   2310 | loss 1.5805 | lr 3.27e-04 | grad 1.12 | tok/s 23954
step   2320 | loss 1.6204 | lr 3.27e-04 | grad 0.87 | tok/s 23463
step   2330 | loss 1.9402 | lr 3.27e-04 | grad 1.44 | tok/s 23545
step   2340 | loss 1.6467 | lr 3.27e-04 | grad 0.95 | tok/s 23560
step   2350 | loss 1.5321 | lr 3.27e-04 | grad 0.93 | tok/s 23324
step   2360 | loss 1.5163 | lr 3.27e-04 | grad 1.28 | tok/s 23124
step   2370 | loss 1.6560 | lr 3.27e-04 | grad 1.45 | tok/s 23213
step   2380 | loss 1.5697 | lr 3.27e-04 | grad 1.22 | tok/s 23049
step   2390 | loss 1.6782 | lr 3.27e-04 | grad 0.95 | tok/s 23426
step   2400 | loss 1.5293 | lr 3.27e-04 | grad 1.53 | tok/s 22867
step   2410 | loss 1.6932 | lr 3.27e-04 | grad 0.98 | tok/s 23559
step   2420 | loss 1.4555 | lr 3.27e-04 | grad 1.16 | tok/s 23328
step   2430 | loss 1.6596 | lr 3.27e-04 | grad 1.48 | tok/s 23645
step   2440 | loss 1.5592 | lr 3.27e-04 | grad 1.30 | tok/s 24288
step   2450 | loss 1.5244 | lr 3.27e-04 | grad 0.86 | tok/s 23323
step   2460 | loss 1.4989 | lr 3.27e-04 | grad 1.09 | tok/s 23059
step   2470 | loss 1.5739 | lr 3.27e-04 | grad 0.95 | tok/s 23617
step   2480 | loss 1.6252 | lr 3.27e-04 | grad 0.95 | tok/s 23643
step   2490 | loss 1.7745 | lr 3.27e-04 | grad 1.40 | tok/s 23205
step   2500 | loss 1.6222 | lr 3.27e-04 | grad 1.16 | tok/s 22921
step   2510 | loss 1.6301 | lr 3.27e-04 | grad 0.88 | tok/s 23136
step   2520 | loss 1.8974 | lr 3.27e-04 | grad 1.77 | tok/s 23354
step   2530 | loss 1.6763 | lr 3.27e-04 | grad 1.19 | tok/s 23755
step   2540 | loss 1.5998 | lr 3.27e-04 | grad 1.23 | tok/s 23084
step   2550 | loss 1.6212 | lr 3.27e-04 | grad 0.79 | tok/s 23556
step   2560 | loss 1.4804 | lr 3.27e-04 | grad 0.84 | tok/s 23237
step   2570 | loss 1.6223 | lr 3.27e-04 | grad 0.71 | tok/s 23900
step   2580 | loss 1.5544 | lr 3.27e-04 | grad 0.92 | tok/s 23810
step   2590 | loss 1.5091 | lr 3.27e-04 | grad 0.92 | tok/s 24289
step   2600 | loss 1.5681 | lr 3.27e-04 | grad 1.34 | tok/s 23593
step   2610 | loss 1.5524 | lr 3.27e-04 | grad 0.99 | tok/s 23134
step   2620 | loss 1.5508 | lr 3.27e-04 | grad 1.34 | tok/s 22509
step   2630 | loss 1.8593 | lr 3.27e-04 | grad 0.77 | tok/s 24019
step   2640 | loss 1.4793 | lr 3.27e-04 | grad 1.09 | tok/s 24290

Training complete! Final step: 2647
