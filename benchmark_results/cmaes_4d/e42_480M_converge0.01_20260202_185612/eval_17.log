Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_185612/eval_17/level42_100m_20260202_195659
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 304,230,528 parameters
Using schedule-free AdamW (lr=5.4668943132556304e-05)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 5.6549 | lr 5.47e-05 | grad 30.62 | tok/s 16783
step     20 | loss 4.3589 | lr 5.47e-05 | grad 29.12 | tok/s 34151
step     30 | loss 3.7432 | lr 5.47e-05 | grad 18.00 | tok/s 34417
step     40 | loss 2.8499 | lr 5.47e-05 | grad 15.69 | tok/s 33983
step     50 | loss 2.4112 | lr 5.47e-05 | grad 10.06 | tok/s 33650
step     60 | loss 3.1409 | lr 5.47e-05 | grad 17.62 | tok/s 32401
step     70 | loss 2.7793 | lr 5.47e-05 | grad 14.38 | tok/s 32723
step     80 | loss 2.7216 | lr 5.47e-05 | grad 9.88 | tok/s 32379
step     90 | loss 2.6643 | lr 5.47e-05 | grad 11.00 | tok/s 31591
step    100 | loss 2.2421 | lr 5.47e-05 | grad 9.62 | tok/s 32670
step    110 | loss 2.4966 | lr 5.47e-05 | grad 6.41 | tok/s 31592
step    120 | loss 2.4850 | lr 5.47e-05 | grad 10.88 | tok/s 31758
step    130 | loss 2.3044 | lr 5.47e-05 | grad 9.56 | tok/s 32479
step    140 | loss 2.1149 | lr 5.47e-05 | grad 7.59 | tok/s 30955
step    150 | loss 2.1837 | lr 5.47e-05 | grad 6.03 | tok/s 31196
step    160 | loss 2.1535 | lr 5.47e-05 | grad 7.66 | tok/s 31238
step    170 | loss 2.3230 | lr 5.47e-05 | grad 10.25 | tok/s 31934
step    180 | loss 2.0657 | lr 5.47e-05 | grad 7.84 | tok/s 31791
step    190 | loss 1.8846 | lr 5.47e-05 | grad 5.72 | tok/s 32970
step    200 | loss 2.0125 | lr 5.47e-05 | grad 6.53 | tok/s 31942
step    210 | loss 2.1528 | lr 5.47e-05 | grad 6.69 | tok/s 32484
step    220 | loss 2.0740 | lr 5.47e-05 | grad 7.16 | tok/s 31781
step    230 | loss 1.9499 | lr 5.47e-05 | grad 6.72 | tok/s 31706
step    240 | loss 2.0531 | lr 5.47e-05 | grad 7.56 | tok/s 32331
step    250 | loss 2.1135 | lr 5.47e-05 | grad 6.28 | tok/s 31569
step    260 | loss 1.9205 | lr 5.47e-05 | grad 6.53 | tok/s 30866
step    270 | loss 1.9713 | lr 5.47e-05 | grad 9.81 | tok/s 31639
step    280 | loss 1.8030 | lr 5.47e-05 | grad 7.78 | tok/s 32597
step    290 | loss 1.6881 | lr 5.47e-05 | grad 7.78 | tok/s 32976
step    300 | loss 1.7117 | lr 5.47e-05 | grad 6.91 | tok/s 32965
step    310 | loss 1.7651 | lr 5.47e-05 | grad 7.03 | tok/s 32597
step    320 | loss 1.9308 | lr 5.47e-05 | grad 5.38 | tok/s 31219
step    330 | loss 1.9608 | lr 5.47e-05 | grad 10.88 | tok/s 32151
step    340 | loss 1.9125 | lr 5.47e-05 | grad 8.81 | tok/s 31121
step    350 | loss 1.8847 | lr 5.47e-05 | grad 6.31 | tok/s 30966
step    360 | loss 1.8082 | lr 5.47e-05 | grad 7.88 | tok/s 31986
step    370 | loss 2.2080 | lr 5.47e-05 | grad 7.91 | tok/s 32084
step    380 | loss 1.8634 | lr 5.47e-05 | grad 6.31 | tok/s 32569
step    390 | loss 1.8324 | lr 5.47e-05 | grad 6.25 | tok/s 31697
step    400 | loss 1.8237 | lr 5.47e-05 | grad 5.53 | tok/s 32099
step    410 | loss 1.8400 | lr 5.47e-05 | grad 7.12 | tok/s 30952
step    420 | loss 1.9102 | lr 5.47e-05 | grad 7.12 | tok/s 31249
step    430 | loss 1.9877 | lr 5.47e-05 | grad 9.25 | tok/s 32167
step    440 | loss 1.8685 | lr 5.47e-05 | grad 6.03 | tok/s 31621
step    450 | loss 1.8164 | lr 5.47e-05 | grad 5.78 | tok/s 31556
step    460 | loss 1.8316 | lr 5.47e-05 | grad 8.19 | tok/s 31601
step    470 | loss 1.7216 | lr 5.47e-05 | grad 6.12 | tok/s 30719
step    480 | loss 1.7092 | lr 5.47e-05 | grad 5.59 | tok/s 31387
step    490 | loss 2.2397 | lr 5.47e-05 | grad 8.50 | tok/s 32374
step    500 | loss 1.7718 | lr 5.47e-05 | grad 17.12 | tok/s 31630
step    510 | loss 1.5901 | lr 5.47e-05 | grad 6.03 | tok/s 32597
step    520 | loss 2.1914 | lr 5.47e-05 | grad 8.81 | tok/s 31619
step    530 | loss 1.6251 | lr 5.47e-05 | grad 8.50 | tok/s 32192
step    540 | loss 1.6454 | lr 5.47e-05 | grad 6.59 | tok/s 32290
step    550 | loss 1.4947 | lr 5.47e-05 | grad 6.34 | tok/s 32925
step    560 | loss 1.6403 | lr 5.47e-05 | grad 15.56 | tok/s 32476
step    570 | loss 2.0625 | lr 5.47e-05 | grad 7.00 | tok/s 32582
step    580 | loss 2.2032 | lr 5.47e-05 | grad 11.12 | tok/s 31388
step    590 | loss 1.6926 | lr 5.47e-05 | grad 7.88 | tok/s 31703
step    600 | loss 1.7539 | lr 5.47e-05 | grad 7.38 | tok/s 32957
step    610 | loss 1.6698 | lr 5.47e-05 | grad 5.50 | tok/s 31274
step    620 | loss 1.6127 | lr 5.47e-05 | grad 6.69 | tok/s 32437
step    630 | loss 1.8478 | lr 5.47e-05 | grad 6.50 | tok/s 32342
step    640 | loss 1.6935 | lr 5.47e-05 | grad 7.78 | tok/s 31643
step    650 | loss 1.8675 | lr 5.47e-05 | grad 27.38 | tok/s 31246
step    660 | loss 1.8095 | lr 5.47e-05 | grad 8.62 | tok/s 32229
step    670 | loss 1.7981 | lr 5.47e-05 | grad 7.59 | tok/s 31595
step    680 | loss 1.7706 | lr 5.47e-05 | grad 9.50 | tok/s 31345
step    690 | loss 1.8447 | lr 5.47e-05 | grad 8.94 | tok/s 31648
step    700 | loss 1.7227 | lr 5.47e-05 | grad 6.34 | tok/s 31936
step    710 | loss 1.7911 | lr 5.47e-05 | grad 17.38 | tok/s 31659
step    720 | loss 1.8272 | lr 5.47e-05 | grad 7.50 | tok/s 31842
step    730 | loss 1.8205 | lr 5.47e-05 | grad 13.75 | tok/s 31789
step    740 | loss 1.6170 | lr 5.47e-05 | grad 6.47 | tok/s 31394
step    750 | loss 1.8624 | lr 5.47e-05 | grad 5.84 | tok/s 31805
step    760 | loss 1.6531 | lr 5.47e-05 | grad 5.53 | tok/s 31771
step    770 | loss 1.7228 | lr 5.47e-05 | grad 7.25 | tok/s 32245
step    780 | loss 1.5878 | lr 5.47e-05 | grad 6.22 | tok/s 32352
step    790 | loss 1.6245 | lr 5.47e-05 | grad 14.31 | tok/s 31974
step    800 | loss 1.6032 | lr 5.47e-05 | grad 6.62 | tok/s 31773
step    810 | loss 2.3388 | lr 5.47e-05 | grad 12.62 | tok/s 32744
step    820 | loss 2.0340 | lr 5.47e-05 | grad 9.50 | tok/s 32971
step    830 | loss 1.8159 | lr 5.47e-05 | grad 8.31 | tok/s 32975
step    840 | loss 1.7733 | lr 5.47e-05 | grad 5.44 | tok/s 31452
step    850 | loss 1.6004 | lr 5.47e-05 | grad 5.34 | tok/s 31838
step    860 | loss 1.6253 | lr 5.47e-05 | grad 7.66 | tok/s 31731
step    870 | loss 1.6996 | lr 5.47e-05 | grad 6.16 | tok/s 32320
step    880 | loss 1.6177 | lr 5.47e-05 | grad 9.81 | tok/s 31378
step    890 | loss 1.9453 | lr 5.47e-05 | grad 6.00 | tok/s 31204
step    900 | loss 1.5690 | lr 5.47e-05 | grad 7.62 | tok/s 31207
step    910 | loss 1.6717 | lr 5.47e-05 | grad 5.38 | tok/s 31508
step    920 | loss 1.6363 | lr 5.47e-05 | grad 6.81 | tok/s 31340
step    930 | loss 1.7111 | lr 5.47e-05 | grad 7.38 | tok/s 31318
step    940 | loss 1.6714 | lr 5.47e-05 | grad 6.19 | tok/s 32041
step    950 | loss 1.4675 | lr 5.47e-05 | grad 6.53 | tok/s 32938
step    960 | loss 1.3908 | lr 5.47e-05 | grad 5.47 | tok/s 32973
step    970 | loss 1.6013 | lr 5.47e-05 | grad 6.50 | tok/s 31800
step    980 | loss 1.7546 | lr 5.47e-05 | grad 8.81 | tok/s 31259
step    990 | loss 1.7258 | lr 5.47e-05 | grad 9.25 | tok/s 31696
step   1000 | loss 1.6266 | lr 5.47e-05 | grad 7.97 | tok/s 32495
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6266.pt
step   1010 | loss 1.5792 | lr 5.47e-05 | grad 5.62 | tok/s 22194
step   1020 | loss 1.7888 | lr 5.47e-05 | grad 6.03 | tok/s 31784
step   1030 | loss 1.7473 | lr 5.47e-05 | grad 8.81 | tok/s 31813
step   1040 | loss 1.3946 | lr 5.47e-05 | grad 5.16 | tok/s 31643
step   1050 | loss 1.9946 | lr 5.47e-05 | grad 5.94 | tok/s 32367
step   1060 | loss 2.1529 | lr 5.47e-05 | grad 14.62 | tok/s 31689
step   1070 | loss 1.7129 | lr 5.47e-05 | grad 8.12 | tok/s 31229
step   1080 | loss 1.8404 | lr 5.47e-05 | grad 5.12 | tok/s 32084
step   1090 | loss 1.6240 | lr 5.47e-05 | grad 12.00 | tok/s 32716
step   1100 | loss 1.5813 | lr 5.47e-05 | grad 8.38 | tok/s 32207
step   1110 | loss 1.6755 | lr 5.47e-05 | grad 5.88 | tok/s 31116
step   1120 | loss 1.6689 | lr 5.47e-05 | grad 7.19 | tok/s 31785
step   1130 | loss 1.7234 | lr 5.47e-05 | grad 13.69 | tok/s 31687
step   1140 | loss 1.6781 | lr 5.47e-05 | grad 11.88 | tok/s 31282
step   1150 | loss 1.7072 | lr 5.47e-05 | grad 8.25 | tok/s 30953
step   1160 | loss 1.5362 | lr 5.47e-05 | grad 6.06 | tok/s 32609
step   1170 | loss 1.5037 | lr 5.47e-05 | grad 5.75 | tok/s 32909
step   1180 | loss 1.4329 | lr 5.47e-05 | grad 6.91 | tok/s 32893
step   1190 | loss 1.3892 | lr 5.47e-05 | grad 5.62 | tok/s 32918
step   1200 | loss 1.3805 | lr 5.47e-05 | grad 5.81 | tok/s 32934
step   1210 | loss 1.4638 | lr 5.47e-05 | grad 3.53 | tok/s 32482
step   1220 | loss 1.5533 | lr 5.47e-05 | grad 5.34 | tok/s 30915
step   1230 | loss 1.6369 | lr 5.47e-05 | grad 5.22 | tok/s 32068
step   1240 | loss 1.6321 | lr 5.47e-05 | grad 12.62 | tok/s 31845
step   1250 | loss 1.8053 | lr 5.47e-05 | grad 7.12 | tok/s 31715
step   1260 | loss 1.6505 | lr 5.47e-05 | grad 6.97 | tok/s 31645
step   1270 | loss 1.6680 | lr 5.47e-05 | grad 6.03 | tok/s 31232
step   1280 | loss 1.6318 | lr 5.47e-05 | grad 6.69 | tok/s 31384
step   1290 | loss 1.7222 | lr 5.47e-05 | grad 5.38 | tok/s 31224
step   1300 | loss 1.6125 | lr 5.47e-05 | grad 8.00 | tok/s 31486
step   1310 | loss 1.6683 | lr 5.47e-05 | grad 6.91 | tok/s 31983
step   1320 | loss 1.4797 | lr 5.47e-05 | grad 5.47 | tok/s 31472
step   1330 | loss 1.4764 | lr 5.47e-05 | grad 5.75 | tok/s 32242
step   1340 | loss 1.6030 | lr 5.47e-05 | grad 5.06 | tok/s 31520
step   1350 | loss 1.5450 | lr 5.47e-05 | grad 6.94 | tok/s 31406
step   1360 | loss 1.7438 | lr 5.47e-05 | grad 6.00 | tok/s 31413
step   1370 | loss 1.5793 | lr 5.47e-05 | grad 8.56 | tok/s 31431
step   1380 | loss 1.5616 | lr 5.47e-05 | grad 5.44 | tok/s 32269
step   1390 | loss 1.6469 | lr 5.47e-05 | grad 7.03 | tok/s 32286
step   1400 | loss 1.6250 | lr 5.47e-05 | grad 11.81 | tok/s 31006
step   1410 | loss 1.5623 | lr 5.47e-05 | grad 5.06 | tok/s 30519
step   1420 | loss 1.4065 | lr 5.47e-05 | grad 5.69 | tok/s 31604
step   1430 | loss 1.4001 | lr 5.47e-05 | grad 5.75 | tok/s 32422
step   1440 | loss 1.6066 | lr 5.47e-05 | grad 9.06 | tok/s 30941
step   1450 | loss 1.6561 | lr 5.47e-05 | grad 7.31 | tok/s 31616
step   1460 | loss 1.5136 | lr 5.47e-05 | grad 14.31 | tok/s 31833
step   1470 | loss 1.5820 | lr 5.47e-05 | grad 8.50 | tok/s 31628
step   1480 | loss 1.8349 | lr 5.47e-05 | grad 11.12 | tok/s 31189
step   1490 | loss 1.5864 | lr 5.47e-05 | grad 7.22 | tok/s 32186
step   1500 | loss 1.6183 | lr 5.47e-05 | grad 6.72 | tok/s 32018
step   1510 | loss 1.5548 | lr 5.47e-05 | grad 5.16 | tok/s 31701
step   1520 | loss 1.5276 | lr 5.47e-05 | grad 5.28 | tok/s 31349
step   1530 | loss 1.4545 | lr 5.47e-05 | grad 12.25 | tok/s 32590
step   1540 | loss 1.9732 | lr 5.47e-05 | grad 7.53 | tok/s 31631
step   1550 | loss 1.5599 | lr 5.47e-05 | grad 6.88 | tok/s 31242
step   1560 | loss 1.6829 | lr 5.47e-05 | grad 7.34 | tok/s 32088
step   1570 | loss 1.4546 | lr 5.47e-05 | grad 7.16 | tok/s 31411
step   1580 | loss 1.5997 | lr 5.47e-05 | grad 5.62 | tok/s 31101
step   1590 | loss 1.4067 | lr 5.47e-05 | grad 5.06 | tok/s 32671
step   1600 | loss 1.5839 | lr 5.47e-05 | grad 6.75 | tok/s 31845
step   1610 | loss 1.5389 | lr 5.47e-05 | grad 5.03 | tok/s 32284
step   1620 | loss 1.4973 | lr 5.47e-05 | grad 6.03 | tok/s 30880
step   1630 | loss 1.5582 | lr 5.47e-05 | grad 12.31 | tok/s 31190
step   1640 | loss 1.5811 | lr 5.47e-05 | grad 9.12 | tok/s 31218
step   1650 | loss 1.5711 | lr 5.47e-05 | grad 12.62 | tok/s 32311
step   1660 | loss 1.8449 | lr 5.47e-05 | grad 10.38 | tok/s 31943
step   1670 | loss 1.4342 | lr 5.47e-05 | grad 5.88 | tok/s 31576
step   1680 | loss 1.6682 | lr 5.47e-05 | grad 8.25 | tok/s 31988
step   1690 | loss 1.6351 | lr 5.47e-05 | grad 6.44 | tok/s 31675
step   1700 | loss 1.5277 | lr 5.47e-05 | grad 11.56 | tok/s 31407
step   1710 | loss 1.6755 | lr 5.47e-05 | grad 5.44 | tok/s 31486
step   1720 | loss 1.5707 | lr 5.47e-05 | grad 6.06 | tok/s 31459
step   1730 | loss 1.5601 | lr 5.47e-05 | grad 7.53 | tok/s 31077
step   1740 | loss 1.6498 | lr 5.47e-05 | grad 7.06 | tok/s 31521
step   1750 | loss 1.6968 | lr 5.47e-05 | grad 9.94 | tok/s 31363
step   1760 | loss 1.5372 | lr 5.47e-05 | grad 9.00 | tok/s 31212
step   1770 | loss 1.6813 | lr 5.47e-05 | grad 7.62 | tok/s 31116
step   1780 | loss 1.5343 | lr 5.47e-05 | grad 11.88 | tok/s 31986
step   1790 | loss 1.4433 | lr 5.47e-05 | grad 11.81 | tok/s 32140
step   1800 | loss 1.4544 | lr 5.47e-05 | grad 4.28 | tok/s 31113
step   1810 | loss 1.5308 | lr 5.47e-05 | grad 4.44 | tok/s 31239
step   1820 | loss 1.6370 | lr 5.47e-05 | grad 6.78 | tok/s 31340
step   1830 | loss 1.6386 | lr 5.47e-05 | grad 7.19 | tok/s 31110
step   1840 | loss 1.4637 | lr 5.47e-05 | grad 6.50 | tok/s 31558
step   1850 | loss 1.4702 | lr 5.47e-05 | grad 7.47 | tok/s 32319
step   1860 | loss 1.5368 | lr 5.47e-05 | grad 5.53 | tok/s 32017
step   1870 | loss 1.6656 | lr 5.47e-05 | grad 9.81 | tok/s 31738
step   1880 | loss 1.5118 | lr 5.47e-05 | grad 6.19 | tok/s 31828
step   1890 | loss 1.6431 | lr 5.47e-05 | grad 4.78 | tok/s 31555
step   1900 | loss 1.3857 | lr 5.47e-05 | grad 8.31 | tok/s 32015
step   1910 | loss 1.4568 | lr 5.47e-05 | grad 5.19 | tok/s 31977
step   1920 | loss 1.5362 | lr 5.47e-05 | grad 7.91 | tok/s 32635
step   1930 | loss 1.5047 | lr 5.47e-05 | grad 8.69 | tok/s 31877
step   1940 | loss 1.6133 | lr 5.47e-05 | grad 7.66 | tok/s 32115
step   1950 | loss 1.4200 | lr 5.47e-05 | grad 6.47 | tok/s 31575
step   1960 | loss 1.6731 | lr 5.47e-05 | grad 17.62 | tok/s 31980
step   1970 | loss 1.4789 | lr 5.47e-05 | grad 9.00 | tok/s 31464
step   1980 | loss 1.4548 | lr 5.47e-05 | grad 6.34 | tok/s 32365
step   1990 | loss 1.2304 | lr 5.47e-05 | grad 9.50 | tok/s 32945
step   2000 | loss 1.2943 | lr 5.47e-05 | grad 20.00 | tok/s 32979
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2943.pt
step   2010 | loss 1.3993 | lr 5.47e-05 | grad 5.22 | tok/s 23385
step   2020 | loss 1.2924 | lr 5.47e-05 | grad 5.78 | tok/s 33146
step   2030 | loss 1.5419 | lr 5.47e-05 | grad 8.12 | tok/s 31752
step   2040 | loss 1.5323 | lr 5.47e-05 | grad 5.72 | tok/s 32270
step   2050 | loss 1.5715 | lr 5.47e-05 | grad 9.31 | tok/s 31892
step   2060 | loss 1.5750 | lr 5.47e-05 | grad 5.53 | tok/s 31864
step   2070 | loss 1.4858 | lr 5.47e-05 | grad 5.88 | tok/s 31532
step   2080 | loss 1.3678 | lr 5.47e-05 | grad 5.59 | tok/s 32138
step   2090 | loss 1.4143 | lr 5.47e-05 | grad 7.56 | tok/s 31616
step   2100 | loss 1.2686 | lr 5.47e-05 | grad 8.00 | tok/s 32197
step   2110 | loss 1.5815 | lr 5.47e-05 | grad 6.91 | tok/s 31428
step   2120 | loss 1.6071 | lr 5.47e-05 | grad 11.62 | tok/s 31717
step   2130 | loss 1.5747 | lr 5.47e-05 | grad 13.12 | tok/s 31406
step   2140 | loss 1.6789 | lr 5.47e-05 | grad 7.88 | tok/s 31948
step   2150 | loss 1.4881 | lr 5.47e-05 | grad 8.00 | tok/s 31757
step   2160 | loss 1.6664 | lr 5.47e-05 | grad 9.94 | tok/s 32493
step   2170 | loss 1.3044 | lr 5.47e-05 | grad 5.91 | tok/s 32600
step   2180 | loss 1.2649 | lr 5.47e-05 | grad 4.84 | tok/s 33027
step   2190 | loss 1.2577 | lr 5.47e-05 | grad 5.22 | tok/s 32966
step   2200 | loss 1.3846 | lr 5.47e-05 | grad 16.88 | tok/s 32343
step   2210 | loss 1.5053 | lr 5.47e-05 | grad 11.06 | tok/s 32404
step   2220 | loss 1.5171 | lr 5.47e-05 | grad 7.41 | tok/s 32648
step   2230 | loss 1.6079 | lr 5.47e-05 | grad 5.38 | tok/s 32201
step   2240 | loss 1.4427 | lr 5.47e-05 | grad 5.47 | tok/s 31734
step   2250 | loss 1.5956 | lr 5.47e-05 | grad 12.00 | tok/s 31817
step   2260 | loss 1.4822 | lr 5.47e-05 | grad 7.12 | tok/s 31471
step   2270 | loss 1.4827 | lr 5.47e-05 | grad 5.84 | tok/s 31283
step   2280 | loss 1.4771 | lr 5.47e-05 | grad 6.66 | tok/s 32188
step   2290 | loss 1.4342 | lr 5.47e-05 | grad 5.28 | tok/s 32971
step   2300 | loss 1.3642 | lr 5.47e-05 | grad 5.03 | tok/s 32970
step   2310 | loss 1.4465 | lr 5.47e-05 | grad 6.03 | tok/s 32519
step   2320 | loss 1.5626 | lr 5.47e-05 | grad 5.41 | tok/s 31838
step   2330 | loss 1.8551 | lr 5.47e-05 | grad 11.06 | tok/s 31951
step   2340 | loss 1.5639 | lr 5.47e-05 | grad 7.22 | tok/s 32044
step   2350 | loss 1.4488 | lr 5.47e-05 | grad 6.41 | tok/s 31656
step   2360 | loss 1.4401 | lr 5.47e-05 | grad 7.78 | tok/s 31391
step   2370 | loss 1.5616 | lr 5.47e-05 | grad 12.44 | tok/s 31517
step   2380 | loss 1.4678 | lr 5.47e-05 | grad 9.00 | tok/s 31313
step   2390 | loss 1.5483 | lr 5.47e-05 | grad 6.09 | tok/s 31782
step   2400 | loss 1.4523 | lr 5.47e-05 | grad 9.62 | tok/s 31071
step   2410 | loss 1.6248 | lr 5.47e-05 | grad 6.59 | tok/s 32008
step   2420 | loss 1.3804 | lr 5.47e-05 | grad 9.38 | tok/s 31782
step   2430 | loss 1.5361 | lr 5.47e-05 | grad 11.31 | tok/s 32097
step   2440 | loss 1.2433 | lr 5.47e-05 | grad 10.94 | tok/s 32975
step   2450 | loss 1.4220 | lr 5.47e-05 | grad 5.88 | tok/s 31664
step   2460 | loss 1.4090 | lr 5.47e-05 | grad 6.41 | tok/s 31341
step   2470 | loss 1.4940 | lr 5.47e-05 | grad 8.38 | tok/s 32059
step   2480 | loss 1.5169 | lr 5.47e-05 | grad 6.38 | tok/s 32128
step   2490 | loss 1.6734 | lr 5.47e-05 | grad 8.88 | tok/s 31488
step   2500 | loss 1.5608 | lr 5.47e-05 | grad 9.69 | tok/s 31142
step   2510 | loss 1.5265 | lr 5.47e-05 | grad 7.44 | tok/s 31459
step   2520 | loss 1.8895 | lr 5.47e-05 | grad 27.62 | tok/s 31780
step   2530 | loss 1.5842 | lr 5.47e-05 | grad 9.88 | tok/s 32256
step   2540 | loss 1.5372 | lr 5.47e-05 | grad 11.00 | tok/s 31429
step   2550 | loss 1.5216 | lr 5.47e-05 | grad 5.78 | tok/s 32015
step   2560 | loss 1.4171 | lr 5.47e-05 | grad 9.12 | tok/s 31567
step   2570 | loss 1.5414 | lr 5.47e-05 | grad 5.28 | tok/s 32429
step   2580 | loss 1.4566 | lr 5.47e-05 | grad 5.97 | tok/s 32324
step   2590 | loss 1.3502 | lr 5.47e-05 | grad 7.38 | tok/s 32960
step   2600 | loss 1.4624 | lr 5.47e-05 | grad 8.38 | tok/s 32029
step   2610 | loss 1.4842 | lr 5.47e-05 | grad 6.94 | tok/s 31398
step   2620 | loss 1.4833 | lr 5.47e-05 | grad 10.38 | tok/s 30530
step   2630 | loss 1.7666 | lr 5.47e-05 | grad 5.22 | tok/s 32595
step   2640 | loss 1.3633 | lr 5.47e-05 | grad 6.16 | tok/s 32974
step   2650 | loss 1.3179 | lr 5.47e-05 | grad 4.97 | tok/s 32975
step   2660 | loss 1.3103 | lr 5.47e-05 | grad 5.72 | tok/s 32973
step   2670 | loss 1.4800 | lr 5.47e-05 | grad 5.47 | tok/s 31561
step   2680 | loss 1.6296 | lr 5.47e-05 | grad 11.25 | tok/s 31293
step   2690 | loss 1.8349 | lr 5.47e-05 | grad 17.50 | tok/s 32421
step   2700 | loss 1.4225 | lr 5.47e-05 | grad 5.19 | tok/s 31647
step   2710 | loss 1.3576 | lr 5.47e-05 | grad 5.56 | tok/s 32011
step   2720 | loss 1.5719 | lr 5.47e-05 | grad 7.81 | tok/s 31566
step   2730 | loss 1.6358 | lr 5.47e-05 | grad 5.69 | tok/s 31053
step   2740 | loss 1.5062 | lr 5.47e-05 | grad 6.12 | tok/s 32217
step   2750 | loss 1.4295 | lr 5.47e-05 | grad 4.94 | tok/s 31227
step   2760 | loss 1.3561 | lr 5.47e-05 | grad 6.66 | tok/s 31597
step   2770 | loss 1.8681 | lr 5.47e-05 | grad 13.81 | tok/s 31758
step   2780 | loss 1.3902 | lr 5.47e-05 | grad 5.78 | tok/s 32369
step   2790 | loss 1.7602 | lr 5.47e-05 | grad 9.31 | tok/s 32155
step   2800 | loss 1.5302 | lr 5.47e-05 | grad 7.66 | tok/s 32239
step   2810 | loss 1.4657 | lr 5.47e-05 | grad 6.53 | tok/s 31780
step   2820 | loss 1.3869 | lr 5.47e-05 | grad 8.19 | tok/s 32105
step   2830 | loss 1.4229 | lr 5.47e-05 | grad 11.38 | tok/s 31714
step   2840 | loss 1.6551 | lr 5.47e-05 | grad 27.00 | tok/s 32151
step   2850 | loss 1.7171 | lr 5.47e-05 | grad 7.72 | tok/s 31891
step   2860 | loss 1.6472 | lr 5.47e-05 | grad 7.66 | tok/s 32269
step   2870 | loss 1.4159 | lr 5.47e-05 | grad 5.97 | tok/s 31323
step   2880 | loss 1.6463 | lr 5.47e-05 | grad 18.12 | tok/s 31421
step   2890 | loss 1.4788 | lr 5.47e-05 | grad 5.28 | tok/s 32104
step   2900 | loss 1.4860 | lr 5.47e-05 | grad 5.12 | tok/s 31607
step   2910 | loss 1.4968 | lr 5.47e-05 | grad 8.81 | tok/s 31472
step   2920 | loss 1.4793 | lr 5.47e-05 | grad 7.59 | tok/s 31581
step   2930 | loss 1.6311 | lr 5.47e-05 | grad 10.62 | tok/s 31326
step   2940 | loss 1.4551 | lr 5.47e-05 | grad 4.91 | tok/s 31589
step   2950 | loss 1.4894 | lr 5.47e-05 | grad 5.78 | tok/s 32234
step   2960 | loss 1.4015 | lr 5.47e-05 | grad 6.47 | tok/s 31865
step   2970 | loss 1.4148 | lr 5.47e-05 | grad 19.88 | tok/s 31932
step   2980 | loss 1.4380 | lr 5.47e-05 | grad 7.50 | tok/s 31884
step   2990 | loss 1.5206 | lr 5.47e-05 | grad 9.56 | tok/s 32095
step   3000 | loss 1.4787 | lr 5.47e-05 | grad 6.34 | tok/s 31717
  >>> saved checkpoint: checkpoint_step_003000_loss_1.4787.pt
step   3010 | loss 1.5609 | lr 5.47e-05 | grad 13.31 | tok/s 22349
step   3020 | loss 1.4036 | lr 5.47e-05 | grad 7.84 | tok/s 31917
step   3030 | loss 1.3779 | lr 5.47e-05 | grad 6.28 | tok/s 31647
step   3040 | loss 1.4468 | lr 5.47e-05 | grad 5.78 | tok/s 32070
step   3050 | loss 1.3664 | lr 5.47e-05 | grad 5.72 | tok/s 32426
step   3060 | loss 1.4544 | lr 5.47e-05 | grad 5.28 | tok/s 31704
step   3070 | loss 1.4039 | lr 5.47e-05 | grad 5.03 | tok/s 31764
step   3080 | loss 1.4235 | lr 5.47e-05 | grad 4.41 | tok/s 31763
step   3090 | loss 1.7551 | lr 5.47e-05 | grad 8.50 | tok/s 32037
step   3100 | loss 1.8027 | lr 5.47e-05 | grad 7.28 | tok/s 33120
step   3110 | loss 1.4865 | lr 5.47e-05 | grad 8.44 | tok/s 33071
step   3120 | loss 1.5858 | lr 5.47e-05 | grad 8.44 | tok/s 31898
step   3130 | loss 2.1406 | lr 5.47e-05 | grad 5.91 | tok/s 31639
step   3140 | loss 1.3245 | lr 5.47e-05 | grad 6.50 | tok/s 32124
step   3150 | loss 1.7330 | lr 5.47e-05 | grad 19.75 | tok/s 32038
step   3160 | loss 1.4370 | lr 5.47e-05 | grad 6.25 | tok/s 31519
step   3170 | loss 1.8025 | lr 5.47e-05 | grad 7.59 | tok/s 32467
step   3180 | loss 1.4520 | lr 5.47e-05 | grad 8.81 | tok/s 32498
step   3190 | loss 1.4833 | lr 5.47e-05 | grad 6.75 | tok/s 31972
step   3200 | loss 1.5690 | lr 5.47e-05 | grad 9.69 | tok/s 30990
step   3210 | loss 1.4917 | lr 5.47e-05 | grad 6.50 | tok/s 30164
step   3220 | loss 1.2948 | lr 5.47e-05 | grad 6.03 | tok/s 31602
step   3230 | loss 1.5773 | lr 5.47e-05 | grad 9.25 | tok/s 31856
step   3240 | loss 1.5188 | lr 5.47e-05 | grad 5.84 | tok/s 31873
step   3250 | loss 1.5138 | lr 5.47e-05 | grad 11.31 | tok/s 32821
step   3260 | loss 1.5217 | lr 5.47e-05 | grad 5.25 | tok/s 31441
step   3270 | loss 1.4743 | lr 5.47e-05 | grad 6.44 | tok/s 32053
step   3280 | loss 1.5481 | lr 5.47e-05 | grad 10.81 | tok/s 31715
step   3290 | loss 1.4984 | lr 5.47e-05 | grad 7.31 | tok/s 32168
step   3300 | loss 1.4795 | lr 5.47e-05 | grad 6.97 | tok/s 31095
step   3310 | loss 1.4929 | lr 5.47e-05 | grad 8.44 | tok/s 30529
step   3320 | loss 1.4762 | lr 5.47e-05 | grad 5.50 | tok/s 33032
step   3330 | loss 1.4201 | lr 5.47e-05 | grad 7.66 | tok/s 32640
step   3340 | loss 1.3642 | lr 5.47e-05 | grad 11.12 | tok/s 31632
step   3350 | loss 1.5854 | lr 5.47e-05 | grad 15.00 | tok/s 32324
step   3360 | loss 1.5025 | lr 5.47e-05 | grad 5.19 | tok/s 31612
step   3370 | loss 1.3354 | lr 5.47e-05 | grad 5.00 | tok/s 31110
step   3380 | loss 1.4478 | lr 5.47e-05 | grad 10.81 | tok/s 32177
step   3390 | loss 1.4664 | lr 5.47e-05 | grad 8.31 | tok/s 32257
step   3400 | loss 1.9697 | lr 5.47e-05 | grad 11.75 | tok/s 32176
step   3410 | loss 1.4258 | lr 5.47e-05 | grad 6.59 | tok/s 31853
step   3420 | loss 1.5912 | lr 5.47e-05 | grad 5.56 | tok/s 31948
step   3430 | loss 1.2641 | lr 5.47e-05 | grad 13.50 | tok/s 32679
step   3440 | loss 1.4727 | lr 5.47e-05 | grad 5.75 | tok/s 30761
step   3450 | loss 1.5481 | lr 5.47e-05 | grad 5.66 | tok/s 31261
step   3460 | loss 1.4504 | lr 5.47e-05 | grad 8.12 | tok/s 32115
step   3470 | loss 1.7353 | lr 5.47e-05 | grad 5.75 | tok/s 31497
step   3480 | loss 1.4928 | lr 5.47e-05 | grad 10.75 | tok/s 31506
step   3490 | loss 1.3606 | lr 5.47e-05 | grad 6.75 | tok/s 31529
step   3500 | loss 1.4802 | lr 5.47e-05 | grad 6.81 | tok/s 31744
step   3510 | loss 1.4911 | lr 5.47e-05 | grad 4.94 | tok/s 31125
step   3520 | loss 1.4656 | lr 5.47e-05 | grad 4.94 | tok/s 32066
step   3530 | loss 1.4809 | lr 5.47e-05 | grad 5.94 | tok/s 31830
step   3540 | loss 1.4257 | lr 5.47e-05 | grad 6.38 | tok/s 31549
step   3550 | loss 1.6114 | lr 5.47e-05 | grad 5.94 | tok/s 31948
step   3560 | loss 1.5334 | lr 5.47e-05 | grad 6.28 | tok/s 31063
step   3570 | loss 1.4594 | lr 5.47e-05 | grad 8.38 | tok/s 31747
step   3580 | loss 1.4180 | lr 5.47e-05 | grad 11.00 | tok/s 31935

Training complete! Final step: 3587
