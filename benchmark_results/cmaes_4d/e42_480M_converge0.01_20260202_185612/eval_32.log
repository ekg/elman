Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_185612/eval_32/level42_100m_20260202_202723
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 393,976,320 parameters
Using schedule-free AdamW (lr=0.00018411327127201732)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 6.0133 | lr 1.84e-04 | grad 20.00 | tok/s 8297
step     20 | loss 3.2891 | lr 1.84e-04 | grad 6.00 | tok/s 13995
step     30 | loss 3.5901 | lr 1.84e-04 | grad 5.19 | tok/s 14771
step     40 | loss 4.6560 | lr 1.84e-04 | grad 25.00 | tok/s 15064
step     50 | loss 4.6208 | lr 1.84e-04 | grad 10.62 | tok/s 15257
step     60 | loss 3.8520 | lr 1.84e-04 | grad 9.38 | tok/s 15202
step     70 | loss 3.2885 | lr 1.84e-04 | grad 6.75 | tok/s 15170
step     80 | loss 2.9710 | lr 1.84e-04 | grad 6.03 | tok/s 15142
step     90 | loss 2.7002 | lr 1.84e-04 | grad 5.44 | tok/s 15116
step    100 | loss 2.5079 | lr 1.84e-04 | grad 4.31 | tok/s 15110
step    110 | loss 2.7060 | lr 1.84e-04 | grad 4.91 | tok/s 14986
step    120 | loss 3.2295 | lr 1.84e-04 | grad 4.06 | tok/s 14284
step    130 | loss 2.5170 | lr 1.84e-04 | grad 5.00 | tok/s 14625
step    140 | loss 2.7001 | lr 1.84e-04 | grad 8.81 | tok/s 14663
step    150 | loss 2.3738 | lr 1.84e-04 | grad 8.56 | tok/s 15017
step    160 | loss 2.7439 | lr 1.84e-04 | grad 2.52 | tok/s 14528
step    170 | loss 2.5565 | lr 1.84e-04 | grad 2.09 | tok/s 14307
step    180 | loss 2.6995 | lr 1.84e-04 | grad 5.38 | tok/s 14658
step    190 | loss 2.2766 | lr 1.84e-04 | grad 2.48 | tok/s 14378
step    200 | loss 2.1514 | lr 1.84e-04 | grad 1.99 | tok/s 15038
step    210 | loss 2.2205 | lr 1.84e-04 | grad 6.47 | tok/s 14253
step    220 | loss 2.5651 | lr 1.84e-04 | grad 6.62 | tok/s 14406
step    230 | loss 2.3479 | lr 1.84e-04 | grad 2.58 | tok/s 14396
step    240 | loss 2.6255 | lr 1.84e-04 | grad 4.97 | tok/s 14570
step    250 | loss 2.1379 | lr 1.84e-04 | grad 1.76 | tok/s 14478
step    260 | loss 2.2715 | lr 1.84e-04 | grad 3.28 | tok/s 14894
step    270 | loss 2.1571 | lr 1.84e-04 | grad 2.67 | tok/s 14554
step    280 | loss 2.0983 | lr 1.84e-04 | grad 1.73 | tok/s 13666
step    290 | loss 2.0278 | lr 1.84e-04 | grad 2.00 | tok/s 14130
step    300 | loss 2.2982 | lr 1.84e-04 | grad 2.73 | tok/s 14242
step    310 | loss 1.9781 | lr 1.84e-04 | grad 1.86 | tok/s 14160
step    320 | loss 2.2421 | lr 1.84e-04 | grad 4.59 | tok/s 14333
step    330 | loss 2.0393 | lr 1.84e-04 | grad 1.88 | tok/s 14487
step    340 | loss 2.3866 | lr 1.84e-04 | grad 3.05 | tok/s 14427
step    350 | loss 2.2734 | lr 1.84e-04 | grad 2.09 | tok/s 14835
step    360 | loss 1.9444 | lr 1.84e-04 | grad 2.09 | tok/s 14200
step    370 | loss 1.9685 | lr 1.84e-04 | grad 1.73 | tok/s 14969
step    380 | loss 1.7585 | lr 1.84e-04 | grad 2.19 | tok/s 15085
step    390 | loss 1.6636 | lr 1.84e-04 | grad 2.05 | tok/s 15095
step    400 | loss 2.1653 | lr 1.84e-04 | grad 2.05 | tok/s 14293
step    410 | loss 2.0847 | lr 1.84e-04 | grad 2.31 | tok/s 14425
step    420 | loss 2.1673 | lr 1.84e-04 | grad 2.42 | tok/s 15040
step    430 | loss 1.9908 | lr 1.84e-04 | grad 2.08 | tok/s 14795
step    440 | loss 2.0687 | lr 1.84e-04 | grad 2.38 | tok/s 14338
step    450 | loss 1.9450 | lr 1.84e-04 | grad 1.45 | tok/s 14513
step    460 | loss 1.9918 | lr 1.84e-04 | grad 1.83 | tok/s 14706
step    470 | loss 1.9775 | lr 1.84e-04 | grad 3.31 | tok/s 14605
step    480 | loss 2.0163 | lr 1.84e-04 | grad 3.03 | tok/s 14925
step    490 | loss 2.0097 | lr 1.84e-04 | grad 2.80 | tok/s 14322
step    500 | loss 2.1844 | lr 1.84e-04 | grad 2.14 | tok/s 14562
step    510 | loss 2.0160 | lr 1.84e-04 | grad 2.30 | tok/s 13902
step    520 | loss 1.8648 | lr 1.84e-04 | grad 2.00 | tok/s 14565
step    530 | loss 2.0319 | lr 1.84e-04 | grad 2.14 | tok/s 14322
step    540 | loss 1.9647 | lr 1.84e-04 | grad 1.40 | tok/s 14020
step    550 | loss 1.6600 | lr 1.84e-04 | grad 3.12 | tok/s 14643
step    560 | loss 1.8034 | lr 1.84e-04 | grad 2.61 | tok/s 15093
step    570 | loss 1.6893 | lr 1.84e-04 | grad 1.92 | tok/s 15080
step    580 | loss 1.6245 | lr 1.84e-04 | grad 1.80 | tok/s 15091
step    590 | loss 1.6762 | lr 1.84e-04 | grad 1.93 | tok/s 15087
step    600 | loss 1.6243 | lr 1.84e-04 | grad 1.95 | tok/s 15096
step    610 | loss 1.6158 | lr 1.84e-04 | grad 1.62 | tok/s 15088
step    620 | loss 1.6018 | lr 1.84e-04 | grad 2.28 | tok/s 15026
step    630 | loss 2.0165 | lr 1.84e-04 | grad 6.00 | tok/s 14197
step    640 | loss 2.0323 | lr 1.84e-04 | grad 2.41 | tok/s 14392
step    650 | loss 1.8480 | lr 1.84e-04 | grad 1.85 | tok/s 14385
step    660 | loss 1.9181 | lr 1.84e-04 | grad 2.12 | tok/s 14935
step    670 | loss 1.9462 | lr 1.84e-04 | grad 4.97 | tok/s 14438
step    680 | loss 1.9454 | lr 1.84e-04 | grad 2.48 | tok/s 14195
step    690 | loss 1.9048 | lr 1.84e-04 | grad 1.69 | tok/s 14100
step    700 | loss 1.8000 | lr 1.84e-04 | grad 2.05 | tok/s 14400
step    710 | loss 1.9852 | lr 1.84e-04 | grad 4.59 | tok/s 14179
step    720 | loss 1.6735 | lr 1.84e-04 | grad 2.08 | tok/s 14725
step    730 | loss 1.7830 | lr 1.84e-04 | grad 1.35 | tok/s 14492
step    740 | loss 2.2441 | lr 1.84e-04 | grad 4.31 | tok/s 14898
step    750 | loss 2.0080 | lr 1.84e-04 | grad 2.00 | tok/s 15063
step    760 | loss 1.8307 | lr 1.84e-04 | grad 3.14 | tok/s 14727
step    770 | loss 1.8661 | lr 1.84e-04 | grad 2.20 | tok/s 14495
step    780 | loss 1.7772 | lr 1.84e-04 | grad 2.06 | tok/s 14589
step    790 | loss 2.1253 | lr 1.84e-04 | grad 6.94 | tok/s 14919
step    800 | loss 1.6567 | lr 1.84e-04 | grad 1.61 | tok/s 14649
step    810 | loss 1.6203 | lr 1.84e-04 | grad 3.52 | tok/s 14156
step    820 | loss 1.7613 | lr 1.84e-04 | grad 2.05 | tok/s 14451
step    830 | loss 1.8143 | lr 1.84e-04 | grad 1.50 | tok/s 14256
step    840 | loss 1.9489 | lr 1.84e-04 | grad 1.92 | tok/s 14189
step    850 | loss 1.9031 | lr 1.84e-04 | grad 1.91 | tok/s 14494
step    860 | loss 1.9478 | lr 1.84e-04 | grad 2.45 | tok/s 14734
step    870 | loss 1.9471 | lr 1.84e-04 | grad 1.95 | tok/s 14841
step    880 | loss 1.8558 | lr 1.84e-04 | grad 1.85 | tok/s 14558
step    890 | loss 1.7271 | lr 1.84e-04 | grad 1.34 | tok/s 14488
step    900 | loss 1.7980 | lr 1.84e-04 | grad 1.94 | tok/s 14421
step    910 | loss 1.8557 | lr 1.84e-04 | grad 6.19 | tok/s 14271
step    920 | loss 1.7446 | lr 1.84e-04 | grad 1.91 | tok/s 14428
step    930 | loss 1.6842 | lr 1.84e-04 | grad 1.93 | tok/s 14607
step    940 | loss 1.6518 | lr 1.84e-04 | grad 2.05 | tok/s 14270
step    950 | loss 1.7595 | lr 1.84e-04 | grad 2.44 | tok/s 14046
step    960 | loss 1.6947 | lr 1.84e-04 | grad 1.83 | tok/s 14431
step    970 | loss 1.6838 | lr 1.84e-04 | grad 1.77 | tok/s 14445
step    980 | loss 2.4679 | lr 1.84e-04 | grad 3.47 | tok/s 15026
step    990 | loss 1.8957 | lr 1.84e-04 | grad 1.92 | tok/s 14411
step   1000 | loss 1.8313 | lr 1.84e-04 | grad 2.28 | tok/s 14447
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8313.pt
step   1010 | loss 1.6164 | lr 1.84e-04 | grad 2.83 | tok/s 9562
step   1020 | loss 1.5585 | lr 1.84e-04 | grad 1.59 | tok/s 15148
step   1030 | loss 1.8241 | lr 1.84e-04 | grad 1.98 | tok/s 14398
step   1040 | loss 2.3534 | lr 1.84e-04 | grad 4.03 | tok/s 14732
step   1050 | loss 1.8480 | lr 1.84e-04 | grad 2.98 | tok/s 14835
step   1060 | loss 1.4136 | lr 1.84e-04 | grad 3.06 | tok/s 14642
step   1070 | loss 1.6973 | lr 1.84e-04 | grad 2.84 | tok/s 14593
step   1080 | loss 1.4866 | lr 1.84e-04 | grad 1.88 | tok/s 15115
step   1090 | loss 1.4461 | lr 1.84e-04 | grad 1.79 | tok/s 15104
step   1100 | loss 1.4377 | lr 1.84e-04 | grad 1.55 | tok/s 15105
step   1110 | loss 1.3654 | lr 1.84e-04 | grad 1.72 | tok/s 15091
step   1120 | loss 1.7035 | lr 1.84e-04 | grad 3.47 | tok/s 14677
step   1130 | loss 2.0731 | lr 1.84e-04 | grad 2.08 | tok/s 14835
step   1140 | loss 2.0606 | lr 1.84e-04 | grad 2.34 | tok/s 14998
step   1150 | loss 2.2212 | lr 1.84e-04 | grad 2.67 | tok/s 14530
step   1160 | loss 2.0189 | lr 1.84e-04 | grad 2.94 | tok/s 14316
step   1170 | loss 1.7332 | lr 1.84e-04 | grad 1.92 | tok/s 14153
step   1180 | loss 1.6082 | lr 1.84e-04 | grad 2.73 | tok/s 14881
step   1190 | loss 2.0510 | lr 1.84e-04 | grad 3.41 | tok/s 15009
step   1200 | loss 1.4022 | lr 1.84e-04 | grad 1.75 | tok/s 15082
step   1210 | loss 1.6013 | lr 1.84e-04 | grad 1.98 | tok/s 14054
step   1220 | loss 1.6418 | lr 1.84e-04 | grad 2.02 | tok/s 14781
step   1230 | loss 1.5898 | lr 1.84e-04 | grad 1.41 | tok/s 14805
step   1240 | loss 1.5374 | lr 1.84e-04 | grad 1.92 | tok/s 14871
step   1250 | loss 1.7948 | lr 1.84e-04 | grad 2.69 | tok/s 14648
step   1260 | loss 1.7977 | lr 1.84e-04 | grad 2.42 | tok/s 14948
step   1270 | loss 1.6040 | lr 1.84e-04 | grad 2.27 | tok/s 14539
step   1280 | loss 1.6487 | lr 1.84e-04 | grad 1.88 | tok/s 14371
step   1290 | loss 1.6205 | lr 1.84e-04 | grad 2.48 | tok/s 14410
step   1300 | loss 1.9641 | lr 1.84e-04 | grad 6.91 | tok/s 14165
step   1310 | loss 1.8124 | lr 1.84e-04 | grad 2.05 | tok/s 14738
step   1320 | loss 1.7290 | lr 1.84e-04 | grad 2.11 | tok/s 14767
step   1330 | loss 1.7470 | lr 1.84e-04 | grad 1.96 | tok/s 14625
step   1340 | loss 1.8815 | lr 1.84e-04 | grad 2.31 | tok/s 14306
step   1350 | loss 1.6657 | lr 1.84e-04 | grad 1.54 | tok/s 14620
step   1360 | loss 1.7817 | lr 1.84e-04 | grad 2.11 | tok/s 14093
step   1370 | loss 1.8854 | lr 1.84e-04 | grad 2.30 | tok/s 14853
step   1380 | loss 1.6730 | lr 1.84e-04 | grad 2.03 | tok/s 14190
step   1390 | loss 1.6564 | lr 1.84e-04 | grad 3.17 | tok/s 14879
step   1400 | loss 1.7683 | lr 1.84e-04 | grad 2.09 | tok/s 14352
step   1410 | loss 1.6386 | lr 1.84e-04 | grad 3.98 | tok/s 14034
step   1420 | loss 1.5992 | lr 1.84e-04 | grad 4.81 | tok/s 14951
step   1430 | loss 2.0141 | lr 1.84e-04 | grad 1.98 | tok/s 14408
step   1440 | loss 1.6972 | lr 1.84e-04 | grad 2.23 | tok/s 14787
step   1450 | loss 1.7677 | lr 1.84e-04 | grad 9.44 | tok/s 14761
step   1460 | loss 1.7615 | lr 1.84e-04 | grad 4.69 | tok/s 14338
step   1470 | loss 1.5638 | lr 1.84e-04 | grad 1.98 | tok/s 14011
step   1480 | loss 1.6054 | lr 1.84e-04 | grad 1.74 | tok/s 14807
step   1490 | loss 2.0192 | lr 1.84e-04 | grad 9.56 | tok/s 14560
step   1500 | loss 1.6763 | lr 1.84e-04 | grad 2.03 | tok/s 14604
step   1510 | loss 1.5318 | lr 1.84e-04 | grad 1.87 | tok/s 14587
step   1520 | loss 1.6823 | lr 1.84e-04 | grad 1.91 | tok/s 14487
step   1530 | loss 1.5964 | lr 1.84e-04 | grad 1.75 | tok/s 14716
step   1540 | loss 1.7158 | lr 1.84e-04 | grad 1.98 | tok/s 14850
step   1550 | loss 1.7088 | lr 1.84e-04 | grad 2.36 | tok/s 14555
step   1560 | loss 1.3935 | lr 1.84e-04 | grad 2.14 | tok/s 15110
step   1570 | loss 1.5313 | lr 1.84e-04 | grad 1.52 | tok/s 14700
step   1580 | loss 1.5063 | lr 1.84e-04 | grad 2.30 | tok/s 14651
step   1590 | loss 1.6801 | lr 1.84e-04 | grad 2.02 | tok/s 14340
step   1600 | loss 1.5458 | lr 1.84e-04 | grad 3.05 | tok/s 14869
step   1610 | loss 2.2033 | lr 1.84e-04 | grad 3.41 | tok/s 14757
step   1620 | loss 2.2188 | lr 1.84e-04 | grad 3.69 | tok/s 15126
step   1630 | loss 1.9535 | lr 1.84e-04 | grad 3.17 | tok/s 15116
step   1640 | loss 1.7857 | lr 1.84e-04 | grad 2.66 | tok/s 15116
step   1650 | loss 1.6873 | lr 1.84e-04 | grad 2.62 | tok/s 15123
step   1660 | loss 1.6352 | lr 1.84e-04 | grad 2.64 | tok/s 15121
step   1670 | loss 1.7862 | lr 1.84e-04 | grad 1.77 | tok/s 14641
step   1680 | loss 1.6196 | lr 1.84e-04 | grad 1.93 | tok/s 14514
step   1690 | loss 1.6468 | lr 1.84e-04 | grad 2.00 | tok/s 14082
step   1700 | loss 1.4841 | lr 1.84e-04 | grad 1.40 | tok/s 14813
step   1710 | loss 1.4910 | lr 1.84e-04 | grad 1.83 | tok/s 14648
step   1720 | loss 1.6488 | lr 1.84e-04 | grad 1.83 | tok/s 14458
step   1730 | loss 1.7018 | lr 1.84e-04 | grad 2.77 | tok/s 14659
step   1740 | loss 1.5889 | lr 1.84e-04 | grad 1.88 | tok/s 14981
step   1750 | loss 1.4231 | lr 1.84e-04 | grad 1.38 | tok/s 14490
step   1760 | loss 1.6268 | lr 1.84e-04 | grad 1.90 | tok/s 14288
step   1770 | loss 1.9352 | lr 1.84e-04 | grad 1.66 | tok/s 14829
step   1780 | loss 1.9183 | lr 1.84e-04 | grad 1.65 | tok/s 13784
step   1790 | loss 1.5281 | lr 1.84e-04 | grad 2.27 | tok/s 14331
step   1800 | loss 1.5504 | lr 1.84e-04 | grad 1.74 | tok/s 14433
step   1810 | loss 1.5720 | lr 1.84e-04 | grad 1.54 | tok/s 14531
step   1820 | loss 1.7519 | lr 1.84e-04 | grad 1.67 | tok/s 14470
step   1830 | loss 1.5524 | lr 1.84e-04 | grad 1.45 | tok/s 13965
step   1840 | loss 1.6203 | lr 1.84e-04 | grad 1.98 | tok/s 14571
step   1850 | loss 1.6555 | lr 1.84e-04 | grad 1.59 | tok/s 14242
step   1860 | loss 1.6310 | lr 1.84e-04 | grad 1.73 | tok/s 14493
step   1870 | loss 1.6062 | lr 1.84e-04 | grad 1.98 | tok/s 14692
step   1880 | loss 1.7179 | lr 1.84e-04 | grad 2.59 | tok/s 14716
step   1890 | loss 1.4265 | lr 1.84e-04 | grad 2.11 | tok/s 15095
step   1900 | loss 1.3588 | lr 1.84e-04 | grad 1.66 | tok/s 15114
step   1910 | loss 1.3330 | lr 1.84e-04 | grad 2.23 | tok/s 15118
step   1920 | loss 1.3184 | lr 1.84e-04 | grad 1.78 | tok/s 15110
step   1930 | loss 1.4002 | lr 1.84e-04 | grad 1.75 | tok/s 14891
step   1940 | loss 1.7455 | lr 1.84e-04 | grad 2.47 | tok/s 14459
step   1950 | loss 1.6482 | lr 1.84e-04 | grad 2.91 | tok/s 14126
step   1960 | loss 1.6922 | lr 1.84e-04 | grad 2.75 | tok/s 14306
step   1970 | loss 1.7298 | lr 1.84e-04 | grad 1.70 | tok/s 14718
step   1980 | loss 1.6827 | lr 1.84e-04 | grad 2.69 | tok/s 14369
step   1990 | loss 1.8052 | lr 1.84e-04 | grad 3.34 | tok/s 14633
step   2000 | loss 1.3522 | lr 1.84e-04 | grad 2.08 | tok/s 15097
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3522.pt
step   2010 | loss 1.5475 | lr 1.84e-04 | grad 1.55 | tok/s 9218
step   2020 | loss 1.5800 | lr 1.84e-04 | grad 1.81 | tok/s 14395
step   2030 | loss 1.9456 | lr 1.84e-04 | grad 2.41 | tok/s 14247
step   2040 | loss 1.5913 | lr 1.84e-04 | grad 2.34 | tok/s 14583
step   2050 | loss 1.5205 | lr 1.84e-04 | grad 2.39 | tok/s 14509
step   2060 | loss 1.8673 | lr 1.84e-04 | grad 5.66 | tok/s 14631
step   2070 | loss 1.3068 | lr 1.84e-04 | grad 2.25 | tok/s 14674
step   2080 | loss 1.4474 | lr 1.84e-04 | grad 1.65 | tok/s 14102
step   2090 | loss 1.7491 | lr 1.84e-04 | grad 2.70 | tok/s 14921
step   2100 | loss 1.9009 | lr 1.84e-04 | grad 2.05 | tok/s 15042
step   2110 | loss 1.5811 | lr 1.84e-04 | grad 2.75 | tok/s 14282
step   2120 | loss 2.5244 | lr 1.84e-04 | grad 2.92 | tok/s 14584
step   2130 | loss 1.6055 | lr 1.84e-04 | grad 2.53 | tok/s 14243
step   2140 | loss 1.7697 | lr 1.84e-04 | grad 2.22 | tok/s 14732
step   2150 | loss 1.9352 | lr 1.84e-04 | grad 2.95 | tok/s 14655
step   2160 | loss 1.6510 | lr 1.84e-04 | grad 1.65 | tok/s 14587
step   2170 | loss 1.6396 | lr 1.84e-04 | grad 1.52 | tok/s 14640
step   2180 | loss 1.3975 | lr 1.84e-04 | grad 2.31 | tok/s 15056
step   2190 | loss 1.6987 | lr 1.84e-04 | grad 2.97 | tok/s 14429
step   2200 | loss 1.4507 | lr 1.84e-04 | grad 1.56 | tok/s 15125
step   2210 | loss 1.7262 | lr 1.84e-04 | grad 1.50 | tok/s 14603
step   2220 | loss 1.6063 | lr 1.84e-04 | grad 1.66 | tok/s 14125
step   2230 | loss 1.7758 | lr 1.84e-04 | grad 1.69 | tok/s 14439
step   2240 | loss 1.4825 | lr 1.84e-04 | grad 2.39 | tok/s 14672
step   2250 | loss 1.5199 | lr 1.84e-04 | grad 1.80 | tok/s 14395
step   2260 | loss 1.7093 | lr 1.84e-04 | grad 1.69 | tok/s 14656
step   2270 | loss 1.9056 | lr 1.84e-04 | grad 1.77 | tok/s 14072
step   2280 | loss 1.5507 | lr 1.84e-04 | grad 1.59 | tok/s 14491
step   2290 | loss 1.4042 | lr 1.84e-04 | grad 2.16 | tok/s 14231
step   2300 | loss 1.8581 | lr 1.84e-04 | grad 2.92 | tok/s 14334
step   2310 | loss 1.5298 | lr 1.84e-04 | grad 1.80 | tok/s 14700
step   2320 | loss 1.5285 | lr 1.84e-04 | grad 2.42 | tok/s 15118
step   2330 | loss 1.4570 | lr 1.84e-04 | grad 1.87 | tok/s 15123
step   2340 | loss 1.4200 | lr 1.84e-04 | grad 2.03 | tok/s 15123
step   2350 | loss 1.3967 | lr 1.84e-04 | grad 1.66 | tok/s 15116
step   2360 | loss 1.3433 | lr 1.84e-04 | grad 1.88 | tok/s 15110
step   2370 | loss 1.3348 | lr 1.84e-04 | grad 2.20 | tok/s 15111
step   2380 | loss 1.3130 | lr 1.84e-04 | grad 1.95 | tok/s 15122
step   2390 | loss 1.3348 | lr 1.84e-04 | grad 1.88 | tok/s 15115
step   2400 | loss 1.2913 | lr 1.84e-04 | grad 1.95 | tok/s 15117
step   2410 | loss 1.5084 | lr 1.84e-04 | grad 6.84 | tok/s 14916
step   2420 | loss 1.4365 | lr 1.84e-04 | grad 0.97 | tok/s 14827
step   2430 | loss 1.3383 | lr 1.84e-04 | grad 2.16 | tok/s 14253
step   2440 | loss 1.5637 | lr 1.84e-04 | grad 1.87 | tok/s 14120
step   2450 | loss 1.5011 | lr 1.84e-04 | grad 2.14 | tok/s 14654
step   2460 | loss 1.7030 | lr 1.84e-04 | grad 1.67 | tok/s 14834
step   2470 | loss 1.4851 | lr 1.84e-04 | grad 1.63 | tok/s 14434
step   2480 | loss 1.6804 | lr 1.84e-04 | grad 3.16 | tok/s 14817
step   2490 | loss 1.7105 | lr 1.84e-04 | grad 2.00 | tok/s 14389
step   2500 | loss 1.7792 | lr 1.84e-04 | grad 1.75 | tok/s 14754
step   2510 | loss 1.6219 | lr 1.84e-04 | grad 1.55 | tok/s 14621
step   2520 | loss 1.5645 | lr 1.84e-04 | grad 1.75 | tok/s 14389
step   2530 | loss 1.7141 | lr 1.84e-04 | grad 2.41 | tok/s 14726
step   2540 | loss 1.5992 | lr 1.84e-04 | grad 6.34 | tok/s 14053
step   2550 | loss 1.4664 | lr 1.84e-04 | grad 2.12 | tok/s 14524
step   2560 | loss 1.7599 | lr 1.84e-04 | grad 1.74 | tok/s 14291
step   2570 | loss 1.5148 | lr 1.84e-04 | grad 1.89 | tok/s 14464
step   2580 | loss 1.8344 | lr 1.84e-04 | grad 2.22 | tok/s 14401
step   2590 | loss 1.4760 | lr 1.84e-04 | grad 1.78 | tok/s 14116
step   2600 | loss 1.6181 | lr 1.84e-04 | grad 1.55 | tok/s 14498
step   2610 | loss 1.5640 | lr 1.84e-04 | grad 1.57 | tok/s 14855
step   2620 | loss 1.7283 | lr 1.84e-04 | grad 2.00 | tok/s 14372
step   2630 | loss 1.3371 | lr 1.84e-04 | grad 2.11 | tok/s 14877
step   2640 | loss 1.5150 | lr 1.84e-04 | grad 1.76 | tok/s 14340
step   2650 | loss 1.4092 | lr 1.84e-04 | grad 2.19 | tok/s 14615
step   2660 | loss 1.4948 | lr 1.84e-04 | grad 1.52 | tok/s 14721
step   2670 | loss 1.4592 | lr 1.84e-04 | grad 1.62 | tok/s 14645
step   2680 | loss 1.5536 | lr 1.84e-04 | grad 1.64 | tok/s 14380
step   2690 | loss 1.4474 | lr 1.84e-04 | grad 1.99 | tok/s 14124
step   2700 | loss 1.5591 | lr 1.84e-04 | grad 1.70 | tok/s 14598
step   2710 | loss 1.8740 | lr 1.84e-04 | grad 2.12 | tok/s 14474
step   2720 | loss 1.5570 | lr 1.84e-04 | grad 1.62 | tok/s 14712
step   2730 | loss 1.4825 | lr 1.84e-04 | grad 3.20 | tok/s 14483
step   2740 | loss 1.5937 | lr 1.84e-04 | grad 2.00 | tok/s 14441
step   2750 | loss 1.5962 | lr 1.84e-04 | grad 2.05 | tok/s 14677
step   2760 | loss 1.3759 | lr 1.84e-04 | grad 1.83 | tok/s 14814
step   2770 | loss 1.6260 | lr 1.84e-04 | grad 1.77 | tok/s 14583
step   2780 | loss 1.4721 | lr 1.84e-04 | grad 3.00 | tok/s 14839
step   2790 | loss 1.6058 | lr 1.84e-04 | grad 4.75 | tok/s 14425
step   2800 | loss 1.5911 | lr 1.84e-04 | grad 2.45 | tok/s 14035
step   2810 | loss 1.5976 | lr 1.84e-04 | grad 2.02 | tok/s 14591
step   2820 | loss 1.4979 | lr 1.84e-04 | grad 2.72 | tok/s 13662
step   2830 | loss 1.3931 | lr 1.84e-04 | grad 1.44 | tok/s 14495
step   2840 | loss 1.3341 | lr 1.84e-04 | grad 1.98 | tok/s 14315
step   2850 | loss 1.3329 | lr 1.84e-04 | grad 1.53 | tok/s 15138
step   2860 | loss 1.3592 | lr 1.84e-04 | grad 2.59 | tok/s 14726
step   2870 | loss 1.5454 | lr 1.84e-04 | grad 1.76 | tok/s 14344
step   2880 | loss 1.5531 | lr 1.84e-04 | grad 2.25 | tok/s 14210
step   2890 | loss 1.7235 | lr 1.84e-04 | grad 5.41 | tok/s 14645
step   2900 | loss 1.5680 | lr 1.84e-04 | grad 3.45 | tok/s 14180
step   2910 | loss 1.5762 | lr 1.84e-04 | grad 1.91 | tok/s 14900
step   2920 | loss 1.3923 | lr 1.84e-04 | grad 4.75 | tok/s 14359
step   2930 | loss 1.5693 | lr 1.84e-04 | grad 2.00 | tok/s 14797
step   2940 | loss 1.4281 | lr 1.84e-04 | grad 1.54 | tok/s 14498
step   2950 | loss 1.8829 | lr 1.84e-04 | grad 1.52 | tok/s 14619
step   2960 | loss 1.6936 | lr 1.84e-04 | grad 3.47 | tok/s 14102
step   2970 | loss 1.6099 | lr 1.84e-04 | grad 1.67 | tok/s 14586
step   2980 | loss 1.4970 | lr 1.84e-04 | grad 1.43 | tok/s 14769
step   2990 | loss 1.7058 | lr 1.84e-04 | grad 2.25 | tok/s 14677
step   3000 | loss 1.4711 | lr 1.84e-04 | grad 2.38 | tok/s 14738
  >>> saved checkpoint: checkpoint_step_003000_loss_1.4711.pt
step   3010 | loss 1.6181 | lr 1.84e-04 | grad 4.38 | tok/s 9687
step   3020 | loss 1.4785 | lr 1.84e-04 | grad 1.82 | tok/s 14473
step   3030 | loss 1.4442 | lr 1.84e-04 | grad 1.94 | tok/s 14184
step   3040 | loss 1.4658 | lr 1.84e-04 | grad 2.00 | tok/s 14672
step   3050 | loss 1.3397 | lr 1.84e-04 | grad 1.40 | tok/s 15071
step   3060 | loss 1.6492 | lr 1.84e-04 | grad 3.19 | tok/s 14804
step   3070 | loss 2.1772 | lr 1.84e-04 | grad 8.94 | tok/s 14518
step   3080 | loss 1.6386 | lr 1.84e-04 | grad 2.88 | tok/s 14624
step   3090 | loss 1.5268 | lr 1.84e-04 | grad 2.25 | tok/s 14092
step   3100 | loss 1.4146 | lr 1.84e-04 | grad 1.80 | tok/s 14684
step   3110 | loss 1.6046 | lr 1.84e-04 | grad 2.41 | tok/s 15010
step   3120 | loss 1.5404 | lr 1.84e-04 | grad 1.77 | tok/s 14389
step   3130 | loss 1.4708 | lr 1.84e-04 | grad 2.22 | tok/s 14204
step   3140 | loss 1.3847 | lr 1.84e-04 | grad 3.09 | tok/s 14474
step   3150 | loss 1.4705 | lr 1.84e-04 | grad 1.96 | tok/s 13807
step   3160 | loss 1.6286 | lr 1.84e-04 | grad 1.58 | tok/s 15069
step   3170 | loss 1.3540 | lr 1.84e-04 | grad 1.42 | tok/s 14915
step   3180 | loss 1.3832 | lr 1.84e-04 | grad 3.22 | tok/s 14823
step   3190 | loss 1.6745 | lr 1.84e-04 | grad 3.67 | tok/s 14802
step   3200 | loss 1.3905 | lr 1.84e-04 | grad 1.42 | tok/s 14587
step   3210 | loss 1.5858 | lr 1.84e-04 | grad 2.75 | tok/s 14671
step   3220 | loss 1.3927 | lr 1.84e-04 | grad 2.31 | tok/s 15010
step   3230 | loss 1.5080 | lr 1.84e-04 | grad 1.64 | tok/s 14126
step   3240 | loss 1.4249 | lr 1.84e-04 | grad 1.64 | tok/s 14031
step   3250 | loss 1.4749 | lr 1.84e-04 | grad 3.20 | tok/s 14307
step   3260 | loss 1.5378 | lr 1.84e-04 | grad 1.58 | tok/s 14538
step   3270 | loss 1.4550 | lr 1.84e-04 | grad 1.95 | tok/s 14190

Training complete! Final step: 3278
