Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_185612/eval_30/level42_100m_20260202_202723
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 313,163,392 parameters
Using schedule-free AdamW (lr=0.00011190458792827088)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 5.7716 | lr 1.12e-04 | grad 20.25 | tok/s 15657
step     20 | loss 4.5956 | lr 1.12e-04 | grad 11.62 | tok/s 29608
step     30 | loss 4.4410 | lr 1.12e-04 | grad 20.50 | tok/s 29895
step     40 | loss 3.2643 | lr 1.12e-04 | grad 9.06 | tok/s 29690
step     50 | loss 2.6087 | lr 1.12e-04 | grad 8.62 | tok/s 29507
step     60 | loss 3.3546 | lr 1.12e-04 | grad 12.44 | tok/s 28451
step     70 | loss 2.8275 | lr 1.12e-04 | grad 8.06 | tok/s 28835
step     80 | loss 2.7088 | lr 1.12e-04 | grad 5.66 | tok/s 28661
step     90 | loss 2.7172 | lr 1.12e-04 | grad 5.06 | tok/s 27935
step    100 | loss 2.2934 | lr 1.12e-04 | grad 5.91 | tok/s 28931
step    110 | loss 2.5123 | lr 1.12e-04 | grad 4.09 | tok/s 27930
step    120 | loss 2.5077 | lr 1.12e-04 | grad 4.53 | tok/s 28106
step    130 | loss 2.3125 | lr 1.12e-04 | grad 4.66 | tok/s 28794
step    140 | loss 2.1201 | lr 1.12e-04 | grad 4.59 | tok/s 27437
step    150 | loss 2.1825 | lr 1.12e-04 | grad 3.08 | tok/s 27675
step    160 | loss 2.1616 | lr 1.12e-04 | grad 3.22 | tok/s 27760
step    170 | loss 2.3288 | lr 1.12e-04 | grad 5.53 | tok/s 28372
step    180 | loss 2.0719 | lr 1.12e-04 | grad 4.06 | tok/s 28346
step    190 | loss 1.8771 | lr 1.12e-04 | grad 3.05 | tok/s 29267
step    200 | loss 2.0210 | lr 1.12e-04 | grad 2.91 | tok/s 28396
step    210 | loss 2.1488 | lr 1.12e-04 | grad 3.30 | tok/s 28850
step    220 | loss 2.0740 | lr 1.12e-04 | grad 3.69 | tok/s 28286
step    230 | loss 1.9462 | lr 1.12e-04 | grad 3.36 | tok/s 28220
step    240 | loss 2.0558 | lr 1.12e-04 | grad 4.41 | tok/s 28846
step    250 | loss 2.1010 | lr 1.12e-04 | grad 3.05 | tok/s 28129
step    260 | loss 1.9188 | lr 1.12e-04 | grad 3.53 | tok/s 27531
step    270 | loss 1.9540 | lr 1.12e-04 | grad 5.59 | tok/s 28173
step    280 | loss 1.7873 | lr 1.12e-04 | grad 3.98 | tok/s 29039
step    290 | loss 1.6697 | lr 1.12e-04 | grad 3.78 | tok/s 29419
step    300 | loss 1.6891 | lr 1.12e-04 | grad 3.95 | tok/s 29377
step    310 | loss 1.7723 | lr 1.12e-04 | grad 4.72 | tok/s 29089
step    320 | loss 1.9234 | lr 1.12e-04 | grad 2.59 | tok/s 27821
step    330 | loss 1.9505 | lr 1.12e-04 | grad 5.50 | tok/s 28685
step    340 | loss 1.9016 | lr 1.12e-04 | grad 3.95 | tok/s 27740
step    350 | loss 1.8766 | lr 1.12e-04 | grad 3.44 | tok/s 27592
step    360 | loss 1.7989 | lr 1.12e-04 | grad 3.38 | tok/s 28509
step    370 | loss 2.2107 | lr 1.12e-04 | grad 10.62 | tok/s 28600
step    380 | loss 1.8551 | lr 1.12e-04 | grad 3.17 | tok/s 29042
step    390 | loss 1.8193 | lr 1.12e-04 | grad 2.84 | tok/s 28339
step    400 | loss 1.8227 | lr 1.12e-04 | grad 2.78 | tok/s 28568
step    410 | loss 1.8296 | lr 1.12e-04 | grad 3.34 | tok/s 27676
step    420 | loss 1.8988 | lr 1.12e-04 | grad 3.83 | tok/s 27872
step    430 | loss 2.0013 | lr 1.12e-04 | grad 10.69 | tok/s 28729
step    440 | loss 1.8685 | lr 1.12e-04 | grad 2.98 | tok/s 28206
step    450 | loss 1.7950 | lr 1.12e-04 | grad 2.95 | tok/s 28195
step    460 | loss 1.8221 | lr 1.12e-04 | grad 3.89 | tok/s 28239
step    470 | loss 1.7042 | lr 1.12e-04 | grad 4.03 | tok/s 27455
step    480 | loss 1.6905 | lr 1.12e-04 | grad 2.80 | tok/s 28051
step    490 | loss 2.2644 | lr 1.12e-04 | grad 4.50 | tok/s 28909
step    500 | loss 1.7563 | lr 1.12e-04 | grad 8.31 | tok/s 28240
step    510 | loss 1.5879 | lr 1.12e-04 | grad 3.44 | tok/s 29082
step    520 | loss 2.1794 | lr 1.12e-04 | grad 4.47 | tok/s 28275
step    530 | loss 1.6166 | lr 1.12e-04 | grad 3.22 | tok/s 28717
step    540 | loss 1.6267 | lr 1.12e-04 | grad 3.52 | tok/s 28873
step    550 | loss 1.4812 | lr 1.12e-04 | grad 3.41 | tok/s 29340
step    560 | loss 1.6338 | lr 1.12e-04 | grad 8.50 | tok/s 29015
step    570 | loss 2.0730 | lr 1.12e-04 | grad 3.64 | tok/s 29030
step    580 | loss 2.1944 | lr 1.12e-04 | grad 5.97 | tok/s 28014
step    590 | loss 1.6811 | lr 1.12e-04 | grad 3.91 | tok/s 28294
step    600 | loss 1.7554 | lr 1.12e-04 | grad 3.53 | tok/s 29342
step    610 | loss 1.6545 | lr 1.12e-04 | grad 3.34 | tok/s 27903
step    620 | loss 1.6003 | lr 1.12e-04 | grad 3.30 | tok/s 28936
step    630 | loss 1.8754 | lr 1.12e-04 | grad 3.61 | tok/s 28885
step    640 | loss 1.6784 | lr 1.12e-04 | grad 4.12 | tok/s 28221
step    650 | loss 1.8562 | lr 1.12e-04 | grad 17.62 | tok/s 27852
step    660 | loss 1.8032 | lr 1.12e-04 | grad 4.44 | tok/s 28713
step    670 | loss 1.7868 | lr 1.12e-04 | grad 3.77 | tok/s 28177
step    680 | loss 1.7591 | lr 1.12e-04 | grad 4.75 | tok/s 27969
step    690 | loss 1.8400 | lr 1.12e-04 | grad 4.97 | tok/s 28190
step    700 | loss 1.7377 | lr 1.12e-04 | grad 4.44 | tok/s 28489
step    710 | loss 1.7744 | lr 1.12e-04 | grad 9.44 | tok/s 28200
step    720 | loss 1.8239 | lr 1.12e-04 | grad 3.73 | tok/s 28402
step    730 | loss 1.8090 | lr 1.12e-04 | grad 7.28 | tok/s 28319
step    740 | loss 1.6032 | lr 1.12e-04 | grad 3.19 | tok/s 27997
step    750 | loss 1.8475 | lr 1.12e-04 | grad 2.80 | tok/s 28371
step    760 | loss 1.6459 | lr 1.12e-04 | grad 2.95 | tok/s 28316
step    770 | loss 1.7083 | lr 1.12e-04 | grad 3.50 | tok/s 28779
step    780 | loss 1.5837 | lr 1.12e-04 | grad 2.98 | tok/s 28873
step    790 | loss 1.6139 | lr 1.12e-04 | grad 8.06 | tok/s 28524
step    800 | loss 1.5873 | lr 1.12e-04 | grad 3.25 | tok/s 28317
step    810 | loss 2.3308 | lr 1.12e-04 | grad 6.12 | tok/s 29220
step    820 | loss 1.9951 | lr 1.12e-04 | grad 4.50 | tok/s 29373
step    830 | loss 1.7934 | lr 1.12e-04 | grad 4.19 | tok/s 29406
step    840 | loss 1.7780 | lr 1.12e-04 | grad 2.88 | tok/s 28018
step    850 | loss 1.5962 | lr 1.12e-04 | grad 2.58 | tok/s 28397
step    860 | loss 1.6212 | lr 1.12e-04 | grad 3.98 | tok/s 28294
step    870 | loss 1.7018 | lr 1.12e-04 | grad 3.81 | tok/s 28821
step    880 | loss 1.6095 | lr 1.12e-04 | grad 5.34 | tok/s 28000
step    890 | loss 1.9598 | lr 1.12e-04 | grad 2.88 | tok/s 27816
step    900 | loss 1.5739 | lr 1.12e-04 | grad 4.16 | tok/s 27856
step    910 | loss 1.6653 | lr 1.12e-04 | grad 2.58 | tok/s 28086
step    920 | loss 1.6392 | lr 1.12e-04 | grad 3.34 | tok/s 27966
step    930 | loss 1.6900 | lr 1.12e-04 | grad 3.64 | tok/s 27952
step    940 | loss 1.6637 | lr 1.12e-04 | grad 3.73 | tok/s 28620
step    950 | loss 1.4633 | lr 1.12e-04 | grad 3.36 | tok/s 29383
step    960 | loss 1.3898 | lr 1.12e-04 | grad 2.98 | tok/s 29376
step    970 | loss 1.6044 | lr 1.12e-04 | grad 3.41 | tok/s 28365
step    980 | loss 1.7397 | lr 1.12e-04 | grad 4.25 | tok/s 27828
step    990 | loss 1.7382 | lr 1.12e-04 | grad 5.59 | tok/s 28287
step   1000 | loss 1.6280 | lr 1.12e-04 | grad 4.31 | tok/s 28972
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6280.pt
step   1010 | loss 1.5823 | lr 1.12e-04 | grad 2.77 | tok/s 19646
step   1020 | loss 1.7808 | lr 1.12e-04 | grad 5.00 | tok/s 28158
step   1030 | loss 1.7411 | lr 1.12e-04 | grad 4.22 | tok/s 28218
step   1040 | loss 1.3975 | lr 1.12e-04 | grad 2.70 | tok/s 28058
step   1050 | loss 2.0302 | lr 1.12e-04 | grad 2.80 | tok/s 28787
step   1060 | loss 2.1263 | lr 1.12e-04 | grad 5.81 | tok/s 28210
step   1070 | loss 1.7092 | lr 1.12e-04 | grad 4.75 | tok/s 27843
step   1080 | loss 1.8465 | lr 1.12e-04 | grad 2.50 | tok/s 28616
step   1090 | loss 1.6291 | lr 1.12e-04 | grad 6.41 | tok/s 29087
step   1100 | loss 1.6068 | lr 1.12e-04 | grad 4.44 | tok/s 28683
step   1110 | loss 1.6723 | lr 1.12e-04 | grad 2.95 | tok/s 27728
step   1120 | loss 1.6732 | lr 1.12e-04 | grad 3.67 | tok/s 28326
step   1130 | loss 1.7120 | lr 1.12e-04 | grad 6.19 | tok/s 28237
step   1140 | loss 1.6708 | lr 1.12e-04 | grad 5.84 | tok/s 27887
step   1150 | loss 1.7458 | lr 1.12e-04 | grad 3.89 | tok/s 27619
step   1160 | loss 1.5328 | lr 1.12e-04 | grad 3.02 | tok/s 29114
step   1170 | loss 1.5096 | lr 1.12e-04 | grad 2.92 | tok/s 29367
step   1180 | loss 1.4349 | lr 1.12e-04 | grad 3.16 | tok/s 29353
step   1190 | loss 1.3927 | lr 1.12e-04 | grad 2.75 | tok/s 29355
step   1200 | loss 1.3806 | lr 1.12e-04 | grad 2.67 | tok/s 29341
step   1210 | loss 1.4698 | lr 1.12e-04 | grad 1.72 | tok/s 28902
step   1220 | loss 1.5526 | lr 1.12e-04 | grad 2.70 | tok/s 27575
step   1230 | loss 1.6318 | lr 1.12e-04 | grad 2.75 | tok/s 28592
step   1240 | loss 1.6242 | lr 1.12e-04 | grad 5.53 | tok/s 28353
step   1250 | loss 1.8100 | lr 1.12e-04 | grad 3.64 | tok/s 28214
step   1260 | loss 1.6519 | lr 1.12e-04 | grad 3.59 | tok/s 28175
step   1270 | loss 1.6677 | lr 1.12e-04 | grad 3.64 | tok/s 27800
step   1280 | loss 1.6294 | lr 1.12e-04 | grad 3.27 | tok/s 27964
step   1290 | loss 1.7201 | lr 1.12e-04 | grad 2.83 | tok/s 27820
step   1300 | loss 1.6186 | lr 1.12e-04 | grad 4.50 | tok/s 28037
step   1310 | loss 1.6671 | lr 1.12e-04 | grad 3.38 | tok/s 28486
step   1320 | loss 1.4798 | lr 1.12e-04 | grad 2.89 | tok/s 28044
step   1330 | loss 1.4781 | lr 1.12e-04 | grad 2.80 | tok/s 28713
step   1340 | loss 1.6093 | lr 1.12e-04 | grad 2.47 | tok/s 28107
step   1350 | loss 1.5407 | lr 1.12e-04 | grad 3.64 | tok/s 27974
step   1360 | loss 1.7445 | lr 1.12e-04 | grad 3.03 | tok/s 27972
step   1370 | loss 1.5777 | lr 1.12e-04 | grad 4.12 | tok/s 28001
step   1380 | loss 1.5561 | lr 1.12e-04 | grad 2.72 | tok/s 28754
step   1390 | loss 1.6182 | lr 1.12e-04 | grad 3.64 | tok/s 28782
step   1400 | loss 1.6174 | lr 1.12e-04 | grad 6.12 | tok/s 27643
step   1410 | loss 1.5574 | lr 1.12e-04 | grad 3.09 | tok/s 27200
step   1420 | loss 1.4093 | lr 1.12e-04 | grad 2.91 | tok/s 28175
step   1430 | loss 1.4059 | lr 1.12e-04 | grad 2.98 | tok/s 28901
step   1440 | loss 1.6085 | lr 1.12e-04 | grad 4.69 | tok/s 27570
step   1450 | loss 1.6484 | lr 1.12e-04 | grad 3.42 | tok/s 28134
step   1460 | loss 1.5103 | lr 1.12e-04 | grad 7.97 | tok/s 28376
step   1470 | loss 1.5922 | lr 1.12e-04 | grad 4.31 | tok/s 28202
step   1480 | loss 1.8339 | lr 1.12e-04 | grad 5.03 | tok/s 27791
step   1490 | loss 1.5937 | lr 1.12e-04 | grad 3.38 | tok/s 28718
step   1500 | loss 1.6140 | lr 1.12e-04 | grad 3.22 | tok/s 28527
step   1510 | loss 1.5554 | lr 1.12e-04 | grad 2.83 | tok/s 28290
step   1520 | loss 1.5283 | lr 1.12e-04 | grad 2.92 | tok/s 27928
step   1530 | loss 1.4845 | lr 1.12e-04 | grad 7.16 | tok/s 29030
step   1540 | loss 2.0119 | lr 1.12e-04 | grad 3.81 | tok/s 28214
step   1550 | loss 1.5597 | lr 1.12e-04 | grad 3.41 | tok/s 27863
step   1560 | loss 1.6898 | lr 1.12e-04 | grad 3.97 | tok/s 28581
step   1570 | loss 1.4543 | lr 1.12e-04 | grad 3.61 | tok/s 27996
step   1580 | loss 1.5953 | lr 1.12e-04 | grad 3.69 | tok/s 27758
step   1590 | loss 1.4078 | lr 1.12e-04 | grad 2.52 | tok/s 29029
step   1600 | loss 1.5881 | lr 1.12e-04 | grad 3.88 | tok/s 28426
step   1610 | loss 1.5362 | lr 1.12e-04 | grad 2.80 | tok/s 28772
step   1620 | loss 1.4950 | lr 1.12e-04 | grad 3.00 | tok/s 27550
step   1630 | loss 1.5615 | lr 1.12e-04 | grad 6.84 | tok/s 27825
step   1640 | loss 1.5791 | lr 1.12e-04 | grad 4.66 | tok/s 27856
step   1650 | loss 1.5703 | lr 1.12e-04 | grad 4.94 | tok/s 28780
step   1660 | loss 1.8316 | lr 1.12e-04 | grad 4.97 | tok/s 28453
step   1670 | loss 1.4372 | lr 1.12e-04 | grad 3.08 | tok/s 28152
step   1680 | loss 1.7026 | lr 1.12e-04 | grad 3.67 | tok/s 28499
step   1690 | loss 1.6404 | lr 1.12e-04 | grad 3.23 | tok/s 28223
step   1700 | loss 1.5342 | lr 1.12e-04 | grad 5.19 | tok/s 28001
step   1710 | loss 1.6705 | lr 1.12e-04 | grad 2.52 | tok/s 28072
step   1720 | loss 1.5718 | lr 1.12e-04 | grad 3.02 | tok/s 28018
step   1730 | loss 1.5538 | lr 1.12e-04 | grad 3.75 | tok/s 27709
step   1740 | loss 1.6666 | lr 1.12e-04 | grad 3.31 | tok/s 28097
step   1750 | loss 1.7091 | lr 1.12e-04 | grad 4.62 | tok/s 27937
step   1760 | loss 1.5360 | lr 1.12e-04 | grad 4.28 | tok/s 27804
step   1770 | loss 1.6804 | lr 1.12e-04 | grad 5.72 | tok/s 27744
step   1780 | loss 1.5337 | lr 1.12e-04 | grad 5.03 | tok/s 28482
step   1790 | loss 1.4429 | lr 1.12e-04 | grad 7.91 | tok/s 28634
step   1800 | loss 1.4579 | lr 1.12e-04 | grad 2.52 | tok/s 27717
step   1810 | loss 1.5329 | lr 1.12e-04 | grad 2.31 | tok/s 27839
step   1820 | loss 1.6374 | lr 1.12e-04 | grad 3.44 | tok/s 27943
step   1830 | loss 1.6358 | lr 1.12e-04 | grad 4.09 | tok/s 27736
step   1840 | loss 1.4619 | lr 1.12e-04 | grad 3.28 | tok/s 28092
step   1850 | loss 1.4806 | lr 1.12e-04 | grad 3.56 | tok/s 28766
step   1860 | loss 1.5366 | lr 1.12e-04 | grad 2.67 | tok/s 28545
step   1870 | loss 1.6659 | lr 1.12e-04 | grad 5.38 | tok/s 28262
step   1880 | loss 1.5066 | lr 1.12e-04 | grad 3.14 | tok/s 28354
step   1890 | loss 1.6454 | lr 1.12e-04 | grad 2.75 | tok/s 28152
step   1900 | loss 1.3979 | lr 1.12e-04 | grad 3.86 | tok/s 28531
step   1910 | loss 1.4589 | lr 1.12e-04 | grad 2.70 | tok/s 28517
step   1920 | loss 1.5504 | lr 1.12e-04 | grad 3.41 | tok/s 29057
step   1930 | loss 1.5062 | lr 1.12e-04 | grad 4.19 | tok/s 28405
step   1940 | loss 1.6211 | lr 1.12e-04 | grad 4.69 | tok/s 28595
step   1950 | loss 1.4306 | lr 1.12e-04 | grad 3.45 | tok/s 28120
step   1960 | loss 1.6922 | lr 1.12e-04 | grad 8.88 | tok/s 28461
step   1970 | loss 1.4779 | lr 1.12e-04 | grad 5.16 | tok/s 28026
step   1980 | loss 1.4652 | lr 1.12e-04 | grad 3.19 | tok/s 28826
step   1990 | loss 1.2307 | lr 1.12e-04 | grad 3.34 | tok/s 29336
step   2000 | loss 1.2990 | lr 1.12e-04 | grad 10.12 | tok/s 29344
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2990.pt
step   2010 | loss 1.4062 | lr 1.12e-04 | grad 2.42 | tok/s 20754
step   2020 | loss 1.2976 | lr 1.12e-04 | grad 2.80 | tok/s 29411
step   2030 | loss 1.5473 | lr 1.12e-04 | grad 4.06 | tok/s 28170
step   2040 | loss 1.5345 | lr 1.12e-04 | grad 2.77 | tok/s 28599
step   2050 | loss 1.5771 | lr 1.12e-04 | grad 6.16 | tok/s 28344
step   2060 | loss 1.5803 | lr 1.12e-04 | grad 2.62 | tok/s 28319
step   2070 | loss 1.4897 | lr 1.12e-04 | grad 3.03 | tok/s 28018
step   2080 | loss 1.3704 | lr 1.12e-04 | grad 3.08 | tok/s 28574
step   2090 | loss 1.4241 | lr 1.12e-04 | grad 3.89 | tok/s 28109
step   2100 | loss 1.2755 | lr 1.12e-04 | grad 3.86 | tok/s 28551
step   2110 | loss 1.5769 | lr 1.12e-04 | grad 3.31 | tok/s 27935
step   2120 | loss 1.6351 | lr 1.12e-04 | grad 5.66 | tok/s 28247
step   2130 | loss 1.5772 | lr 1.12e-04 | grad 6.50 | tok/s 27928
step   2140 | loss 1.6842 | lr 1.12e-04 | grad 3.98 | tok/s 28432
step   2150 | loss 1.4911 | lr 1.12e-04 | grad 4.25 | tok/s 28210
step   2160 | loss 1.6977 | lr 1.12e-04 | grad 4.78 | tok/s 28897
step   2170 | loss 1.3115 | lr 1.12e-04 | grad 2.84 | tok/s 28994
step   2180 | loss 1.2718 | lr 1.12e-04 | grad 2.53 | tok/s 29351
step   2190 | loss 1.2644 | lr 1.12e-04 | grad 2.48 | tok/s 29347
step   2200 | loss 1.3986 | lr 1.12e-04 | grad 9.81 | tok/s 28751
step   2210 | loss 1.5170 | lr 1.12e-04 | grad 6.72 | tok/s 28825
step   2220 | loss 1.5467 | lr 1.12e-04 | grad 5.09 | tok/s 29022
step   2230 | loss 1.6069 | lr 1.12e-04 | grad 2.69 | tok/s 28608
step   2240 | loss 1.4460 | lr 1.12e-04 | grad 2.62 | tok/s 28245
step   2250 | loss 1.5966 | lr 1.12e-04 | grad 8.38 | tok/s 28337
step   2260 | loss 1.4817 | lr 1.12e-04 | grad 3.08 | tok/s 28023
step   2270 | loss 1.4831 | lr 1.12e-04 | grad 2.98 | tok/s 27889
step   2280 | loss 1.4789 | lr 1.12e-04 | grad 3.12 | tok/s 28669
step   2290 | loss 1.4380 | lr 1.12e-04 | grad 2.45 | tok/s 29353
step   2300 | loss 1.3692 | lr 1.12e-04 | grad 2.36 | tok/s 29342
step   2310 | loss 1.4476 | lr 1.12e-04 | grad 2.83 | tok/s 28942
step   2320 | loss 1.5680 | lr 1.12e-04 | grad 3.02 | tok/s 28350
step   2330 | loss 1.8885 | lr 1.12e-04 | grad 7.59 | tok/s 28438
step   2340 | loss 1.5635 | lr 1.12e-04 | grad 3.81 | tok/s 28493
step   2350 | loss 1.4549 | lr 1.12e-04 | grad 3.02 | tok/s 28190
step   2360 | loss 1.4387 | lr 1.12e-04 | grad 3.84 | tok/s 27953
step   2370 | loss 1.5625 | lr 1.12e-04 | grad 6.56 | tok/s 28068
step   2380 | loss 1.4742 | lr 1.12e-04 | grad 4.25 | tok/s 27871
step   2390 | loss 1.5518 | lr 1.12e-04 | grad 2.78 | tok/s 28323
step   2400 | loss 1.4818 | lr 1.12e-04 | grad 4.81 | tok/s 27666
step   2410 | loss 1.6248 | lr 1.12e-04 | grad 3.16 | tok/s 28501
step   2420 | loss 1.3837 | lr 1.12e-04 | grad 5.09 | tok/s 28225
step   2430 | loss 1.5499 | lr 1.12e-04 | grad 6.59 | tok/s 28612
step   2440 | loss 1.3240 | lr 1.12e-04 | grad 5.09 | tok/s 29359
step   2450 | loss 1.4301 | lr 1.12e-04 | grad 2.72 | tok/s 28223
step   2460 | loss 1.4070 | lr 1.12e-04 | grad 3.03 | tok/s 27897
step   2470 | loss 1.4979 | lr 1.12e-04 | grad 3.75 | tok/s 28565
step   2480 | loss 1.5234 | lr 1.12e-04 | grad 3.08 | tok/s 28615
step   2490 | loss 1.6822 | lr 1.12e-04 | grad 4.53 | tok/s 28074
step   2500 | loss 1.5554 | lr 1.12e-04 | grad 4.16 | tok/s 27739
step   2510 | loss 1.5332 | lr 1.12e-04 | grad 3.52 | tok/s 27992
step   2520 | loss 2.0576 | lr 1.12e-04 | grad 18.75 | tok/s 28295
step   2530 | loss 1.5910 | lr 1.12e-04 | grad 5.16 | tok/s 28760
step   2540 | loss 1.5520 | lr 1.12e-04 | grad 6.31 | tok/s 27987
step   2550 | loss 1.5189 | lr 1.12e-04 | grad 2.70 | tok/s 28530
step   2560 | loss 1.4134 | lr 1.12e-04 | grad 4.53 | tok/s 28166
step   2570 | loss 1.5510 | lr 1.12e-04 | grad 3.31 | tok/s 28945
step   2580 | loss 1.4622 | lr 1.12e-04 | grad 3.00 | tok/s 28810
step   2590 | loss 1.3549 | lr 1.12e-04 | grad 3.56 | tok/s 29428
step   2600 | loss 1.4639 | lr 1.12e-04 | grad 4.12 | tok/s 28577
step   2610 | loss 1.4885 | lr 1.12e-04 | grad 3.22 | tok/s 28022
step   2620 | loss 1.4826 | lr 1.12e-04 | grad 5.06 | tok/s 27253
step   2630 | loss 1.7817 | lr 1.12e-04 | grad 2.48 | tok/s 29092
step   2640 | loss 1.3650 | lr 1.12e-04 | grad 3.00 | tok/s 29390
step   2650 | loss 1.3254 | lr 1.12e-04 | grad 2.38 | tok/s 29408
step   2660 | loss 1.3177 | lr 1.12e-04 | grad 2.72 | tok/s 29424
step   2670 | loss 1.4787 | lr 1.12e-04 | grad 2.64 | tok/s 28157
step   2680 | loss 1.6443 | lr 1.12e-04 | grad 6.97 | tok/s 27914
step   2690 | loss 1.8416 | lr 1.12e-04 | grad 10.00 | tok/s 28905
step   2700 | loss 1.4328 | lr 1.12e-04 | grad 2.42 | tok/s 28198
step   2710 | loss 1.3612 | lr 1.12e-04 | grad 2.42 | tok/s 28576
step   2720 | loss 1.5761 | lr 1.12e-04 | grad 3.77 | tok/s 28152
step   2730 | loss 1.6257 | lr 1.12e-04 | grad 2.67 | tok/s 27730
step   2740 | loss 1.5076 | lr 1.12e-04 | grad 6.59 | tok/s 28751
step   2750 | loss 1.4280 | lr 1.12e-04 | grad 2.19 | tok/s 27859
step   2760 | loss 1.3626 | lr 1.12e-04 | grad 3.77 | tok/s 28127
step   2770 | loss 1.9051 | lr 1.12e-04 | grad 6.78 | tok/s 28312
step   2780 | loss 1.4018 | lr 1.12e-04 | grad 3.91 | tok/s 28884
step   2790 | loss 1.7635 | lr 1.12e-04 | grad 4.31 | tok/s 28686
step   2800 | loss 1.5397 | lr 1.12e-04 | grad 4.44 | tok/s 28753
step   2810 | loss 1.4599 | lr 1.12e-04 | grad 3.33 | tok/s 28344
step   2820 | loss 1.3985 | lr 1.12e-04 | grad 5.66 | tok/s 28629
step   2830 | loss 1.4307 | lr 1.12e-04 | grad 6.38 | tok/s 28296
step   2840 | loss 1.6436 | lr 1.12e-04 | grad 14.31 | tok/s 28687
step   2850 | loss 1.7264 | lr 1.12e-04 | grad 3.73 | tok/s 28487
step   2860 | loss 1.6503 | lr 1.12e-04 | grad 3.61 | tok/s 28797
step   2870 | loss 1.4113 | lr 1.12e-04 | grad 2.80 | tok/s 27951
step   2880 | loss 1.6547 | lr 1.12e-04 | grad 9.69 | tok/s 28018
step   2890 | loss 1.4824 | lr 1.12e-04 | grad 2.50 | tok/s 28653
step   2900 | loss 1.4876 | lr 1.12e-04 | grad 2.67 | tok/s 28201
step   2910 | loss 1.4979 | lr 1.12e-04 | grad 4.59 | tok/s 28058
step   2920 | loss 1.4877 | lr 1.12e-04 | grad 3.73 | tok/s 28155
step   2930 | loss 1.6306 | lr 1.12e-04 | grad 6.53 | tok/s 27881
step   2940 | loss 1.4555 | lr 1.12e-04 | grad 2.70 | tok/s 28187
step   2950 | loss 1.4973 | lr 1.12e-04 | grad 2.92 | tok/s 28758
step   2960 | loss 1.4047 | lr 1.12e-04 | grad 3.19 | tok/s 28405
step   2970 | loss 1.4232 | lr 1.12e-04 | grad 9.62 | tok/s 28478
step   2980 | loss 1.4433 | lr 1.12e-04 | grad 3.77 | tok/s 28428
step   2990 | loss 1.5222 | lr 1.12e-04 | grad 4.97 | tok/s 28585
step   3000 | loss 1.4787 | lr 1.12e-04 | grad 2.97 | tok/s 28253
  >>> saved checkpoint: checkpoint_step_003000_loss_1.4787.pt
step   3010 | loss 1.5678 | lr 1.12e-04 | grad 9.88 | tok/s 19790
step   3020 | loss 1.4062 | lr 1.12e-04 | grad 3.73 | tok/s 28306
step   3030 | loss 1.3819 | lr 1.12e-04 | grad 4.22 | tok/s 28099
step   3040 | loss 1.4298 | lr 1.12e-04 | grad 2.61 | tok/s 28511
step   3050 | loss 1.3708 | lr 1.12e-04 | grad 2.73 | tok/s 28822
step   3060 | loss 1.4598 | lr 1.12e-04 | grad 2.55 | tok/s 28150
step   3070 | loss 1.4012 | lr 1.12e-04 | grad 2.56 | tok/s 28217
step   3080 | loss 1.4319 | lr 1.12e-04 | grad 2.05 | tok/s 28227
step   3090 | loss 1.7357 | lr 1.12e-04 | grad 4.25 | tok/s 28462
step   3100 | loss 1.7937 | lr 1.12e-04 | grad 4.28 | tok/s 29428
step   3110 | loss 1.5078 | lr 1.12e-04 | grad 3.94 | tok/s 29427
step   3120 | loss 1.5917 | lr 1.12e-04 | grad 4.06 | tok/s 28347
step   3130 | loss 2.0875 | lr 1.12e-04 | grad 2.72 | tok/s 28114
step   3140 | loss 1.3275 | lr 1.12e-04 | grad 3.38 | tok/s 28525
step   3150 | loss 1.7201 | lr 1.12e-04 | grad 10.06 | tok/s 28449
step   3160 | loss 1.4345 | lr 1.12e-04 | grad 2.88 | tok/s 27989
step   3170 | loss 1.8041 | lr 1.12e-04 | grad 4.03 | tok/s 28828
step   3180 | loss 1.4704 | lr 1.12e-04 | grad 4.31 | tok/s 28877
step   3190 | loss 1.4866 | lr 1.12e-04 | grad 4.16 | tok/s 28408

Training complete! Final step: 3194
