Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002322/eval_4/level42_100m_20260202_002329
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 472,640,000 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 5.9972 | lr 3.00e-04 | grad 9.50 | tok/s 7612
step     20 | loss 3.2432 | lr 3.00e-04 | grad 8.00 | tok/s 11939
step     30 | loss 3.5094 | lr 3.00e-04 | grad 8.06 | tok/s 6848
step     40 | loss 5.1095 | lr 3.00e-04 | grad 9.62 | tok/s 6008
step     50 | loss 3.8682 | lr 3.00e-04 | grad 4.81 | tok/s 5879
step     60 | loss 3.5865 | lr 3.00e-04 | grad 5.78 | tok/s 5875
step     70 | loss 3.5872 | lr 3.00e-04 | grad 4.28 | tok/s 5639
step     80 | loss 2.8711 | lr 3.00e-04 | grad 2.94 | tok/s 5385
step     90 | loss 3.3202 | lr 3.00e-04 | grad 4.50 | tok/s 5651
step    100 | loss 2.7973 | lr 3.00e-04 | grad 6.16 | tok/s 5422
step    110 | loss 2.6379 | lr 3.00e-04 | grad 3.70 | tok/s 5551
step    120 | loss 2.6087 | lr 3.00e-04 | grad 2.78 | tok/s 5505
step    130 | loss 2.7465 | lr 3.00e-04 | grad 3.16 | tok/s 5404
step    140 | loss 2.6956 | lr 3.00e-04 | grad 2.22 | tok/s 5554
step    150 | loss 2.4071 | lr 3.00e-04 | grad 3.23 | tok/s 5457
step    160 | loss 2.5375 | lr 3.00e-04 | grad 3.89 | tok/s 5350
step    170 | loss 2.3108 | lr 3.00e-04 | grad 3.08 | tok/s 5285
step    180 | loss 2.5327 | lr 3.00e-04 | grad 3.73 | tok/s 5442
step    190 | loss 2.4871 | lr 3.00e-04 | grad 2.83 | tok/s 5350
step    200 | loss 2.2198 | lr 3.00e-04 | grad 2.69 | tok/s 5565
step    210 | loss 2.0775 | lr 3.00e-04 | grad 2.17 | tok/s 5574
step    220 | loss 2.4858 | lr 3.00e-04 | grad 1.98 | tok/s 5521
step    230 | loss 2.3403 | lr 3.00e-04 | grad 1.91 | tok/s 5626
step    240 | loss 2.2437 | lr 3.00e-04 | grad 2.03 | tok/s 5428
step    250 | loss 2.3110 | lr 3.00e-04 | grad 2.03 | tok/s 5672
step    260 | loss 2.2529 | lr 3.00e-04 | grad 2.33 | tok/s 5506
step    270 | loss 2.3490 | lr 3.00e-04 | grad 2.30 | tok/s 5397
step    280 | loss 2.3182 | lr 3.00e-04 | grad 2.05 | tok/s 5461
step    290 | loss 2.0014 | lr 3.00e-04 | grad 1.59 | tok/s 5420
step    300 | loss 2.0152 | lr 3.00e-04 | grad 2.12 | tok/s 5777
step    310 | loss 1.9579 | lr 3.00e-04 | grad 2.31 | tok/s 5709
step    320 | loss 1.9417 | lr 3.00e-04 | grad 2.62 | tok/s 5682
step    330 | loss 2.1074 | lr 3.00e-04 | grad 1.65 | tok/s 5371
step    340 | loss 2.1319 | lr 3.00e-04 | grad 1.77 | tok/s 5392
step    350 | loss 2.2424 | lr 3.00e-04 | grad 1.75 | tok/s 5387
step    360 | loss 2.1423 | lr 3.00e-04 | grad 1.69 | tok/s 5385
step    370 | loss 2.1812 | lr 3.00e-04 | grad 1.72 | tok/s 5319
step    380 | loss 2.0623 | lr 3.00e-04 | grad 1.63 | tok/s 5493
step    390 | loss 2.4481 | lr 3.00e-04 | grad 2.08 | tok/s 5560
step    400 | loss 2.0091 | lr 3.00e-04 | grad 2.69 | tok/s 5431
step    410 | loss 2.3627 | lr 3.00e-04 | grad 1.69 | tok/s 5463
step    420 | loss 1.9107 | lr 3.00e-04 | grad 2.02 | tok/s 5393
step    430 | loss 2.0504 | lr 3.00e-04 | grad 5.12 | tok/s 5381
step    440 | loss 2.1879 | lr 3.00e-04 | grad 2.14 | tok/s 5375
step    450 | loss 2.3117 | lr 3.00e-04 | grad 3.28 | tok/s 5513
step    460 | loss 1.9206 | lr 3.00e-04 | grad 2.34 | tok/s 5402
step    470 | loss 2.0577 | lr 3.00e-04 | grad 2.16 | tok/s 5350
step    480 | loss 1.9792 | lr 3.00e-04 | grad 1.90 | tok/s 5408
step    490 | loss 1.9111 | lr 3.00e-04 | grad 1.38 | tok/s 5246
step    500 | loss 2.2290 | lr 3.00e-04 | grad 5.44 | tok/s 5564
step    510 | loss 2.3910 | lr 3.00e-04 | grad 4.41 | tok/s 5411
step    520 | loss 1.8915 | lr 3.00e-04 | grad 2.08 | tok/s 5565
step    530 | loss 1.9856 | lr 3.00e-04 | grad 1.63 | tok/s 5417
step    540 | loss 2.3136 | lr 3.00e-04 | grad 1.55 | tok/s 5460
step    550 | loss 1.7821 | lr 3.00e-04 | grad 1.95 | tok/s 5550
step    560 | loss 1.7284 | lr 3.00e-04 | grad 1.80 | tok/s 5625
step    570 | loss 1.6565 | lr 3.00e-04 | grad 1.88 | tok/s 5629
step    580 | loss 2.2227 | lr 3.00e-04 | grad 2.08 | tok/s 5420
step    590 | loss 2.3293 | lr 3.00e-04 | grad 3.17 | tok/s 5386
step    600 | loss 2.0813 | lr 3.00e-04 | grad 1.78 | tok/s 5375
step    610 | loss 2.1288 | lr 3.00e-04 | grad 4.91 | tok/s 5561
step    620 | loss 1.7869 | lr 3.00e-04 | grad 2.06 | tok/s 5474
step    630 | loss 1.8559 | lr 3.00e-04 | grad 1.29 | tok/s 5453
step    640 | loss 2.0355 | lr 3.00e-04 | grad 3.48 | tok/s 5556
step    650 | loss 1.9504 | lr 3.00e-04 | grad 1.80 | tok/s 5440
step    660 | loss 1.9016 | lr 3.00e-04 | grad 1.60 | tok/s 5378
step    670 | loss 2.0720 | lr 3.00e-04 | grad 2.56 | tok/s 5367
step    680 | loss 2.0595 | lr 3.00e-04 | grad 1.94 | tok/s 5516
step    690 | loss 2.1236 | lr 3.00e-04 | grad 3.97 | tok/s 5461
step    700 | loss 2.0775 | lr 3.00e-04 | grad 2.83 | tok/s 5426
step    710 | loss 1.9198 | lr 3.00e-04 | grad 2.05 | tok/s 5463
step    720 | loss 1.8742 | lr 3.00e-04 | grad 3.88 | tok/s 5312
step    730 | loss 2.1336 | lr 3.00e-04 | grad 3.14 | tok/s 5511
step    740 | loss 1.9224 | lr 3.00e-04 | grad 3.12 | tok/s 5455
step    750 | loss 1.8617 | lr 3.00e-04 | grad 2.34 | tok/s 5263
step    760 | loss 2.1821 | lr 3.00e-04 | grad 9.94 | tok/s 5585
step    770 | loss 1.8006 | lr 3.00e-04 | grad 1.76 | tok/s 5480
step    780 | loss 1.8813 | lr 3.00e-04 | grad 2.09 | tok/s 5541
step    790 | loss 1.9092 | lr 3.00e-04 | grad 2.28 | tok/s 5504
step    800 | loss 1.6746 | lr 3.00e-04 | grad 1.82 | tok/s 5583
step    810 | loss 1.8684 | lr 3.00e-04 | grad 1.80 | tok/s 5482
step    820 | loss 2.3053 | lr 3.00e-04 | grad 2.91 | tok/s 5522
step    830 | loss 2.3279 | lr 3.00e-04 | grad 3.39 | tok/s 5645
step    840 | loss 2.0272 | lr 3.00e-04 | grad 3.14 | tok/s 5628
step    850 | loss 1.9967 | lr 3.00e-04 | grad 2.30 | tok/s 5548
step    860 | loss 1.8635 | lr 3.00e-04 | grad 1.91 | tok/s 5266
step    870 | loss 1.7440 | lr 3.00e-04 | grad 1.71 | tok/s 5470
step    880 | loss 1.8962 | lr 3.00e-04 | grad 2.22 | tok/s 5394
step    890 | loss 1.7854 | lr 3.00e-04 | grad 1.57 | tok/s 5399
step    900 | loss 2.0499 | lr 3.00e-04 | grad 1.81 | tok/s 5467
step    910 | loss 1.8172 | lr 3.00e-04 | grad 4.06 | tok/s 5320
step    920 | loss 1.7895 | lr 3.00e-04 | grad 2.30 | tok/s 5379
step    930 | loss 1.8233 | lr 3.00e-04 | grad 2.05 | tok/s 5210
step    940 | loss 1.8519 | lr 3.00e-04 | grad 2.81 | tok/s 5326
step    950 | loss 1.7925 | lr 3.00e-04 | grad 1.38 | tok/s 5485
step    960 | loss 1.7886 | lr 3.00e-04 | grad 2.31 | tok/s 5638
step    970 | loss 1.5534 | lr 3.00e-04 | grad 2.11 | tok/s 5645
step    980 | loss 1.5885 | lr 3.00e-04 | grad 2.38 | tok/s 5612
step    990 | loss 1.8996 | lr 3.00e-04 | grad 1.80 | tok/s 5260
step   1000 | loss 1.9033 | lr 3.00e-04 | grad 1.97 | tok/s 5421
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9033.pt
step   1010 | loss 1.9347 | lr 3.00e-04 | grad 1.77 | tok/s 5448
step   1020 | loss 1.6368 | lr 3.00e-04 | grad 2.47 | tok/s 5510
step   1030 | loss 2.0879 | lr 3.00e-04 | grad 17.75 | tok/s 5295
step   1040 | loss 1.7469 | lr 3.00e-04 | grad 2.31 | tok/s 5447
step   1050 | loss 1.7419 | lr 3.00e-04 | grad 1.90 | tok/s 5421
step   1060 | loss 1.7533 | lr 3.00e-04 | grad 2.14 | tok/s 5431
step   1070 | loss 2.1617 | lr 3.00e-04 | grad 2.86 | tok/s 5509
step   1080 | loss 2.2525 | lr 3.00e-04 | grad 2.30 | tok/s 5358
step   1090 | loss 2.1064 | lr 3.00e-04 | grad 2.42 | tok/s 5420
step   1100 | loss 1.7996 | lr 3.00e-04 | grad 5.00 | tok/s 5455
step   1110 | loss 1.7267 | lr 3.00e-04 | grad 2.17 | tok/s 5641
step   1120 | loss 1.9395 | lr 3.00e-04 | grad 2.12 | tok/s 5613
step   1130 | loss 1.9580 | lr 3.00e-04 | grad 3.80 | tok/s 5314
step   1140 | loss 1.6672 | lr 3.00e-04 | grad 1.69 | tok/s 5393
step   1150 | loss 2.0340 | lr 3.00e-04 | grad 3.17 | tok/s 5469
step   1160 | loss 1.7293 | lr 3.00e-04 | grad 1.95 | tok/s 5395
step   1170 | loss 1.9268 | lr 3.00e-04 | grad 1.16 | tok/s 5313
step   1180 | loss 1.7191 | lr 3.00e-04 | grad 2.17 | tok/s 5671
step   1190 | loss 1.6347 | lr 3.00e-04 | grad 1.66 | tok/s 5662
step   1200 | loss 1.5613 | lr 3.00e-04 | grad 1.88 | tok/s 5661
step   1210 | loss 1.5189 | lr 3.00e-04 | grad 2.11 | tok/s 5667
step   1220 | loss 1.5584 | lr 3.00e-04 | grad 2.33 | tok/s 5656
step   1230 | loss 1.6674 | lr 3.00e-04 | grad 1.82 | tok/s 5492
step   1240 | loss 1.7470 | lr 3.00e-04 | grad 2.44 | tok/s 5224

Training complete! Final step: 1244
