Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002322/eval_1/level42_100m_20260202_002329
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 463,429,120 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 9.1921 | lr 3.00e-04 | grad 24.12 | tok/s 8698
step     20 | loss 3.0771 | lr 3.00e-04 | grad 5.31 | tok/s 15060
step     30 | loss 3.3266 | lr 3.00e-04 | grad 5.41 | tok/s 12875
step     40 | loss 4.9112 | lr 3.00e-04 | grad 13.50 | tok/s 6961
step     50 | loss 4.0169 | lr 3.00e-04 | grad 8.69 | tok/s 6850
step     60 | loss 3.4764 | lr 3.00e-04 | grad 9.56 | tok/s 7389
step     70 | loss 3.0988 | lr 3.00e-04 | grad 10.44 | tok/s 5596
step     80 | loss 3.2670 | lr 3.00e-04 | grad 9.38 | tok/s 6858
step     90 | loss 3.2044 | lr 3.00e-04 | grad 6.97 | tok/s 7135
step    100 | loss 3.0507 | lr 3.00e-04 | grad 5.03 | tok/s 6907
step    110 | loss 2.8078 | lr 3.00e-04 | grad 6.06 | tok/s 6876
step    120 | loss 2.4199 | lr 3.00e-04 | grad 4.47 | tok/s 6972
step    130 | loss 2.8658 | lr 3.00e-04 | grad 4.09 | tok/s 6851
step    140 | loss 2.6128 | lr 3.00e-04 | grad 5.69 | tok/s 6410
step    150 | loss 2.5096 | lr 3.00e-04 | grad 4.53 | tok/s 6120
step    160 | loss 2.1902 | lr 3.00e-04 | grad 2.33 | tok/s 6405
step    170 | loss 2.3489 | lr 3.00e-04 | grad 4.28 | tok/s 6738
step    180 | loss 2.3660 | lr 3.00e-04 | grad 3.48 | tok/s 6593
step    190 | loss 2.5620 | lr 3.00e-04 | grad 3.30 | tok/s 5002
step    200 | loss 2.3213 | lr 3.00e-04 | grad 3.53 | tok/s 5227
step    210 | loss 2.1326 | lr 3.00e-04 | grad 3.36 | tok/s 5401
step    220 | loss 2.2750 | lr 3.00e-04 | grad 3.12 | tok/s 5125
step    230 | loss 2.4199 | lr 3.00e-04 | grad 2.80 | tok/s 6375
step    240 | loss 2.2674 | lr 3.00e-04 | grad 2.77 | tok/s 6562
step    250 | loss 2.2316 | lr 3.00e-04 | grad 4.03 | tok/s 5701
step    260 | loss 2.3845 | lr 3.00e-04 | grad 3.59 | tok/s 5360
step    270 | loss 2.3058 | lr 3.00e-04 | grad 3.44 | tok/s 5164
step    280 | loss 2.0830 | lr 3.00e-04 | grad 3.42 | tok/s 5289
step    290 | loss 2.1005 | lr 3.00e-04 | grad 3.17 | tok/s 5479
step    300 | loss 2.0648 | lr 3.00e-04 | grad 2.69 | tok/s 7031
step    310 | loss 1.9436 | lr 3.00e-04 | grad 3.08 | tok/s 7119
step    320 | loss 1.9464 | lr 3.00e-04 | grad 3.38 | tok/s 7085
step    330 | loss 2.0258 | lr 3.00e-04 | grad 2.48 | tok/s 6860
step    340 | loss 2.2392 | lr 3.00e-04 | grad 2.47 | tok/s 6919
step    350 | loss 2.1776 | lr 3.00e-04 | grad 1.95 | tok/s 6865
step    360 | loss 2.1403 | lr 3.00e-04 | grad 4.22 | tok/s 6666
step    370 | loss 2.1263 | lr 3.00e-04 | grad 2.69 | tok/s 6671
step    380 | loss 1.9463 | lr 3.00e-04 | grad 3.34 | tok/s 6818
step    390 | loss 2.5421 | lr 3.00e-04 | grad 3.52 | tok/s 6323
step    400 | loss 2.1338 | lr 3.00e-04 | grad 4.56 | tok/s 6389
step    410 | loss 2.0410 | lr 3.00e-04 | grad 3.73 | tok/s 6178
step    420 | loss 1.9632 | lr 3.00e-04 | grad 2.33 | tok/s 4767
step    430 | loss 2.1405 | lr 3.00e-04 | grad 2.31 | tok/s 6002
step    440 | loss 2.1870 | lr 3.00e-04 | grad 2.81 | tok/s 5989
step    450 | loss 2.2734 | lr 3.00e-04 | grad 2.09 | tok/s 6011
step    460 | loss 2.0662 | lr 3.00e-04 | grad 3.17 | tok/s 5983
step    470 | loss 2.0983 | lr 3.00e-04 | grad 1.74 | tok/s 5927
step    480 | loss 2.0412 | lr 3.00e-04 | grad 3.14 | tok/s 6040
step    490 | loss 1.9692 | lr 3.00e-04 | grad 2.83 | tok/s 5721
step    500 | loss 1.8978 | lr 3.00e-04 | grad 2.22 | tok/s 6240
step    510 | loss 2.5206 | lr 3.00e-04 | grad 3.19 | tok/s 6949
step    520 | loss 1.9141 | lr 3.00e-04 | grad 1.99 | tok/s 6834
step    530 | loss 1.9058 | lr 3.00e-04 | grad 2.50 | tok/s 6937
step    540 | loss 2.3667 | lr 3.00e-04 | grad 3.02 | tok/s 6915
step    550 | loss 1.8786 | lr 3.00e-04 | grad 3.09 | tok/s 6857
step    560 | loss 1.8399 | lr 3.00e-04 | grad 2.77 | tok/s 6014
step    570 | loss 1.7161 | lr 3.00e-04 | grad 2.31 | tok/s 6426
step    580 | loss 2.0415 | lr 3.00e-04 | grad 5.19 | tok/s 6091
step    590 | loss 2.3543 | lr 3.00e-04 | grad 11.06 | tok/s 6266
step    600 | loss 2.3673 | lr 3.00e-04 | grad 2.77 | tok/s 5523
step    610 | loss 2.0720 | lr 3.00e-04 | grad 3.72 | tok/s 5444
step    620 | loss 1.9121 | lr 3.00e-04 | grad 2.19 | tok/s 6997
step    630 | loss 1.9438 | lr 3.00e-04 | grad 1.90 | tok/s 6778
step    640 | loss 1.8291 | lr 3.00e-04 | grad 2.39 | tok/s 6913
step    650 | loss 2.1289 | lr 3.00e-04 | grad 2.47 | tok/s 6999
step    660 | loss 1.9589 | lr 3.00e-04 | grad 2.42 | tok/s 6810
step    670 | loss 2.2624 | lr 3.00e-04 | grad 3.30 | tok/s 6813
step    680 | loss 2.0141 | lr 3.00e-04 | grad 1.84 | tok/s 6853
step    690 | loss 1.9913 | lr 3.00e-04 | grad 2.50 | tok/s 6753
step    700 | loss 2.0254 | lr 3.00e-04 | grad 1.27 | tok/s 6753
step    710 | loss 2.1134 | lr 3.00e-04 | grad 2.56 | tok/s 6706
step    720 | loss 2.0591 | lr 3.00e-04 | grad 2.17 | tok/s 6753
step    730 | loss 2.0561 | lr 3.00e-04 | grad 3.55 | tok/s 6806
step    740 | loss 2.0602 | lr 3.00e-04 | grad 3.05 | tok/s 6920
step    750 | loss 2.0262 | lr 3.00e-04 | grad 2.23 | tok/s 6817
step    760 | loss 1.8178 | lr 3.00e-04 | grad 2.44 | tok/s 6635
step    770 | loss 2.2013 | lr 3.00e-04 | grad 3.83 | tok/s 6845
step    780 | loss 1.8765 | lr 3.00e-04 | grad 1.84 | tok/s 6732
step    790 | loss 1.9975 | lr 3.00e-04 | grad 2.86 | tok/s 6964
step    800 | loss 1.8935 | lr 3.00e-04 | grad 4.62 | tok/s 6906
step    810 | loss 1.8337 | lr 3.00e-04 | grad 1.47 | tok/s 6777
step    820 | loss 1.7972 | lr 3.00e-04 | grad 1.84 | tok/s 7007
step    830 | loss 2.4909 | lr 3.00e-04 | grad 4.06 | tok/s 7040
step    840 | loss 2.1170 | lr 3.00e-04 | grad 4.44 | tok/s 7080
step    850 | loss 2.0099 | lr 3.00e-04 | grad 2.97 | tok/s 7080
step    860 | loss 1.9935 | lr 3.00e-04 | grad 2.31 | tok/s 6793
step    870 | loss 1.7894 | lr 3.00e-04 | grad 2.47 | tok/s 6876
step    880 | loss 1.8784 | lr 3.00e-04 | grad 2.75 | tok/s 6876
step    890 | loss 1.8825 | lr 3.00e-04 | grad 3.05 | tok/s 6825
step    900 | loss 1.9975 | lr 3.00e-04 | grad 4.34 | tok/s 6809
step    910 | loss 2.2103 | lr 3.00e-04 | grad 1.65 | tok/s 6606
step    920 | loss 1.8186 | lr 3.00e-04 | grad 2.28 | tok/s 6777
step    930 | loss 1.8606 | lr 3.00e-04 | grad 2.50 | tok/s 6834
step    940 | loss 1.9139 | lr 3.00e-04 | grad 2.70 | tok/s 6828
step    950 | loss 1.9726 | lr 3.00e-04 | grad 3.16 | tok/s 6690
step    960 | loss 1.8435 | lr 3.00e-04 | grad 2.61 | tok/s 6932
step    970 | loss 1.6369 | lr 3.00e-04 | grad 3.02 | tok/s 7096
step    980 | loss 1.5621 | lr 3.00e-04 | grad 2.70 | tok/s 7108
step    990 | loss 1.9716 | lr 3.00e-04 | grad 3.11 | tok/s 6707
step   1000 | loss 1.9708 | lr 3.00e-04 | grad 2.00 | tok/s 6852
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9708.pt
step   1010 | loss 2.0708 | lr 3.00e-04 | grad 1.65 | tok/s 5371
step   1020 | loss 1.7531 | lr 3.00e-04 | grad 2.64 | tok/s 7005
step   1030 | loss 1.7984 | lr 3.00e-04 | grad 2.39 | tok/s 6595
step   1040 | loss 2.0800 | lr 3.00e-04 | grad 2.02 | tok/s 6786
step   1050 | loss 2.0288 | lr 3.00e-04 | grad 2.59 | tok/s 6912
step   1060 | loss 1.5989 | lr 3.00e-04 | grad 2.12 | tok/s 6748
step   1070 | loss 2.2928 | lr 3.00e-04 | grad 4.66 | tok/s 7048
step   1080 | loss 2.3892 | lr 3.00e-04 | grad 2.64 | tok/s 6757
step   1090 | loss 2.0686 | lr 3.00e-04 | grad 4.41 | tok/s 6858
step   1100 | loss 2.0422 | lr 3.00e-04 | grad 2.94 | tok/s 6953
step   1110 | loss 1.8952 | lr 3.00e-04 | grad 1.93 | tok/s 6912
step   1120 | loss 1.9764 | lr 3.00e-04 | grad 2.56 | tok/s 6957
step   1130 | loss 1.7400 | lr 3.00e-04 | grad 2.64 | tok/s 6616
step   1140 | loss 1.8776 | lr 3.00e-04 | grad 2.66 | tok/s 6857
step   1150 | loss 1.8781 | lr 3.00e-04 | grad 2.34 | tok/s 6783
step   1160 | loss 1.8312 | lr 3.00e-04 | grad 1.76 | tok/s 6806
step   1170 | loss 2.0396 | lr 3.00e-04 | grad 3.36 | tok/s 6680
step   1180 | loss 1.7551 | lr 3.00e-04 | grad 2.42 | tok/s 7059
step   1190 | loss 1.7015 | lr 3.00e-04 | grad 2.66 | tok/s 7106
step   1200 | loss 1.6411 | lr 3.00e-04 | grad 2.59 | tok/s 7194
step   1210 | loss 1.5769 | lr 3.00e-04 | grad 2.48 | tok/s 11029
step   1220 | loss 1.5951 | lr 3.00e-04 | grad 2.61 | tok/s 7090
step   1230 | loss 1.8487 | lr 3.00e-04 | grad 2.39 | tok/s 6945
step   1240 | loss 1.5893 | lr 3.00e-04 | grad 1.93 | tok/s 6662
step   1250 | loss 1.8622 | lr 3.00e-04 | grad 2.23 | tok/s 6856
step   1260 | loss 1.7575 | lr 3.00e-04 | grad 3.08 | tok/s 6907
step   1270 | loss 2.0996 | lr 3.00e-04 | grad 3.05 | tok/s 7028
step   1280 | loss 1.8484 | lr 3.00e-04 | grad 2.59 | tok/s 6912
step   1290 | loss 1.9145 | lr 3.00e-04 | grad 1.45 | tok/s 6767
step   1300 | loss 1.9762 | lr 3.00e-04 | grad 6.44 | tok/s 6827
step   1310 | loss 1.9387 | lr 3.00e-04 | grad 3.00 | tok/s 6886
step   1320 | loss 1.7860 | lr 3.00e-04 | grad 2.77 | tok/s 6616
step   1330 | loss 1.9340 | lr 3.00e-04 | grad 3.80 | tok/s 6886
step   1340 | loss 1.7786 | lr 3.00e-04 | grad 1.37 | tok/s 6934
step   1350 | loss 1.6988 | lr 3.00e-04 | grad 2.12 | tok/s 6948
step   1360 | loss 1.8170 | lr 3.00e-04 | grad 2.17 | tok/s 6887
step   1370 | loss 1.7082 | lr 3.00e-04 | grad 2.30 | tok/s 6661
step   1380 | loss 2.0189 | lr 3.00e-04 | grad 2.34 | tok/s 6930
step   1390 | loss 1.7801 | lr 3.00e-04 | grad 2.66 | tok/s 6844
step   1400 | loss 1.7705 | lr 3.00e-04 | grad 2.47 | tok/s 6923
step   1410 | loss 1.8497 | lr 3.00e-04 | grad 2.23 | tok/s 6961
step   1420 | loss 1.7567 | lr 3.00e-04 | grad 3.08 | tok/s 6795
step   1430 | loss 1.8700 | lr 3.00e-04 | grad 2.78 | tok/s 6693
step   1440 | loss 1.5487 | lr 3.00e-04 | grad 1.59 | tok/s 6825
step   1450 | loss 1.6358 | lr 3.00e-04 | grad 2.78 | tok/s 7070
step   1460 | loss 1.7838 | lr 3.00e-04 | grad 1.62 | tok/s 6631
step   1470 | loss 1.8722 | lr 3.00e-04 | grad 4.16 | tok/s 6844
step   1480 | loss 1.7590 | lr 3.00e-04 | grad 2.38 | tok/s 6867
step   1490 | loss 1.7209 | lr 3.00e-04 | grad 3.56 | tok/s 6893
step   1500 | loss 1.9454 | lr 3.00e-04 | grad 2.56 | tok/s 6698

Training complete! Final step: 1502
