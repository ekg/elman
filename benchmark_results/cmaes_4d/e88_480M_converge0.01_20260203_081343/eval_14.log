Using device: cuda
Output directory: benchmark_results/cmaes_4d/e88_480M_converge0.01_20260203_081343/eval_14/levelE88_100m_20260203_084412
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 309,256,064 parameters
Using schedule-free AdamW (lr=0.0008148532555279302)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.7128 | lr 8.15e-04 | grad 10.44 | tok/s 14220
step     20 | loss 4.2713 | lr 8.15e-04 | grad 8.00 | tok/s 24733
step     30 | loss 3.2922 | lr 8.15e-04 | grad 3.62 | tok/s 24917
step     40 | loss 2.6400 | lr 8.15e-04 | grad 3.08 | tok/s 24750
step     50 | loss 2.2254 | lr 8.15e-04 | grad 1.95 | tok/s 24624
step     60 | loss 2.8439 | lr 8.15e-04 | grad 2.92 | tok/s 23612
step     70 | loss 2.5074 | lr 8.15e-04 | grad 3.58 | tok/s 23939
step     80 | loss 2.4329 | lr 8.15e-04 | grad 1.88 | tok/s 23710
step     90 | loss 2.3474 | lr 8.15e-04 | grad 2.14 | tok/s 23180
step    100 | loss 2.0033 | lr 8.15e-04 | grad 1.78 | tok/s 23953
step    110 | loss 2.3162 | lr 8.15e-04 | grad 1.44 | tok/s 23184
step    120 | loss 2.2951 | lr 8.15e-04 | grad 1.88 | tok/s 23334
step    130 | loss 2.0824 | lr 8.15e-04 | grad 2.09 | tok/s 23851
step    140 | loss 1.9059 | lr 8.15e-04 | grad 1.68 | tok/s 22765
step    150 | loss 1.9526 | lr 8.15e-04 | grad 1.25 | tok/s 22922
step    160 | loss 1.9548 | lr 8.15e-04 | grad 2.08 | tok/s 22981
step    170 | loss 2.0588 | lr 8.15e-04 | grad 2.12 | tok/s 23492
step    180 | loss 1.7214 | lr 8.15e-04 | grad 1.74 | tok/s 23411
step    190 | loss 1.4551 | lr 8.15e-04 | grad 1.10 | tok/s 24219
step    200 | loss 1.6669 | lr 8.15e-04 | grad 1.60 | tok/s 23498
step    210 | loss 1.8270 | lr 8.15e-04 | grad 1.38 | tok/s 23905
step    220 | loss 1.7648 | lr 8.15e-04 | grad 1.60 | tok/s 23337
step    230 | loss 1.6688 | lr 8.15e-04 | grad 1.34 | tok/s 23314
step    240 | loss 1.7184 | lr 8.15e-04 | grad 1.42 | tok/s 23859
step    250 | loss 1.8328 | lr 8.15e-04 | grad 1.53 | tok/s 23200
step    260 | loss 1.6493 | lr 8.15e-04 | grad 1.52 | tok/s 22766
step    270 | loss 1.6702 | lr 8.15e-04 | grad 3.84 | tok/s 23240
step    280 | loss 1.4790 | lr 8.15e-04 | grad 1.55 | tok/s 23953
step    290 | loss 1.3621 | lr 8.15e-04 | grad 1.45 | tok/s 24250
step    300 | loss 1.3720 | lr 8.15e-04 | grad 1.60 | tok/s 24291
step    310 | loss 1.4835 | lr 8.15e-04 | grad 1.38 | tok/s 24014
step    320 | loss 1.6584 | lr 8.15e-04 | grad 0.87 | tok/s 22953
step    330 | loss 1.6628 | lr 8.15e-04 | grad 1.93 | tok/s 23705
step    340 | loss 1.6356 | lr 8.15e-04 | grad 1.57 | tok/s 22973
step    350 | loss 1.5812 | lr 8.15e-04 | grad 0.90 | tok/s 22814
step    360 | loss 1.4724 | lr 8.15e-04 | grad 1.33 | tok/s 23612
step    370 | loss 1.8016 | lr 8.15e-04 | grad 1.20 | tok/s 23702
step    380 | loss 1.5420 | lr 8.15e-04 | grad 1.20 | tok/s 24098
step    390 | loss 1.5326 | lr 8.15e-04 | grad 1.66 | tok/s 23368
step    400 | loss 1.5005 | lr 8.15e-04 | grad 0.99 | tok/s 23664
step    410 | loss 1.5298 | lr 8.15e-04 | grad 1.24 | tok/s 22907
step    420 | loss 1.5713 | lr 8.15e-04 | grad 1.12 | tok/s 23067
step    430 | loss 1.6008 | lr 8.15e-04 | grad 2.27 | tok/s 23797
step    440 | loss 1.5505 | lr 8.15e-04 | grad 1.16 | tok/s 23330
step    450 | loss 1.5344 | lr 8.15e-04 | grad 1.12 | tok/s 23357
step    460 | loss 1.5437 | lr 8.15e-04 | grad 1.38 | tok/s 23390
step    470 | loss 1.4214 | lr 8.15e-04 | grad 1.00 | tok/s 22766
step    480 | loss 1.4544 | lr 8.15e-04 | grad 1.09 | tok/s 23245
step    490 | loss 1.8193 | lr 8.15e-04 | grad 1.30 | tok/s 23930
step    500 | loss 1.4945 | lr 8.15e-04 | grad 3.14 | tok/s 23441
step    510 | loss 1.3048 | lr 8.15e-04 | grad 1.06 | tok/s 24170
step    520 | loss 1.9313 | lr 8.15e-04 | grad 2.36 | tok/s 23485
step    530 | loss 1.3327 | lr 8.15e-04 | grad 3.53 | tok/s 23789
step    540 | loss 1.3838 | lr 8.15e-04 | grad 1.26 | tok/s 23873
step    550 | loss 1.2414 | lr 8.15e-04 | grad 1.20 | tok/s 24281
step    560 | loss 1.3691 | lr 8.15e-04 | grad 2.81 | tok/s 24028
step    570 | loss 1.7232 | lr 8.15e-04 | grad 1.07 | tok/s 24052
step    580 | loss 1.7831 | lr 8.15e-04 | grad 3.52 | tok/s 23222
step    590 | loss 1.4215 | lr 8.15e-04 | grad 1.27 | tok/s 23440
step    600 | loss 1.3963 | lr 8.15e-04 | grad 1.25 | tok/s 24388
step    610 | loss 1.4088 | lr 8.15e-04 | grad 1.13 | tok/s 23223
step    620 | loss 1.3465 | lr 8.15e-04 | grad 1.32 | tok/s 24005
step    630 | loss 1.4971 | lr 8.15e-04 | grad 1.29 | tok/s 24014
step    640 | loss 1.4144 | lr 8.15e-04 | grad 1.30 | tok/s 23432
step    650 | loss 1.5320 | lr 8.15e-04 | grad 3.86 | tok/s 23116
step    660 | loss 1.5234 | lr 8.15e-04 | grad 1.32 | tok/s 23858
step    670 | loss 1.4901 | lr 8.15e-04 | grad 1.20 | tok/s 23416
step    680 | loss 1.4589 | lr 8.15e-04 | grad 1.30 | tok/s 23276
step    690 | loss 1.5604 | lr 8.15e-04 | grad 1.66 | tok/s 23480
step    700 | loss 1.4391 | lr 8.15e-04 | grad 1.09 | tok/s 23694
step    710 | loss 1.3966 | lr 8.15e-04 | grad 3.89 | tok/s 23467
step    720 | loss 1.5528 | lr 8.15e-04 | grad 0.96 | tok/s 23702
step    730 | loss 1.5472 | lr 8.15e-04 | grad 2.33 | tok/s 23568
step    740 | loss 1.3738 | lr 8.15e-04 | grad 1.09 | tok/s 23323
step    750 | loss 1.6779 | lr 8.15e-04 | grad 1.25 | tok/s 23658
step    760 | loss 1.3922 | lr 8.15e-04 | grad 1.52 | tok/s 23609
step    770 | loss 1.4749 | lr 8.15e-04 | grad 1.84 | tok/s 23994
step    780 | loss 1.3051 | lr 8.15e-04 | grad 1.13 | tok/s 24057
step    790 | loss 1.3567 | lr 8.15e-04 | grad 3.28 | tok/s 23768
step    800 | loss 1.3430 | lr 8.15e-04 | grad 1.08 | tok/s 23603
step    810 | loss 2.0116 | lr 8.15e-04 | grad 2.48 | tok/s 24341
step    820 | loss 1.6887 | lr 8.15e-04 | grad 2.50 | tok/s 24565
step    830 | loss 1.4634 | lr 8.15e-04 | grad 1.52 | tok/s 24580
step    840 | loss 1.4709 | lr 8.15e-04 | grad 1.01 | tok/s 23445
step    850 | loss 1.3807 | lr 8.15e-04 | grad 0.87 | tok/s 23737
step    860 | loss 1.3780 | lr 8.15e-04 | grad 1.14 | tok/s 23652
step    870 | loss 1.4401 | lr 8.15e-04 | grad 1.73 | tok/s 24053
step    880 | loss 1.3678 | lr 8.15e-04 | grad 1.63 | tok/s 22999
step    890 | loss 1.6344 | lr 8.15e-04 | grad 1.50 | tok/s 23245
step    900 | loss 1.2981 | lr 8.15e-04 | grad 1.54 | tok/s 23270
step    910 | loss 1.4461 | lr 8.15e-04 | grad 0.91 | tok/s 23491
step    920 | loss 1.3636 | lr 8.15e-04 | grad 1.03 | tok/s 23362
step    930 | loss 1.4553 | lr 8.15e-04 | grad 1.55 | tok/s 23352
step    940 | loss 1.4114 | lr 8.15e-04 | grad 2.16 | tok/s 23930
step    950 | loss 1.2365 | lr 8.15e-04 | grad 1.98 | tok/s 24581
step    960 | loss 1.1733 | lr 8.15e-04 | grad 1.88 | tok/s 24586
step    970 | loss 1.3651 | lr 8.15e-04 | grad 1.33 | tok/s 23706
step    980 | loss 1.4527 | lr 8.15e-04 | grad 2.02 | tok/s 23268
step    990 | loss 1.4364 | lr 8.15e-04 | grad 1.59 | tok/s 23630
step   1000 | loss 1.3100 | lr 8.15e-04 | grad 1.05 | tok/s 24170
  >>> saved checkpoint: checkpoint_step_001000_loss_1.3100.pt
step   1010 | loss 1.3364 | lr 8.15e-04 | grad 1.34 | tok/s 17986
step   1020 | loss 1.6927 | lr 8.15e-04 | grad 1.20 | tok/s 23614
step   1030 | loss 1.4853 | lr 8.15e-04 | grad 0.97 | tok/s 23701
step   1040 | loss 1.1617 | lr 8.15e-04 | grad 1.06 | tok/s 23492
step   1050 | loss 1.6265 | lr 8.15e-04 | grad 0.92 | tok/s 24155
step   1060 | loss 1.7994 | lr 8.15e-04 | grad 1.23 | tok/s 23663
step   1070 | loss 1.4386 | lr 8.15e-04 | grad 1.60 | tok/s 23307
step   1080 | loss 1.5566 | lr 8.15e-04 | grad 1.05 | tok/s 23949
step   1090 | loss 1.3478 | lr 8.15e-04 | grad 1.45 | tok/s 24323
step   1100 | loss 1.3147 | lr 8.15e-04 | grad 1.36 | tok/s 24032
step   1110 | loss 1.4248 | lr 8.15e-04 | grad 0.92 | tok/s 23227
step   1120 | loss 1.4624 | lr 8.15e-04 | grad 1.13 | tok/s 23712
step   1130 | loss 1.4644 | lr 8.15e-04 | grad 2.19 | tok/s 23625
step   1140 | loss 1.4448 | lr 8.15e-04 | grad 1.90 | tok/s 23350
step   1150 | loss 1.5061 | lr 8.15e-04 | grad 1.30 | tok/s 23123
step   1160 | loss 1.3424 | lr 8.15e-04 | grad 1.53 | tok/s 24384
step   1170 | loss 1.2811 | lr 8.15e-04 | grad 1.30 | tok/s 24592
step   1180 | loss 1.2108 | lr 8.15e-04 | grad 1.30 | tok/s 24587
step   1190 | loss 1.1770 | lr 8.15e-04 | grad 1.42 | tok/s 24614
step   1200 | loss 1.1500 | lr 8.15e-04 | grad 1.12 | tok/s 24616
step   1210 | loss 1.2527 | lr 8.15e-04 | grad 0.89 | tok/s 24218
step   1220 | loss 1.3452 | lr 8.15e-04 | grad 0.79 | tok/s 23122
step   1230 | loss 1.4176 | lr 8.15e-04 | grad 1.15 | tok/s 24012
step   1240 | loss 1.3542 | lr 8.15e-04 | grad 2.08 | tok/s 23767
step   1250 | loss 1.5443 | lr 8.15e-04 | grad 0.82 | tok/s 23652
step   1260 | loss 1.4099 | lr 8.15e-04 | grad 1.52 | tok/s 23611
step   1270 | loss 1.4493 | lr 8.15e-04 | grad 2.55 | tok/s 23304
step   1280 | loss 1.3796 | lr 8.15e-04 | grad 1.14 | tok/s 23432
step   1290 | loss 1.4394 | lr 8.15e-04 | grad 0.95 | tok/s 23274
step   1300 | loss 1.3950 | lr 8.15e-04 | grad 1.09 | tok/s 23468
step   1310 | loss 1.3990 | lr 8.15e-04 | grad 0.92 | tok/s 23842
step   1320 | loss 1.2594 | lr 8.15e-04 | grad 1.06 | tok/s 23512
step   1330 | loss 1.2642 | lr 8.15e-04 | grad 1.06 | tok/s 24076
step   1340 | loss 1.2816 | lr 8.15e-04 | grad 1.14 | tok/s 23561
step   1350 | loss 1.3454 | lr 8.15e-04 | grad 1.70 | tok/s 23433
step   1360 | loss 1.4723 | lr 8.15e-04 | grad 1.13 | tok/s 23482
step   1370 | loss 1.3465 | lr 8.15e-04 | grad 0.87 | tok/s 23446
step   1380 | loss 1.2961 | lr 8.15e-04 | grad 1.05 | tok/s 24090
step   1390 | loss 1.3483 | lr 8.15e-04 | grad 1.47 | tok/s 24119
step   1400 | loss 1.4114 | lr 8.15e-04 | grad 1.70 | tok/s 23169
step   1410 | loss 1.3290 | lr 8.15e-04 | grad 0.94 | tok/s 22760
step   1420 | loss 1.2174 | lr 8.15e-04 | grad 0.88 | tok/s 23576
step   1430 | loss 1.1799 | lr 8.15e-04 | grad 1.01 | tok/s 24189
step   1440 | loss 1.3853 | lr 8.15e-04 | grad 1.96 | tok/s 23092
step   1450 | loss 1.4130 | lr 8.15e-04 | grad 1.61 | tok/s 23561
step   1460 | loss 1.2686 | lr 8.15e-04 | grad 3.02 | tok/s 23799
step   1470 | loss 1.3468 | lr 8.15e-04 | grad 1.49 | tok/s 23644
step   1480 | loss 1.5630 | lr 8.15e-04 | grad 2.17 | tok/s 23278
step   1490 | loss 1.3147 | lr 8.15e-04 | grad 1.02 | tok/s 24033
step   1500 | loss 1.3793 | lr 8.15e-04 | grad 1.35 | tok/s 23891
step   1510 | loss 1.3400 | lr 8.15e-04 | grad 0.97 | tok/s 23702
step   1520 | loss 1.2751 | lr 8.15e-04 | grad 1.32 | tok/s 23375
step   1530 | loss 1.2912 | lr 8.15e-04 | grad 5.47 | tok/s 24254
step   1540 | loss 1.7331 | lr 8.15e-04 | grad 1.13 | tok/s 23586
step   1550 | loss 1.3433 | lr 8.15e-04 | grad 1.16 | tok/s 23313
step   1560 | loss 1.3791 | lr 8.15e-04 | grad 1.06 | tok/s 23916
step   1570 | loss 1.2617 | lr 8.15e-04 | grad 1.48 | tok/s 23403
step   1580 | loss 1.3950 | lr 8.15e-04 | grad 0.79 | tok/s 23190
step   1590 | loss 1.2049 | lr 8.15e-04 | grad 1.05 | tok/s 24238
step   1600 | loss 1.3975 | lr 8.15e-04 | grad 1.16 | tok/s 23755
step   1610 | loss 1.3272 | lr 8.15e-04 | grad 1.61 | tok/s 24047
step   1620 | loss 1.2873 | lr 8.15e-04 | grad 1.21 | tok/s 23022
step   1630 | loss 1.3291 | lr 8.15e-04 | grad 2.61 | tok/s 23197
step   1640 | loss 1.3541 | lr 8.15e-04 | grad 1.04 | tok/s 23246
step   1650 | loss 1.3133 | lr 8.15e-04 | grad 2.16 | tok/s 24033
step   1660 | loss 1.5730 | lr 8.15e-04 | grad 1.41 | tok/s 23806
step   1670 | loss 1.2406 | lr 8.15e-04 | grad 0.88 | tok/s 23500
step   1680 | loss 1.3795 | lr 8.15e-04 | grad 1.60 | tok/s 23813
step   1690 | loss 1.3917 | lr 8.15e-04 | grad 1.43 | tok/s 23600
step   1700 | loss 1.2799 | lr 8.15e-04 | grad 1.49 | tok/s 23398
step   1710 | loss 1.3771 | lr 8.15e-04 | grad 0.99 | tok/s 23482
step   1720 | loss 1.3455 | lr 8.15e-04 | grad 0.94 | tok/s 23402
step   1730 | loss 1.3534 | lr 8.15e-04 | grad 1.41 | tok/s 23123
step   1740 | loss 1.3968 | lr 8.15e-04 | grad 1.11 | tok/s 23447
step   1750 | loss 1.4543 | lr 8.15e-04 | grad 1.44 | tok/s 23342
step   1760 | loss 1.3291 | lr 8.15e-04 | grad 1.62 | tok/s 22959
step   1770 | loss 1.4371 | lr 8.15e-04 | grad 0.96 | tok/s 23171
step   1780 | loss 1.3424 | lr 8.15e-04 | grad 1.59 | tok/s 23847
step   1790 | loss 1.2587 | lr 8.15e-04 | grad 1.22 | tok/s 23941
step   1800 | loss 1.2548 | lr 8.15e-04 | grad 0.74 | tok/s 23177
step   1810 | loss 1.3273 | lr 8.15e-04 | grad 0.90 | tok/s 23283
step   1820 | loss 1.4240 | lr 8.15e-04 | grad 1.19 | tok/s 23367
step   1830 | loss 1.4282 | lr 8.15e-04 | grad 0.97 | tok/s 23197
step   1840 | loss 1.2546 | lr 8.15e-04 | grad 0.89 | tok/s 23521
step   1850 | loss 1.2474 | lr 8.15e-04 | grad 1.18 | tok/s 24065
step   1860 | loss 1.3346 | lr 8.15e-04 | grad 1.20 | tok/s 23847
step   1870 | loss 1.4076 | lr 8.15e-04 | grad 3.02 | tok/s 23593
step   1880 | loss 1.2969 | lr 8.15e-04 | grad 1.23 | tok/s 23711
step   1890 | loss 1.4191 | lr 8.15e-04 | grad 0.96 | tok/s 23524
step   1900 | loss 1.1812 | lr 8.15e-04 | grad 0.93 | tok/s 23834
step   1910 | loss 1.2675 | lr 8.15e-04 | grad 1.10 | tok/s 23817
step   1920 | loss 1.4090 | lr 8.15e-04 | grad 0.91 | tok/s 24290
step   1930 | loss 1.3025 | lr 8.15e-04 | grad 1.54 | tok/s 23728
step   1940 | loss 1.3472 | lr 8.15e-04 | grad 1.05 | tok/s 23898
step   1950 | loss 1.2065 | lr 8.15e-04 | grad 1.48 | tok/s 23468
step   1960 | loss 1.3733 | lr 8.15e-04 | grad 1.91 | tok/s 23804
step   1970 | loss 1.2735 | lr 8.15e-04 | grad 1.26 | tok/s 23418
step   1980 | loss 1.2440 | lr 8.15e-04 | grad 0.91 | tok/s 24077
step   1990 | loss 1.0661 | lr 8.15e-04 | grad 1.36 | tok/s 24470
step   2000 | loss 1.0949 | lr 8.15e-04 | grad 3.83 | tok/s 24528
  >>> saved checkpoint: checkpoint_step_002000_loss_1.0949.pt
step   2010 | loss 1.2316 | lr 8.15e-04 | grad 1.17 | tok/s 18704
step   2020 | loss 1.1261 | lr 8.15e-04 | grad 0.98 | tok/s 24583
step   2030 | loss 1.3366 | lr 8.15e-04 | grad 1.13 | tok/s 23579
step   2040 | loss 1.3713 | lr 8.15e-04 | grad 1.33 | tok/s 23915
step   2050 | loss 1.3242 | lr 8.15e-04 | grad 2.20 | tok/s 23671
step   2060 | loss 1.3677 | lr 8.15e-04 | grad 0.92 | tok/s 23669
step   2070 | loss 1.2886 | lr 8.15e-04 | grad 1.17 | tok/s 23414
step   2080 | loss 1.1669 | lr 8.15e-04 | grad 0.83 | tok/s 23846
step   2090 | loss 1.1982 | lr 8.15e-04 | grad 0.98 | tok/s 23485
step   2100 | loss 1.0837 | lr 8.15e-04 | grad 1.41 | tok/s 23855
step   2110 | loss 1.3962 | lr 8.15e-04 | grad 0.93 | tok/s 23374
step   2120 | loss 1.3960 | lr 8.15e-04 | grad 1.20 | tok/s 23614
step   2130 | loss 1.3932 | lr 8.15e-04 | grad 1.74 | tok/s 23379
step   2140 | loss 1.4203 | lr 8.15e-04 | grad 1.02 | tok/s 23789
step   2150 | loss 1.2878 | lr 8.15e-04 | grad 1.51 | tok/s 23596
step   2160 | loss 1.4270 | lr 8.15e-04 | grad 1.19 | tok/s 24202
step   2170 | loss 1.1428 | lr 8.15e-04 | grad 1.22 | tok/s 24230
step   2180 | loss 1.1099 | lr 8.15e-04 | grad 0.85 | tok/s 24556
step   2190 | loss 1.1017 | lr 8.15e-04 | grad 1.02 | tok/s 24540
step   2200 | loss 1.1816 | lr 8.15e-04 | grad 1.66 | tok/s 24037
step   2210 | loss 1.2998 | lr 8.15e-04 | grad 3.75 | tok/s 24118
step   2220 | loss 1.3003 | lr 8.15e-04 | grad 1.06 | tok/s 24298
step   2230 | loss 1.3733 | lr 8.15e-04 | grad 1.00 | tok/s 23916
step   2240 | loss 1.2348 | lr 8.15e-04 | grad 0.91 | tok/s 23664
step   2250 | loss 1.3568 | lr 8.15e-04 | grad 2.17 | tok/s 23693
step   2260 | loss 1.3024 | lr 8.15e-04 | grad 0.93 | tok/s 23422
step   2270 | loss 1.2745 | lr 8.15e-04 | grad 1.30 | tok/s 23277
step   2280 | loss 1.3144 | lr 8.15e-04 | grad 1.35 | tok/s 23935
step   2290 | loss 1.2708 | lr 8.15e-04 | grad 1.23 | tok/s 24532
step   2300 | loss 1.2062 | lr 8.15e-04 | grad 1.27 | tok/s 24011
step   2310 | loss 1.2668 | lr 8.15e-04 | grad 0.92 | tok/s 24169
step   2320 | loss 1.3179 | lr 8.15e-04 | grad 0.83 | tok/s 23661
step   2330 | loss 1.5635 | lr 8.15e-04 | grad 2.05 | tok/s 23720
step   2340 | loss 1.3456 | lr 8.15e-04 | grad 1.76 | tok/s 23780
step   2350 | loss 1.2531 | lr 8.15e-04 | grad 1.07 | tok/s 23519
step   2360 | loss 1.2513 | lr 8.15e-04 | grad 1.13 | tok/s 23343
step   2370 | loss 1.3430 | lr 8.15e-04 | grad 2.73 | tok/s 23423
step   2380 | loss 1.2909 | lr 8.15e-04 | grad 1.30 | tok/s 23275
step   2390 | loss 1.3382 | lr 8.15e-04 | grad 0.86 | tok/s 23683
step   2400 | loss 1.2906 | lr 8.15e-04 | grad 0.96 | tok/s 23045
step   2410 | loss 1.3940 | lr 8.15e-04 | grad 0.93 | tok/s 23769
step   2420 | loss 1.2152 | lr 8.15e-04 | grad 1.41 | tok/s 23518
step   2430 | loss 1.3091 | lr 8.15e-04 | grad 2.33 | tok/s 23809
step   2440 | loss 1.0322 | lr 8.15e-04 | grad 1.70 | tok/s 24409
step   2450 | loss 1.2212 | lr 8.15e-04 | grad 0.77 | tok/s 23455
step   2460 | loss 1.2359 | lr 8.15e-04 | grad 0.96 | tok/s 23242
step   2470 | loss 1.2882 | lr 8.15e-04 | grad 1.27 | tok/s 23819
step   2480 | loss 1.2726 | lr 8.15e-04 | grad 1.01 | tok/s 23780
step   2490 | loss 1.4615 | lr 8.15e-04 | grad 2.34 | tok/s 23362
step   2500 | loss 1.3576 | lr 8.15e-04 | grad 1.27 | tok/s 23083
step   2510 | loss 1.2982 | lr 8.15e-04 | grad 0.91 | tok/s 23331
step   2520 | loss 1.6517 | lr 8.15e-04 | grad 1.38 | tok/s 23521
step   2530 | loss 1.3748 | lr 8.15e-04 | grad 1.16 | tok/s 23923
step   2540 | loss 1.3248 | lr 8.15e-04 | grad 2.83 | tok/s 23286
step   2550 | loss 1.2955 | lr 8.15e-04 | grad 0.76 | tok/s 23736
step   2560 | loss 1.2464 | lr 8.15e-04 | grad 1.16 | tok/s 23432
step   2570 | loss 1.3547 | lr 8.15e-04 | grad 1.27 | tok/s 24104
step   2580 | loss 1.2651 | lr 8.15e-04 | grad 1.12 | tok/s 23998
step   2590 | loss 1.1958 | lr 8.15e-04 | grad 1.31 | tok/s 24492
step   2600 | loss 1.2949 | lr 8.15e-04 | grad 1.14 | tok/s 23814
step   2610 | loss 1.2593 | lr 8.15e-04 | grad 1.02 | tok/s 23321
step   2620 | loss 1.3021 | lr 8.15e-04 | grad 1.40 | tok/s 22679
step   2630 | loss 1.5423 | lr 8.15e-04 | grad 0.98 | tok/s 24234
step   2640 | loss 1.2071 | lr 8.15e-04 | grad 0.92 | tok/s 24464
step   2650 | loss 1.1715 | lr 8.15e-04 | grad 0.88 | tok/s 24454
step   2660 | loss 1.1523 | lr 8.15e-04 | grad 0.72 | tok/s 24432

Training complete! Final step: 2668
