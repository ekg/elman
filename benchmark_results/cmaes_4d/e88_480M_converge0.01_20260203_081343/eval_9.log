Using device: cuda
Output directory: benchmark_results/cmaes_4d/e88_480M_converge0.01_20260203_081343/eval_9/levelE88_100m_20260203_084412
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 302,260,006 parameters
Using schedule-free AdamW (lr=0.00035923336728666954)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.1498 | lr 3.59e-04 | grad 10.31 | tok/s 13512
step     20 | loss 4.0268 | lr 3.59e-04 | grad 10.50 | tok/s 22577
step     30 | loss 3.2839 | lr 3.59e-04 | grad 3.61 | tok/s 22699
step     40 | loss 2.5875 | lr 3.59e-04 | grad 4.75 | tok/s 22487
step     50 | loss 2.2233 | lr 3.59e-04 | grad 2.06 | tok/s 22369
step     60 | loss 2.8337 | lr 3.59e-04 | grad 3.53 | tok/s 21421
step     70 | loss 2.4850 | lr 3.59e-04 | grad 4.12 | tok/s 21724
step     80 | loss 2.3854 | lr 3.59e-04 | grad 1.97 | tok/s 21557
step     90 | loss 2.3637 | lr 3.59e-04 | grad 2.17 | tok/s 20991
step    100 | loss 1.9855 | lr 3.59e-04 | grad 2.20 | tok/s 21739
step    110 | loss 2.2945 | lr 3.59e-04 | grad 1.45 | tok/s 20976
step    120 | loss 2.2856 | lr 3.59e-04 | grad 1.92 | tok/s 21140
step    130 | loss 2.0723 | lr 3.59e-04 | grad 1.90 | tok/s 21648
step    140 | loss 1.8862 | lr 3.59e-04 | grad 1.98 | tok/s 20597
step    150 | loss 1.9464 | lr 3.59e-04 | grad 1.42 | tok/s 20805
step    160 | loss 1.9329 | lr 3.59e-04 | grad 1.44 | tok/s 20846
step    170 | loss 2.0446 | lr 3.59e-04 | grad 2.41 | tok/s 21318
step    180 | loss 1.7210 | lr 3.59e-04 | grad 1.42 | tok/s 21227
step    190 | loss 1.4513 | lr 3.59e-04 | grad 1.16 | tok/s 21967
step    200 | loss 1.6698 | lr 3.59e-04 | grad 1.47 | tok/s 21327
step    210 | loss 1.8463 | lr 3.59e-04 | grad 1.81 | tok/s 21652
step    220 | loss 1.7727 | lr 3.59e-04 | grad 2.09 | tok/s 21202
step    230 | loss 1.6729 | lr 3.59e-04 | grad 1.72 | tok/s 21172
step    240 | loss 1.7298 | lr 3.59e-04 | grad 1.56 | tok/s 21618
step    250 | loss 1.8296 | lr 3.59e-04 | grad 1.31 | tok/s 21097
step    260 | loss 1.6582 | lr 3.59e-04 | grad 1.64 | tok/s 20660
step    270 | loss 1.6829 | lr 3.59e-04 | grad 3.02 | tok/s 21120
step    280 | loss 1.5059 | lr 3.59e-04 | grad 1.58 | tok/s 21758
step    290 | loss 1.3845 | lr 3.59e-04 | grad 1.52 | tok/s 22036
step    300 | loss 1.3826 | lr 3.59e-04 | grad 1.18 | tok/s 22036
step    310 | loss 1.4831 | lr 3.59e-04 | grad 1.95 | tok/s 21813
step    320 | loss 1.6904 | lr 3.59e-04 | grad 1.20 | tok/s 20865
step    330 | loss 1.6862 | lr 3.59e-04 | grad 2.41 | tok/s 21492
step    340 | loss 1.6610 | lr 3.59e-04 | grad 1.34 | tok/s 20820
step    350 | loss 1.6079 | lr 3.59e-04 | grad 1.15 | tok/s 20691
step    360 | loss 1.5023 | lr 3.59e-04 | grad 1.14 | tok/s 21400
step    370 | loss 1.8613 | lr 3.59e-04 | grad 1.45 | tok/s 21461
step    380 | loss 1.5805 | lr 3.59e-04 | grad 1.30 | tok/s 21825
step    390 | loss 1.5559 | lr 3.59e-04 | grad 1.27 | tok/s 21254
step    400 | loss 1.5539 | lr 3.59e-04 | grad 1.28 | tok/s 21445
step    410 | loss 1.5482 | lr 3.59e-04 | grad 1.36 | tok/s 20787
step    420 | loss 1.6026 | lr 3.59e-04 | grad 1.57 | tok/s 20900
step    430 | loss 1.6650 | lr 3.59e-04 | grad 2.47 | tok/s 21569
step    440 | loss 1.5863 | lr 3.59e-04 | grad 1.17 | tok/s 21166
step    450 | loss 1.5677 | lr 3.59e-04 | grad 1.28 | tok/s 21180
step    460 | loss 1.5781 | lr 3.59e-04 | grad 1.81 | tok/s 21196
step    470 | loss 1.4617 | lr 3.59e-04 | grad 1.12 | tok/s 20614
step    480 | loss 1.4862 | lr 3.59e-04 | grad 1.16 | tok/s 21095
step    490 | loss 1.8760 | lr 3.59e-04 | grad 1.70 | tok/s 21720
step    500 | loss 1.5245 | lr 3.59e-04 | grad 3.20 | tok/s 21225
step    510 | loss 1.3293 | lr 3.59e-04 | grad 1.15 | tok/s 21830
step    520 | loss 1.9707 | lr 3.59e-04 | grad 1.96 | tok/s 21251
step    530 | loss 1.3694 | lr 3.59e-04 | grad 6.44 | tok/s 21553
step    540 | loss 1.4178 | lr 3.59e-04 | grad 1.30 | tok/s 21712
step    550 | loss 1.2718 | lr 3.59e-04 | grad 1.28 | tok/s 22085
step    560 | loss 1.4034 | lr 3.59e-04 | grad 3.91 | tok/s 21786
step    570 | loss 1.7788 | lr 3.59e-04 | grad 1.40 | tok/s 21832
step    580 | loss 1.8287 | lr 3.59e-04 | grad 2.83 | tok/s 21043
step    590 | loss 1.4617 | lr 3.59e-04 | grad 1.74 | tok/s 21257
step    600 | loss 1.4561 | lr 3.59e-04 | grad 1.55 | tok/s 22088
step    610 | loss 1.4508 | lr 3.59e-04 | grad 1.45 | tok/s 20987
step    620 | loss 1.3798 | lr 3.59e-04 | grad 1.19 | tok/s 21771
step    630 | loss 1.5539 | lr 3.59e-04 | grad 1.43 | tok/s 21745
step    640 | loss 1.4663 | lr 3.59e-04 | grad 1.27 | tok/s 21237
step    650 | loss 1.5573 | lr 3.59e-04 | grad 4.78 | tok/s 20985
step    660 | loss 1.5704 | lr 3.59e-04 | grad 1.76 | tok/s 21669
step    670 | loss 1.5424 | lr 3.59e-04 | grad 1.67 | tok/s 21244
step    680 | loss 1.5072 | lr 3.59e-04 | grad 1.61 | tok/s 21073
step    690 | loss 1.6135 | lr 3.59e-04 | grad 2.19 | tok/s 21273
step    700 | loss 1.4805 | lr 3.59e-04 | grad 1.20 | tok/s 21501
step    710 | loss 1.4433 | lr 3.59e-04 | grad 4.22 | tok/s 21273
step    720 | loss 1.5956 | lr 3.59e-04 | grad 1.45 | tok/s 21421
step    730 | loss 1.5954 | lr 3.59e-04 | grad 3.12 | tok/s 21418
step    740 | loss 1.4143 | lr 3.59e-04 | grad 1.29 | tok/s 21137
step    750 | loss 1.6860 | lr 3.59e-04 | grad 1.20 | tok/s 21404
step    760 | loss 1.4259 | lr 3.59e-04 | grad 1.17 | tok/s 21411
step    770 | loss 1.5290 | lr 3.59e-04 | grad 1.43 | tok/s 21742
step    780 | loss 1.3556 | lr 3.59e-04 | grad 1.15 | tok/s 21800
step    790 | loss 1.3866 | lr 3.59e-04 | grad 2.62 | tok/s 21547
step    800 | loss 1.3920 | lr 3.59e-04 | grad 1.50 | tok/s 21388
step    810 | loss 2.1049 | lr 3.59e-04 | grad 2.14 | tok/s 22099
step    820 | loss 1.7141 | lr 3.59e-04 | grad 1.58 | tok/s 22269
step    830 | loss 1.4935 | lr 3.59e-04 | grad 1.56 | tok/s 22265
step    840 | loss 1.5290 | lr 3.59e-04 | grad 1.19 | tok/s 21260
step    850 | loss 1.4211 | lr 3.59e-04 | grad 1.07 | tok/s 21497
step    860 | loss 1.4294 | lr 3.59e-04 | grad 1.56 | tok/s 21447
step    870 | loss 1.4985 | lr 3.59e-04 | grad 1.20 | tok/s 21820
step    880 | loss 1.4151 | lr 3.59e-04 | grad 1.91 | tok/s 21105
step    890 | loss 1.7148 | lr 3.59e-04 | grad 1.42 | tok/s 21109
step    900 | loss 1.3511 | lr 3.59e-04 | grad 1.47 | tok/s 21122
step    910 | loss 1.4914 | lr 3.59e-04 | grad 1.09 | tok/s 21339
step    920 | loss 1.4203 | lr 3.59e-04 | grad 1.43 | tok/s 21203
step    930 | loss 1.5125 | lr 3.59e-04 | grad 1.50 | tok/s 21228
step    940 | loss 1.4617 | lr 3.59e-04 | grad 1.21 | tok/s 21734
step    950 | loss 1.2696 | lr 3.59e-04 | grad 1.18 | tok/s 22344
step    960 | loss 1.2104 | lr 3.59e-04 | grad 1.04 | tok/s 22362
step    970 | loss 1.4169 | lr 3.59e-04 | grad 1.38 | tok/s 21550
step    980 | loss 1.5234 | lr 3.59e-04 | grad 2.27 | tok/s 21169
step    990 | loss 1.5032 | lr 3.59e-04 | grad 1.75 | tok/s 21485
step   1000 | loss 1.3674 | lr 3.59e-04 | grad 1.22 | tok/s 21988
  >>> saved checkpoint: checkpoint_step_001000_loss_1.3674.pt
step   1010 | loss 1.3828 | lr 3.59e-04 | grad 1.15 | tok/s 16295
step   1020 | loss 1.6641 | lr 3.59e-04 | grad 1.01 | tok/s 21551
step   1030 | loss 1.5541 | lr 3.59e-04 | grad 1.46 | tok/s 21559
step   1040 | loss 1.2100 | lr 3.59e-04 | grad 1.01 | tok/s 21381
step   1050 | loss 1.6730 | lr 3.59e-04 | grad 1.08 | tok/s 21962
step   1060 | loss 1.8695 | lr 3.59e-04 | grad 1.45 | tok/s 21518
step   1070 | loss 1.5047 | lr 3.59e-04 | grad 1.50 | tok/s 21215
step   1080 | loss 1.6166 | lr 3.59e-04 | grad 1.02 | tok/s 21804
step   1090 | loss 1.4155 | lr 3.59e-04 | grad 1.91 | tok/s 22121
step   1100 | loss 1.3629 | lr 3.59e-04 | grad 1.64 | tok/s 21903
step   1110 | loss 1.4911 | lr 3.59e-04 | grad 0.96 | tok/s 21129
step   1120 | loss 1.5109 | lr 3.59e-04 | grad 1.45 | tok/s 21609
step   1130 | loss 1.5282 | lr 3.59e-04 | grad 2.78 | tok/s 21529
step   1140 | loss 1.5052 | lr 3.59e-04 | grad 2.05 | tok/s 21223
step   1150 | loss 1.5464 | lr 3.59e-04 | grad 1.40 | tok/s 21043
step   1160 | loss 1.3822 | lr 3.59e-04 | grad 1.38 | tok/s 22209
step   1170 | loss 1.3233 | lr 3.59e-04 | grad 0.96 | tok/s 22381
step   1180 | loss 1.2564 | lr 3.59e-04 | grad 1.26 | tok/s 22379
step   1190 | loss 1.2178 | lr 3.59e-04 | grad 1.22 | tok/s 22392
step   1200 | loss 1.1942 | lr 3.59e-04 | grad 0.96 | tok/s 22411
step   1210 | loss 1.3034 | lr 3.59e-04 | grad 0.95 | tok/s 22020
step   1220 | loss 1.4007 | lr 3.59e-04 | grad 1.01 | tok/s 21052
step   1230 | loss 1.4758 | lr 3.59e-04 | grad 1.06 | tok/s 21852
step   1240 | loss 1.4144 | lr 3.59e-04 | grad 1.87 | tok/s 21614
step   1250 | loss 1.6082 | lr 3.59e-04 | grad 1.21 | tok/s 21523
step   1260 | loss 1.4676 | lr 3.59e-04 | grad 1.57 | tok/s 21526
step   1270 | loss 1.5096 | lr 3.59e-04 | grad 3.00 | tok/s 21200
step   1280 | loss 1.4504 | lr 3.59e-04 | grad 1.30 | tok/s 21329
step   1290 | loss 1.5195 | lr 3.59e-04 | grad 1.04 | tok/s 21232
step   1300 | loss 1.4411 | lr 3.59e-04 | grad 1.31 | tok/s 21384
step   1310 | loss 1.4627 | lr 3.59e-04 | grad 1.20 | tok/s 21726
step   1320 | loss 1.3049 | lr 3.59e-04 | grad 1.21 | tok/s 21371
step   1330 | loss 1.2984 | lr 3.59e-04 | grad 0.98 | tok/s 21892
step   1340 | loss 1.3329 | lr 3.59e-04 | grad 1.02 | tok/s 21402
step   1350 | loss 1.3952 | lr 3.59e-04 | grad 1.94 | tok/s 21322
step   1360 | loss 1.5317 | lr 3.59e-04 | grad 1.30 | tok/s 21290
step   1370 | loss 1.4093 | lr 3.59e-04 | grad 1.15 | tok/s 21308
step   1380 | loss 1.3571 | lr 3.59e-04 | grad 1.05 | tok/s 21892
step   1390 | loss 1.4111 | lr 3.59e-04 | grad 1.44 | tok/s 21920
step   1400 | loss 1.4583 | lr 3.59e-04 | grad 2.31 | tok/s 21021
step   1410 | loss 1.3923 | lr 3.59e-04 | grad 0.97 | tok/s 20690
step   1420 | loss 1.2669 | lr 3.59e-04 | grad 1.14 | tok/s 21416
step   1430 | loss 1.2331 | lr 3.59e-04 | grad 1.02 | tok/s 21643
step   1440 | loss 1.4435 | lr 3.59e-04 | grad 1.77 | tok/s 20934
step   1450 | loss 1.4764 | lr 3.59e-04 | grad 1.41 | tok/s 21376
step   1460 | loss 1.3249 | lr 3.59e-04 | grad 3.47 | tok/s 21545
step   1470 | loss 1.4077 | lr 3.59e-04 | grad 1.52 | tok/s 21395
step   1480 | loss 1.6421 | lr 3.59e-04 | grad 2.58 | tok/s 21077
step   1490 | loss 1.3882 | lr 3.59e-04 | grad 1.33 | tok/s 21766
step   1500 | loss 1.4403 | lr 3.59e-04 | grad 1.34 | tok/s 21655
step   1510 | loss 1.3933 | lr 3.59e-04 | grad 1.10 | tok/s 21427
step   1520 | loss 1.3417 | lr 3.59e-04 | grad 1.05 | tok/s 21170
step   1530 | loss 1.3293 | lr 3.59e-04 | grad 6.50 | tok/s 21990
step   1540 | loss 1.7993 | lr 3.59e-04 | grad 1.55 | tok/s 21371
step   1550 | loss 1.4035 | lr 3.59e-04 | grad 1.16 | tok/s 21097
step   1560 | loss 1.4455 | lr 3.59e-04 | grad 1.38 | tok/s 21642
step   1570 | loss 1.3135 | lr 3.59e-04 | grad 1.48 | tok/s 21198
step   1580 | loss 1.4527 | lr 3.59e-04 | grad 0.93 | tok/s 21005
step   1590 | loss 1.2602 | lr 3.59e-04 | grad 1.01 | tok/s 21911
step   1600 | loss 1.4506 | lr 3.59e-04 | grad 1.34 | tok/s 21503
step   1610 | loss 1.3779 | lr 3.59e-04 | grad 1.17 | tok/s 21732
step   1620 | loss 1.3440 | lr 3.59e-04 | grad 1.14 | tok/s 20785
step   1630 | loss 1.3899 | lr 3.59e-04 | grad 3.16 | tok/s 21018
step   1640 | loss 1.4228 | lr 3.59e-04 | grad 1.71 | tok/s 21081
step   1650 | loss 1.3920 | lr 3.59e-04 | grad 2.36 | tok/s 21763
step   1660 | loss 1.6379 | lr 3.59e-04 | grad 1.60 | tok/s 21539
step   1670 | loss 1.2958 | lr 3.59e-04 | grad 1.20 | tok/s 21317
step   1680 | loss 1.4231 | lr 3.59e-04 | grad 1.71 | tok/s 21579
step   1690 | loss 1.4646 | lr 3.59e-04 | grad 1.18 | tok/s 21373
step   1700 | loss 1.3482 | lr 3.59e-04 | grad 2.06 | tok/s 21186
step   1710 | loss 1.4660 | lr 3.59e-04 | grad 1.04 | tok/s 21248
step   1720 | loss 1.4156 | lr 3.59e-04 | grad 1.12 | tok/s 21222
step   1730 | loss 1.4091 | lr 3.59e-04 | grad 1.38 | tok/s 20972
step   1740 | loss 1.4705 | lr 3.59e-04 | grad 1.22 | tok/s 21276
step   1750 | loss 1.5199 | lr 3.59e-04 | grad 2.11 | tok/s 21153
step   1760 | loss 1.3822 | lr 3.59e-04 | grad 1.89 | tok/s 21056
step   1770 | loss 1.4978 | lr 3.59e-04 | grad 0.94 | tok/s 21001
step   1780 | loss 1.3964 | lr 3.59e-04 | grad 2.12 | tok/s 21559
step   1790 | loss 1.3133 | lr 3.59e-04 | grad 1.21 | tok/s 21700
step   1800 | loss 1.3137 | lr 3.59e-04 | grad 0.95 | tok/s 20996
step   1810 | loss 1.3864 | lr 3.59e-04 | grad 0.96 | tok/s 21078
step   1820 | loss 1.4766 | lr 3.59e-04 | grad 1.30 | tok/s 21145
step   1830 | loss 1.4994 | lr 3.59e-04 | grad 1.28 | tok/s 20989
step   1840 | loss 1.3197 | lr 3.59e-04 | grad 1.16 | tok/s 21297
step   1850 | loss 1.3091 | lr 3.59e-04 | grad 1.53 | tok/s 21793
step   1860 | loss 1.3928 | lr 3.59e-04 | grad 1.09 | tok/s 21609
step   1870 | loss 1.4876 | lr 3.59e-04 | grad 2.83 | tok/s 21394
step   1880 | loss 1.3534 | lr 3.59e-04 | grad 1.25 | tok/s 21485
step   1890 | loss 1.4781 | lr 3.59e-04 | grad 0.96 | tok/s 21276
step   1900 | loss 1.2385 | lr 3.59e-04 | grad 1.18 | tok/s 21607
step   1910 | loss 1.3209 | lr 3.59e-04 | grad 1.05 | tok/s 21605
step   1920 | loss 1.4303 | lr 3.59e-04 | grad 1.23 | tok/s 22017
step   1930 | loss 1.3591 | lr 3.59e-04 | grad 1.77 | tok/s 21506
step   1940 | loss 1.3680 | lr 3.59e-04 | grad 1.41 | tok/s 21657
step   1950 | loss 1.2638 | lr 3.59e-04 | grad 1.15 | tok/s 21293
step   1960 | loss 1.4630 | lr 3.59e-04 | grad 3.33 | tok/s 21592
step   1970 | loss 1.3301 | lr 3.59e-04 | grad 1.48 | tok/s 21217
step   1980 | loss 1.3066 | lr 3.59e-04 | grad 1.01 | tok/s 21846
step   1990 | loss 1.1095 | lr 3.59e-04 | grad 1.56 | tok/s 22246
step   2000 | loss 1.1517 | lr 3.59e-04 | grad 4.66 | tok/s 22189
  >>> saved checkpoint: checkpoint_step_002000_loss_1.1517.pt
step   2010 | loss 1.2737 | lr 3.59e-04 | grad 0.97 | tok/s 17223
step   2020 | loss 1.1724 | lr 3.59e-04 | grad 1.00 | tok/s 22365
step   2030 | loss 1.4065 | lr 3.59e-04 | grad 1.42 | tok/s 21399
step   2040 | loss 1.4214 | lr 3.59e-04 | grad 1.20 | tok/s 21674
step   2050 | loss 1.3811 | lr 3.59e-04 | grad 1.75 | tok/s 21455
step   2060 | loss 1.4260 | lr 3.59e-04 | grad 1.17 | tok/s 21438
step   2070 | loss 1.3565 | lr 3.59e-04 | grad 1.12 | tok/s 21189
step   2080 | loss 1.2262 | lr 3.59e-04 | grad 1.08 | tok/s 21593
step   2090 | loss 1.2659 | lr 3.59e-04 | grad 1.41 | tok/s 21240
step   2100 | loss 1.1459 | lr 3.59e-04 | grad 1.66 | tok/s 21557
step   2110 | loss 1.4534 | lr 3.59e-04 | grad 1.29 | tok/s 21144
step   2120 | loss 1.4471 | lr 3.59e-04 | grad 1.70 | tok/s 21365
step   2130 | loss 1.4489 | lr 3.59e-04 | grad 2.47 | tok/s 21137
step   2140 | loss 1.4896 | lr 3.59e-04 | grad 1.17 | tok/s 21530
step   2150 | loss 1.3514 | lr 3.59e-04 | grad 1.71 | tok/s 21359
step   2160 | loss 1.5070 | lr 3.59e-04 | grad 1.76 | tok/s 21880
step   2170 | loss 1.1943 | lr 3.59e-04 | grad 1.07 | tok/s 21902
step   2180 | loss 1.1562 | lr 3.59e-04 | grad 0.91 | tok/s 22182
step   2190 | loss 1.1486 | lr 3.59e-04 | grad 0.95 | tok/s 22219
step   2200 | loss 1.2405 | lr 3.59e-04 | grad 2.52 | tok/s 21788
step   2210 | loss 1.3584 | lr 3.59e-04 | grad 3.36 | tok/s 21833
step   2220 | loss 1.3551 | lr 3.59e-04 | grad 1.21 | tok/s 21991
step   2230 | loss 1.4422 | lr 3.59e-04 | grad 1.23 | tok/s 21636
step   2240 | loss 1.3029 | lr 3.59e-04 | grad 1.07 | tok/s 21372
step   2250 | loss 1.4427 | lr 3.59e-04 | grad 2.47 | tok/s 21442
step   2260 | loss 1.3608 | lr 3.59e-04 | grad 1.29 | tok/s 21227
step   2270 | loss 1.3246 | lr 3.59e-04 | grad 1.29 | tok/s 21106
step   2280 | loss 1.3628 | lr 3.59e-04 | grad 1.30 | tok/s 21674
step   2290 | loss 1.3151 | lr 3.59e-04 | grad 1.01 | tok/s 22224
step   2300 | loss 1.2489 | lr 3.59e-04 | grad 0.95 | tok/s 22200
step   2310 | loss 1.3155 | lr 3.59e-04 | grad 1.22 | tok/s 21889
step   2320 | loss 1.3903 | lr 3.59e-04 | grad 1.12 | tok/s 21480
step   2330 | loss 1.6541 | lr 3.59e-04 | grad 2.25 | tok/s 21491
step   2340 | loss 1.4103 | lr 3.59e-04 | grad 1.52 | tok/s 21503
step   2350 | loss 1.3190 | lr 3.59e-04 | grad 1.16 | tok/s 21289
step   2360 | loss 1.3197 | lr 3.59e-04 | grad 1.54 | tok/s 21097
step   2370 | loss 1.3997 | lr 3.59e-04 | grad 2.62 | tok/s 21178
step   2380 | loss 1.3495 | lr 3.59e-04 | grad 1.43 | tok/s 21013
step   2390 | loss 1.4018 | lr 3.59e-04 | grad 1.04 | tok/s 21254
step   2400 | loss 1.3217 | lr 3.59e-04 | grad 1.57 | tok/s 20914
step   2410 | loss 1.4510 | lr 3.59e-04 | grad 1.18 | tok/s 21482
step   2420 | loss 1.2687 | lr 3.59e-04 | grad 1.73 | tok/s 21276

Training complete! Final step: 2422
