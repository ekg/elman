Using device: cuda
Output directory: benchmark_results/cmaes_4d/e88_480M_converge0.01_20260203_081343/eval_22/levelE88_100m_20260203_091435
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 216,167,552 parameters
Using schedule-free AdamW (lr=0.0003856731620676956)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.7118 | lr 3.86e-04 | grad 13.06 | tok/s 16397
step     20 | loss 4.2218 | lr 3.86e-04 | grad 6.56 | tok/s 31999
step     30 | loss 3.1317 | lr 3.86e-04 | grad 5.94 | tok/s 32268
step     40 | loss 2.7415 | lr 3.86e-04 | grad 5.94 | tok/s 32119
step     50 | loss 2.3838 | lr 3.86e-04 | grad 2.59 | tok/s 31935
step     60 | loss 3.0149 | lr 3.86e-04 | grad 4.44 | tok/s 30592
step     70 | loss 2.5554 | lr 3.86e-04 | grad 4.28 | tok/s 30956
step     80 | loss 2.4309 | lr 3.86e-04 | grad 3.97 | tok/s 30737
step     90 | loss 2.3447 | lr 3.86e-04 | grad 2.75 | tok/s 29992
step    100 | loss 2.0370 | lr 3.86e-04 | grad 3.02 | tok/s 31087
step    110 | loss 2.3005 | lr 3.86e-04 | grad 1.55 | tok/s 30028
step    120 | loss 2.2952 | lr 3.86e-04 | grad 2.77 | tok/s 30238
step    130 | loss 2.1039 | lr 3.86e-04 | grad 2.17 | tok/s 30988
step    140 | loss 1.9164 | lr 3.86e-04 | grad 2.16 | tok/s 29515
step    150 | loss 1.9640 | lr 3.86e-04 | grad 1.59 | tok/s 29781
step    160 | loss 1.9643 | lr 3.86e-04 | grad 1.83 | tok/s 29856
step    170 | loss 2.0548 | lr 3.86e-04 | grad 2.31 | tok/s 30472
step    180 | loss 1.7187 | lr 3.86e-04 | grad 1.66 | tok/s 30478
step    190 | loss 1.4601 | lr 3.86e-04 | grad 1.38 | tok/s 31477
step    200 | loss 1.6662 | lr 3.86e-04 | grad 1.94 | tok/s 30556
step    210 | loss 1.8295 | lr 3.86e-04 | grad 1.80 | tok/s 31046
step    220 | loss 1.7762 | lr 3.86e-04 | grad 2.17 | tok/s 30434
step    230 | loss 1.6780 | lr 3.86e-04 | grad 1.76 | tok/s 30360
step    240 | loss 1.7159 | lr 3.86e-04 | grad 1.59 | tok/s 31042
step    250 | loss 1.8419 | lr 3.86e-04 | grad 1.41 | tok/s 30284
step    260 | loss 1.6698 | lr 3.86e-04 | grad 1.69 | tok/s 29636
step    270 | loss 1.6863 | lr 3.86e-04 | grad 2.20 | tok/s 30292
step    280 | loss 1.5169 | lr 3.86e-04 | grad 1.54 | tok/s 31262
step    290 | loss 1.3961 | lr 3.86e-04 | grad 1.59 | tok/s 31664
step    300 | loss 1.4085 | lr 3.86e-04 | grad 1.67 | tok/s 31661
step    310 | loss 1.5080 | lr 3.86e-04 | grad 2.00 | tok/s 31341
step    320 | loss 1.6996 | lr 3.86e-04 | grad 1.25 | tok/s 29922
step    330 | loss 1.6971 | lr 3.86e-04 | grad 2.47 | tok/s 30879
step    340 | loss 1.6636 | lr 3.86e-04 | grad 1.44 | tok/s 29878
step    350 | loss 1.6137 | lr 3.86e-04 | grad 1.10 | tok/s 29687
step    360 | loss 1.5018 | lr 3.86e-04 | grad 1.33 | tok/s 30678
step    370 | loss 1.8376 | lr 3.86e-04 | grad 1.48 | tok/s 30774
step    380 | loss 1.5767 | lr 3.86e-04 | grad 1.33 | tok/s 31227
step    390 | loss 1.5645 | lr 3.86e-04 | grad 1.56 | tok/s 30489
step    400 | loss 1.5441 | lr 3.86e-04 | grad 1.27 | tok/s 30704
step    410 | loss 1.5647 | lr 3.86e-04 | grad 1.36 | tok/s 29782
step    420 | loss 1.6123 | lr 3.86e-04 | grad 1.50 | tok/s 29984
step    430 | loss 1.6807 | lr 3.86e-04 | grad 2.83 | tok/s 30915
step    440 | loss 1.5946 | lr 3.86e-04 | grad 1.25 | tok/s 30305
step    450 | loss 1.5745 | lr 3.86e-04 | grad 1.30 | tok/s 30315
step    460 | loss 1.5877 | lr 3.86e-04 | grad 1.82 | tok/s 30319
step    470 | loss 1.4662 | lr 3.86e-04 | grad 1.12 | tok/s 29470
step    480 | loss 1.4947 | lr 3.86e-04 | grad 1.23 | tok/s 30159
step    490 | loss 1.8857 | lr 3.86e-04 | grad 1.71 | tok/s 31135
step    500 | loss 1.5301 | lr 3.86e-04 | grad 3.05 | tok/s 30370
step    510 | loss 1.3269 | lr 3.86e-04 | grad 1.09 | tok/s 31231
step    520 | loss 1.9946 | lr 3.86e-04 | grad 2.11 | tok/s 30320
step    530 | loss 1.3657 | lr 3.86e-04 | grad 5.06 | tok/s 30781
step    540 | loss 1.4301 | lr 3.86e-04 | grad 1.33 | tok/s 31122
step    550 | loss 1.2828 | lr 3.86e-04 | grad 1.40 | tok/s 31654
step    560 | loss 1.4142 | lr 3.86e-04 | grad 3.92 | tok/s 31165
step    570 | loss 1.7766 | lr 3.86e-04 | grad 1.33 | tok/s 31289
step    580 | loss 1.8304 | lr 3.86e-04 | grad 3.27 | tok/s 30124
step    590 | loss 1.4712 | lr 3.86e-04 | grad 1.64 | tok/s 30453
step    600 | loss 1.4489 | lr 3.86e-04 | grad 1.52 | tok/s 31573
step    610 | loss 1.4552 | lr 3.86e-04 | grad 1.23 | tok/s 29973
step    620 | loss 1.3832 | lr 3.86e-04 | grad 1.31 | tok/s 31155
step    630 | loss 1.5679 | lr 3.86e-04 | grad 1.43 | tok/s 31112
step    640 | loss 1.4825 | lr 3.86e-04 | grad 1.27 | tok/s 30399
step    650 | loss 1.5734 | lr 3.86e-04 | grad 4.03 | tok/s 29992
step    660 | loss 1.5675 | lr 3.86e-04 | grad 1.73 | tok/s 30950
step    670 | loss 1.5474 | lr 3.86e-04 | grad 1.65 | tok/s 30338
step    680 | loss 1.5138 | lr 3.86e-04 | grad 1.61 | tok/s 30089
step    690 | loss 1.6253 | lr 3.86e-04 | grad 2.27 | tok/s 30378
step    700 | loss 1.4975 | lr 3.86e-04 | grad 1.26 | tok/s 30689
step    710 | loss 1.4738 | lr 3.86e-04 | grad 4.38 | tok/s 30388
step    720 | loss 1.5983 | lr 3.86e-04 | grad 1.37 | tok/s 30586
step    730 | loss 1.5989 | lr 3.86e-04 | grad 2.89 | tok/s 30484
step    740 | loss 1.4239 | lr 3.86e-04 | grad 1.25 | tok/s 30155
step    750 | loss 1.7886 | lr 3.86e-04 | grad 1.23 | tok/s 30530
step    760 | loss 1.4370 | lr 3.86e-04 | grad 1.40 | tok/s 30466
step    770 | loss 1.5425 | lr 3.86e-04 | grad 1.46 | tok/s 30964
step    780 | loss 1.3576 | lr 3.86e-04 | grad 1.09 | tok/s 31066
step    790 | loss 1.4140 | lr 3.86e-04 | grad 3.05 | tok/s 30689
step    800 | loss 1.4007 | lr 3.86e-04 | grad 1.37 | tok/s 30434
step    810 | loss 2.0890 | lr 3.86e-04 | grad 2.45 | tok/s 31437
step    820 | loss 1.7239 | lr 3.86e-04 | grad 1.87 | tok/s 31661
step    830 | loss 1.5213 | lr 3.86e-04 | grad 1.45 | tok/s 31660
step    840 | loss 1.5503 | lr 3.86e-04 | grad 1.28 | tok/s 30204
step    850 | loss 1.4320 | lr 3.86e-04 | grad 1.08 | tok/s 30570
step    860 | loss 1.4389 | lr 3.86e-04 | grad 1.50 | tok/s 30473
step    870 | loss 1.5133 | lr 3.86e-04 | grad 1.38 | tok/s 31028
step    880 | loss 1.4234 | lr 3.86e-04 | grad 1.96 | tok/s 30131
step    890 | loss 1.7053 | lr 3.86e-04 | grad 1.42 | tok/s 29952
step    900 | loss 1.3552 | lr 3.86e-04 | grad 1.62 | tok/s 29963
step    910 | loss 1.5049 | lr 3.86e-04 | grad 1.09 | tok/s 30260
step    920 | loss 1.4284 | lr 3.86e-04 | grad 1.45 | tok/s 30090
step    930 | loss 1.5265 | lr 3.86e-04 | grad 1.55 | tok/s 30074
step    940 | loss 1.4814 | lr 3.86e-04 | grad 1.45 | tok/s 30768
step    950 | loss 1.2793 | lr 3.86e-04 | grad 1.41 | tok/s 31661
step    960 | loss 1.2330 | lr 3.86e-04 | grad 1.31 | tok/s 31658
step    970 | loss 1.4287 | lr 3.86e-04 | grad 1.38 | tok/s 30536
step    980 | loss 1.5431 | lr 3.86e-04 | grad 2.41 | tok/s 29967
step    990 | loss 1.5067 | lr 3.86e-04 | grad 1.87 | tok/s 30446
step   1000 | loss 1.3775 | lr 3.86e-04 | grad 1.24 | tok/s 31105
  >>> saved checkpoint: checkpoint_step_001000_loss_1.3775.pt
step   1010 | loss 1.3900 | lr 3.86e-04 | grad 1.12 | tok/s 22690
step   1020 | loss 1.7145 | lr 3.86e-04 | grad 1.10 | tok/s 30298
step   1030 | loss 1.5587 | lr 3.86e-04 | grad 1.53 | tok/s 30402
step   1040 | loss 1.2342 | lr 3.86e-04 | grad 1.38 | tok/s 30138
step   1050 | loss 1.6769 | lr 3.86e-04 | grad 1.03 | tok/s 31010
step   1060 | loss 1.8817 | lr 3.86e-04 | grad 1.49 | tok/s 30389
step   1070 | loss 1.5140 | lr 3.86e-04 | grad 1.64 | tok/s 29980
step   1080 | loss 1.6353 | lr 3.86e-04 | grad 1.05 | tok/s 30830
step   1090 | loss 1.4200 | lr 3.86e-04 | grad 1.93 | tok/s 31275
step   1100 | loss 1.3788 | lr 3.86e-04 | grad 1.72 | tok/s 30928
step   1110 | loss 1.5037 | lr 3.86e-04 | grad 0.89 | tok/s 29874
step   1120 | loss 1.5259 | lr 3.86e-04 | grad 1.38 | tok/s 30527
step   1130 | loss 1.5438 | lr 3.86e-04 | grad 2.72 | tok/s 30414
step   1140 | loss 1.5164 | lr 3.86e-04 | grad 1.95 | tok/s 30025
step   1150 | loss 1.5701 | lr 3.86e-04 | grad 1.34 | tok/s 29734
step   1160 | loss 1.3952 | lr 3.86e-04 | grad 1.51 | tok/s 31374
step   1170 | loss 1.3397 | lr 3.86e-04 | grad 1.17 | tok/s 31659
step   1180 | loss 1.2769 | lr 3.86e-04 | grad 1.33 | tok/s 31664
step   1190 | loss 1.2449 | lr 3.86e-04 | grad 1.41 | tok/s 31661
step   1200 | loss 1.2201 | lr 3.86e-04 | grad 0.91 | tok/s 31659
step   1210 | loss 1.3123 | lr 3.86e-04 | grad 0.88 | tok/s 31101
step   1220 | loss 1.4009 | lr 3.86e-04 | grad 0.94 | tok/s 29716
step   1230 | loss 1.4849 | lr 3.86e-04 | grad 1.12 | tok/s 30854
step   1240 | loss 1.4258 | lr 3.86e-04 | grad 1.79 | tok/s 30565
step   1250 | loss 1.6068 | lr 3.86e-04 | grad 1.10 | tok/s 30416
step   1260 | loss 1.4776 | lr 3.86e-04 | grad 1.55 | tok/s 30390
step   1270 | loss 1.5260 | lr 3.86e-04 | grad 3.69 | tok/s 29979
step   1280 | loss 1.4614 | lr 3.86e-04 | grad 1.31 | tok/s 30139
step   1290 | loss 1.5176 | lr 3.86e-04 | grad 1.01 | tok/s 29979
step   1300 | loss 1.4503 | lr 3.86e-04 | grad 1.38 | tok/s 30231
step   1310 | loss 1.4745 | lr 3.86e-04 | grad 1.16 | tok/s 30699
step   1320 | loss 1.3160 | lr 3.86e-04 | grad 1.23 | tok/s 30228
step   1330 | loss 1.3122 | lr 3.86e-04 | grad 1.07 | tok/s 30974
step   1340 | loss 1.3440 | lr 3.86e-04 | grad 1.04 | tok/s 30291
step   1350 | loss 1.4028 | lr 3.86e-04 | grad 1.89 | tok/s 30146
step   1360 | loss 1.5420 | lr 3.86e-04 | grad 1.26 | tok/s 30159
step   1370 | loss 1.4187 | lr 3.86e-04 | grad 1.10 | tok/s 30176
step   1380 | loss 1.3632 | lr 3.86e-04 | grad 1.16 | tok/s 30993
step   1390 | loss 1.4895 | lr 3.86e-04 | grad 1.41 | tok/s 31007
step   1400 | loss 1.4779 | lr 3.86e-04 | grad 2.08 | tok/s 29759
step   1410 | loss 1.4038 | lr 3.86e-04 | grad 0.91 | tok/s 29283
step   1420 | loss 1.2734 | lr 3.86e-04 | grad 1.06 | tok/s 30335
step   1430 | loss 1.2337 | lr 3.86e-04 | grad 0.98 | tok/s 31175
step   1440 | loss 1.4530 | lr 3.86e-04 | grad 1.77 | tok/s 29713
step   1450 | loss 1.4871 | lr 3.86e-04 | grad 1.52 | tok/s 30312
step   1460 | loss 1.3280 | lr 3.86e-04 | grad 3.20 | tok/s 30564
step   1470 | loss 1.4184 | lr 3.86e-04 | grad 1.55 | tok/s 30408
step   1480 | loss 1.6575 | lr 3.86e-04 | grad 2.53 | tok/s 29946
step   1490 | loss 1.3904 | lr 3.86e-04 | grad 1.24 | tok/s 30951
step   1500 | loss 1.4604 | lr 3.86e-04 | grad 1.39 | tok/s 30758
step   1510 | loss 1.4044 | lr 3.86e-04 | grad 1.12 | tok/s 30493
step   1520 | loss 1.3445 | lr 3.86e-04 | grad 1.20 | tok/s 30104
step   1530 | loss 1.3386 | lr 3.86e-04 | grad 6.16 | tok/s 31260
step   1540 | loss 1.8572 | lr 3.86e-04 | grad 1.54 | tok/s 30378
step   1550 | loss 1.4094 | lr 3.86e-04 | grad 1.16 | tok/s 30026
step   1560 | loss 1.4574 | lr 3.86e-04 | grad 1.38 | tok/s 30797
step   1570 | loss 1.3267 | lr 3.86e-04 | grad 1.50 | tok/s 30144
step   1580 | loss 1.4671 | lr 3.86e-04 | grad 1.05 | tok/s 29897
step   1590 | loss 1.2701 | lr 3.86e-04 | grad 1.04 | tok/s 31197
step   1600 | loss 1.4643 | lr 3.86e-04 | grad 1.38 | tok/s 30622
step   1610 | loss 1.3978 | lr 3.86e-04 | grad 1.34 | tok/s 30994
step   1620 | loss 1.3548 | lr 3.86e-04 | grad 1.23 | tok/s 29648
step   1630 | loss 1.4003 | lr 3.86e-04 | grad 2.36 | tok/s 29924
step   1640 | loss 1.4346 | lr 3.86e-04 | grad 1.62 | tok/s 30006
step   1650 | loss 1.3966 | lr 3.86e-04 | grad 2.31 | tok/s 31021
step   1660 | loss 1.6756 | lr 3.86e-04 | grad 1.52 | tok/s 30657
step   1670 | loss 1.3095 | lr 3.86e-04 | grad 1.40 | tok/s 30325
step   1680 | loss 1.4292 | lr 3.86e-04 | grad 1.66 | tok/s 30709
step   1690 | loss 1.4759 | lr 3.86e-04 | grad 1.35 | tok/s 30415
step   1700 | loss 1.3535 | lr 3.86e-04 | grad 1.98 | tok/s 30155
step   1710 | loss 1.4735 | lr 3.86e-04 | grad 1.03 | tok/s 30238
step   1720 | loss 1.4196 | lr 3.86e-04 | grad 1.06 | tok/s 30189
step   1730 | loss 1.4238 | lr 3.86e-04 | grad 1.30 | tok/s 29843
step   1740 | loss 1.4698 | lr 3.86e-04 | grad 1.20 | tok/s 30279
step   1750 | loss 1.5283 | lr 3.86e-04 | grad 1.98 | tok/s 30095
step   1760 | loss 1.3981 | lr 3.86e-04 | grad 1.81 | tok/s 29973
step   1770 | loss 1.5150 | lr 3.86e-04 | grad 1.05 | tok/s 29888
step   1780 | loss 1.4132 | lr 3.86e-04 | grad 2.22 | tok/s 30703
step   1790 | loss 1.3121 | lr 3.86e-04 | grad 1.17 | tok/s 30856
step   1800 | loss 1.3220 | lr 3.86e-04 | grad 0.86 | tok/s 29877
step   1810 | loss 1.3984 | lr 3.86e-04 | grad 0.91 | tok/s 27831
step   1820 | loss 1.4893 | lr 3.86e-04 | grad 1.31 | tok/s 30109
step   1830 | loss 1.5189 | lr 3.86e-04 | grad 1.22 | tok/s 29878
step   1840 | loss 1.3262 | lr 3.86e-04 | grad 1.17 | tok/s 30294
step   1850 | loss 1.3178 | lr 3.86e-04 | grad 1.41 | tok/s 31021
step   1860 | loss 1.4053 | lr 3.86e-04 | grad 1.18 | tok/s 30749
step   1870 | loss 1.4942 | lr 3.86e-04 | grad 2.97 | tok/s 30440
step   1880 | loss 1.3732 | lr 3.86e-04 | grad 1.26 | tok/s 30564
step   1890 | loss 1.4912 | lr 3.86e-04 | grad 0.93 | tok/s 30292
step   1900 | loss 1.2432 | lr 3.86e-04 | grad 1.18 | tok/s 30749
step   1910 | loss 1.3323 | lr 3.86e-04 | grad 1.07 | tok/s 30733
step   1920 | loss 1.4437 | lr 3.86e-04 | grad 1.20 | tok/s 31330
step   1930 | loss 1.3697 | lr 3.86e-04 | grad 1.84 | tok/s 30605
step   1940 | loss 1.3990 | lr 3.86e-04 | grad 1.32 | tok/s 30830
step   1950 | loss 1.2784 | lr 3.86e-04 | grad 1.28 | tok/s 30288
step   1960 | loss 1.4664 | lr 3.86e-04 | grad 3.33 | tok/s 30718
step   1970 | loss 1.3441 | lr 3.86e-04 | grad 1.53 | tok/s 30193
step   1980 | loss 1.3171 | lr 3.86e-04 | grad 1.03 | tok/s 31094
step   1990 | loss 1.1219 | lr 3.86e-04 | grad 1.70 | tok/s 31659
step   2000 | loss 1.1661 | lr 3.86e-04 | grad 4.78 | tok/s 31659
  >>> saved checkpoint: checkpoint_step_002000_loss_1.1661.pt
step   2010 | loss 1.2873 | lr 3.86e-04 | grad 1.11 | tok/s 24392
step   2020 | loss 1.1899 | lr 3.86e-04 | grad 1.38 | tok/s 31658
step   2030 | loss 1.4147 | lr 3.86e-04 | grad 1.33 | tok/s 30331
step   2040 | loss 1.4350 | lr 3.86e-04 | grad 1.37 | tok/s 30797
step   2050 | loss 1.3936 | lr 3.86e-04 | grad 2.27 | tok/s 30492
step   2060 | loss 1.4395 | lr 3.86e-04 | grad 1.12 | tok/s 30480
step   2070 | loss 1.3651 | lr 3.86e-04 | grad 1.09 | tok/s 30155
step   2080 | loss 1.2303 | lr 3.86e-04 | grad 1.11 | tok/s 30713
step   2090 | loss 1.2775 | lr 3.86e-04 | grad 1.33 | tok/s 30238
step   2100 | loss 1.1534 | lr 3.86e-04 | grad 1.59 | tok/s 30682
step   2110 | loss 1.4673 | lr 3.86e-04 | grad 1.23 | tok/s 30103
step   2120 | loss 1.4658 | lr 3.86e-04 | grad 1.56 | tok/s 30409
step   2130 | loss 1.4558 | lr 3.86e-04 | grad 2.16 | tok/s 30077
step   2140 | loss 1.4930 | lr 3.86e-04 | grad 1.22 | tok/s 30631
step   2150 | loss 1.3629 | lr 3.86e-04 | grad 1.58 | tok/s 30401
step   2160 | loss 1.5247 | lr 3.86e-04 | grad 1.67 | tok/s 31164
step   2170 | loss 1.2019 | lr 3.86e-04 | grad 1.25 | tok/s 31265
step   2180 | loss 1.1655 | lr 3.86e-04 | grad 0.93 | tok/s 31658
step   2190 | loss 1.1570 | lr 3.86e-04 | grad 1.03 | tok/s 31658
step   2200 | loss 1.2484 | lr 3.86e-04 | grad 2.33 | tok/s 31011
step   2210 | loss 1.3741 | lr 3.86e-04 | grad 3.59 | tok/s 31080
step   2220 | loss 1.3725 | lr 3.86e-04 | grad 1.23 | tok/s 31320
step   2230 | loss 1.4548 | lr 3.86e-04 | grad 1.07 | tok/s 30856
step   2240 | loss 1.3128 | lr 3.86e-04 | grad 1.02 | tok/s 30472
step   2250 | loss 1.4505 | lr 3.86e-04 | grad 2.75 | tok/s 30534
step   2260 | loss 1.3692 | lr 3.86e-04 | grad 1.27 | tok/s 30204
step   2270 | loss 1.3440 | lr 3.86e-04 | grad 1.13 | tok/s 30030
step   2280 | loss 1.3760 | lr 3.86e-04 | grad 1.56 | tok/s 30910
step   2290 | loss 1.3323 | lr 3.86e-04 | grad 1.22 | tok/s 31665
step   2300 | loss 1.2708 | lr 3.86e-04 | grad 1.20 | tok/s 31655
step   2310 | loss 1.3364 | lr 3.86e-04 | grad 1.13 | tok/s 31221
step   2320 | loss 1.4021 | lr 3.86e-04 | grad 1.15 | tok/s 30567
step   2330 | loss 1.6489 | lr 3.86e-04 | grad 2.41 | tok/s 30665
step   2340 | loss 1.4195 | lr 3.86e-04 | grad 1.51 | tok/s 30694
step   2350 | loss 1.3269 | lr 3.86e-04 | grad 1.24 | tok/s 30390
step   2360 | loss 1.3289 | lr 3.86e-04 | grad 1.45 | tok/s 30118
step   2370 | loss 1.4091 | lr 3.86e-04 | grad 2.53 | tok/s 30239
step   2380 | loss 1.3573 | lr 3.86e-04 | grad 1.45 | tok/s 30025
step   2390 | loss 1.4232 | lr 3.86e-04 | grad 1.04 | tok/s 30525
step   2400 | loss 1.3656 | lr 3.86e-04 | grad 1.49 | tok/s 29802
step   2410 | loss 1.4656 | lr 3.86e-04 | grad 1.16 | tok/s 30695
step   2420 | loss 1.2785 | lr 3.86e-04 | grad 1.80 | tok/s 30397
step   2430 | loss 1.3871 | lr 3.86e-04 | grad 2.12 | tok/s 30809
step   2440 | loss 1.0592 | lr 3.86e-04 | grad 1.86 | tok/s 31660
step   2450 | loss 1.2872 | lr 3.86e-04 | grad 1.12 | tok/s 30396
step   2460 | loss 1.3033 | lr 3.86e-04 | grad 1.10 | tok/s 30046
step   2470 | loss 1.3604 | lr 3.86e-04 | grad 1.48 | tok/s 30777
step   2480 | loss 1.3564 | lr 3.86e-04 | grad 1.22 | tok/s 30815
step   2490 | loss 1.5460 | lr 3.86e-04 | grad 2.30 | tok/s 30230
step   2500 | loss 1.4369 | lr 3.86e-04 | grad 1.78 | tok/s 29866
step   2510 | loss 1.3801 | lr 3.86e-04 | grad 1.23 | tok/s 30148
step   2520 | loss 1.7118 | lr 3.86e-04 | grad 1.64 | tok/s 30425
step   2530 | loss 1.4498 | lr 3.86e-04 | grad 1.39 | tok/s 30956
step   2540 | loss 1.3988 | lr 3.86e-04 | grad 2.19 | tok/s 30067
step   2550 | loss 1.3778 | lr 3.86e-04 | grad 1.01 | tok/s 30703
step   2560 | loss 1.3187 | lr 3.86e-04 | grad 1.23 | tok/s 30284
step   2570 | loss 1.4203 | lr 3.86e-04 | grad 1.18 | tok/s 31153
step   2580 | loss 1.3351 | lr 3.86e-04 | grad 1.13 | tok/s 31028
step   2590 | loss 1.2528 | lr 3.86e-04 | grad 1.47 | tok/s 31661
step   2600 | loss 1.3645 | lr 3.86e-04 | grad 1.44 | tok/s 30755
step   2610 | loss 1.3504 | lr 3.86e-04 | grad 1.36 | tok/s 30134
step   2620 | loss 1.3779 | lr 3.86e-04 | grad 1.74 | tok/s 29319
step   2630 | loss 1.6166 | lr 3.86e-04 | grad 0.96 | tok/s 31300
step   2640 | loss 1.2677 | lr 3.86e-04 | grad 1.04 | tok/s 31664
step   2650 | loss 1.2314 | lr 3.86e-04 | grad 0.95 | tok/s 31656
step   2660 | loss 1.2109 | lr 3.86e-04 | grad 0.81 | tok/s 31663
step   2670 | loss 1.3662 | lr 3.86e-04 | grad 1.19 | tok/s 30290
step   2680 | loss 1.4830 | lr 3.86e-04 | grad 2.23 | tok/s 30018
step   2690 | loss 1.6662 | lr 3.86e-04 | grad 2.78 | tok/s 31111
step   2700 | loss 1.3019 | lr 3.86e-04 | grad 1.03 | tok/s 30322
step   2710 | loss 1.2306 | lr 3.86e-04 | grad 1.03 | tok/s 30745
step   2720 | loss 1.4451 | lr 3.86e-04 | grad 1.05 | tok/s 30298
step   2730 | loss 1.5020 | lr 3.86e-04 | grad 1.16 | tok/s 29821
step   2740 | loss 1.3740 | lr 3.86e-04 | grad 1.22 | tok/s 30928
step   2750 | loss 1.3128 | lr 3.86e-04 | grad 0.93 | tok/s 29954
step   2760 | loss 1.2481 | lr 3.86e-04 | grad 1.08 | tok/s 30279
step   2770 | loss 1.6592 | lr 3.86e-04 | grad 2.52 | tok/s 30457
step   2780 | loss 1.2441 | lr 3.86e-04 | grad 0.77 | tok/s 31076
step   2790 | loss 1.5928 | lr 3.86e-04 | grad 1.49 | tok/s 30870
step   2800 | loss 1.3680 | lr 3.86e-04 | grad 1.26 | tok/s 30948
step   2810 | loss 1.3591 | lr 3.86e-04 | grad 1.04 | tok/s 30522
step   2820 | loss 1.2842 | lr 3.86e-04 | grad 1.72 | tok/s 30827
step   2830 | loss 1.2708 | lr 3.86e-04 | grad 1.95 | tok/s 30451
step   2840 | loss 1.4677 | lr 3.86e-04 | grad 4.97 | tok/s 30868
step   2850 | loss 1.5870 | lr 3.86e-04 | grad 1.26 | tok/s 30625
step   2860 | loss 1.5372 | lr 3.86e-04 | grad 1.26 | tok/s 30987
step   2870 | loss 1.3127 | lr 3.86e-04 | grad 0.86 | tok/s 30058
step   2880 | loss 1.5136 | lr 3.86e-04 | grad 3.20 | tok/s 30153
step   2890 | loss 1.3532 | lr 3.86e-04 | grad 0.96 | tok/s 30829
step   2900 | loss 1.3839 | lr 3.86e-04 | grad 1.09 | tok/s 30346
step   2910 | loss 1.3611 | lr 3.86e-04 | grad 1.66 | tok/s 30186
step   2920 | loss 1.3528 | lr 3.86e-04 | grad 1.52 | tok/s 30277
step   2930 | loss 1.4520 | lr 3.86e-04 | grad 1.77 | tok/s 30029
step   2940 | loss 1.3571 | lr 3.86e-04 | grad 1.12 | tok/s 30333
step   2950 | loss 1.3399 | lr 3.86e-04 | grad 0.97 | tok/s 30949
step   2960 | loss 1.2553 | lr 3.86e-04 | grad 1.35 | tok/s 30536
step   2970 | loss 1.2899 | lr 3.86e-04 | grad 3.27 | tok/s 30651
step   2980 | loss 1.3152 | lr 3.86e-04 | grad 1.41 | tok/s 30595
step   2990 | loss 1.3929 | lr 3.86e-04 | grad 1.74 | tok/s 30780
step   3000 | loss 1.3677 | lr 3.86e-04 | grad 1.10 | tok/s 30396
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3677.pt
step   3010 | loss 1.4135 | lr 3.86e-04 | grad 2.19 | tok/s 23155
step   3020 | loss 1.3006 | lr 3.86e-04 | grad 1.45 | tok/s 30428
step   3030 | loss 1.2871 | lr 3.86e-04 | grad 1.03 | tok/s 30214
step   3040 | loss 1.3364 | lr 3.86e-04 | grad 1.01 | tok/s 30657
step   3050 | loss 1.2532 | lr 3.86e-04 | grad 1.08 | tok/s 30985
step   3060 | loss 1.3399 | lr 3.86e-04 | grad 1.03 | tok/s 30254
step   3070 | loss 1.3027 | lr 3.86e-04 | grad 1.03 | tok/s 30360
step   3080 | loss 1.3245 | lr 3.86e-04 | grad 1.04 | tok/s 30364
step   3090 | loss 1.5624 | lr 3.86e-04 | grad 2.89 | tok/s 30622
step   3100 | loss 1.7132 | lr 3.86e-04 | grad 1.88 | tok/s 31659
step   3110 | loss 1.4246 | lr 3.86e-04 | grad 1.83 | tok/s 31659
step   3120 | loss 1.4578 | lr 3.86e-04 | grad 1.28 | tok/s 30496
step   3130 | loss 1.8626 | lr 3.86e-04 | grad 1.08 | tok/s 30247
step   3140 | loss 1.2138 | lr 3.86e-04 | grad 1.28 | tok/s 30715
step   3150 | loss 1.5762 | lr 3.86e-04 | grad 3.94 | tok/s 30630
step   3160 | loss 1.3442 | lr 3.86e-04 | grad 0.89 | tok/s 30139
step   3170 | loss 1.5762 | lr 3.86e-04 | grad 1.28 | tok/s 31042
step   3180 | loss 1.3033 | lr 3.86e-04 | grad 1.36 | tok/s 31124
step   3190 | loss 1.3787 | lr 3.86e-04 | grad 1.11 | tok/s 30607
step   3200 | loss 1.4262 | lr 3.86e-04 | grad 1.41 | tok/s 29703
step   3210 | loss 1.3820 | lr 3.86e-04 | grad 1.01 | tok/s 28861
step   3220 | loss 1.2048 | lr 3.86e-04 | grad 1.11 | tok/s 30310
step   3230 | loss 1.4508 | lr 3.86e-04 | grad 1.44 | tok/s 30544
step   3240 | loss 1.3955 | lr 3.86e-04 | grad 1.09 | tok/s 30552
step   3250 | loss 1.3658 | lr 3.86e-04 | grad 2.36 | tok/s 31478
step   3260 | loss 1.3840 | lr 3.86e-04 | grad 1.02 | tok/s 30158
step   3270 | loss 1.3506 | lr 3.86e-04 | grad 1.23 | tok/s 30713
step   3280 | loss 1.4700 | lr 3.86e-04 | grad 2.05 | tok/s 30395
step   3290 | loss 1.3339 | lr 3.86e-04 | grad 1.20 | tok/s 30800
step   3300 | loss 1.3617 | lr 3.86e-04 | grad 1.23 | tok/s 29787
step   3310 | loss 1.3704 | lr 3.86e-04 | grad 1.95 | tok/s 29214
step   3320 | loss 1.3955 | lr 3.86e-04 | grad 1.09 | tok/s 31660
step   3330 | loss 1.3152 | lr 3.86e-04 | grad 1.24 | tok/s 31291
step   3340 | loss 1.2511 | lr 3.86e-04 | grad 2.14 | tok/s 30272
step   3350 | loss 1.5360 | lr 3.86e-04 | grad 33.25 | tok/s 30918
step   3360 | loss 1.4125 | lr 3.86e-04 | grad 0.89 | tok/s 30233
step   3370 | loss 1.2415 | lr 3.86e-04 | grad 0.92 | tok/s 29837
step   3380 | loss 1.3272 | lr 3.86e-04 | grad 2.06 | tok/s 30843
step   3390 | loss 1.3356 | lr 3.86e-04 | grad 1.41 | tok/s 30904
step   3400 | loss 1.7652 | lr 3.86e-04 | grad 2.69 | tok/s 30870
step   3410 | loss 1.3176 | lr 3.86e-04 | grad 1.12 | tok/s 30563
step   3420 | loss 1.4825 | lr 3.86e-04 | grad 1.16 | tok/s 30642
step   3430 | loss 1.1365 | lr 3.86e-04 | grad 2.66 | tok/s 31327
step   3440 | loss 1.3471 | lr 3.86e-04 | grad 1.05 | tok/s 29508

Training complete! Final step: 3446
