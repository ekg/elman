Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_18/level1_100m_20260202_132240
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 332,338,560 parameters
Using schedule-free AdamW (lr=0.0009770605920889366)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 6.5247 | lr 9.77e-04 | grad 18.25 | tok/s 17448
step     20 | loss 5.0744 | lr 9.77e-04 | grad 11.25 | tok/s 35769
step     30 | loss 4.4349 | lr 9.77e-04 | grad 4.22 | tok/s 35787
step     40 | loss 4.0937 | lr 9.77e-04 | grad 5.34 | tok/s 36138
step     50 | loss 3.5200 | lr 9.77e-04 | grad 5.00 | tok/s 36006
step     60 | loss 3.4798 | lr 9.77e-04 | grad 5.59 | tok/s 34568
step     70 | loss 3.2200 | lr 9.77e-04 | grad 7.38 | tok/s 35084
step     80 | loss 2.9655 | lr 9.77e-04 | grad 3.78 | tok/s 34833
step     90 | loss 2.9549 | lr 9.77e-04 | grad 3.47 | tok/s 33792
step    100 | loss 2.5541 | lr 9.77e-04 | grad 2.69 | tok/s 35098
step    110 | loss 2.9458 | lr 9.77e-04 | grad 2.48 | tok/s 33915
step    120 | loss 2.7379 | lr 9.77e-04 | grad 3.53 | tok/s 34132
step    130 | loss 2.5410 | lr 9.77e-04 | grad 4.00 | tok/s 34828
step    140 | loss 2.3384 | lr 9.77e-04 | grad 1.98 | tok/s 33296
step    150 | loss 2.4035 | lr 9.77e-04 | grad 1.38 | tok/s 33561
step    160 | loss 2.3863 | lr 9.77e-04 | grad 4.50 | tok/s 33544
step    170 | loss 2.5447 | lr 9.77e-04 | grad 2.83 | tok/s 34332
step    180 | loss 2.3035 | lr 9.77e-04 | grad 3.48 | tok/s 34112
step    190 | loss 2.1288 | lr 9.77e-04 | grad 2.48 | tok/s 35262
step    200 | loss 2.2305 | lr 9.77e-04 | grad 2.39 | tok/s 34346
step    210 | loss 2.3536 | lr 9.77e-04 | grad 1.88 | tok/s 34695
step    220 | loss 2.2635 | lr 9.77e-04 | grad 1.48 | tok/s 34021
step    230 | loss 2.1396 | lr 9.77e-04 | grad 1.87 | tok/s 34059
step    240 | loss 2.2161 | lr 9.77e-04 | grad 1.84 | tok/s 34416
step    250 | loss 2.2577 | lr 9.77e-04 | grad 2.31 | tok/s 33943
step    260 | loss 2.0659 | lr 9.77e-04 | grad 2.66 | tok/s 33240
step    270 | loss 2.1104 | lr 9.77e-04 | grad 3.30 | tok/s 33891
step    280 | loss 1.9263 | lr 9.77e-04 | grad 2.89 | tok/s 34999
step    290 | loss 1.8134 | lr 9.77e-04 | grad 2.92 | tok/s 35340
step    300 | loss 1.8442 | lr 9.77e-04 | grad 2.73 | tok/s 35192
step    310 | loss 1.9243 | lr 9.77e-04 | grad 1.32 | tok/s 35145
step    320 | loss 2.0585 | lr 9.77e-04 | grad 1.21 | tok/s 33465
step    330 | loss 2.0943 | lr 9.77e-04 | grad 1.98 | tok/s 34410
step    340 | loss 2.0274 | lr 9.77e-04 | grad 2.34 | tok/s 33448
step    350 | loss 2.0194 | lr 9.77e-04 | grad 2.23 | tok/s 33045
step    360 | loss 1.9507 | lr 9.77e-04 | grad 2.97 | tok/s 34271
step    370 | loss 2.3403 | lr 9.77e-04 | grad 2.31 | tok/s 34521
step    380 | loss 1.9742 | lr 9.77e-04 | grad 1.50 | tok/s 34895
step    390 | loss 1.9426 | lr 9.77e-04 | grad 1.65 | tok/s 34042
step    400 | loss 1.9685 | lr 9.77e-04 | grad 1.45 | tok/s 34307
step    410 | loss 1.9300 | lr 9.77e-04 | grad 1.69 | tok/s 33227
step    420 | loss 2.0081 | lr 9.77e-04 | grad 1.54 | tok/s 33473
step    430 | loss 2.1098 | lr 9.77e-04 | grad 2.00 | tok/s 34544
step    440 | loss 1.9517 | lr 9.77e-04 | grad 1.98 | tok/s 33883
step    450 | loss 1.9075 | lr 9.77e-04 | grad 1.42 | tok/s 33867
step    460 | loss 1.9473 | lr 9.77e-04 | grad 1.55 | tok/s 34087
step    470 | loss 1.8275 | lr 9.77e-04 | grad 1.83 | tok/s 32935
step    480 | loss 1.7858 | lr 9.77e-04 | grad 1.35 | tok/s 33681
step    490 | loss 2.3056 | lr 9.77e-04 | grad 1.84 | tok/s 34713
step    500 | loss 1.8912 | lr 9.77e-04 | grad 2.73 | tok/s 33991
step    510 | loss 1.6763 | lr 9.77e-04 | grad 1.26 | tok/s 34969
step    520 | loss 2.2650 | lr 9.77e-04 | grad 2.89 | tok/s 34104
step    530 | loss 1.7234 | lr 9.77e-04 | grad 1.21 | tok/s 34476
step    540 | loss 1.7306 | lr 9.77e-04 | grad 1.51 | tok/s 34807
step    550 | loss 1.5561 | lr 9.77e-04 | grad 2.28 | tok/s 35425
step    560 | loss 1.7312 | lr 9.77e-04 | grad 2.86 | tok/s 34842
step    570 | loss 2.1387 | lr 9.77e-04 | grad 1.54 | tok/s 35022
step    580 | loss 2.2851 | lr 9.77e-04 | grad 2.70 | tok/s 33607
step    590 | loss 1.8128 | lr 9.77e-04 | grad 1.44 | tok/s 33780
step    600 | loss 1.8219 | lr 9.77e-04 | grad 1.35 | tok/s 35389
step    610 | loss 1.7524 | lr 9.77e-04 | grad 1.45 | tok/s 33620
step    620 | loss 1.6911 | lr 9.77e-04 | grad 2.30 | tok/s 34536
step    630 | loss 1.9169 | lr 9.77e-04 | grad 1.58 | tok/s 34714
step    640 | loss 1.7810 | lr 9.77e-04 | grad 2.44 | tok/s 33971
step    650 | loss 1.9605 | lr 9.77e-04 | grad 3.62 | tok/s 33391
step    660 | loss 1.8970 | lr 9.77e-04 | grad 1.96 | tok/s 34539
step    670 | loss 1.8880 | lr 9.77e-04 | grad 1.16 | tok/s 33953
step    680 | loss 1.8493 | lr 9.77e-04 | grad 1.91 | tok/s 33537
step    690 | loss 1.9206 | lr 9.77e-04 | grad 1.97 | tok/s 33933
step    700 | loss 1.8514 | lr 9.77e-04 | grad 1.69 | tok/s 34328
step    710 | loss 1.7837 | lr 9.77e-04 | grad 3.89 | tok/s 33862
step    720 | loss 1.9164 | lr 9.77e-04 | grad 1.17 | tok/s 34206
step    730 | loss 1.8947 | lr 9.77e-04 | grad 2.36 | tok/s 34070
step    740 | loss 1.6942 | lr 9.77e-04 | grad 1.17 | tok/s 33543
step    750 | loss 2.0002 | lr 9.77e-04 | grad 1.28 | tok/s 34162
step    760 | loss 1.7133 | lr 9.77e-04 | grad 1.45 | tok/s 34153
step    770 | loss 1.7886 | lr 9.77e-04 | grad 1.66 | tok/s 34488
step    780 | loss 1.6566 | lr 9.77e-04 | grad 1.80 | tok/s 34771
step    790 | loss 1.7103 | lr 9.77e-04 | grad 3.00 | tok/s 34363
step    800 | loss 1.6426 | lr 9.77e-04 | grad 1.45 | tok/s 33952
step    810 | loss 2.2843 | lr 9.77e-04 | grad 2.61 | tok/s 35194
step    820 | loss 1.8982 | lr 9.77e-04 | grad 2.53 | tok/s 35381
step    830 | loss 1.7576 | lr 9.77e-04 | grad 2.27 | tok/s 35352
step    840 | loss 1.8467 | lr 9.77e-04 | grad 1.55 | tok/s 33829
step    850 | loss 1.6794 | lr 9.77e-04 | grad 0.93 | tok/s 34171
step    860 | loss 1.6888 | lr 9.77e-04 | grad 1.45 | tok/s 34030
step    870 | loss 1.7702 | lr 9.77e-04 | grad 1.46 | tok/s 34793
step    880 | loss 1.6820 | lr 9.77e-04 | grad 2.05 | tok/s 33586
step    890 | loss 1.9940 | lr 9.77e-04 | grad 1.41 | tok/s 33480
step    900 | loss 1.6606 | lr 9.77e-04 | grad 1.99 | tok/s 33673
step    910 | loss 1.7240 | lr 9.77e-04 | grad 1.02 | tok/s 33717
step    920 | loss 1.6919 | lr 9.77e-04 | grad 1.50 | tok/s 33500
step    930 | loss 1.7616 | lr 9.77e-04 | grad 1.40 | tok/s 33749
step    940 | loss 1.7466 | lr 9.77e-04 | grad 1.82 | tok/s 34458
step    950 | loss 1.4677 | lr 9.77e-04 | grad 2.42 | tok/s 35509
step    960 | loss 1.4115 | lr 9.77e-04 | grad 1.73 | tok/s 35384
step    970 | loss 1.6699 | lr 9.77e-04 | grad 1.52 | tok/s 34104
step    980 | loss 1.8126 | lr 9.77e-04 | grad 1.35 | tok/s 33550
step    990 | loss 1.7782 | lr 9.77e-04 | grad 1.27 | tok/s 34066
step   1000 | loss 1.6680 | lr 9.77e-04 | grad 1.18 | tok/s 34601
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6680.pt
step   1010 | loss 1.6532 | lr 9.77e-04 | grad 1.63 | tok/s 22791
step   1020 | loss 1.8989 | lr 9.77e-04 | grad 1.37 | tok/s 33895
step   1030 | loss 1.7974 | lr 9.77e-04 | grad 1.35 | tok/s 33910
step   1040 | loss 1.4516 | lr 9.77e-04 | grad 1.09 | tok/s 33817
step   1050 | loss 1.9685 | lr 9.77e-04 | grad 1.12 | tok/s 34678
step   1060 | loss 2.1296 | lr 9.77e-04 | grad 1.82 | tok/s 33917
step   1070 | loss 1.7717 | lr 9.77e-04 | grad 1.22 | tok/s 33517
step   1080 | loss 1.8767 | lr 9.77e-04 | grad 1.00 | tok/s 34546
step   1090 | loss 1.6865 | lr 9.77e-04 | grad 1.55 | tok/s 34922
step   1100 | loss 1.6678 | lr 9.77e-04 | grad 1.28 | tok/s 34642
step   1110 | loss 1.7502 | lr 9.77e-04 | grad 1.03 | tok/s 33420
step   1120 | loss 1.7148 | lr 9.77e-04 | grad 0.99 | tok/s 34101
step   1130 | loss 1.7688 | lr 9.77e-04 | grad 2.61 | tok/s 34002
step   1140 | loss 1.7125 | lr 9.77e-04 | grad 2.88 | tok/s 33649
step   1150 | loss 1.8993 | lr 9.77e-04 | grad 1.18 | tok/s 33283
step   1160 | loss 1.5614 | lr 9.77e-04 | grad 1.66 | tok/s 35247
step   1170 | loss 1.5065 | lr 9.77e-04 | grad 1.41 | tok/s 35313
step   1180 | loss 1.4450 | lr 9.77e-04 | grad 1.63 | tok/s 35209
step   1190 | loss 1.4043 | lr 9.77e-04 | grad 1.45 | tok/s 35459
step   1200 | loss 1.3834 | lr 9.77e-04 | grad 1.34 | tok/s 35435
step   1210 | loss 1.5072 | lr 9.77e-04 | grad 0.47 | tok/s 34746
step   1220 | loss 1.5982 | lr 9.77e-04 | grad 0.79 | tok/s 33407
step   1230 | loss 1.6733 | lr 9.77e-04 | grad 1.10 | tok/s 34406
step   1240 | loss 1.6648 | lr 9.77e-04 | grad 1.44 | tok/s 34204
step   1250 | loss 1.8827 | lr 9.77e-04 | grad 1.02 | tok/s 34137
step   1260 | loss 1.6981 | lr 9.77e-04 | grad 1.73 | tok/s 33967
step   1270 | loss 1.7281 | lr 9.77e-04 | grad 0.88 | tok/s 33614
step   1280 | loss 1.6825 | lr 9.77e-04 | grad 1.12 | tok/s 33786
step   1290 | loss 1.7313 | lr 9.77e-04 | grad 0.79 | tok/s 33425
step   1300 | loss 1.6628 | lr 9.77e-04 | grad 1.30 | tok/s 33870
step   1310 | loss 1.7261 | lr 9.77e-04 | grad 1.00 | tok/s 34354
step   1320 | loss 1.5551 | lr 9.77e-04 | grad 1.03 | tok/s 33736
step   1330 | loss 1.5264 | lr 9.77e-04 | grad 1.13 | tok/s 34746
step   1340 | loss 1.6108 | lr 9.77e-04 | grad 1.01 | tok/s 33897
step   1350 | loss 1.5659 | lr 9.77e-04 | grad 1.19 | tok/s 33624
step   1360 | loss 1.8003 | lr 9.77e-04 | grad 1.09 | tok/s 33894
step   1370 | loss 1.6122 | lr 9.77e-04 | grad 0.88 | tok/s 33770
step   1380 | loss 1.5860 | lr 9.77e-04 | grad 1.27 | tok/s 34615
step   1390 | loss 1.6758 | lr 9.77e-04 | grad 1.32 | tok/s 34799
step   1400 | loss 1.6870 | lr 9.77e-04 | grad 2.12 | tok/s 33284
step   1410 | loss 1.5971 | lr 9.77e-04 | grad 1.08 | tok/s 32833
step   1420 | loss 1.4496 | lr 9.77e-04 | grad 0.87 | tok/s 34066
step   1430 | loss 1.4289 | lr 9.77e-04 | grad 1.10 | tok/s 34473
step   1440 | loss 1.6606 | lr 9.77e-04 | grad 1.27 | tok/s 33195
step   1450 | loss 1.6926 | lr 9.77e-04 | grad 1.09 | tok/s 34013
step   1460 | loss 1.5511 | lr 9.77e-04 | grad 2.17 | tok/s 34049
step   1470 | loss 1.6434 | lr 9.77e-04 | grad 1.47 | tok/s 34019
step   1480 | loss 1.8759 | lr 9.77e-04 | grad 1.63 | tok/s 33553
step   1490 | loss 1.6159 | lr 9.77e-04 | grad 1.30 | tok/s 34434
step   1500 | loss 1.6708 | lr 9.77e-04 | grad 1.12 | tok/s 34479
step   1510 | loss 1.5947 | lr 9.77e-04 | grad 0.98 | tok/s 34018
step   1520 | loss 1.5662 | lr 9.77e-04 | grad 0.97 | tok/s 33533
step   1530 | loss 1.5448 | lr 9.77e-04 | grad 2.80 | tok/s 34959
step   1540 | loss 2.0627 | lr 9.77e-04 | grad 1.23 | tok/s 34026
step   1550 | loss 1.5897 | lr 9.77e-04 | grad 0.91 | tok/s 33474
step   1560 | loss 1.7049 | lr 9.77e-04 | grad 1.16 | tok/s 34543
step   1570 | loss 1.5036 | lr 9.77e-04 | grad 1.20 | tok/s 33754
step   1580 | loss 1.6304 | lr 9.77e-04 | grad 1.04 | tok/s 33296
step   1590 | loss 1.4522 | lr 9.77e-04 | grad 0.81 | tok/s 35015
step   1600 | loss 1.6156 | lr 9.77e-04 | grad 1.17 | tok/s 34243
step   1610 | loss 1.5756 | lr 9.77e-04 | grad 1.23 | tok/s 34521
step   1620 | loss 1.5302 | lr 9.77e-04 | grad 1.13 | tok/s 33269
step   1630 | loss 1.6061 | lr 9.77e-04 | grad 2.20 | tok/s 33511
step   1640 | loss 1.6176 | lr 9.77e-04 | grad 1.35 | tok/s 33477
step   1650 | loss 1.6062 | lr 9.77e-04 | grad 1.70 | tok/s 34844
step   1660 | loss 1.8798 | lr 9.77e-04 | grad 1.30 | tok/s 34212
step   1670 | loss 1.4614 | lr 9.77e-04 | grad 1.03 | tok/s 33844
step   1680 | loss 1.6877 | lr 9.77e-04 | grad 1.41 | tok/s 34323
step   1690 | loss 1.6877 | lr 9.77e-04 | grad 1.05 | tok/s 33929
step   1700 | loss 1.5859 | lr 9.77e-04 | grad 1.69 | tok/s 33728
step   1710 | loss 1.6886 | lr 9.77e-04 | grad 0.86 | tok/s 33907
step   1720 | loss 1.5965 | lr 9.77e-04 | grad 0.85 | tok/s 33643
step   1730 | loss 1.5923 | lr 9.77e-04 | grad 1.51 | tok/s 33290
step   1740 | loss 1.7013 | lr 9.77e-04 | grad 1.01 | tok/s 33908
step   1750 | loss 1.7325 | lr 9.77e-04 | grad 1.52 | tok/s 33619
step   1760 | loss 1.5600 | lr 9.77e-04 | grad 1.79 | tok/s 33496
step   1770 | loss 1.7161 | lr 9.77e-04 | grad 1.04 | tok/s 33563
step   1780 | loss 1.5578 | lr 9.77e-04 | grad 1.62 | tok/s 34122
step   1790 | loss 1.4679 | lr 9.77e-04 | grad 3.11 | tok/s 34556
step   1800 | loss 1.5099 | lr 9.77e-04 | grad 0.78 | tok/s 33513
step   1810 | loss 1.5757 | lr 9.77e-04 | grad 0.98 | tok/s 33400
step   1820 | loss 1.6706 | lr 9.77e-04 | grad 1.01 | tok/s 33652
step   1830 | loss 1.6658 | lr 9.77e-04 | grad 0.95 | tok/s 33529
step   1840 | loss 1.5004 | lr 9.77e-04 | grad 0.78 | tok/s 33823
step   1850 | loss 1.4997 | lr 9.77e-04 | grad 1.25 | tok/s 34695
step   1860 | loss 1.5581 | lr 9.77e-04 | grad 0.95 | tok/s 34412
step   1870 | loss 1.6868 | lr 9.77e-04 | grad 1.61 | tok/s 33900
step   1880 | loss 1.5336 | lr 9.77e-04 | grad 1.14 | tok/s 34200
step   1890 | loss 1.6847 | lr 9.77e-04 | grad 0.81 | tok/s 33956
step   1900 | loss 1.4319 | lr 9.77e-04 | grad 1.09 | tok/s 34314
step   1910 | loss 1.5013 | lr 9.77e-04 | grad 1.04 | tok/s 34454
step   1920 | loss 1.5968 | lr 9.77e-04 | grad 0.86 | tok/s 34920
step   1930 | loss 1.5295 | lr 9.77e-04 | grad 1.15 | tok/s 34220
step   1940 | loss 1.6871 | lr 9.77e-04 | grad 1.12 | tok/s 34552
step   1950 | loss 1.4679 | lr 9.77e-04 | grad 1.52 | tok/s 33832
step   1960 | loss 1.6957 | lr 9.77e-04 | grad 2.30 | tok/s 34208
step   1970 | loss 1.5189 | lr 9.77e-04 | grad 1.45 | tok/s 33834
step   1980 | loss 1.4790 | lr 9.77e-04 | grad 0.76 | tok/s 34739
step   1990 | loss 1.2346 | lr 9.77e-04 | grad 1.62 | tok/s 35320
step   2000 | loss 1.2928 | lr 9.77e-04 | grad 3.31 | tok/s 35512
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2928.pt
step   2010 | loss 1.4269 | lr 9.77e-04 | grad 0.91 | tok/s 23770
step   2020 | loss 1.2994 | lr 9.77e-04 | grad 1.07 | tok/s 35366
step   2030 | loss 1.5732 | lr 9.77e-04 | grad 0.86 | tok/s 34068
step   2040 | loss 1.5341 | lr 9.77e-04 | grad 1.20 | tok/s 34490
step   2050 | loss 1.6091 | lr 9.77e-04 | grad 1.74 | tok/s 33989
step   2060 | loss 1.6169 | lr 9.77e-04 | grad 0.94 | tok/s 34245
step   2070 | loss 1.5187 | lr 9.77e-04 | grad 1.23 | tok/s 33710
step   2080 | loss 1.4125 | lr 9.77e-04 | grad 0.88 | tok/s 34335
step   2090 | loss 1.4544 | lr 9.77e-04 | grad 1.00 | tok/s 33981
step   2100 | loss 1.3079 | lr 9.77e-04 | grad 1.68 | tok/s 34260
step   2110 | loss 1.6039 | lr 9.77e-04 | grad 1.02 | tok/s 33702
step   2120 | loss 1.6622 | lr 9.77e-04 | grad 1.25 | tok/s 34070
step   2130 | loss 1.6057 | lr 9.77e-04 | grad 1.53 | tok/s 33458
step   2140 | loss 1.6742 | lr 9.77e-04 | grad 1.27 | tok/s 34348
step   2150 | loss 1.5230 | lr 9.77e-04 | grad 1.65 | tok/s 34032
step   2160 | loss 1.6857 | lr 9.77e-04 | grad 1.11 | tok/s 34631
step   2170 | loss 1.3383 | lr 9.77e-04 | grad 0.92 | tok/s 34871
step   2180 | loss 1.2884 | lr 9.77e-04 | grad 0.96 | tok/s 35474
step   2190 | loss 1.2804 | lr 9.77e-04 | grad 0.82 | tok/s 35255
step   2200 | loss 1.4285 | lr 9.77e-04 | grad 2.09 | tok/s 34689
step   2210 | loss 1.5432 | lr 9.77e-04 | grad 2.55 | tok/s 34786
step   2220 | loss 1.5494 | lr 9.77e-04 | grad 0.91 | tok/s 34801
step   2230 | loss 1.6332 | lr 9.77e-04 | grad 1.00 | tok/s 34542
step   2240 | loss 1.4811 | lr 9.77e-04 | grad 0.86 | tok/s 34141
step   2250 | loss 1.6097 | lr 9.77e-04 | grad 2.50 | tok/s 34086
step   2260 | loss 1.5121 | lr 9.77e-04 | grad 1.20 | tok/s 33920
step   2270 | loss 1.5127 | lr 9.77e-04 | grad 0.86 | tok/s 33630
step   2280 | loss 1.4880 | lr 9.77e-04 | grad 1.41 | tok/s 34539
step   2290 | loss 1.4298 | lr 9.77e-04 | grad 1.22 | tok/s 35493
step   2300 | loss 1.3646 | lr 9.77e-04 | grad 1.18 | tok/s 35189
step   2310 | loss 1.4709 | lr 9.77e-04 | grad 0.95 | tok/s 34820
step   2320 | loss 1.5998 | lr 9.77e-04 | grad 1.05 | tok/s 34320
step   2330 | loss 1.8932 | lr 9.77e-04 | grad 1.41 | tok/s 34264
step   2340 | loss 1.6061 | lr 9.77e-04 | grad 1.30 | tok/s 34306
step   2350 | loss 1.4803 | lr 9.77e-04 | grad 1.01 | tok/s 34026
step   2360 | loss 1.4486 | lr 9.77e-04 | grad 1.26 | tok/s 33746
step   2370 | loss 1.5925 | lr 9.77e-04 | grad 1.70 | tok/s 33807
step   2380 | loss 1.5097 | lr 9.77e-04 | grad 1.45 | tok/s 33702
step   2390 | loss 1.6942 | lr 9.77e-04 | grad 0.77 | tok/s 33973
step   2400 | loss 1.5721 | lr 9.77e-04 | grad 1.00 | tok/s 33270
step   2410 | loss 1.6699 | lr 9.77e-04 | grad 0.87 | tok/s 34394
step   2420 | loss 1.4120 | lr 9.77e-04 | grad 1.75 | tok/s 33980
step   2430 | loss 1.5652 | lr 9.77e-04 | grad 1.63 | tok/s 34348
step   2440 | loss 1.3071 | lr 9.77e-04 | grad 1.28 | tok/s 35500
step   2450 | loss 1.4683 | lr 9.77e-04 | grad 0.94 | tok/s 33845
step   2460 | loss 1.4417 | lr 9.77e-04 | grad 0.87 | tok/s 33614
step   2470 | loss 1.5161 | lr 9.77e-04 | grad 1.50 | tok/s 34440
step   2480 | loss 1.5381 | lr 9.77e-04 | grad 0.82 | tok/s 34345
step   2490 | loss 1.7369 | lr 9.77e-04 | grad 1.77 | tok/s 33910
step   2500 | loss 1.5840 | lr 9.77e-04 | grad 1.05 | tok/s 33468
step   2510 | loss 1.5568 | lr 9.77e-04 | grad 0.90 | tok/s 33641
step   2520 | loss 1.8879 | lr 9.77e-04 | grad 2.30 | tok/s 34092
step   2530 | loss 1.6045 | lr 9.77e-04 | grad 1.60 | tok/s 34541
step   2540 | loss 1.5688 | lr 9.77e-04 | grad 1.88 | tok/s 33615
step   2550 | loss 1.5316 | lr 9.77e-04 | grad 0.89 | tok/s 34452
step   2560 | loss 1.4327 | lr 9.77e-04 | grad 0.91 | tok/s 33845
step   2570 | loss 1.5830 | lr 9.77e-04 | grad 0.77 | tok/s 34783
step   2580 | loss 1.4918 | lr 9.77e-04 | grad 0.91 | tok/s 34752
step   2590 | loss 1.3557 | lr 9.77e-04 | grad 1.59 | tok/s 35344
step   2600 | loss 1.4797 | lr 9.77e-04 | grad 1.15 | tok/s 34293
step   2610 | loss 1.5133 | lr 9.77e-04 | grad 0.98 | tok/s 33719
step   2620 | loss 1.5124 | lr 9.77e-04 | grad 1.51 | tok/s 32788
step   2630 | loss 1.7512 | lr 9.77e-04 | grad 0.98 | tok/s 34962
step   2640 | loss 1.3634 | lr 9.77e-04 | grad 0.77 | tok/s 35434
step   2650 | loss 1.3286 | lr 9.77e-04 | grad 0.73 | tok/s 35249
step   2660 | loss 1.3180 | lr 9.77e-04 | grad 0.68 | tok/s 35336
step   2670 | loss 1.5059 | lr 9.77e-04 | grad 1.02 | tok/s 33934
step   2680 | loss 1.6732 | lr 9.77e-04 | grad 1.32 | tok/s 33440
step   2690 | loss 1.8445 | lr 9.77e-04 | grad 2.73 | tok/s 34727
step   2700 | loss 1.4584 | lr 9.77e-04 | grad 0.91 | tok/s 33920
step   2710 | loss 1.3964 | lr 9.77e-04 | grad 0.69 | tok/s 34348
step   2720 | loss 1.6113 | lr 9.77e-04 | grad 0.79 | tok/s 34017
step   2730 | loss 1.6826 | lr 9.77e-04 | grad 0.76 | tok/s 33521
step   2740 | loss 1.5274 | lr 9.77e-04 | grad 1.02 | tok/s 34756
step   2750 | loss 1.4561 | lr 9.77e-04 | grad 0.74 | tok/s 33672
step   2760 | loss 1.3844 | lr 9.77e-04 | grad 0.93 | tok/s 33941
step   2770 | loss 1.9107 | lr 9.77e-04 | grad 2.38 | tok/s 34236
step   2780 | loss 1.4222 | lr 9.77e-04 | grad 0.61 | tok/s 34897
step   2790 | loss 1.7910 | lr 9.77e-04 | grad 1.18 | tok/s 34718
step   2800 | loss 1.5605 | lr 9.77e-04 | grad 1.01 | tok/s 34739
step   2810 | loss 1.4825 | lr 9.77e-04 | grad 1.12 | tok/s 34320
step   2820 | loss 1.4262 | lr 9.77e-04 | grad 1.60 | tok/s 34614
step   2830 | loss 1.4500 | lr 9.77e-04 | grad 1.53 | tok/s 34223
step   2840 | loss 1.6922 | lr 9.77e-04 | grad 3.67 | tok/s 34682
step   2850 | loss 1.7307 | lr 9.77e-04 | grad 1.04 | tok/s 34413
step   2860 | loss 1.6783 | lr 9.77e-04 | grad 1.02 | tok/s 34788
step   2870 | loss 1.4393 | lr 9.77e-04 | grad 1.19 | tok/s 33686
step   2880 | loss 1.8358 | lr 9.77e-04 | grad 7.16 | tok/s 33872
step   2890 | loss 1.5305 | lr 9.77e-04 | grad 0.78 | tok/s 34639
step   2900 | loss 1.5084 | lr 9.77e-04 | grad 0.78 | tok/s 34109
step   2910 | loss 1.5360 | lr 9.77e-04 | grad 1.14 | tok/s 33919
step   2920 | loss 1.5098 | lr 9.77e-04 | grad 1.02 | tok/s 34027
step   2930 | loss 1.6347 | lr 9.77e-04 | grad 1.55 | tok/s 33751
step   2940 | loss 1.4835 | lr 9.77e-04 | grad 1.09 | tok/s 34019
step   2950 | loss 1.5178 | lr 9.77e-04 | grad 0.64 | tok/s 34765
step   2960 | loss 1.4343 | lr 9.77e-04 | grad 0.74 | tok/s 34364
step   2970 | loss 1.4427 | lr 9.77e-04 | grad 2.27 | tok/s 34450
step   2980 | loss 1.4832 | lr 9.77e-04 | grad 1.14 | tok/s 34398
step   2990 | loss 1.5361 | lr 9.77e-04 | grad 1.62 | tok/s 34528
step   3000 | loss 1.5080 | lr 9.77e-04 | grad 1.00 | tok/s 34153
  >>> saved checkpoint: checkpoint_step_003000_loss_1.5080.pt
step   3010 | loss 1.5816 | lr 9.77e-04 | grad 1.85 | tok/s 22737
step   3020 | loss 1.4248 | lr 9.77e-04 | grad 0.94 | tok/s 34289
step   3030 | loss 1.4297 | lr 9.77e-04 | grad 0.76 | tok/s 34017
step   3040 | loss 1.4549 | lr 9.77e-04 | grad 0.92 | tok/s 34485
step   3050 | loss 1.3924 | lr 9.77e-04 | grad 0.84 | tok/s 34854
step   3060 | loss 1.4713 | lr 9.77e-04 | grad 0.79 | tok/s 34074
step   3070 | loss 1.4187 | lr 9.77e-04 | grad 0.92 | tok/s 34166
step   3080 | loss 1.4407 | lr 9.77e-04 | grad 0.68 | tok/s 34048
step   3090 | loss 1.7454 | lr 9.77e-04 | grad 1.48 | tok/s 34472
step   3100 | loss 1.7752 | lr 9.77e-04 | grad 2.41 | tok/s 35570
step   3110 | loss 1.4974 | lr 9.77e-04 | grad 1.80 | tok/s 35567
step   3120 | loss 1.6165 | lr 9.77e-04 | grad 1.19 | tok/s 34300
step   3130 | loss 2.1535 | lr 9.77e-04 | grad 0.99 | tok/s 34053
step   3140 | loss 1.3537 | lr 9.77e-04 | grad 1.00 | tok/s 34461
step   3150 | loss 1.7362 | lr 9.77e-04 | grad 3.36 | tok/s 34276
step   3160 | loss 1.4542 | lr 9.77e-04 | grad 1.70 | tok/s 33879
step   3170 | loss 1.7722 | lr 9.77e-04 | grad 1.20 | tok/s 34899
step   3180 | loss 1.4864 | lr 9.77e-04 | grad 1.11 | tok/s 34914
step   3190 | loss 1.5059 | lr 9.77e-04 | grad 0.75 | tok/s 34410
step   3200 | loss 1.6023 | lr 9.77e-04 | grad 1.22 | tok/s 33425
step   3210 | loss 1.5157 | lr 9.77e-04 | grad 0.63 | tok/s 32480
step   3220 | loss 1.3238 | lr 9.77e-04 | grad 0.89 | tok/s 34060
step   3230 | loss 1.6020 | lr 9.77e-04 | grad 1.46 | tok/s 34203
step   3240 | loss 1.5197 | lr 9.77e-04 | grad 0.66 | tok/s 34368
step   3250 | loss 1.5395 | lr 9.77e-04 | grad 1.65 | tok/s 35380
step   3260 | loss 1.5654 | lr 9.77e-04 | grad 0.91 | tok/s 33839
step   3270 | loss 1.5151 | lr 9.77e-04 | grad 0.86 | tok/s 34496
step   3280 | loss 1.5735 | lr 9.77e-04 | grad 1.17 | tok/s 34163
step   3290 | loss 1.5449 | lr 9.77e-04 | grad 1.20 | tok/s 34609
step   3300 | loss 1.4972 | lr 9.77e-04 | grad 1.11 | tok/s 33483
step   3310 | loss 1.5266 | lr 9.77e-04 | grad 1.20 | tok/s 32871
step   3320 | loss 1.4761 | lr 9.77e-04 | grad 0.98 | tok/s 35525
step   3330 | loss 1.4359 | lr 9.77e-04 | grad 0.81 | tok/s 35097
step   3340 | loss 1.4008 | lr 9.77e-04 | grad 1.63 | tok/s 34031
step   3350 | loss 1.6169 | lr 9.77e-04 | grad 1.85 | tok/s 34853
step   3360 | loss 1.5454 | lr 9.77e-04 | grad 0.73 | tok/s 34062
step   3370 | loss 1.3520 | lr 9.77e-04 | grad 0.66 | tok/s 33445
step   3380 | loss 1.4665 | lr 9.77e-04 | grad 1.34 | tok/s 34630
step   3390 | loss 1.4849 | lr 9.77e-04 | grad 0.78 | tok/s 34692
step   3400 | loss 2.0305 | lr 9.77e-04 | grad 1.42 | tok/s 34589
step   3410 | loss 1.4489 | lr 9.77e-04 | grad 0.89 | tok/s 34351
step   3420 | loss 1.6008 | lr 9.77e-04 | grad 1.05 | tok/s 34412
step   3430 | loss 1.3263 | lr 9.77e-04 | grad 1.38 | tok/s 35083
step   3440 | loss 1.5402 | lr 9.77e-04 | grad 0.95 | tok/s 32974
step   3450 | loss 1.5657 | lr 9.77e-04 | grad 0.98 | tok/s 33746
step   3460 | loss 1.4696 | lr 9.77e-04 | grad 1.33 | tok/s 34558
step   3470 | loss 1.7224 | lr 9.77e-04 | grad 0.82 | tok/s 33760
step   3480 | loss 1.5261 | lr 9.77e-04 | grad 1.34 | tok/s 34032
step   3490 | loss 1.3781 | lr 9.77e-04 | grad 0.79 | tok/s 33841
step   3500 | loss 1.5042 | lr 9.77e-04 | grad 1.05 | tok/s 34120
step   3510 | loss 1.5099 | lr 9.77e-04 | grad 0.81 | tok/s 33581
step   3520 | loss 1.4873 | lr 9.77e-04 | grad 0.62 | tok/s 34409
step   3530 | loss 1.5054 | lr 9.77e-04 | grad 0.95 | tok/s 34038
step   3540 | loss 1.4551 | lr 9.77e-04 | grad 0.99 | tok/s 33948
step   3550 | loss 1.6079 | lr 9.77e-04 | grad 1.01 | tok/s 34337
step   3560 | loss 1.5541 | lr 9.77e-04 | grad 0.92 | tok/s 33339
step   3570 | loss 1.4879 | lr 9.77e-04 | grad 0.86 | tok/s 34161
step   3580 | loss 1.4431 | lr 9.77e-04 | grad 1.41 | tok/s 34316
step   3590 | loss 1.4862 | lr 9.77e-04 | grad 1.55 | tok/s 35128
step   3600 | loss 1.3422 | lr 9.77e-04 | grad 1.12 | tok/s 35506
step   3610 | loss 1.3136 | lr 9.77e-04 | grad 1.05 | tok/s 35211
step   3620 | loss 1.2720 | lr 9.77e-04 | grad 0.70 | tok/s 35489
step   3630 | loss 1.2502 | lr 9.77e-04 | grad 0.97 | tok/s 35475
step   3640 | loss 1.2625 | lr 9.77e-04 | grad 1.00 | tok/s 35483
step   3650 | loss 1.3869 | lr 9.77e-04 | grad 1.42 | tok/s 34010
step   3660 | loss 1.6630 | lr 9.77e-04 | grad 1.25 | tok/s 34429
step   3670 | loss 1.3812 | lr 9.77e-04 | grad 0.77 | tok/s 34696
step   3680 | loss 1.3773 | lr 9.77e-04 | grad 1.06 | tok/s 33866
step   3690 | loss 1.3805 | lr 9.77e-04 | grad 1.36 | tok/s 34034
step   3700 | loss 1.4275 | lr 9.77e-04 | grad 1.05 | tok/s 34336
step   3710 | loss 1.4878 | lr 9.77e-04 | grad 1.43 | tok/s 35099
step   3720 | loss 1.4472 | lr 9.77e-04 | grad 2.22 | tok/s 34237
step   3730 | loss 1.4700 | lr 9.77e-04 | grad 1.02 | tok/s 34850
step   3740 | loss 1.6060 | lr 9.77e-04 | grad 0.73 | tok/s 34304
step   3750 | loss 1.4364 | lr 9.77e-04 | grad 0.80 | tok/s 35070
step   3760 | loss 1.4078 | lr 9.77e-04 | grad 1.09 | tok/s 34224
step   3770 | loss 1.5343 | lr 9.77e-04 | grad 2.91 | tok/s 34319
step   3780 | loss 1.4034 | lr 9.77e-04 | grad 1.07 | tok/s 34066
step   3790 | loss 1.4532 | lr 9.77e-04 | grad 1.16 | tok/s 34806
step   3800 | loss 1.4446 | lr 9.77e-04 | grad 0.95 | tok/s 33865
step   3810 | loss 1.4178 | lr 9.77e-04 | grad 0.76 | tok/s 34990
step   3820 | loss 1.3661 | lr 9.77e-04 | grad 0.76 | tok/s 33657
step   3830 | loss 1.3455 | lr 9.77e-04 | grad 0.70 | tok/s 34190
step   3840 | loss 1.3232 | lr 9.77e-04 | grad 1.50 | tok/s 34531

Training complete! Final step: 3847
