Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_94/level1_100m_20260202_175455
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 472,260,096 parameters
Using schedule-free AdamW (lr=0.0002735500198295742)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 4.2374 | lr 2.74e-04 | grad 5.00 | tok/s 8640
step     20 | loss 2.5257 | lr 2.74e-04 | grad 1.46 | tok/s 14865
step     30 | loss 2.8605 | lr 2.74e-04 | grad 2.70 | tok/s 15705
step     40 | loss 3.8775 | lr 2.74e-04 | grad 11.88 | tok/s 16013
step     50 | loss 3.9990 | lr 2.74e-04 | grad 5.22 | tok/s 16212
step     60 | loss 2.9208 | lr 2.74e-04 | grad 2.86 | tok/s 16185
step     70 | loss 2.5056 | lr 2.74e-04 | grad 4.03 | tok/s 16191
step     80 | loss 2.3260 | lr 2.74e-04 | grad 2.50 | tok/s 16172
step     90 | loss 2.2423 | lr 2.74e-04 | grad 2.08 | tok/s 16170
step    100 | loss 2.0818 | lr 2.74e-04 | grad 1.70 | tok/s 16174
step    110 | loss 2.2284 | lr 2.74e-04 | grad 3.12 | tok/s 16039
step    120 | loss 2.7041 | lr 2.74e-04 | grad 1.37 | tok/s 15252
step    130 | loss 2.1463 | lr 2.74e-04 | grad 2.97 | tok/s 15618
step    140 | loss 2.4356 | lr 2.74e-04 | grad 4.38 | tok/s 15658
step    150 | loss 1.7408 | lr 2.74e-04 | grad 4.34 | tok/s 16038
step    160 | loss 2.3992 | lr 2.74e-04 | grad 1.47 | tok/s 15507
step    170 | loss 2.2639 | lr 2.74e-04 | grad 1.02 | tok/s 15282
step    180 | loss 2.0068 | lr 2.74e-04 | grad 1.74 | tok/s 15634
step    190 | loss 1.9352 | lr 2.74e-04 | grad 1.30 | tok/s 15357
step    200 | loss 1.6873 | lr 2.74e-04 | grad 1.27 | tok/s 16066
step    210 | loss 1.8827 | lr 2.74e-04 | grad 2.33 | tok/s 15243
step    220 | loss 2.1960 | lr 2.74e-04 | grad 1.95 | tok/s 15403
step    230 | loss 2.0146 | lr 2.74e-04 | grad 1.57 | tok/s 15379
step    240 | loss 2.2304 | lr 2.74e-04 | grad 3.08 | tok/s 15588
step    250 | loss 1.7820 | lr 2.74e-04 | grad 1.07 | tok/s 15491
step    260 | loss 1.8962 | lr 2.74e-04 | grad 1.88 | tok/s 15914
step    270 | loss 1.8282 | lr 2.74e-04 | grad 1.36 | tok/s 15560
step    280 | loss 1.7852 | lr 2.74e-04 | grad 1.21 | tok/s 14599
step    290 | loss 1.6827 | lr 2.74e-04 | grad 1.43 | tok/s 15102
step    300 | loss 1.9656 | lr 2.74e-04 | grad 1.55 | tok/s 15211
step    310 | loss 1.6732 | lr 2.74e-04 | grad 1.13 | tok/s 15157
step    320 | loss 1.8830 | lr 2.74e-04 | grad 2.17 | tok/s 15326
step    330 | loss 1.7178 | lr 2.74e-04 | grad 1.16 | tok/s 15490
step    340 | loss 2.0137 | lr 2.74e-04 | grad 1.59 | tok/s 15411
step    350 | loss 1.7541 | lr 2.74e-04 | grad 1.37 | tok/s 15871
step    360 | loss 1.5891 | lr 2.74e-04 | grad 1.17 | tok/s 15181
step    370 | loss 1.5331 | lr 2.74e-04 | grad 1.16 | tok/s 16004
step    380 | loss 1.2749 | lr 2.74e-04 | grad 1.26 | tok/s 16137
step    390 | loss 1.1841 | lr 2.74e-04 | grad 0.98 | tok/s 16131
step    400 | loss 1.7779 | lr 2.74e-04 | grad 1.23 | tok/s 15284
step    410 | loss 1.7645 | lr 2.74e-04 | grad 1.59 | tok/s 15420
step    420 | loss 1.6724 | lr 2.74e-04 | grad 1.68 | tok/s 16095
step    430 | loss 1.5996 | lr 2.74e-04 | grad 1.26 | tok/s 15827
step    440 | loss 1.7163 | lr 2.74e-04 | grad 1.51 | tok/s 15338
step    450 | loss 1.6250 | lr 2.74e-04 | grad 0.96 | tok/s 15513
step    460 | loss 1.6184 | lr 2.74e-04 | grad 1.36 | tok/s 15744
step    470 | loss 1.5967 | lr 2.74e-04 | grad 2.14 | tok/s 15619
step    480 | loss 1.5974 | lr 2.74e-04 | grad 1.88 | tok/s 15966
step    490 | loss 1.6990 | lr 2.74e-04 | grad 1.70 | tok/s 15328
step    500 | loss 1.8417 | lr 2.74e-04 | grad 1.18 | tok/s 15572
step    510 | loss 1.7087 | lr 2.74e-04 | grad 0.98 | tok/s 14881
step    520 | loss 1.5537 | lr 2.74e-04 | grad 1.38 | tok/s 15583
step    530 | loss 1.7164 | lr 2.74e-04 | grad 1.35 | tok/s 15313
step    540 | loss 1.6029 | lr 2.74e-04 | grad 1.05 | tok/s 15001
step    550 | loss 1.3620 | lr 2.74e-04 | grad 2.00 | tok/s 15667
step    560 | loss 1.4461 | lr 2.74e-04 | grad 1.21 | tok/s 16129
step    570 | loss 1.3532 | lr 2.74e-04 | grad 1.35 | tok/s 16139
step    580 | loss 1.3131 | lr 2.74e-04 | grad 0.91 | tok/s 16137
step    590 | loss 1.3454 | lr 2.74e-04 | grad 0.88 | tok/s 16129
step    600 | loss 1.2881 | lr 2.74e-04 | grad 1.07 | tok/s 16139
step    610 | loss 1.3128 | lr 2.74e-04 | grad 1.05 | tok/s 16128
step    620 | loss 1.3079 | lr 2.74e-04 | grad 1.04 | tok/s 16069
step    630 | loss 1.7408 | lr 2.74e-04 | grad 3.45 | tok/s 15182
step    640 | loss 1.7285 | lr 2.74e-04 | grad 1.13 | tok/s 15379
step    650 | loss 1.5545 | lr 2.74e-04 | grad 1.16 | tok/s 15365
step    660 | loss 1.5991 | lr 2.74e-04 | grad 1.25 | tok/s 15962
step    670 | loss 1.6395 | lr 2.74e-04 | grad 3.56 | tok/s 15422
step    680 | loss 1.6421 | lr 2.74e-04 | grad 1.55 | tok/s 15167
step    690 | loss 1.6040 | lr 2.74e-04 | grad 1.21 | tok/s 15051
step    700 | loss 1.4900 | lr 2.74e-04 | grad 0.99 | tok/s 15397
step    710 | loss 1.6514 | lr 2.74e-04 | grad 2.19 | tok/s 15144
step    720 | loss 1.3265 | lr 2.74e-04 | grad 1.13 | tok/s 15741
step    730 | loss 1.4855 | lr 2.74e-04 | grad 1.02 | tok/s 15483
step    740 | loss 1.7846 | lr 2.74e-04 | grad 2.62 | tok/s 15915
step    750 | loss 1.5594 | lr 2.74e-04 | grad 1.16 | tok/s 16099
step    760 | loss 1.5327 | lr 2.74e-04 | grad 2.38 | tok/s 15772
step    770 | loss 1.5685 | lr 2.74e-04 | grad 1.29 | tok/s 15513
step    780 | loss 1.5062 | lr 2.74e-04 | grad 1.34 | tok/s 15617
step    790 | loss 1.6596 | lr 2.74e-04 | grad 3.25 | tok/s 15970
step    800 | loss 1.3551 | lr 2.74e-04 | grad 0.94 | tok/s 15682
step    810 | loss 1.3461 | lr 2.74e-04 | grad 2.03 | tok/s 15154
step    820 | loss 1.4646 | lr 2.74e-04 | grad 1.38 | tok/s 15469
step    830 | loss 1.5164 | lr 2.74e-04 | grad 0.91 | tok/s 15276
step    840 | loss 1.6516 | lr 2.74e-04 | grad 1.08 | tok/s 15188
step    850 | loss 1.5520 | lr 2.74e-04 | grad 1.05 | tok/s 15511
step    860 | loss 1.6116 | lr 2.74e-04 | grad 1.70 | tok/s 15780
step    870 | loss 1.4425 | lr 2.74e-04 | grad 1.23 | tok/s 15879
step    880 | loss 1.5960 | lr 2.74e-04 | grad 1.18 | tok/s 15561
step    890 | loss 1.4887 | lr 2.74e-04 | grad 0.87 | tok/s 15502
step    900 | loss 1.5341 | lr 2.74e-04 | grad 1.07 | tok/s 15451
step    910 | loss 1.5259 | lr 2.74e-04 | grad 3.72 | tok/s 15279
step    920 | loss 1.4905 | lr 2.74e-04 | grad 1.09 | tok/s 15449
step    930 | loss 1.4019 | lr 2.74e-04 | grad 1.25 | tok/s 15654
step    940 | loss 1.3872 | lr 2.74e-04 | grad 1.35 | tok/s 15288
step    950 | loss 1.4829 | lr 2.74e-04 | grad 1.71 | tok/s 15035
step    960 | loss 1.4414 | lr 2.74e-04 | grad 0.91 | tok/s 15449
step    970 | loss 1.4653 | lr 2.74e-04 | grad 1.12 | tok/s 15455
step    980 | loss 1.8972 | lr 2.74e-04 | grad 2.64 | tok/s 16070
step    990 | loss 1.5781 | lr 2.74e-04 | grad 1.22 | tok/s 15415
step   1000 | loss 1.5536 | lr 2.74e-04 | grad 1.31 | tok/s 15454
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5536.pt
step   1010 | loss 1.3718 | lr 2.74e-04 | grad 1.66 | tok/s 9609
step   1020 | loss 1.2675 | lr 2.74e-04 | grad 0.94 | tok/s 16216
step   1030 | loss 1.5500 | lr 2.74e-04 | grad 1.31 | tok/s 15399
step   1040 | loss 2.1115 | lr 2.74e-04 | grad 2.30 | tok/s 15755
step   1050 | loss 1.5261 | lr 2.74e-04 | grad 1.81 | tok/s 15861
step   1060 | loss 1.1684 | lr 2.74e-04 | grad 1.02 | tok/s 15671
step   1070 | loss 1.4493 | lr 2.74e-04 | grad 1.33 | tok/s 15629
step   1080 | loss 1.2708 | lr 2.74e-04 | grad 1.08 | tok/s 16173
step   1090 | loss 1.2332 | lr 2.74e-04 | grad 0.93 | tok/s 16185
step   1100 | loss 1.2191 | lr 2.74e-04 | grad 0.90 | tok/s 16174
step   1110 | loss 1.1634 | lr 2.74e-04 | grad 0.92 | tok/s 16185
step   1120 | loss 1.4547 | lr 2.74e-04 | grad 2.88 | tok/s 15725
step   1130 | loss 1.6524 | lr 2.74e-04 | grad 1.12 | tok/s 15894
step   1140 | loss 1.7916 | lr 2.74e-04 | grad 1.53 | tok/s 16078
step   1150 | loss 1.7055 | lr 2.74e-04 | grad 1.62 | tok/s 15580
step   1160 | loss 1.7493 | lr 2.74e-04 | grad 1.59 | tok/s 15334
step   1170 | loss 1.5024 | lr 2.74e-04 | grad 1.31 | tok/s 15164
step   1180 | loss 1.3561 | lr 2.74e-04 | grad 2.08 | tok/s 15923
step   1190 | loss 1.6152 | lr 2.74e-04 | grad 1.68 | tok/s 16055
step   1200 | loss 1.1627 | lr 2.74e-04 | grad 1.12 | tok/s 16148
step   1210 | loss 1.3970 | lr 2.74e-04 | grad 1.31 | tok/s 15037
step   1220 | loss 1.4158 | lr 2.74e-04 | grad 1.52 | tok/s 15824
step   1230 | loss 1.3458 | lr 2.74e-04 | grad 0.93 | tok/s 15853
step   1240 | loss 1.3218 | lr 2.74e-04 | grad 1.16 | tok/s 15917
step   1250 | loss 1.4935 | lr 2.74e-04 | grad 1.66 | tok/s 15677
step   1260 | loss 1.4180 | lr 2.74e-04 | grad 1.44 | tok/s 16020
step   1270 | loss 1.3849 | lr 2.74e-04 | grad 1.28 | tok/s 15557
step   1280 | loss 1.3972 | lr 2.74e-04 | grad 1.27 | tok/s 15393
step   1290 | loss 1.3666 | lr 2.74e-04 | grad 1.45 | tok/s 15425
step   1300 | loss 1.6656 | lr 2.74e-04 | grad 4.09 | tok/s 15191
step   1310 | loss 1.5182 | lr 2.74e-04 | grad 1.24 | tok/s 15795
step   1320 | loss 1.4942 | lr 2.74e-04 | grad 1.20 | tok/s 15797
step   1330 | loss 1.4346 | lr 2.74e-04 | grad 1.16 | tok/s 15647
step   1340 | loss 1.6195 | lr 2.74e-04 | grad 1.59 | tok/s 15322
step   1350 | loss 1.4242 | lr 2.74e-04 | grad 1.02 | tok/s 15643
step   1360 | loss 1.4661 | lr 2.74e-04 | grad 1.29 | tok/s 15074
step   1370 | loss 1.5917 | lr 2.74e-04 | grad 1.84 | tok/s 15886
step   1380 | loss 1.4584 | lr 2.74e-04 | grad 1.27 | tok/s 15176
step   1390 | loss 1.3829 | lr 2.74e-04 | grad 1.93 | tok/s 15914
step   1400 | loss 1.5263 | lr 2.74e-04 | grad 1.29 | tok/s 15345
step   1410 | loss 1.4320 | lr 2.74e-04 | grad 2.11 | tok/s 15007
step   1420 | loss 1.1680 | lr 2.74e-04 | grad 5.66 | tok/s 15997
step   1430 | loss 1.7215 | lr 2.74e-04 | grad 1.28 | tok/s 15417
step   1440 | loss 1.4529 | lr 2.74e-04 | grad 1.35 | tok/s 15828
step   1450 | loss 1.5015 | lr 2.74e-04 | grad 5.50 | tok/s 15796
step   1460 | loss 1.5784 | lr 2.74e-04 | grad 2.81 | tok/s 15349
step   1470 | loss 1.3562 | lr 2.74e-04 | grad 1.20 | tok/s 15002
step   1480 | loss 1.3968 | lr 2.74e-04 | grad 1.05 | tok/s 15826
step   1490 | loss 1.8113 | lr 2.74e-04 | grad 4.47 | tok/s 15571
step   1500 | loss 1.4653 | lr 2.74e-04 | grad 1.30 | tok/s 15613
step   1510 | loss 1.2727 | lr 2.74e-04 | grad 1.25 | tok/s 15620
step   1520 | loss 1.4803 | lr 2.74e-04 | grad 1.09 | tok/s 15494
step   1530 | loss 1.4031 | lr 2.74e-04 | grad 1.19 | tok/s 15738
step   1540 | loss 1.5095 | lr 2.74e-04 | grad 1.23 | tok/s 15888
step   1550 | loss 1.5078 | lr 2.74e-04 | grad 1.48 | tok/s 15580
step   1560 | loss 1.1783 | lr 2.74e-04 | grad 1.34 | tok/s 16166
step   1570 | loss 1.3172 | lr 2.74e-04 | grad 0.98 | tok/s 15717
step   1580 | loss 1.2957 | lr 2.74e-04 | grad 1.77 | tok/s 15678
step   1590 | loss 1.4697 | lr 2.74e-04 | grad 1.38 | tok/s 15337
step   1600 | loss 1.2896 | lr 2.74e-04 | grad 2.02 | tok/s 15915
step   1610 | loss 1.9476 | lr 2.74e-04 | grad 2.08 | tok/s 15793
step   1620 | loss 1.8415 | lr 2.74e-04 | grad 1.55 | tok/s 16166
step   1630 | loss 1.5440 | lr 2.74e-04 | grad 1.64 | tok/s 16179
step   1640 | loss 1.4042 | lr 2.74e-04 | grad 1.88 | tok/s 16168
step   1650 | loss 1.3329 | lr 2.74e-04 | grad 1.52 | tok/s 16178
step   1660 | loss 1.2916 | lr 2.74e-04 | grad 1.36 | tok/s 16168
step   1670 | loss 1.5175 | lr 2.74e-04 | grad 1.26 | tok/s 15670
step   1680 | loss 1.4150 | lr 2.74e-04 | grad 1.28 | tok/s 15522
step   1690 | loss 1.4725 | lr 2.74e-04 | grad 1.49 | tok/s 15067
step   1700 | loss 1.3184 | lr 2.74e-04 | grad 0.93 | tok/s 15849
step   1710 | loss 1.2911 | lr 2.74e-04 | grad 1.44 | tok/s 15680
step   1720 | loss 1.4694 | lr 2.74e-04 | grad 1.22 | tok/s 15463
step   1730 | loss 1.4506 | lr 2.74e-04 | grad 1.89 | tok/s 15689
step   1740 | loss 1.4011 | lr 2.74e-04 | grad 1.23 | tok/s 16010
step   1750 | loss 1.2270 | lr 2.74e-04 | grad 0.93 | tok/s 15494
step   1760 | loss 1.4579 | lr 2.74e-04 | grad 1.31 | tok/s 15289
step   1770 | loss 1.6157 | lr 2.74e-04 | grad 1.31 | tok/s 15871
step   1780 | loss 1.7388 | lr 2.74e-04 | grad 1.01 | tok/s 14735
step   1790 | loss 1.3434 | lr 2.74e-04 | grad 1.39 | tok/s 15332
step   1800 | loss 1.2965 | lr 2.74e-04 | grad 1.16 | tok/s 15442
step   1810 | loss 1.4003 | lr 2.74e-04 | grad 1.00 | tok/s 15546
step   1820 | loss 1.5528 | lr 2.74e-04 | grad 1.20 | tok/s 15468
step   1830 | loss 1.3896 | lr 2.74e-04 | grad 0.98 | tok/s 14927
step   1840 | loss 1.3545 | lr 2.74e-04 | grad 1.28 | tok/s 15573
step   1850 | loss 1.4165 | lr 2.74e-04 | grad 1.07 | tok/s 15231
step   1860 | loss 1.4591 | lr 2.74e-04 | grad 1.12 | tok/s 15500
step   1870 | loss 1.4153 | lr 2.74e-04 | grad 1.27 | tok/s 15712
step   1880 | loss 1.4996 | lr 2.74e-04 | grad 1.35 | tok/s 15736
step   1890 | loss 1.2420 | lr 2.74e-04 | grad 1.12 | tok/s 16170
step   1900 | loss 1.1892 | lr 2.74e-04 | grad 0.96 | tok/s 16163
step   1910 | loss 1.1678 | lr 2.74e-04 | grad 1.14 | tok/s 16161
step   1920 | loss 1.1561 | lr 2.74e-04 | grad 0.95 | tok/s 16164
step   1930 | loss 1.2292 | lr 2.74e-04 | grad 1.32 | tok/s 15940
step   1940 | loss 1.5355 | lr 2.74e-04 | grad 1.64 | tok/s 15473
step   1950 | loss 1.4411 | lr 2.74e-04 | grad 2.22 | tok/s 15112
step   1960 | loss 1.4713 | lr 2.74e-04 | grad 1.84 | tok/s 15315
step   1970 | loss 1.5284 | lr 2.74e-04 | grad 1.05 | tok/s 15742
step   1980 | loss 1.4428 | lr 2.74e-04 | grad 1.93 | tok/s 15356
step   1990 | loss 1.5104 | lr 2.74e-04 | grad 1.78 | tok/s 15654
step   2000 | loss 1.1488 | lr 2.74e-04 | grad 1.31 | tok/s 16160
  >>> saved checkpoint: checkpoint_step_002000_loss_1.1488.pt
step   2010 | loss 1.3579 | lr 2.74e-04 | grad 1.01 | tok/s 9186
step   2020 | loss 1.3980 | lr 2.74e-04 | grad 1.21 | tok/s 15403
step   2030 | loss 1.7352 | lr 2.74e-04 | grad 1.57 | tok/s 15235
step   2040 | loss 1.4324 | lr 2.74e-04 | grad 1.19 | tok/s 15594
step   2050 | loss 1.3541 | lr 2.74e-04 | grad 1.55 | tok/s 15520
step   2060 | loss 1.6507 | lr 2.74e-04 | grad 3.84 | tok/s 15661
step   2070 | loss 1.1639 | lr 2.74e-04 | grad 1.43 | tok/s 15703
step   2080 | loss 1.2745 | lr 2.74e-04 | grad 1.18 | tok/s 15081
step   2090 | loss 1.4696 | lr 2.74e-04 | grad 2.53 | tok/s 15962
step   2100 | loss 1.5543 | lr 2.74e-04 | grad 1.45 | tok/s 16090
step   2110 | loss 1.3993 | lr 2.74e-04 | grad 2.00 | tok/s 15292
step   2120 | loss 2.1453 | lr 2.74e-04 | grad 1.95 | tok/s 15611
step   2130 | loss 1.4479 | lr 2.74e-04 | grad 2.00 | tok/s 15236
step   2140 | loss 1.5263 | lr 2.74e-04 | grad 1.25 | tok/s 15764
step   2150 | loss 1.7038 | lr 2.74e-04 | grad 1.41 | tok/s 15691
step   2160 | loss 1.4536 | lr 2.74e-04 | grad 0.95 | tok/s 15624
step   2170 | loss 1.4760 | lr 2.74e-04 | grad 1.04 | tok/s 15676
step   2180 | loss 1.2057 | lr 2.74e-04 | grad 1.61 | tok/s 16122
step   2190 | loss 1.4965 | lr 2.74e-04 | grad 2.33 | tok/s 15446
step   2200 | loss 1.1919 | lr 2.74e-04 | grad 0.93 | tok/s 16194
step   2210 | loss 1.4763 | lr 2.74e-04 | grad 1.10 | tok/s 15637
step   2220 | loss 1.4495 | lr 2.74e-04 | grad 1.25 | tok/s 15110
step   2230 | loss 1.6086 | lr 2.74e-04 | grad 0.93 | tok/s 15449
step   2240 | loss 1.3397 | lr 2.74e-04 | grad 2.05 | tok/s 15711
step   2250 | loss 1.3698 | lr 2.74e-04 | grad 0.99 | tok/s 15386
step   2260 | loss 1.5327 | lr 2.74e-04 | grad 1.26 | tok/s 15687
step   2270 | loss 1.7063 | lr 2.74e-04 | grad 1.20 | tok/s 15058
step   2280 | loss 1.4055 | lr 2.74e-04 | grad 1.04 | tok/s 15513
step   2290 | loss 1.2608 | lr 2.74e-04 | grad 1.23 | tok/s 15232
step   2300 | loss 1.6572 | lr 2.74e-04 | grad 2.03 | tok/s 15350
step   2310 | loss 1.3921 | lr 2.74e-04 | grad 1.34 | tok/s 15744
step   2320 | loss 1.3521 | lr 2.74e-04 | grad 1.42 | tok/s 16169
step   2330 | loss 1.2737 | lr 2.74e-04 | grad 1.27 | tok/s 16165
step   2340 | loss 1.2447 | lr 2.74e-04 | grad 1.06 | tok/s 16180
step   2350 | loss 1.2281 | lr 2.74e-04 | grad 1.13 | tok/s 16170
step   2360 | loss 1.1684 | lr 2.74e-04 | grad 1.16 | tok/s 16170
step   2370 | loss 1.1751 | lr 2.74e-04 | grad 1.14 | tok/s 16166
step   2380 | loss 1.1610 | lr 2.74e-04 | grad 1.03 | tok/s 16177
step   2390 | loss 1.1740 | lr 2.74e-04 | grad 0.98 | tok/s 16178
step   2400 | loss 1.1287 | lr 2.74e-04 | grad 1.02 | tok/s 16166
step   2410 | loss 1.3580 | lr 2.74e-04 | grad 6.75 | tok/s 15954
step   2420 | loss 1.2835 | lr 2.74e-04 | grad 0.95 | tok/s 15879
step   2430 | loss 1.2036 | lr 2.74e-04 | grad 1.56 | tok/s 15249
step   2440 | loss 1.4084 | lr 2.74e-04 | grad 1.34 | tok/s 15108
step   2450 | loss 1.3502 | lr 2.74e-04 | grad 1.09 | tok/s 15662
step   2460 | loss 1.5421 | lr 2.74e-04 | grad 1.05 | tok/s 15860
step   2470 | loss 1.3393 | lr 2.74e-04 | grad 1.21 | tok/s 15444
step   2480 | loss 1.4845 | lr 2.74e-04 | grad 1.84 | tok/s 15846
step   2490 | loss 1.5282 | lr 2.74e-04 | grad 1.50 | tok/s 15386
step   2500 | loss 1.5748 | lr 2.74e-04 | grad 1.29 | tok/s 15770
step   2510 | loss 1.4426 | lr 2.74e-04 | grad 1.20 | tok/s 15618
step   2520 | loss 1.4037 | lr 2.74e-04 | grad 1.41 | tok/s 15395
step   2530 | loss 1.5364 | lr 2.74e-04 | grad 1.95 | tok/s 15725
step   2540 | loss 1.4461 | lr 2.74e-04 | grad 3.33 | tok/s 15039
step   2550 | loss 1.3268 | lr 2.74e-04 | grad 1.48 | tok/s 15537
step   2560 | loss 1.5787 | lr 2.74e-04 | grad 1.30 | tok/s 15292
step   2570 | loss 1.3679 | lr 2.74e-04 | grad 1.13 | tok/s 15482
step   2580 | loss 1.5808 | lr 2.74e-04 | grad 1.26 | tok/s 15397
step   2590 | loss 1.3296 | lr 2.74e-04 | grad 1.23 | tok/s 15101
step   2600 | loss 1.4625 | lr 2.74e-04 | grad 1.05 | tok/s 15516
step   2610 | loss 1.3426 | lr 2.74e-04 | grad 1.22 | tok/s 15866
step   2620 | loss 1.5701 | lr 2.74e-04 | grad 1.35 | tok/s 15368
step   2630 | loss 1.1775 | lr 2.74e-04 | grad 1.56 | tok/s 15894
step   2640 | loss 1.3301 | lr 2.74e-04 | grad 1.05 | tok/s 15331
step   2650 | loss 1.2599 | lr 2.74e-04 | grad 1.47 | tok/s 15627
step   2660 | loss 1.3457 | lr 2.74e-04 | grad 0.93 | tok/s 15736
step   2670 | loss 1.1527 | lr 2.74e-04 | grad 1.17 | tok/s 15665
step   2680 | loss 1.4016 | lr 2.74e-04 | grad 1.29 | tok/s 15360
step   2690 | loss 1.3298 | lr 2.74e-04 | grad 1.30 | tok/s 15106
step   2700 | loss 1.4116 | lr 2.74e-04 | grad 1.14 | tok/s 15613
step   2710 | loss 1.6846 | lr 2.74e-04 | grad 1.55 | tok/s 15462
step   2720 | loss 1.3605 | lr 2.74e-04 | grad 1.30 | tok/s 15710
step   2730 | loss 1.3280 | lr 2.74e-04 | grad 2.56 | tok/s 15485
step   2740 | loss 1.4421 | lr 2.74e-04 | grad 1.87 | tok/s 15423
step   2750 | loss 1.3965 | lr 2.74e-04 | grad 1.44 | tok/s 15683
step   2760 | loss 1.2266 | lr 2.74e-04 | grad 1.04 | tok/s 15827
step   2770 | loss 1.3728 | lr 2.74e-04 | grad 1.30 | tok/s 15579
step   2780 | loss 1.3195 | lr 2.74e-04 | grad 2.36 | tok/s 15848
step   2790 | loss 1.4456 | lr 2.74e-04 | grad 2.19 | tok/s 15432
step   2800 | loss 1.4654 | lr 2.74e-04 | grad 2.22 | tok/s 14986
step   2810 | loss 1.3941 | lr 2.74e-04 | grad 1.47 | tok/s 15598
step   2820 | loss 1.3637 | lr 2.74e-04 | grad 1.99 | tok/s 14606
step   2830 | loss 1.2758 | lr 2.74e-04 | grad 0.97 | tok/s 15469
step   2840 | loss 1.2024 | lr 2.74e-04 | grad 1.33 | tok/s 15297
step   2850 | loss 1.1871 | lr 2.74e-04 | grad 0.92 | tok/s 16161
step   2860 | loss 1.2080 | lr 2.74e-04 | grad 1.82 | tok/s 15724
step   2870 | loss 1.3717 | lr 2.74e-04 | grad 1.20 | tok/s 15312
step   2880 | loss 1.4146 | lr 2.74e-04 | grad 1.26 | tok/s 15167
step   2890 | loss 1.5752 | lr 2.74e-04 | grad 3.92 | tok/s 15623
step   2900 | loss 1.4264 | lr 2.74e-04 | grad 1.75 | tok/s 15130
step   2910 | loss 1.3587 | lr 2.74e-04 | grad 1.41 | tok/s 15909
step   2920 | loss 1.2670 | lr 2.74e-04 | grad 2.00 | tok/s 15334
step   2930 | loss 1.3983 | lr 2.74e-04 | grad 1.52 | tok/s 15818
step   2940 | loss 1.2559 | lr 2.74e-04 | grad 1.02 | tok/s 15481
step   2950 | loss 1.6776 | lr 2.74e-04 | grad 1.05 | tok/s 15599
step   2960 | loss 1.5519 | lr 2.74e-04 | grad 1.27 | tok/s 15042
step   2970 | loss 1.4501 | lr 2.74e-04 | grad 1.15 | tok/s 15582
step   2980 | loss 1.3421 | lr 2.74e-04 | grad 1.12 | tok/s 15775
step   2990 | loss 1.4823 | lr 2.74e-04 | grad 1.54 | tok/s 15679
step   3000 | loss 1.3481 | lr 2.74e-04 | grad 1.79 | tok/s 15761
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3481.pt
step   3010 | loss 1.4692 | lr 2.74e-04 | grad 2.52 | tok/s 9608
step   3020 | loss 1.3380 | lr 2.74e-04 | grad 1.46 | tok/s 15413
step   3030 | loss 1.2810 | lr 2.74e-04 | grad 1.30 | tok/s 15134
step   3040 | loss 1.3087 | lr 2.74e-04 | grad 1.28 | tok/s 15651
step   3050 | loss 1.2113 | lr 2.74e-04 | grad 0.95 | tok/s 16107
step   3060 | loss 1.4577 | lr 2.74e-04 | grad 3.02 | tok/s 15817
step   3070 | loss 1.8866 | lr 2.74e-04 | grad 2.95 | tok/s 15502
step   3080 | loss 1.4886 | lr 2.74e-04 | grad 2.36 | tok/s 15645
step   3090 | loss 1.3794 | lr 2.74e-04 | grad 1.84 | tok/s 15050
step   3100 | loss 1.2859 | lr 2.74e-04 | grad 1.24 | tok/s 15684
step   3110 | loss 1.3839 | lr 2.74e-04 | grad 1.58 | tok/s 16022
step   3120 | loss 1.3918 | lr 2.74e-04 | grad 1.06 | tok/s 15388
step   3130 | loss 1.3570 | lr 2.74e-04 | grad 1.62 | tok/s 15174
step   3140 | loss 1.2578 | lr 2.74e-04 | grad 2.56 | tok/s 15449
step   3150 | loss 1.3559 | lr 2.74e-04 | grad 1.71 | tok/s 14744
step   3160 | loss 1.4971 | lr 2.74e-04 | grad 1.02 | tok/s 16098
step   3170 | loss 1.2280 | lr 2.74e-04 | grad 0.92 | tok/s 15922
step   3180 | loss 1.2569 | lr 2.74e-04 | grad 1.60 | tok/s 15844
step   3190 | loss 1.5435 | lr 2.74e-04 | grad 2.23 | tok/s 15803
step   3200 | loss 1.2878 | lr 2.74e-04 | grad 1.03 | tok/s 15589
step   3210 | loss 1.4311 | lr 2.74e-04 | grad 1.16 | tok/s 15679
step   3220 | loss 1.2722 | lr 2.74e-04 | grad 1.13 | tok/s 16046
step   3230 | loss 1.3725 | lr 2.74e-04 | grad 1.12 | tok/s 15077
step   3240 | loss 1.3046 | lr 2.74e-04 | grad 1.09 | tok/s 14977
step   3250 | loss 1.3411 | lr 2.74e-04 | grad 1.73 | tok/s 15288
step   3260 | loss 1.3795 | lr 2.74e-04 | grad 1.29 | tok/s 15513
step   3270 | loss 1.3459 | lr 2.74e-04 | grad 1.30 | tok/s 15140
step   3280 | loss 1.4670 | lr 2.74e-04 | grad 1.02 | tok/s 15528
step   3290 | loss 1.2566 | lr 2.74e-04 | grad 1.33 | tok/s 15589
step   3300 | loss 1.4299 | lr 2.74e-04 | grad 1.77 | tok/s 16081
step   3310 | loss 1.7356 | lr 2.74e-04 | grad 3.80 | tok/s 15678
step   3320 | loss 1.4801 | lr 2.74e-04 | grad 1.77 | tok/s 15552
step   3330 | loss 1.2880 | lr 2.74e-04 | grad 1.58 | tok/s 15022
step   3340 | loss 1.1522 | lr 2.74e-04 | grad 0.86 | tok/s 16042
step   3350 | loss 1.3975 | lr 2.74e-04 | grad 1.72 | tok/s 15961
step   3360 | loss 1.3640 | lr 2.74e-04 | grad 1.33 | tok/s 15324
step   3370 | loss 1.5126 | lr 2.74e-04 | grad 2.59 | tok/s 15303
step   3380 | loss 1.3617 | lr 2.74e-04 | grad 1.21 | tok/s 15578
step   3390 | loss 1.2654 | lr 2.74e-04 | grad 1.91 | tok/s 15326
step   3400 | loss 1.4052 | lr 2.74e-04 | grad 1.13 | tok/s 15567
step   3410 | loss 1.4429 | lr 2.74e-04 | grad 1.21 | tok/s 15338
step   3420 | loss 1.4734 | lr 2.74e-04 | grad 1.48 | tok/s 15530
step   3430 | loss 1.3942 | lr 2.74e-04 | grad 1.53 | tok/s 15560
step   3440 | loss 1.3219 | lr 2.74e-04 | grad 1.77 | tok/s 15349
step   3450 | loss 1.3993 | lr 2.74e-04 | grad 1.47 | tok/s 15279
step   3460 | loss 1.3911 | lr 2.74e-04 | grad 1.47 | tok/s 15290
step   3470 | loss 1.5927 | lr 2.74e-04 | grad 1.02 | tok/s 15653
step   3480 | loss 1.2598 | lr 2.74e-04 | grad 1.94 | tok/s 15211
step   3490 | loss 1.5081 | lr 2.74e-04 | grad 1.11 | tok/s 15693

Training complete! Final step: 3498
