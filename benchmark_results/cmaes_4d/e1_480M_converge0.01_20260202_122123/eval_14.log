Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_14/level1_100m_20260202_125213
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 764,832,768 parameters
Using schedule-free AdamW (lr=0.00023846313351298854)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 5.0712 | lr 2.38e-04 | grad 10.12 | tok/s 4162
step     20 | loss 2.6332 | lr 2.38e-04 | grad 2.39 | tok/s 6747
step     30 | loss 2.4893 | lr 2.38e-04 | grad 2.66 | tok/s 6820
step     40 | loss 2.3680 | lr 2.38e-04 | grad 1.85 | tok/s 6535
step     50 | loss 3.1102 | lr 2.38e-04 | grad 7.00 | tok/s 6640
step     60 | loss 2.0744 | lr 2.38e-04 | grad 2.12 | tok/s 6858
step     70 | loss 2.0526 | lr 2.38e-04 | grad 2.94 | tok/s 6950
step     80 | loss 4.9347 | lr 2.38e-04 | grad 19.25 | tok/s 6987
step     90 | loss 4.2297 | lr 2.38e-04 | grad 2.84 | tok/s 7112
step    100 | loss 3.7091 | lr 2.38e-04 | grad 3.41 | tok/s 7112
step    110 | loss 3.1523 | lr 2.38e-04 | grad 10.69 | tok/s 7112
step    120 | loss 2.7872 | lr 2.38e-04 | grad 11.75 | tok/s 7107
step    130 | loss 2.6345 | lr 2.38e-04 | grad 4.75 | tok/s 7108
step    140 | loss 2.4118 | lr 2.38e-04 | grad 2.88 | tok/s 7108
step    150 | loss 2.4672 | lr 2.38e-04 | grad 6.12 | tok/s 7107
step    160 | loss 2.2221 | lr 2.38e-04 | grad 4.53 | tok/s 7106
step    170 | loss 2.3185 | lr 2.38e-04 | grad 5.97 | tok/s 7108
step    180 | loss 2.1341 | lr 2.38e-04 | grad 4.59 | tok/s 7109
step    190 | loss 2.2689 | lr 2.38e-04 | grad 6.12 | tok/s 7106
step    200 | loss 2.0580 | lr 2.38e-04 | grad 2.38 | tok/s 7107
step    210 | loss 2.0889 | lr 2.38e-04 | grad 4.72 | tok/s 7106
step    220 | loss 2.2401 | lr 2.38e-04 | grad 2.09 | tok/s 7018
step    230 | loss 2.2777 | lr 2.38e-04 | grad 2.17 | tok/s 6934
step    240 | loss 2.3085 | lr 2.38e-04 | grad 2.11 | tok/s 6587
step    250 | loss 2.1354 | lr 2.38e-04 | grad 1.30 | tok/s 6774
step    260 | loss 1.7253 | lr 2.38e-04 | grad 1.77 | tok/s 6986
step    270 | loss 2.1345 | lr 2.38e-04 | grad 1.62 | tok/s 6895
step    280 | loss 2.3401 | lr 2.38e-04 | grad 2.33 | tok/s 6763
step    290 | loss 1.8347 | lr 2.38e-04 | grad 1.76 | tok/s 7113
step    300 | loss 0.7390 | lr 2.38e-04 | grad 3.62 | tok/s 7113
step    310 | loss 2.5547 | lr 2.38e-04 | grad 2.61 | tok/s 6998
step    320 | loss 2.0592 | lr 2.38e-04 | grad 3.53 | tok/s 6847
step    330 | loss 2.0135 | lr 2.38e-04 | grad 1.67 | tok/s 6614
step    340 | loss 2.2437 | lr 2.38e-04 | grad 1.41 | tok/s 6720
step    350 | loss 1.9621 | lr 2.38e-04 | grad 1.71 | tok/s 6890
step    360 | loss 1.5013 | lr 2.38e-04 | grad 4.75 | tok/s 7042
step    370 | loss 1.9044 | lr 2.38e-04 | grad 1.52 | tok/s 6380
step    380 | loss 1.7987 | lr 2.38e-04 | grad 1.54 | tok/s 6801
step    390 | loss 1.5606 | lr 2.38e-04 | grad 1.30 | tok/s 7107
step    400 | loss 1.5420 | lr 2.38e-04 | grad 1.71 | tok/s 7043
step    410 | loss 1.3815 | lr 2.38e-04 | grad 1.22 | tok/s 6887
step    420 | loss 1.8408 | lr 2.38e-04 | grad 2.59 | tok/s 6573
step    430 | loss 2.1385 | lr 2.38e-04 | grad 1.84 | tok/s 6994
step    440 | loss 2.1556 | lr 2.38e-04 | grad 2.28 | tok/s 6612
step    450 | loss 2.0155 | lr 2.38e-04 | grad 1.41 | tok/s 6844
step    460 | loss 1.7957 | lr 2.38e-04 | grad 1.91 | tok/s 6700
step    470 | loss 1.8402 | lr 2.38e-04 | grad 1.50 | tok/s 6903
step    480 | loss 2.2197 | lr 2.38e-04 | grad 3.62 | tok/s 6907
step    490 | loss 1.8063 | lr 2.38e-04 | grad 1.55 | tok/s 6528
step    500 | loss 1.7019 | lr 2.38e-04 | grad 2.22 | tok/s 6972
step    510 | loss 1.7155 | lr 2.38e-04 | grad 1.30 | tok/s 7068
step    520 | loss 1.7200 | lr 2.38e-04 | grad 1.42 | tok/s 7053
step    530 | loss 1.8918 | lr 2.38e-04 | grad 1.41 | tok/s 6780
step    540 | loss 1.7475 | lr 2.38e-04 | grad 1.42 | tok/s 6781
step    550 | loss 1.5854 | lr 2.38e-04 | grad 1.80 | tok/s 6641
step    560 | loss 1.7281 | lr 2.38e-04 | grad 1.60 | tok/s 6469
step    570 | loss 1.6672 | lr 2.38e-04 | grad 2.22 | tok/s 6640
step    580 | loss 1.5754 | lr 2.38e-04 | grad 1.46 | tok/s 6624
step    590 | loss 1.8727 | lr 2.38e-04 | grad 1.85 | tok/s 6797
step    600 | loss 1.8340 | lr 2.38e-04 | grad 1.34 | tok/s 6562
step    610 | loss 1.6490 | lr 2.38e-04 | grad 1.39 | tok/s 6902
step    620 | loss 1.5578 | lr 2.38e-04 | grad 1.44 | tok/s 6538
step    630 | loss 1.6866 | lr 2.38e-04 | grad 2.58 | tok/s 6591
step    640 | loss 1.8090 | lr 2.38e-04 | grad 1.42 | tok/s 6764
step    650 | loss 1.6757 | lr 2.38e-04 | grad 1.55 | tok/s 6799
step    660 | loss 1.6995 | lr 2.38e-04 | grad 1.14 | tok/s 6829
step    670 | loss 1.8749 | lr 2.38e-04 | grad 1.95 | tok/s 6877
step    680 | loss 1.7254 | lr 2.38e-04 | grad 1.48 | tok/s 6736
step    690 | loss 1.8554 | lr 2.38e-04 | grad 2.41 | tok/s 6973
step    700 | loss 1.4534 | lr 2.38e-04 | grad 1.76 | tok/s 7107
step    710 | loss 1.6029 | lr 2.38e-04 | grad 1.42 | tok/s 6631
step    720 | loss 1.4738 | lr 2.38e-04 | grad 2.02 | tok/s 6537
step    730 | loss 1.3831 | lr 2.38e-04 | grad 1.92 | tok/s 7096
step    740 | loss 1.5420 | lr 2.38e-04 | grad 1.57 | tok/s 7002
step    750 | loss 1.2842 | lr 2.38e-04 | grad 1.46 | tok/s 7106
step    760 | loss 1.1818 | lr 2.38e-04 | grad 1.43 | tok/s 7111
step    770 | loss 1.1292 | lr 2.38e-04 | grad 1.28 | tok/s 7111
step    780 | loss 1.0823 | lr 2.38e-04 | grad 1.23 | tok/s 7110
step    790 | loss 1.1861 | lr 2.38e-04 | grad 2.14 | tok/s 6889
step    800 | loss 1.8330 | lr 2.38e-04 | grad 3.36 | tok/s 6864
step    810 | loss 1.6873 | lr 2.38e-04 | grad 1.21 | tok/s 6826
step    820 | loss 1.6965 | lr 2.38e-04 | grad 2.31 | tok/s 6555
step    830 | loss 1.5910 | lr 2.38e-04 | grad 1.62 | tok/s 7036
step    840 | loss 1.4302 | lr 2.38e-04 | grad 1.31 | tok/s 7107
step    850 | loss 1.5719 | lr 2.38e-04 | grad 1.27 | tok/s 7070
step    860 | loss 1.4894 | lr 2.38e-04 | grad 2.61 | tok/s 6992
step    870 | loss 1.5127 | lr 2.38e-04 | grad 1.59 | tok/s 6734
step    880 | loss 1.6906 | lr 2.38e-04 | grad 1.57 | tok/s 6768
step    890 | loss 1.6788 | lr 2.38e-04 | grad 1.89 | tok/s 6866
step    900 | loss 1.5595 | lr 2.38e-04 | grad 1.54 | tok/s 6872
step    910 | loss 1.4338 | lr 2.38e-04 | grad 2.27 | tok/s 6730
step    920 | loss 1.5610 | lr 2.38e-04 | grad 2.67 | tok/s 6997
step    930 | loss 1.6010 | lr 2.38e-04 | grad 2.44 | tok/s 6677
step    940 | loss 1.4322 | lr 2.38e-04 | grad 1.25 | tok/s 7044
step    950 | loss 1.4961 | lr 2.38e-04 | grad 1.55 | tok/s 7073
step    960 | loss 1.3297 | lr 2.38e-04 | grad 1.62 | tok/s 7084
step    970 | loss 1.7294 | lr 2.38e-04 | grad 2.22 | tok/s 6665
step    980 | loss 1.6211 | lr 2.38e-04 | grad 1.50 | tok/s 6845
step    990 | loss 1.4781 | lr 2.38e-04 | grad 1.24 | tok/s 6962
step   1000 | loss 1.8597 | lr 2.38e-04 | grad 6.12 | tok/s 6679
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8597.pt
step   1010 | loss 1.7126 | lr 2.38e-04 | grad 2.31 | tok/s 3749
step   1020 | loss 1.6428 | lr 2.38e-04 | grad 1.45 | tok/s 6489
step   1030 | loss 1.4294 | lr 2.38e-04 | grad 1.40 | tok/s 6873
step   1040 | loss 1.5157 | lr 2.38e-04 | grad 1.46 | tok/s 6832
step   1050 | loss 1.6125 | lr 2.38e-04 | grad 1.74 | tok/s 6639
step   1060 | loss 1.6890 | lr 2.38e-04 | grad 1.39 | tok/s 6935
step   1070 | loss 1.6863 | lr 2.38e-04 | grad 1.52 | tok/s 6803
step   1080 | loss 1.3998 | lr 2.38e-04 | grad 1.59 | tok/s 6390
step   1090 | loss 1.0338 | lr 2.38e-04 | grad 3.66 | tok/s 7081
step   1100 | loss 1.5313 | lr 2.38e-04 | grad 1.51 | tok/s 6809
step   1110 | loss 1.4070 | lr 2.38e-04 | grad 1.38 | tok/s 7116
step   1120 | loss 1.3448 | lr 2.38e-04 | grad 1.56 | tok/s 7114
step   1130 | loss 1.2859 | lr 2.38e-04 | grad 1.42 | tok/s 7099
step   1140 | loss 1.2897 | lr 2.38e-04 | grad 1.45 | tok/s 7101
step   1150 | loss 1.2892 | lr 2.38e-04 | grad 1.20 | tok/s 7111
step   1160 | loss 1.2181 | lr 2.38e-04 | grad 1.21 | tok/s 7111
step   1170 | loss 1.2914 | lr 2.38e-04 | grad 1.31 | tok/s 7110
step   1180 | loss 1.3096 | lr 2.38e-04 | grad 1.69 | tok/s 7114
step   1190 | loss 1.2297 | lr 2.38e-04 | grad 1.22 | tok/s 7114
step   1200 | loss 1.2414 | lr 2.38e-04 | grad 1.12 | tok/s 7101
step   1210 | loss 1.2681 | lr 2.38e-04 | grad 1.23 | tok/s 7107
step   1220 | loss 1.2754 | lr 2.38e-04 | grad 1.27 | tok/s 7113
step   1230 | loss 1.2699 | lr 2.38e-04 | grad 1.12 | tok/s 7111
step   1240 | loss 1.4009 | lr 2.38e-04 | grad 5.53 | tok/s 6954
step   1250 | loss 1.7511 | lr 2.38e-04 | grad 1.26 | tok/s 6769
step   1260 | loss 1.3316 | lr 2.38e-04 | grad 1.58 | tok/s 6657
step   1270 | loss 1.7149 | lr 2.38e-04 | grad 1.58 | tok/s 6576
step   1280 | loss 1.5575 | lr 2.38e-04 | grad 1.34 | tok/s 6970
step   1290 | loss 1.5080 | lr 2.38e-04 | grad 1.76 | tok/s 6853
step   1300 | loss 1.5039 | lr 2.38e-04 | grad 2.14 | tok/s 6740
step   1310 | loss 1.4533 | lr 2.38e-04 | grad 2.30 | tok/s 7075
step   1320 | loss 1.6121 | lr 2.38e-04 | grad 2.67 | tok/s 6989
step   1330 | loss 1.4250 | lr 2.38e-04 | grad 1.34 | tok/s 6994
step   1340 | loss 1.6180 | lr 2.38e-04 | grad 1.34 | tok/s 6473
step   1350 | loss 1.7098 | lr 2.38e-04 | grad 2.03 | tok/s 6600
step   1360 | loss 1.4600 | lr 2.38e-04 | grad 1.13 | tok/s 6903
step   1370 | loss 1.5842 | lr 2.38e-04 | grad 2.98 | tok/s 6766
step   1380 | loss 1.5622 | lr 2.38e-04 | grad 1.95 | tok/s 6509
step   1390 | loss 1.4238 | lr 2.38e-04 | grad 1.38 | tok/s 6753
step   1400 | loss 1.3803 | lr 2.38e-04 | grad 1.27 | tok/s 6813
step   1410 | loss 1.5547 | lr 2.38e-04 | grad 2.17 | tok/s 6675
step   1420 | loss 1.6159 | lr 2.38e-04 | grad 1.59 | tok/s 6673
step   1430 | loss 1.3424 | lr 2.38e-04 | grad 1.43 | tok/s 6767
step   1440 | loss 1.1373 | lr 2.38e-04 | grad 1.28 | tok/s 7114
step   1450 | loss 1.2865 | lr 2.38e-04 | grad 5.34 | tok/s 7022
step   1460 | loss 1.6545 | lr 2.38e-04 | grad 3.77 | tok/s 6635
step   1470 | loss 1.4351 | lr 2.38e-04 | grad 1.23 | tok/s 7053
step   1480 | loss 1.8301 | lr 2.38e-04 | grad 2.56 | tok/s 6977
step   1490 | loss 1.5689 | lr 2.38e-04 | grad 3.30 | tok/s 7083
step   1500 | loss 1.2890 | lr 2.38e-04 | grad 1.28 | tok/s 7112
step   1510 | loss 1.5752 | lr 2.38e-04 | grad 1.49 | tok/s 7012
step   1520 | loss 1.4366 | lr 2.38e-04 | grad 1.64 | tok/s 6876
step   1530 | loss 1.3817 | lr 2.38e-04 | grad 1.33 | tok/s 6894
step   1540 | loss 1.6155 | lr 2.38e-04 | grad 1.53 | tok/s 6758
step   1550 | loss 1.2746 | lr 2.38e-04 | grad 1.36 | tok/s 7057
step   1560 | loss 1.5980 | lr 2.38e-04 | grad 1.49 | tok/s 6687
step   1570 | loss 1.3579 | lr 2.38e-04 | grad 2.17 | tok/s 7041
step   1580 | loss 1.6426 | lr 2.38e-04 | grad 2.31 | tok/s 7009
step   1590 | loss 1.5459 | lr 2.38e-04 | grad 1.26 | tok/s 6668
step   1600 | loss 0.8791 | lr 2.38e-04 | grad 0.55 | tok/s 7129
step   1610 | loss 1.1821 | lr 2.38e-04 | grad 1.22 | tok/s 6588
step   1620 | loss 1.4148 | lr 2.38e-04 | grad 1.95 | tok/s 6743
step   1630 | loss 1.4048 | lr 2.38e-04 | grad 1.51 | tok/s 6910
step   1640 | loss 1.4486 | lr 2.38e-04 | grad 3.67 | tok/s 6686
step   1650 | loss 1.5365 | lr 2.38e-04 | grad 2.58 | tok/s 6327
step   1660 | loss 1.2829 | lr 2.38e-04 | grad 1.09 | tok/s 7110
step   1670 | loss 1.6256 | lr 2.38e-04 | grad 5.50 | tok/s 6754
step   1680 | loss 1.5384 | lr 2.38e-04 | grad 1.65 | tok/s 6606
step   1690 | loss 1.4181 | lr 2.38e-04 | grad 1.98 | tok/s 6857
step   1700 | loss 1.4996 | lr 2.38e-04 | grad 1.38 | tok/s 6669
step   1710 | loss 1.4587 | lr 2.38e-04 | grad 1.55 | tok/s 6887
step   1720 | loss 1.4988 | lr 2.38e-04 | grad 1.98 | tok/s 7106
step   1730 | loss 1.1957 | lr 2.38e-04 | grad 2.81 | tok/s 7107
step   1740 | loss 1.4108 | lr 2.38e-04 | grad 1.64 | tok/s 6804
step   1750 | loss 1.5127 | lr 2.38e-04 | grad 1.53 | tok/s 6934
step   1760 | loss 1.5652 | lr 2.38e-04 | grad 1.56 | tok/s 6839
step   1770 | loss 1.4160 | lr 2.38e-04 | grad 1.16 | tok/s 6716
step   1780 | loss 1.4758 | lr 2.38e-04 | grad 1.79 | tok/s 6878
step   1790 | loss 1.4091 | lr 2.38e-04 | grad 1.21 | tok/s 6826
step   1800 | loss 1.5492 | lr 2.38e-04 | grad 1.62 | tok/s 6676
step   1810 | loss 1.4726 | lr 2.38e-04 | grad 2.73 | tok/s 6762
step   1820 | loss 1.4518 | lr 2.38e-04 | grad 2.61 | tok/s 6823
step   1830 | loss 1.4417 | lr 2.38e-04 | grad 1.54 | tok/s 6940
step   1840 | loss 1.4429 | lr 2.38e-04 | grad 1.08 | tok/s 6616
step   1850 | loss 1.3016 | lr 2.38e-04 | grad 1.29 | tok/s 7077
step   1860 | loss 1.3453 | lr 2.38e-04 | grad 1.56 | tok/s 6620
step   1870 | loss 1.3553 | lr 2.38e-04 | grad 0.94 | tok/s 6979
step   1880 | loss 1.2892 | lr 2.38e-04 | grad 1.64 | tok/s 6313
step   1890 | loss 1.5045 | lr 2.38e-04 | grad 1.10 | tok/s 6664
step   1900 | loss 1.3534 | lr 2.38e-04 | grad 1.16 | tok/s 6752
step   1910 | loss 1.4614 | lr 2.38e-04 | grad 1.34 | tok/s 6574
step   1920 | loss 1.3506 | lr 2.38e-04 | grad 1.24 | tok/s 7043
step   1930 | loss 1.4586 | lr 2.38e-04 | grad 1.36 | tok/s 6614
step   1940 | loss 1.4560 | lr 2.38e-04 | grad 2.12 | tok/s 6986
step   1950 | loss 1.7937 | lr 2.38e-04 | grad 2.52 | tok/s 7112
step   1960 | loss 1.4789 | lr 2.38e-04 | grad 2.50 | tok/s 7113
step   1970 | loss 1.5314 | lr 2.38e-04 | grad 2.98 | tok/s 6875
step   1980 | loss 1.4733 | lr 2.38e-04 | grad 1.13 | tok/s 6690
step   1990 | loss 1.5725 | lr 2.38e-04 | grad 1.30 | tok/s 6752
step   2000 | loss 1.4790 | lr 2.38e-04 | grad 1.34 | tok/s 6857
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4790.pt
step   2010 | loss 1.2073 | lr 2.38e-04 | grad 1.57 | tok/s 3910
step   2020 | loss 1.3017 | lr 2.38e-04 | grad 1.65 | tok/s 6937
step   2030 | loss 1.0117 | lr 2.38e-04 | grad 1.88 | tok/s 7127
step   2040 | loss 1.2357 | lr 2.38e-04 | grad 1.22 | tok/s 7118
step   2050 | loss 1.3477 | lr 2.38e-04 | grad 1.99 | tok/s 6821
step   2060 | loss 1.6380 | lr 2.38e-04 | grad 1.75 | tok/s 6718
step   2070 | loss 1.8829 | lr 2.38e-04 | grad 4.53 | tok/s 6746
step   2080 | loss 1.9960 | lr 2.38e-04 | grad 2.86 | tok/s 7102
step   2090 | loss 1.5721 | lr 2.38e-04 | grad 3.08 | tok/s 6927
step   2100 | loss 1.3174 | lr 2.38e-04 | grad 1.53 | tok/s 7035
step   2110 | loss 1.4583 | lr 2.38e-04 | grad 1.30 | tok/s 6634
step   2120 | loss 0.7144 | lr 2.38e-04 | grad 0.86 | tok/s 7138
step   2130 | loss 1.4284 | lr 2.38e-04 | grad 1.44 | tok/s 6717
step   2140 | loss 1.4030 | lr 2.38e-04 | grad 1.48 | tok/s 7023
step   2150 | loss 1.2545 | lr 2.38e-04 | grad 1.11 | tok/s 7111
step   2160 | loss 1.1986 | lr 2.38e-04 | grad 1.34 | tok/s 7112
step   2170 | loss 1.1879 | lr 2.38e-04 | grad 1.21 | tok/s 7112
step   2180 | loss 1.1942 | lr 2.38e-04 | grad 1.21 | tok/s 7112
step   2190 | loss 1.2161 | lr 2.38e-04 | grad 1.25 | tok/s 7113
step   2200 | loss 1.1523 | lr 2.38e-04 | grad 1.17 | tok/s 7113
step   2210 | loss 1.1397 | lr 2.38e-04 | grad 1.02 | tok/s 7111
step   2220 | loss 1.1177 | lr 2.38e-04 | grad 1.04 | tok/s 7111
step   2230 | loss 1.4354 | lr 2.38e-04 | grad 1.33 | tok/s 6977
step   2240 | loss 1.3541 | lr 2.38e-04 | grad 2.00 | tok/s 6855
step   2250 | loss 1.4481 | lr 2.38e-04 | grad 1.87 | tok/s 7109
step   2260 | loss 1.5736 | lr 2.38e-04 | grad 1.15 | tok/s 6852
step   2270 | loss 1.9679 | lr 2.38e-04 | grad 1.56 | tok/s 7019
step   2280 | loss 1.3836 | lr 2.38e-04 | grad 1.67 | tok/s 7107
step   2290 | loss 1.5617 | lr 2.38e-04 | grad 1.42 | tok/s 6779
step   2300 | loss 1.4755 | lr 2.38e-04 | grad 2.78 | tok/s 6914
step   2310 | loss 1.4536 | lr 2.38e-04 | grad 3.38 | tok/s 6756
step   2320 | loss 1.9245 | lr 2.38e-04 | grad 3.00 | tok/s 6726
step   2330 | loss 1.4366 | lr 2.38e-04 | grad 1.62 | tok/s 6567
step   2340 | loss 1.4490 | lr 2.38e-04 | grad 1.72 | tok/s 6764
step   2350 | loss 1.3492 | lr 2.38e-04 | grad 1.71 | tok/s 6963
step   2360 | loss 1.2303 | lr 2.38e-04 | grad 1.48 | tok/s 7051
step   2370 | loss 1.6001 | lr 2.38e-04 | grad 2.17 | tok/s 7022
step   2380 | loss 1.3662 | lr 2.38e-04 | grad 2.69 | tok/s 7112
step   2390 | loss 1.0957 | lr 2.38e-04 | grad 1.13 | tok/s 7098
step   2400 | loss 1.0246 | lr 2.38e-04 | grad 2.45 | tok/s 7100
step   2410 | loss 1.2535 | lr 2.38e-04 | grad 2.47 | tok/s 6781
step   2420 | loss 1.4974 | lr 2.38e-04 | grad 1.73 | tok/s 6458
step   2430 | loss 1.2258 | lr 2.38e-04 | grad 1.45 | tok/s 7076
step   2440 | loss 1.4291 | lr 2.38e-04 | grad 1.42 | tok/s 6842
step   2450 | loss 1.4534 | lr 2.38e-04 | grad 1.62 | tok/s 6828
step   2460 | loss 1.0675 | lr 2.38e-04 | grad 1.42 | tok/s 7116
step   2470 | loss 1.2107 | lr 2.38e-04 | grad 1.62 | tok/s 7014
step   2480 | loss 1.2742 | lr 2.38e-04 | grad 1.75 | tok/s 6996
step   2490 | loss 1.4318 | lr 2.38e-04 | grad 1.50 | tok/s 6781
step   2500 | loss 1.3916 | lr 2.38e-04 | grad 2.03 | tok/s 7020
step   2510 | loss 1.0467 | lr 2.38e-04 | grad 2.12 | tok/s 7111
step   2520 | loss 1.5554 | lr 2.38e-04 | grad 1.56 | tok/s 6991
step   2530 | loss 1.3046 | lr 2.38e-04 | grad 1.61 | tok/s 6803
step   2540 | loss 1.3551 | lr 2.38e-04 | grad 1.29 | tok/s 6901
step   2550 | loss 1.1374 | lr 2.38e-04 | grad 1.48 | tok/s 7039
step   2560 | loss 1.5299 | lr 2.38e-04 | grad 1.45 | tok/s 6504
step   2570 | loss 1.2841 | lr 2.38e-04 | grad 1.27 | tok/s 6678
step   2580 | loss 1.3067 | lr 2.38e-04 | grad 2.00 | tok/s 6900
step   2590 | loss 1.4355 | lr 2.38e-04 | grad 1.29 | tok/s 6489
step   2600 | loss 1.6303 | lr 2.38e-04 | grad 3.20 | tok/s 6870
step   2610 | loss 1.2741 | lr 2.38e-04 | grad 2.88 | tok/s 6930
step   2620 | loss 1.5173 | lr 2.38e-04 | grad 2.38 | tok/s 6970
step   2630 | loss 1.3718 | lr 2.38e-04 | grad 3.88 | tok/s 7058
step   2640 | loss 1.5332 | lr 2.38e-04 | grad 1.37 | tok/s 6857
step   2650 | loss 1.4009 | lr 2.38e-04 | grad 1.75 | tok/s 6952
step   2660 | loss 1.2888 | lr 2.38e-04 | grad 1.31 | tok/s 6823
step   2670 | loss 1.4384 | lr 2.38e-04 | grad 1.47 | tok/s 6657
step   2680 | loss 1.6029 | lr 2.38e-04 | grad 1.13 | tok/s 6832
step   2690 | loss 1.2828 | lr 2.38e-04 | grad 2.19 | tok/s 7062
step   2700 | loss 1.4278 | lr 2.38e-04 | grad 1.65 | tok/s 6707
step   2710 | loss 1.4309 | lr 2.38e-04 | grad 1.82 | tok/s 6684
step   2720 | loss 1.3452 | lr 2.38e-04 | grad 1.80 | tok/s 6590
step   2730 | loss 1.1476 | lr 2.38e-04 | grad 2.17 | tok/s 7043
step   2740 | loss 1.7532 | lr 2.38e-04 | grad 1.94 | tok/s 6945
step   2750 | loss 1.4954 | lr 2.38e-04 | grad 3.61 | tok/s 6942
step   2760 | loss 1.3377 | lr 2.38e-04 | grad 2.14 | tok/s 6429
step   2770 | loss 1.3623 | lr 2.38e-04 | grad 1.31 | tok/s 6986
step   2780 | loss 1.1875 | lr 2.38e-04 | grad 1.70 | tok/s 7020
step   2790 | loss 1.7928 | lr 2.38e-04 | grad 2.25 | tok/s 6408
step   2800 | loss 1.1416 | lr 2.38e-04 | grad 1.45 | tok/s 7112
step   2810 | loss 1.3400 | lr 2.38e-04 | grad 1.29 | tok/s 6554
step   2820 | loss 1.4209 | lr 2.38e-04 | grad 2.78 | tok/s 6662
step   2830 | loss 0.9095 | lr 2.38e-04 | grad 4.31 | tok/s 7112
step   2840 | loss 1.0802 | lr 2.38e-04 | grad 4.50 | tok/s 6964
step   2850 | loss 1.6940 | lr 2.38e-04 | grad 4.41 | tok/s 6815
step   2860 | loss 1.5300 | lr 2.38e-04 | grad 1.88 | tok/s 6748
step   2870 | loss 1.4296 | lr 2.38e-04 | grad 2.50 | tok/s 6886
step   2880 | loss 1.3062 | lr 2.38e-04 | grad 1.59 | tok/s 7038
step   2890 | loss 1.3899 | lr 2.38e-04 | grad 1.82 | tok/s 6948
step   2900 | loss 1.3986 | lr 2.38e-04 | grad 3.19 | tok/s 6959
step   2910 | loss 1.3814 | lr 2.38e-04 | grad 1.51 | tok/s 6660
step   2920 | loss 1.6719 | lr 2.38e-04 | grad 4.47 | tok/s 6844
step   2930 | loss 1.3561 | lr 2.38e-04 | grad 1.27 | tok/s 6718
step   2940 | loss 1.2779 | lr 2.38e-04 | grad 1.33 | tok/s 6473
step   2950 | loss 1.2816 | lr 2.38e-04 | grad 1.76 | tok/s 6975
step   2960 | loss 1.3077 | lr 2.38e-04 | grad 1.53 | tok/s 6950
step   2970 | loss 1.6504 | lr 2.38e-04 | grad 6.12 | tok/s 6733
step   2980 | loss 1.8790 | lr 2.38e-04 | grad 6.44 | tok/s 6962
step   2990 | loss 1.4563 | lr 2.38e-04 | grad 1.46 | tok/s 6959
step   3000 | loss 1.3337 | lr 2.38e-04 | grad 1.73 | tok/s 6779
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3337.pt
step   3010 | loss 1.3133 | lr 2.38e-04 | grad 1.60 | tok/s 3835
step   3020 | loss 1.4029 | lr 2.38e-04 | grad 1.48 | tok/s 6914
step   3030 | loss 1.4343 | lr 2.38e-04 | grad 1.54 | tok/s 6634
step   3040 | loss 1.4154 | lr 2.38e-04 | grad 1.27 | tok/s 7059
step   3050 | loss 1.3281 | lr 2.38e-04 | grad 2.08 | tok/s 6880
step   3060 | loss 1.4994 | lr 2.38e-04 | grad 1.85 | tok/s 7000

Training complete! Final step: 3064
