Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_27/level1_100m_20260202_135257
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 561,985,536 parameters
Using schedule-free AdamW (lr=0.0003729398516545373)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 4.6718 | lr 3.73e-04 | grad 5.56 | tok/s 4487
step     20 | loss 2.6921 | lr 3.73e-04 | grad 3.34 | tok/s 7797
step     30 | loss 2.6119 | lr 3.73e-04 | grad 3.94 | tok/s 7898
step     40 | loss 2.4606 | lr 3.73e-04 | grad 2.11 | tok/s 7562
step     50 | loss 3.1327 | lr 3.73e-04 | grad 5.84 | tok/s 7685
step     60 | loss 2.1581 | lr 3.73e-04 | grad 1.67 | tok/s 7946
step     70 | loss 2.1747 | lr 3.73e-04 | grad 2.64 | tok/s 8056
step     80 | loss 5.0877 | lr 3.73e-04 | grad 15.50 | tok/s 8106
step     90 | loss 4.2380 | lr 3.73e-04 | grad 2.56 | tok/s 8249
step    100 | loss 3.8202 | lr 3.73e-04 | grad 3.05 | tok/s 8246
step    110 | loss 3.3304 | lr 3.73e-04 | grad 9.50 | tok/s 8243
step    120 | loss 3.0776 | lr 3.73e-04 | grad 12.25 | tok/s 8244
step    130 | loss 2.9602 | lr 3.73e-04 | grad 6.22 | tok/s 8243
step    140 | loss 2.5608 | lr 3.73e-04 | grad 3.09 | tok/s 8241
step    150 | loss 2.7055 | lr 3.73e-04 | grad 7.44 | tok/s 8238
step    160 | loss 2.3712 | lr 3.73e-04 | grad 6.19 | tok/s 8243
step    170 | loss 2.5054 | lr 3.73e-04 | grad 5.62 | tok/s 8244
step    180 | loss 2.2742 | lr 3.73e-04 | grad 5.62 | tok/s 8241
step    190 | loss 2.4206 | lr 3.73e-04 | grad 6.38 | tok/s 8244
step    200 | loss 2.2257 | lr 3.73e-04 | grad 2.94 | tok/s 8245
step    210 | loss 2.2809 | lr 3.73e-04 | grad 4.44 | tok/s 8244
step    220 | loss 2.4132 | lr 3.73e-04 | grad 2.50 | tok/s 8143
step    230 | loss 2.5935 | lr 3.73e-04 | grad 2.80 | tok/s 8045
step    240 | loss 2.4159 | lr 3.73e-04 | grad 1.91 | tok/s 7642
step    250 | loss 2.2285 | lr 3.73e-04 | grad 1.50 | tok/s 7857
step    260 | loss 1.8756 | lr 3.73e-04 | grad 2.14 | tok/s 8108
step    270 | loss 2.2432 | lr 3.73e-04 | grad 1.50 | tok/s 7999
step    280 | loss 2.4008 | lr 3.73e-04 | grad 2.00 | tok/s 7845
step    290 | loss 1.9026 | lr 3.73e-04 | grad 3.17 | tok/s 8252
step    300 | loss 0.8416 | lr 3.73e-04 | grad 1.67 | tok/s 8249
step    310 | loss 2.6049 | lr 3.73e-04 | grad 2.75 | tok/s 8111
step    320 | loss 2.2299 | lr 3.73e-04 | grad 2.86 | tok/s 7942
step    330 | loss 2.0740 | lr 3.73e-04 | grad 1.35 | tok/s 7673
step    340 | loss 2.3219 | lr 3.73e-04 | grad 1.56 | tok/s 7796
step    350 | loss 2.0478 | lr 3.73e-04 | grad 1.80 | tok/s 7993
step    360 | loss 1.6843 | lr 3.73e-04 | grad 3.09 | tok/s 8167
step    370 | loss 1.9892 | lr 3.73e-04 | grad 1.49 | tok/s 7404
step    380 | loss 1.8790 | lr 3.73e-04 | grad 1.26 | tok/s 7892
step    390 | loss 1.6401 | lr 3.73e-04 | grad 1.29 | tok/s 8243
step    400 | loss 1.6443 | lr 3.73e-04 | grad 1.53 | tok/s 8170
step    410 | loss 1.4752 | lr 3.73e-04 | grad 1.14 | tok/s 7994
step    420 | loss 1.9205 | lr 3.73e-04 | grad 2.25 | tok/s 7628
step    430 | loss 2.1873 | lr 3.73e-04 | grad 1.55 | tok/s 8115
step    440 | loss 2.2302 | lr 3.73e-04 | grad 1.92 | tok/s 7672
step    450 | loss 2.2488 | lr 3.73e-04 | grad 1.34 | tok/s 7941
step    460 | loss 1.8460 | lr 3.73e-04 | grad 1.79 | tok/s 7770
step    470 | loss 1.8981 | lr 3.73e-04 | grad 1.51 | tok/s 8011
step    480 | loss 2.2588 | lr 3.73e-04 | grad 2.81 | tok/s 8014
step    490 | loss 1.8800 | lr 3.73e-04 | grad 1.29 | tok/s 7573
step    500 | loss 1.7850 | lr 3.73e-04 | grad 1.98 | tok/s 8090
step    510 | loss 1.7827 | lr 3.73e-04 | grad 1.12 | tok/s 8197
step    520 | loss 1.7883 | lr 3.73e-04 | grad 1.34 | tok/s 8181
step    530 | loss 1.9577 | lr 3.73e-04 | grad 1.13 | tok/s 7864
step    540 | loss 1.8084 | lr 3.73e-04 | grad 1.43 | tok/s 7872
step    550 | loss 1.6409 | lr 3.73e-04 | grad 1.39 | tok/s 7700
step    560 | loss 1.7917 | lr 3.73e-04 | grad 1.41 | tok/s 7502
step    570 | loss 1.7335 | lr 3.73e-04 | grad 1.71 | tok/s 7703
step    580 | loss 1.6318 | lr 3.73e-04 | grad 1.27 | tok/s 7680
step    590 | loss 1.9308 | lr 3.73e-04 | grad 1.56 | tok/s 7875
step    600 | loss 1.8945 | lr 3.73e-04 | grad 1.12 | tok/s 7604
step    610 | loss 1.7031 | lr 3.73e-04 | grad 1.17 | tok/s 7999
step    620 | loss 1.6081 | lr 3.73e-04 | grad 1.16 | tok/s 7579
step    630 | loss 1.7214 | lr 3.73e-04 | grad 1.88 | tok/s 7643
step    640 | loss 1.8809 | lr 3.73e-04 | grad 1.15 | tok/s 7849
step    650 | loss 1.7324 | lr 3.73e-04 | grad 1.41 | tok/s 7886
step    660 | loss 1.7544 | lr 3.73e-04 | grad 0.95 | tok/s 7919
step    670 | loss 1.9386 | lr 3.73e-04 | grad 1.61 | tok/s 7975
step    680 | loss 1.7696 | lr 3.73e-04 | grad 1.28 | tok/s 7815
step    690 | loss 1.9070 | lr 3.73e-04 | grad 1.61 | tok/s 8084
step    700 | loss 1.4896 | lr 3.73e-04 | grad 1.31 | tok/s 8243
step    710 | loss 1.6361 | lr 3.73e-04 | grad 1.24 | tok/s 7687
step    720 | loss 1.5098 | lr 3.73e-04 | grad 1.63 | tok/s 7578
step    730 | loss 1.4180 | lr 3.73e-04 | grad 1.62 | tok/s 8222
step    740 | loss 1.5806 | lr 3.73e-04 | grad 1.20 | tok/s 8120
step    750 | loss 1.3204 | lr 3.73e-04 | grad 1.13 | tok/s 8247
step    760 | loss 1.2194 | lr 3.73e-04 | grad 1.03 | tok/s 8245
step    770 | loss 1.1788 | lr 3.73e-04 | grad 1.09 | tok/s 8246
step    780 | loss 1.1280 | lr 3.73e-04 | grad 0.88 | tok/s 8243
step    790 | loss 1.2187 | lr 3.73e-04 | grad 1.70 | tok/s 7982
step    800 | loss 1.8984 | lr 3.73e-04 | grad 2.42 | tok/s 7956
step    810 | loss 1.7238 | lr 3.73e-04 | grad 1.08 | tok/s 7916
step    820 | loss 1.7397 | lr 3.73e-04 | grad 1.84 | tok/s 7599
step    830 | loss 1.6319 | lr 3.73e-04 | grad 1.19 | tok/s 8156
step    840 | loss 1.4661 | lr 3.73e-04 | grad 1.05 | tok/s 8245
step    850 | loss 1.6060 | lr 3.73e-04 | grad 1.08 | tok/s 8192
step    860 | loss 1.5374 | lr 3.73e-04 | grad 2.17 | tok/s 8115
step    870 | loss 1.5480 | lr 3.73e-04 | grad 1.30 | tok/s 7814
step    880 | loss 1.7241 | lr 3.73e-04 | grad 1.18 | tok/s 7845
step    890 | loss 1.7145 | lr 3.73e-04 | grad 1.56 | tok/s 7958
step    900 | loss 1.5925 | lr 3.73e-04 | grad 1.20 | tok/s 7965
step    910 | loss 1.4700 | lr 3.73e-04 | grad 1.52 | tok/s 7797
step    920 | loss 1.5961 | lr 3.73e-04 | grad 2.03 | tok/s 8109
step    930 | loss 1.6367 | lr 3.73e-04 | grad 1.75 | tok/s 7741
step    940 | loss 1.4636 | lr 3.73e-04 | grad 0.98 | tok/s 8167
step    950 | loss 1.5065 | lr 3.73e-04 | grad 1.11 | tok/s 8200
step    960 | loss 1.3710 | lr 3.73e-04 | grad 1.26 | tok/s 8213
step    970 | loss 1.7470 | lr 3.73e-04 | grad 1.74 | tok/s 7724
step    980 | loss 1.6504 | lr 3.73e-04 | grad 1.17 | tok/s 7933
step    990 | loss 1.5134 | lr 3.73e-04 | grad 1.01 | tok/s 8074
step   1000 | loss 1.8992 | lr 3.73e-04 | grad 5.09 | tok/s 7744
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8992.pt
step   1010 | loss 1.8186 | lr 3.73e-04 | grad 1.27 | tok/s 4291
step   1020 | loss 1.7055 | lr 3.73e-04 | grad 1.34 | tok/s 7564
step   1030 | loss 1.4663 | lr 3.73e-04 | grad 0.92 | tok/s 7895
step   1040 | loss 1.5561 | lr 3.73e-04 | grad 1.92 | tok/s 8051
step   1050 | loss 1.6127 | lr 3.73e-04 | grad 1.08 | tok/s 7572
step   1060 | loss 1.7451 | lr 3.73e-04 | grad 1.30 | tok/s 8097
step   1070 | loss 1.7045 | lr 3.73e-04 | grad 1.85 | tok/s 7967
step   1080 | loss 1.4325 | lr 3.73e-04 | grad 1.10 | tok/s 7374
step   1090 | loss 1.0506 | lr 3.73e-04 | grad 0.54 | tok/s 8174
step   1100 | loss 1.5703 | lr 3.73e-04 | grad 1.46 | tok/s 7838
step   1110 | loss 1.4438 | lr 3.73e-04 | grad 1.16 | tok/s 8243
step   1120 | loss 1.3731 | lr 3.73e-04 | grad 1.17 | tok/s 8253
step   1130 | loss 1.3146 | lr 3.73e-04 | grad 1.06 | tok/s 8249
step   1140 | loss 1.3171 | lr 3.73e-04 | grad 1.05 | tok/s 8252
step   1150 | loss 1.3222 | lr 3.73e-04 | grad 1.19 | tok/s 8254
step   1160 | loss 1.2433 | lr 3.73e-04 | grad 1.29 | tok/s 8249
step   1170 | loss 1.2870 | lr 3.73e-04 | grad 1.19 | tok/s 8231
step   1180 | loss 1.3446 | lr 3.73e-04 | grad 0.92 | tok/s 8251
step   1190 | loss 1.2568 | lr 3.73e-04 | grad 1.21 | tok/s 8253
step   1200 | loss 1.2544 | lr 3.73e-04 | grad 0.93 | tok/s 8245
step   1210 | loss 1.2896 | lr 3.73e-04 | grad 0.86 | tok/s 8250
step   1220 | loss 1.3020 | lr 3.73e-04 | grad 0.96 | tok/s 8252
step   1230 | loss 1.2927 | lr 3.73e-04 | grad 0.96 | tok/s 8249
step   1240 | loss 1.2847 | lr 3.73e-04 | grad 1.30 | tok/s 8182
step   1250 | loss 1.9397 | lr 3.73e-04 | grad 1.20 | tok/s 7811
step   1260 | loss 1.3566 | lr 3.73e-04 | grad 1.59 | tok/s 7694
step   1270 | loss 1.7648 | lr 3.73e-04 | grad 1.36 | tok/s 7688
step   1280 | loss 1.5917 | lr 3.73e-04 | grad 1.48 | tok/s 8036
step   1290 | loss 1.4954 | lr 3.73e-04 | grad 1.28 | tok/s 7888
step   1300 | loss 1.5377 | lr 3.73e-04 | grad 1.01 | tok/s 7821
step   1310 | loss 1.4801 | lr 3.73e-04 | grad 0.97 | tok/s 8209
step   1320 | loss 1.5895 | lr 3.73e-04 | grad 1.16 | tok/s 8111
step   1330 | loss 1.5669 | lr 3.73e-04 | grad 1.15 | tok/s 8121
step   1340 | loss 1.5927 | lr 3.73e-04 | grad 1.60 | tok/s 7654
step   1350 | loss 1.6962 | lr 3.73e-04 | grad 1.16 | tok/s 7569
step   1360 | loss 1.5207 | lr 3.73e-04 | grad 0.82 | tok/s 7950
step   1370 | loss 1.5465 | lr 3.73e-04 | grad 4.25 | tok/s 7850
step   1380 | loss 1.6283 | lr 3.73e-04 | grad 1.72 | tok/s 7547
step   1390 | loss 1.4964 | lr 3.73e-04 | grad 1.82 | tok/s 7969
step   1400 | loss 1.3909 | lr 3.73e-04 | grad 0.82 | tok/s 7773
step   1410 | loss 1.5541 | lr 3.73e-04 | grad 5.94 | tok/s 7747
step   1420 | loss 1.6735 | lr 3.73e-04 | grad 1.15 | tok/s 7735
step   1430 | loss 1.4008 | lr 3.73e-04 | grad 1.22 | tok/s 7850
step   1440 | loss 1.1595 | lr 3.73e-04 | grad 1.20 | tok/s 8250
step   1450 | loss 1.1807 | lr 3.73e-04 | grad 1.18 | tok/s 8134
step   1460 | loss 1.7189 | lr 3.73e-04 | grad 1.12 | tok/s 7691
step   1470 | loss 1.5308 | lr 3.73e-04 | grad 1.07 | tok/s 8179
step   1480 | loss 1.8643 | lr 3.73e-04 | grad 2.03 | tok/s 8090
step   1490 | loss 1.6009 | lr 3.73e-04 | grad 1.30 | tok/s 8209
step   1500 | loss 1.3637 | lr 3.73e-04 | grad 0.96 | tok/s 8248
step   1510 | loss 1.5795 | lr 3.73e-04 | grad 1.48 | tok/s 8132
step   1520 | loss 1.4355 | lr 3.73e-04 | grad 1.73 | tok/s 7976
step   1530 | loss 1.4211 | lr 3.73e-04 | grad 1.55 | tok/s 8164
step   1540 | loss 1.6108 | lr 3.73e-04 | grad 1.39 | tok/s 7671
step   1550 | loss 1.3120 | lr 3.73e-04 | grad 1.38 | tok/s 8189
step   1560 | loss 1.6053 | lr 3.73e-04 | grad 0.94 | tok/s 7759
step   1570 | loss 1.3287 | lr 3.73e-04 | grad 1.27 | tok/s 8169
step   1580 | loss 1.7407 | lr 3.73e-04 | grad 2.67 | tok/s 8134
step   1590 | loss 1.6023 | lr 3.73e-04 | grad 1.09 | tok/s 7735
step   1600 | loss 0.9626 | lr 3.73e-04 | grad 0.56 | tok/s 8261
step   1610 | loss 1.1107 | lr 3.73e-04 | grad 0.96 | tok/s 7797
step   1620 | loss 1.4516 | lr 3.73e-04 | grad 1.85 | tok/s 7666
step   1630 | loss 1.4414 | lr 3.73e-04 | grad 1.00 | tok/s 8017
step   1640 | loss 1.3767 | lr 3.73e-04 | grad 1.13 | tok/s 7775
step   1650 | loss 1.5439 | lr 3.73e-04 | grad 1.21 | tok/s 7336
step   1660 | loss 1.3882 | lr 3.73e-04 | grad 0.90 | tok/s 8227
step   1670 | loss 1.4966 | lr 3.73e-04 | grad 1.92 | tok/s 7923
step   1680 | loss 1.6665 | lr 3.73e-04 | grad 0.84 | tok/s 7579
step   1690 | loss 1.4872 | lr 3.73e-04 | grad 1.77 | tok/s 7949
step   1700 | loss 1.4919 | lr 3.73e-04 | grad 1.00 | tok/s 7877
step   1710 | loss 1.4720 | lr 3.73e-04 | grad 1.03 | tok/s 7874
step   1720 | loss 1.5265 | lr 3.73e-04 | grad 1.37 | tok/s 8224
step   1730 | loss 1.2558 | lr 3.73e-04 | grad 1.23 | tok/s 8247
step   1740 | loss 1.4383 | lr 3.73e-04 | grad 1.18 | tok/s 7963
step   1750 | loss 1.5753 | lr 3.73e-04 | grad 1.38 | tok/s 7968
step   1760 | loss 1.5480 | lr 3.73e-04 | grad 1.01 | tok/s 7937
step   1770 | loss 1.4346 | lr 3.73e-04 | grad 1.12 | tok/s 7793
step   1780 | loss 1.4873 | lr 3.73e-04 | grad 0.97 | tok/s 8036
step   1790 | loss 1.4304 | lr 3.73e-04 | grad 1.62 | tok/s 7954
step   1800 | loss 1.5676 | lr 3.73e-04 | grad 1.03 | tok/s 7804
step   1810 | loss 1.4564 | lr 3.73e-04 | grad 2.12 | tok/s 7676
step   1820 | loss 1.4994 | lr 3.73e-04 | grad 3.34 | tok/s 7911
step   1830 | loss 1.4664 | lr 3.73e-04 | grad 2.09 | tok/s 8081
step   1840 | loss 1.4943 | lr 3.73e-04 | grad 0.84 | tok/s 7688
step   1850 | loss 1.3189 | lr 3.73e-04 | grad 1.09 | tok/s 8201
step   1860 | loss 1.3663 | lr 3.73e-04 | grad 1.29 | tok/s 7770
step   1870 | loss 1.4002 | lr 3.73e-04 | grad 0.84 | tok/s 7956
step   1880 | loss 1.2853 | lr 3.73e-04 | grad 1.32 | tok/s 7645
step   1890 | loss 1.5031 | lr 3.73e-04 | grad 1.12 | tok/s 7406
step   1900 | loss 1.3838 | lr 3.73e-04 | grad 1.12 | tok/s 7930
step   1910 | loss 1.4621 | lr 3.73e-04 | grad 1.23 | tok/s 7518
step   1920 | loss 1.3843 | lr 3.73e-04 | grad 0.98 | tok/s 8250
step   1930 | loss 1.4487 | lr 3.73e-04 | grad 1.53 | tok/s 7588
step   1940 | loss 1.4457 | lr 3.73e-04 | grad 1.39 | tok/s 8190
step   1950 | loss 1.8623 | lr 3.73e-04 | grad 1.90 | tok/s 8161
step   1960 | loss 1.5290 | lr 3.73e-04 | grad 2.03 | tok/s 8255
step   1970 | loss 1.5325 | lr 3.73e-04 | grad 1.12 | tok/s 8049
step   1980 | loss 1.5543 | lr 3.73e-04 | grad 1.14 | tok/s 7689
step   1990 | loss 1.6120 | lr 3.73e-04 | grad 1.27 | tok/s 7830
step   2000 | loss 1.4923 | lr 3.73e-04 | grad 1.27 | tok/s 7952
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4923.pt
step   2010 | loss 1.1754 | lr 3.73e-04 | grad 1.06 | tok/s 4460
step   2020 | loss 1.3175 | lr 3.73e-04 | grad 1.43 | tok/s 8043
step   2030 | loss 0.9797 | lr 3.73e-04 | grad 1.94 | tok/s 8260
step   2040 | loss 1.3590 | lr 3.73e-04 | grad 1.26 | tok/s 8254
step   2050 | loss 1.2668 | lr 3.73e-04 | grad 1.01 | tok/s 7956
step   2060 | loss 1.6265 | lr 3.73e-04 | grad 1.10 | tok/s 7741
step   2070 | loss 1.7833 | lr 3.73e-04 | grad 3.08 | tok/s 7825
step   2080 | loss 2.0820 | lr 3.73e-04 | grad 2.42 | tok/s 8248
step   2090 | loss 1.6329 | lr 3.73e-04 | grad 1.48 | tok/s 8084
step   2100 | loss 1.4069 | lr 3.73e-04 | grad 1.43 | tok/s 8100
step   2110 | loss 1.4967 | lr 3.73e-04 | grad 1.28 | tok/s 7677
step   2120 | loss 0.8686 | lr 3.73e-04 | grad 1.18 | tok/s 8263
step   2130 | loss 1.2900 | lr 3.73e-04 | grad 1.66 | tok/s 7927
step   2140 | loss 1.4359 | lr 3.73e-04 | grad 0.94 | tok/s 8010
step   2150 | loss 1.2848 | lr 3.73e-04 | grad 1.14 | tok/s 8251
step   2160 | loss 1.1799 | lr 3.73e-04 | grad 1.25 | tok/s 8248
step   2170 | loss 1.2404 | lr 3.73e-04 | grad 0.83 | tok/s 8247
step   2180 | loss 1.1914 | lr 3.73e-04 | grad 0.83 | tok/s 8237
step   2190 | loss 1.2166 | lr 3.73e-04 | grad 1.27 | tok/s 8248
step   2200 | loss 1.1832 | lr 3.73e-04 | grad 0.89 | tok/s 8246
step   2210 | loss 1.1569 | lr 3.73e-04 | grad 1.28 | tok/s 8248
step   2220 | loss 1.1425 | lr 3.73e-04 | grad 1.04 | tok/s 8243
step   2230 | loss 1.3972 | lr 3.73e-04 | grad 1.38 | tok/s 8091
step   2240 | loss 1.3119 | lr 3.73e-04 | grad 1.09 | tok/s 7951
step   2250 | loss 1.5549 | lr 3.73e-04 | grad 2.88 | tok/s 8247
step   2260 | loss 1.5696 | lr 3.73e-04 | grad 1.23 | tok/s 7960
step   2270 | loss 1.9618 | lr 3.73e-04 | grad 1.64 | tok/s 8148
step   2280 | loss 1.4317 | lr 3.73e-04 | grad 1.90 | tok/s 8239
step   2290 | loss 1.5399 | lr 3.73e-04 | grad 3.44 | tok/s 7949
step   2300 | loss 1.5493 | lr 3.73e-04 | grad 1.73 | tok/s 8071
step   2310 | loss 1.4281 | lr 3.73e-04 | grad 1.41 | tok/s 7769
step   2320 | loss 1.8389 | lr 3.73e-04 | grad 1.82 | tok/s 7741
step   2330 | loss 1.5916 | lr 3.73e-04 | grad 1.75 | tok/s 7779
step   2340 | loss 1.4673 | lr 3.73e-04 | grad 1.38 | tok/s 7673
step   2350 | loss 1.3820 | lr 3.73e-04 | grad 1.53 | tok/s 8046
step   2360 | loss 1.2699 | lr 3.73e-04 | grad 0.79 | tok/s 8247
step   2370 | loss 1.5668 | lr 3.73e-04 | grad 1.95 | tok/s 8069
step   2380 | loss 1.4485 | lr 3.73e-04 | grad 1.28 | tok/s 8249
step   2390 | loss 1.1357 | lr 3.73e-04 | grad 0.98 | tok/s 8229
step   2400 | loss 1.0729 | lr 3.73e-04 | grad 0.81 | tok/s 8241
step   2410 | loss 1.2062 | lr 3.73e-04 | grad 0.96 | tok/s 7894
step   2420 | loss 1.4874 | lr 3.73e-04 | grad 1.20 | tok/s 7612
step   2430 | loss 1.3444 | lr 3.73e-04 | grad 1.39 | tok/s 8049
step   2440 | loss 1.3292 | lr 3.73e-04 | grad 1.04 | tok/s 7971
step   2450 | loss 1.4737 | lr 3.73e-04 | grad 1.04 | tok/s 7947
step   2460 | loss 1.2333 | lr 3.73e-04 | grad 0.94 | tok/s 8185
step   2470 | loss 1.1947 | lr 3.73e-04 | grad 0.99 | tok/s 8126
step   2480 | loss 1.2252 | lr 3.73e-04 | grad 1.41 | tok/s 8184
step   2490 | loss 1.4028 | lr 3.73e-04 | grad 1.23 | tok/s 7794
step   2500 | loss 1.4974 | lr 3.73e-04 | grad 1.66 | tok/s 8137
step   2510 | loss 1.0983 | lr 3.73e-04 | grad 1.33 | tok/s 8244
step   2520 | loss 1.5111 | lr 3.73e-04 | grad 1.18 | tok/s 8122
step   2530 | loss 1.2973 | lr 3.73e-04 | grad 1.19 | tok/s 7952
step   2540 | loss 1.3996 | lr 3.73e-04 | grad 1.29 | tok/s 7912
step   2550 | loss 1.1568 | lr 3.73e-04 | grad 0.71 | tok/s 8248
step   2560 | loss 1.5169 | lr 3.73e-04 | grad 1.99 | tok/s 7662
step   2570 | loss 1.3157 | lr 3.73e-04 | grad 1.27 | tok/s 7906
step   2580 | loss 1.3635 | lr 3.73e-04 | grad 1.22 | tok/s 7644
step   2590 | loss 1.4176 | lr 3.73e-04 | grad 1.11 | tok/s 7831
step   2600 | loss 1.6415 | lr 3.73e-04 | grad 2.38 | tok/s 7666
step   2610 | loss 1.2650 | lr 3.73e-04 | grad 1.50 | tok/s 8183
step   2620 | loss 1.5342 | lr 3.73e-04 | grad 1.08 | tok/s 7963
step   2630 | loss 1.3728 | lr 3.73e-04 | grad 1.00 | tok/s 8163
step   2640 | loss 1.5968 | lr 3.73e-04 | grad 1.71 | tok/s 7935
step   2650 | loss 1.4155 | lr 3.73e-04 | grad 1.37 | tok/s 8103
step   2660 | loss 1.3232 | lr 3.73e-04 | grad 1.46 | tok/s 7957
step   2670 | loss 1.4003 | lr 3.73e-04 | grad 1.07 | tok/s 7624
step   2680 | loss 1.6836 | lr 3.73e-04 | grad 1.30 | tok/s 7918
step   2690 | loss 1.2995 | lr 3.73e-04 | grad 1.55 | tok/s 8182
step   2700 | loss 1.4549 | lr 3.73e-04 | grad 1.41 | tok/s 7976
step   2710 | loss 1.4903 | lr 3.73e-04 | grad 0.99 | tok/s 7655
step   2720 | loss 1.3460 | lr 3.73e-04 | grad 1.45 | tok/s 7533
step   2730 | loss 1.1691 | lr 3.73e-04 | grad 1.23 | tok/s 8167
step   2740 | loss 1.7243 | lr 3.73e-04 | grad 2.47 | tok/s 8055
step   2750 | loss 1.5499 | lr 3.73e-04 | grad 1.05 | tok/s 8079
step   2760 | loss 1.3648 | lr 3.73e-04 | grad 1.41 | tok/s 7640
step   2770 | loss 1.4588 | lr 3.73e-04 | grad 1.61 | tok/s 7884
step   2780 | loss 1.1253 | lr 3.73e-04 | grad 0.95 | tok/s 8142
step   2790 | loss 1.8326 | lr 3.73e-04 | grad 1.45 | tok/s 7677
step   2800 | loss 1.2407 | lr 3.73e-04 | grad 0.90 | tok/s 7996
step   2810 | loss 1.2991 | lr 3.73e-04 | grad 1.45 | tok/s 7699
step   2820 | loss 1.3806 | lr 3.73e-04 | grad 1.12 | tok/s 7697
step   2830 | loss 1.0970 | lr 3.73e-04 | grad 1.65 | tok/s 8164
step   2840 | loss 0.9575 | lr 3.73e-04 | grad 1.11 | tok/s 8184
step   2850 | loss 1.6893 | lr 3.73e-04 | grad 1.41 | tok/s 7878
step   2860 | loss 1.6649 | lr 3.73e-04 | grad 1.15 | tok/s 7931
step   2870 | loss 1.4288 | lr 3.73e-04 | grad 1.21 | tok/s 7862
step   2880 | loss 1.4146 | lr 3.73e-04 | grad 1.21 | tok/s 8085
step   2890 | loss 1.3184 | lr 3.73e-04 | grad 2.22 | tok/s 8222
step   2900 | loss 1.3936 | lr 3.73e-04 | grad 1.18 | tok/s 7905
step   2910 | loss 1.4761 | lr 3.73e-04 | grad 1.55 | tok/s 7902
step   2920 | loss 1.5936 | lr 3.73e-04 | grad 3.16 | tok/s 7745
step   2930 | loss 1.4613 | lr 3.73e-04 | grad 1.73 | tok/s 7950
step   2940 | loss 1.2790 | lr 3.73e-04 | grad 1.09 | tok/s 7485
step   2950 | loss 1.2954 | lr 3.73e-04 | grad 1.09 | tok/s 7972
step   2960 | loss 1.3628 | lr 3.73e-04 | grad 1.20 | tok/s 8035
step   2970 | loss 1.3764 | lr 3.73e-04 | grad 1.85 | tok/s 7808
step   2980 | loss 1.8423 | lr 3.73e-04 | grad 3.91 | tok/s 8075
step   2990 | loss 1.8367 | lr 3.73e-04 | grad 1.20 | tok/s 8065
step   3000 | loss 1.3279 | lr 3.73e-04 | grad 0.99 | tok/s 8052
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3279.pt
step   3010 | loss 1.1248 | lr 3.73e-04 | grad 1.07 | tok/s 4280
step   3020 | loss 1.3947 | lr 3.73e-04 | grad 0.99 | tok/s 7894
step   3030 | loss 1.4440 | lr 3.73e-04 | grad 2.33 | tok/s 7912
step   3040 | loss 1.4524 | lr 3.73e-04 | grad 1.03 | tok/s 8017
step   3050 | loss 1.4163 | lr 3.73e-04 | grad 1.12 | tok/s 7929
step   3060 | loss 1.3291 | lr 3.73e-04 | grad 1.16 | tok/s 8134
step   3070 | loss 1.6010 | lr 3.73e-04 | grad 1.09 | tok/s 8217
step   3080 | loss 1.3620 | lr 3.73e-04 | grad 1.02 | tok/s 8000
step   3090 | loss 1.3756 | lr 3.73e-04 | grad 1.55 | tok/s 7984
step   3100 | loss 1.4785 | lr 3.73e-04 | grad 1.19 | tok/s 7899
step   3110 | loss 1.1100 | lr 3.73e-04 | grad 0.98 | tok/s 8250
step   3120 | loss 1.0324 | lr 3.73e-04 | grad 0.88 | tok/s 8252
step   3130 | loss 1.4688 | lr 3.73e-04 | grad 1.33 | tok/s 7786
step   3140 | loss 1.0294 | lr 3.73e-04 | grad 0.94 | tok/s 8250
step   3150 | loss 1.1965 | lr 3.73e-04 | grad 2.02 | tok/s 8171
step   3160 | loss 1.5407 | lr 3.73e-04 | grad 3.92 | tok/s 7780
step   3170 | loss 1.3038 | lr 3.73e-04 | grad 0.87 | tok/s 7826
step   3180 | loss 1.3960 | lr 3.73e-04 | grad 0.97 | tok/s 7861
step   3190 | loss 0.9770 | lr 3.73e-04 | grad 1.23 | tok/s 8103
step   3200 | loss 1.4549 | lr 3.73e-04 | grad 2.02 | tok/s 7944
step   3210 | loss 1.6977 | lr 3.73e-04 | grad 2.03 | tok/s 8116
step   3220 | loss 2.0426 | lr 3.73e-04 | grad 1.44 | tok/s 8150
step   3230 | loss 1.6868 | lr 3.73e-04 | grad 1.65 | tok/s 8247
step   3240 | loss 1.4896 | lr 3.73e-04 | grad 1.20 | tok/s 8248
step   3250 | loss 1.4673 | lr 3.73e-04 | grad 1.35 | tok/s 8249
step   3260 | loss 1.3664 | lr 3.73e-04 | grad 1.60 | tok/s 8246
step   3270 | loss 1.3584 | lr 3.73e-04 | grad 1.40 | tok/s 8248
step   3280 | loss 1.2777 | lr 3.73e-04 | grad 1.55 | tok/s 8248
step   3290 | loss 1.2821 | lr 3.73e-04 | grad 1.42 | tok/s 8243
step   3300 | loss 1.2674 | lr 3.73e-04 | grad 1.17 | tok/s 8243
step   3310 | loss 1.2243 | lr 3.73e-04 | grad 1.22 | tok/s 8245
step   3320 | loss 1.2486 | lr 3.73e-04 | grad 1.30 | tok/s 8247
step   3330 | loss 1.4986 | lr 3.73e-04 | grad 2.56 | tok/s 8112
step   3340 | loss 1.4085 | lr 3.73e-04 | grad 2.12 | tok/s 7841
step   3350 | loss 1.3846 | lr 3.73e-04 | grad 1.15 | tok/s 7960
step   3360 | loss 1.3450 | lr 3.73e-04 | grad 1.01 | tok/s 7562
step   3370 | loss 1.4659 | lr 3.73e-04 | grad 1.18 | tok/s 7693
step   3380 | loss 1.4555 | lr 3.73e-04 | grad 1.15 | tok/s 7936
step   3390 | loss 1.3648 | lr 3.73e-04 | grad 1.12 | tok/s 7977
step   3400 | loss 1.1343 | lr 3.73e-04 | grad 0.89 | tok/s 8247
step   3410 | loss 1.0454 | lr 3.73e-04 | grad 1.15 | tok/s 8246
step   3420 | loss 1.5180 | lr 3.73e-04 | grad 1.09 | tok/s 7681
step   3430 | loss 1.3927 | lr 3.73e-04 | grad 1.29 | tok/s 8026
step   3440 | loss 1.4418 | lr 3.73e-04 | grad 1.19 | tok/s 7802
step   3450 | loss 1.3827 | lr 3.73e-04 | grad 1.83 | tok/s 7975
step   3460 | loss 1.4391 | lr 3.73e-04 | grad 1.19 | tok/s 8025
step   3470 | loss 1.3452 | lr 3.73e-04 | grad 1.24 | tok/s 8086
step   3480 | loss 1.3423 | lr 3.73e-04 | grad 1.11 | tok/s 8251
step   3490 | loss 1.2260 | lr 3.73e-04 | grad 1.40 | tok/s 7618
step   3500 | loss 1.1489 | lr 3.73e-04 | grad 1.36 | tok/s 8027
step   3510 | loss 1.4191 | lr 3.73e-04 | grad 1.00 | tok/s 7916
step   3520 | loss 1.4874 | lr 3.73e-04 | grad 1.55 | tok/s 7817
step   3530 | loss 1.6179 | lr 3.73e-04 | grad 3.44 | tok/s 8217
step   3540 | loss 1.4266 | lr 3.73e-04 | grad 2.53 | tok/s 7870
step   3550 | loss 1.9884 | lr 3.73e-04 | grad 1.96 | tok/s 7543
step   3560 | loss 1.4397 | lr 3.73e-04 | grad 1.81 | tok/s 7581

Training complete! Final step: 3560
