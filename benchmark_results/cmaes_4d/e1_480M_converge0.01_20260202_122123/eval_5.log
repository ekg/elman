Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_5/level1_100m_20260202_122130
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 1,574,053,632 parameters
Using schedule-free AdamW (lr=0.0002674699353697692)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 7.2172 | lr 2.67e-04 | grad 8.69 | tok/s 3265
step     20 | loss 3.0624 | lr 2.67e-04 | grad 5.88 | tok/s 4672
step     30 | loss 2.7705 | lr 2.67e-04 | grad 3.58 | tok/s 4716
step     40 | loss 2.6101 | lr 2.67e-04 | grad 2.75 | tok/s 4477
step     50 | loss 3.7003 | lr 2.67e-04 | grad 7.25 | tok/s 4527
step     60 | loss 2.4103 | lr 2.67e-04 | grad 2.58 | tok/s 4652
step     70 | loss 2.2891 | lr 2.67e-04 | grad 3.75 | tok/s 4688
step     80 | loss 5.2246 | lr 2.67e-04 | grad 14.56 | tok/s 4711
step     90 | loss 4.6386 | lr 2.67e-04 | grad 4.81 | tok/s 4787
step    100 | loss 3.7102 | lr 2.67e-04 | grad 3.88 | tok/s 4774
step    110 | loss 3.3412 | lr 2.67e-04 | grad 11.81 | tok/s 4769
step    120 | loss 3.0893 | lr 2.67e-04 | grad 12.31 | tok/s 4764
step    130 | loss 2.9568 | lr 2.67e-04 | grad 9.50 | tok/s 4759
step    140 | loss 2.6215 | lr 2.67e-04 | grad 3.23 | tok/s 4752
step    150 | loss 2.6756 | lr 2.67e-04 | grad 9.94 | tok/s 4737
step    160 | loss 2.3775 | lr 2.67e-04 | grad 7.06 | tok/s 4739
step    170 | loss 2.5271 | lr 2.67e-04 | grad 9.88 | tok/s 4728
step    180 | loss 2.3238 | lr 2.67e-04 | grad 10.81 | tok/s 4727
step    190 | loss 2.4493 | lr 2.67e-04 | grad 8.06 | tok/s 4729
step    200 | loss 2.2809 | lr 2.67e-04 | grad 3.97 | tok/s 4731
step    210 | loss 2.3552 | lr 2.67e-04 | grad 7.88 | tok/s 4728
step    220 | loss 2.5013 | lr 2.67e-04 | grad 5.84 | tok/s 4670
step    230 | loss 2.6574 | lr 2.67e-04 | grad 3.89 | tok/s 4611
step    240 | loss 2.4892 | lr 2.67e-04 | grad 2.84 | tok/s 4393
step    250 | loss 2.3115 | lr 2.67e-04 | grad 2.45 | tok/s 4510
step    260 | loss 1.9953 | lr 2.67e-04 | grad 2.97 | tok/s 4661
step    270 | loss 2.3283 | lr 2.67e-04 | grad 2.12 | tok/s 4593
step    280 | loss 2.4829 | lr 2.67e-04 | grad 2.56 | tok/s 4500
step    290 | loss 1.9767 | lr 2.67e-04 | grad 1.71 | tok/s 4747
step    300 | loss 1.0058 | lr 2.67e-04 | grad 2.48 | tok/s 4747
step    310 | loss 2.7020 | lr 2.67e-04 | grad 3.64 | tok/s 4663
step    320 | loss 2.3660 | lr 2.67e-04 | grad 3.75 | tok/s 4567
step    330 | loss 2.1597 | lr 2.67e-04 | grad 1.84 | tok/s 4411
step    340 | loss 2.4269 | lr 2.67e-04 | grad 2.30 | tok/s 4483
step    350 | loss 2.1770 | lr 2.67e-04 | grad 2.58 | tok/s 4593
step    360 | loss 1.8996 | lr 2.67e-04 | grad 3.75 | tok/s 4690
step    370 | loss 2.0891 | lr 2.67e-04 | grad 2.52 | tok/s 4258
step    380 | loss 1.9982 | lr 2.67e-04 | grad 1.98 | tok/s 4540
step    390 | loss 1.7883 | lr 2.67e-04 | grad 1.59 | tok/s 4738
step    400 | loss 1.7906 | lr 2.67e-04 | grad 2.33 | tok/s 4692
step    410 | loss 1.6415 | lr 2.67e-04 | grad 1.55 | tok/s 4594
step    420 | loss 2.0339 | lr 2.67e-04 | grad 3.02 | tok/s 4380
step    430 | loss 2.2798 | lr 2.67e-04 | grad 2.48 | tok/s 4665
step    440 | loss 2.3166 | lr 2.67e-04 | grad 2.70 | tok/s 4412
step    450 | loss 2.2907 | lr 2.67e-04 | grad 1.99 | tok/s 4558
step    460 | loss 1.9393 | lr 2.67e-04 | grad 3.64 | tok/s 4468
step    470 | loss 2.0313 | lr 2.67e-04 | grad 2.67 | tok/s 4598
step    480 | loss 2.3853 | lr 2.67e-04 | grad 3.66 | tok/s 4610
step    490 | loss 1.9793 | lr 2.67e-04 | grad 1.90 | tok/s 4348
step    500 | loss 1.9105 | lr 2.67e-04 | grad 2.73 | tok/s 4639
step    510 | loss 1.8773 | lr 2.67e-04 | grad 1.65 | tok/s 4714
step    520 | loss 1.8881 | lr 2.67e-04 | grad 1.53 | tok/s 4705
step    530 | loss 2.0590 | lr 2.67e-04 | grad 1.50 | tok/s 4519
step    540 | loss 1.9004 | lr 2.67e-04 | grad 2.14 | tok/s 4525
step    550 | loss 1.7218 | lr 2.67e-04 | grad 2.16 | tok/s 4433
step    560 | loss 1.8877 | lr 2.67e-04 | grad 1.88 | tok/s 4316
step    570 | loss 1.8278 | lr 2.67e-04 | grad 1.88 | tok/s 4431
step    580 | loss 1.7246 | lr 2.67e-04 | grad 1.72 | tok/s 4414
step    590 | loss 2.0268 | lr 2.67e-04 | grad 2.20 | tok/s 4524
step    600 | loss 1.9617 | lr 2.67e-04 | grad 1.41 | tok/s 4377
step    610 | loss 1.7810 | lr 2.67e-04 | grad 1.65 | tok/s 4600
step    620 | loss 1.6781 | lr 2.67e-04 | grad 1.35 | tok/s 4359
step    630 | loss 1.7823 | lr 2.67e-04 | grad 2.34 | tok/s 4396
step    640 | loss 1.9638 | lr 2.67e-04 | grad 1.33 | tok/s 4515
step    650 | loss 1.7982 | lr 2.67e-04 | grad 2.39 | tok/s 4528
step    660 | loss 1.8368 | lr 2.67e-04 | grad 1.30 | tok/s 4549
step    670 | loss 2.0528 | lr 2.67e-04 | grad 1.91 | tok/s 4590
step    680 | loss 1.8251 | lr 2.67e-04 | grad 1.66 | tok/s 4492
step    690 | loss 1.9787 | lr 2.67e-04 | grad 2.25 | tok/s 4646
step    700 | loss 1.5836 | lr 2.67e-04 | grad 1.71 | tok/s 4724
step    710 | loss 1.7155 | lr 2.67e-04 | grad 1.90 | tok/s 4422
step    720 | loss 1.5685 | lr 2.67e-04 | grad 2.19 | tok/s 4359
step    730 | loss 1.4751 | lr 2.67e-04 | grad 2.06 | tok/s 4727
step    740 | loss 1.6399 | lr 2.67e-04 | grad 1.65 | tok/s 4667
step    750 | loss 1.3737 | lr 2.67e-04 | grad 1.50 | tok/s 4741
step    760 | loss 1.2715 | lr 2.67e-04 | grad 1.51 | tok/s 4743
step    770 | loss 1.2251 | lr 2.67e-04 | grad 1.75 | tok/s 4743
step    780 | loss 1.1797 | lr 2.67e-04 | grad 1.40 | tok/s 4742
step    790 | loss 1.2737 | lr 2.67e-04 | grad 2.12 | tok/s 4599
step    800 | loss 1.9711 | lr 2.67e-04 | grad 2.92 | tok/s 4575
step    810 | loss 1.7607 | lr 2.67e-04 | grad 1.23 | tok/s 4550
step    820 | loss 1.7973 | lr 2.67e-04 | grad 2.33 | tok/s 4376
step    830 | loss 1.6813 | lr 2.67e-04 | grad 1.54 | tok/s 4697
step    840 | loss 1.5245 | lr 2.67e-04 | grad 1.27 | tok/s 4740
step    850 | loss 1.6501 | lr 2.67e-04 | grad 1.78 | tok/s 4719
step    860 | loss 1.5890 | lr 2.67e-04 | grad 2.86 | tok/s 4667
step    870 | loss 1.6038 | lr 2.67e-04 | grad 1.72 | tok/s 4498
step    880 | loss 1.7786 | lr 2.67e-04 | grad 1.69 | tok/s 4508
step    890 | loss 1.7625 | lr 2.67e-04 | grad 2.28 | tok/s 4575
step    900 | loss 1.6373 | lr 2.67e-04 | grad 1.40 | tok/s 4576
step    910 | loss 1.5023 | lr 2.67e-04 | grad 1.96 | tok/s 4480
step    920 | loss 1.6367 | lr 2.67e-04 | grad 2.53 | tok/s 4660
step    930 | loss 1.6917 | lr 2.67e-04 | grad 2.16 | tok/s 4449
step    940 | loss 1.5009 | lr 2.67e-04 | grad 1.40 | tok/s 4685
step    950 | loss 1.5942 | lr 2.67e-04 | grad 1.38 | tok/s 4713
step    960 | loss 1.4283 | lr 2.67e-04 | grad 1.62 | tok/s 4721
step    970 | loss 1.7904 | lr 2.67e-04 | grad 2.02 | tok/s 4438
step    980 | loss 1.6876 | lr 2.67e-04 | grad 1.33 | tok/s 4556
step    990 | loss 1.5571 | lr 2.67e-04 | grad 1.30 | tok/s 4641
step   1000 | loss 1.9575 | lr 2.67e-04 | grad 6.78 | tok/s 4448
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9575.pt
step   1010 | loss 1.7494 | lr 2.67e-04 | grad 2.17 | tok/s 1702
step   1020 | loss 1.5038 | lr 2.67e-04 | grad 1.32 | tok/s 4600
step   1030 | loss 1.6002 | lr 2.67e-04 | grad 2.42 | tok/s 4682
step   1040 | loss 1.6617 | lr 2.67e-04 | grad 1.29 | tok/s 4394
step   1050 | loss 1.7877 | lr 2.67e-04 | grad 1.63 | tok/s 4686
step   1060 | loss 1.7564 | lr 2.67e-04 | grad 2.27 | tok/s 4602
step   1070 | loss 1.4783 | lr 2.67e-04 | grad 1.31 | tok/s 4260
step   1080 | loss 1.0752 | lr 2.67e-04 | grad 0.63 | tok/s 4722
step   1090 | loss 1.6168 | lr 2.67e-04 | grad 1.85 | tok/s 4514
step   1100 | loss 1.4791 | lr 2.67e-04 | grad 1.74 | tok/s 4743
step   1110 | loss 1.4025 | lr 2.67e-04 | grad 1.47 | tok/s 4742
step   1120 | loss 1.3400 | lr 2.67e-04 | grad 1.43 | tok/s 4737
step   1130 | loss 1.3426 | lr 2.67e-04 | grad 1.24 | tok/s 4739
step   1140 | loss 1.3479 | lr 2.67e-04 | grad 1.70 | tok/s 4736
step   1150 | loss 1.2662 | lr 2.67e-04 | grad 1.44 | tok/s 4733
step   1160 | loss 1.3094 | lr 2.67e-04 | grad 1.45 | tok/s 4736
step   1170 | loss 1.3748 | lr 2.67e-04 | grad 1.83 | tok/s 4725
step   1180 | loss 1.2809 | lr 2.67e-04 | grad 1.72 | tok/s 4728
step   1190 | loss 1.2811 | lr 2.67e-04 | grad 1.21 | tok/s 4726
step   1200 | loss 1.3200 | lr 2.67e-04 | grad 1.16 | tok/s 4734
step   1210 | loss 1.3305 | lr 2.67e-04 | grad 1.25 | tok/s 4729
step   1220 | loss 1.3195 | lr 2.67e-04 | grad 1.29 | tok/s 4724
step   1230 | loss 1.3161 | lr 2.67e-04 | grad 1.81 | tok/s 4695
step   1240 | loss 1.9427 | lr 2.67e-04 | grad 1.67 | tok/s 4483
step   1250 | loss 1.3862 | lr 2.67e-04 | grad 4.47 | tok/s 4430
step   1260 | loss 1.8104 | lr 2.67e-04 | grad 1.61 | tok/s 4415
step   1270 | loss 1.6210 | lr 2.67e-04 | grad 2.09 | tok/s 4613
step   1280 | loss 1.5177 | lr 2.67e-04 | grad 1.49 | tok/s 4529
step   1290 | loss 1.5655 | lr 2.67e-04 | grad 1.18 | tok/s 4488
step   1300 | loss 1.5079 | lr 2.67e-04 | grad 1.16 | tok/s 4715
step   1310 | loss 1.6253 | lr 2.67e-04 | grad 1.47 | tok/s 4657
step   1320 | loss 1.5893 | lr 2.67e-04 | grad 1.27 | tok/s 4659
step   1330 | loss 1.6277 | lr 2.67e-04 | grad 2.22 | tok/s 4405
step   1340 | loss 1.7386 | lr 2.67e-04 | grad 1.52 | tok/s 4350
step   1350 | loss 1.5661 | lr 2.67e-04 | grad 0.98 | tok/s 4566
step   1360 | loss 1.5697 | lr 2.67e-04 | grad 4.94 | tok/s 4514
step   1370 | loss 1.6697 | lr 2.67e-04 | grad 2.17 | tok/s 4335
step   1380 | loss 1.5271 | lr 2.67e-04 | grad 2.48 | tok/s 4575
step   1390 | loss 1.4266 | lr 2.67e-04 | grad 1.19 | tok/s 4460
step   1400 | loss 1.5856 | lr 2.67e-04 | grad 7.00 | tok/s 4460
step   1410 | loss 1.6844 | lr 2.67e-04 | grad 1.34 | tok/s 4460
step   1420 | loss 1.4141 | lr 2.67e-04 | grad 1.56 | tok/s 4515
step   1430 | loss 1.1822 | lr 2.67e-04 | grad 1.41 | tok/s 4744
step   1440 | loss 1.2045 | lr 2.67e-04 | grad 1.35 | tok/s 4692
step   1450 | loss 1.7490 | lr 2.67e-04 | grad 1.33 | tok/s 4424
step   1460 | loss 1.5713 | lr 2.67e-04 | grad 1.60 | tok/s 4707
step   1470 | loss 1.9374 | lr 2.67e-04 | grad 2.22 | tok/s 4648
step   1480 | loss 1.6279 | lr 2.67e-04 | grad 1.70 | tok/s 4723
step   1490 | loss 1.3695 | lr 2.67e-04 | grad 1.41 | tok/s 4743
step   1500 | loss 1.6033 | lr 2.67e-04 | grad 1.88 | tok/s 4676
step   1510 | loss 1.4624 | lr 2.67e-04 | grad 2.09 | tok/s 4583
step   1520 | loss 1.4398 | lr 2.67e-04 | grad 3.34 | tok/s 4697
step   1530 | loss 1.6454 | lr 2.67e-04 | grad 1.56 | tok/s 4423
step   1540 | loss 1.3388 | lr 2.67e-04 | grad 1.71 | tok/s 4717
step   1550 | loss 1.6328 | lr 2.67e-04 | grad 1.12 | tok/s 4468
step   1560 | loss 1.3535 | lr 2.67e-04 | grad 1.40 | tok/s 4701
step   1570 | loss 1.7907 | lr 2.67e-04 | grad 2.55 | tok/s 4675
step   1580 | loss 1.6611 | lr 2.67e-04 | grad 1.29 | tok/s 4447
step   1590 | loss 0.9738 | lr 2.67e-04 | grad 0.66 | tok/s 4762
step   1600 | loss 1.1324 | lr 2.67e-04 | grad 1.30 | tok/s 4491
step   1610 | loss 1.4842 | lr 2.67e-04 | grad 1.98 | tok/s 4417
step   1620 | loss 1.4714 | lr 2.67e-04 | grad 1.50 | tok/s 4617
step   1630 | loss 1.4087 | lr 2.67e-04 | grad 1.35 | tok/s 4481
step   1640 | loss 1.5707 | lr 2.67e-04 | grad 1.38 | tok/s 4242
step   1650 | loss 1.4147 | lr 2.67e-04 | grad 1.49 | tok/s 4735
step   1660 | loss 1.5437 | lr 2.67e-04 | grad 3.22 | tok/s 4563
step   1670 | loss 1.7195 | lr 2.67e-04 | grad 1.08 | tok/s 4370
step   1680 | loss 1.5380 | lr 2.67e-04 | grad 2.33 | tok/s 4579
step   1690 | loss 1.5259 | lr 2.67e-04 | grad 1.32 | tok/s 4536
step   1700 | loss 1.4999 | lr 2.67e-04 | grad 1.45 | tok/s 4535
step   1710 | loss 1.5757 | lr 2.67e-04 | grad 1.31 | tok/s 4728
step   1720 | loss 1.2765 | lr 2.67e-04 | grad 2.31 | tok/s 4741
step   1730 | loss 1.4608 | lr 2.67e-04 | grad 1.49 | tok/s 4573
step   1740 | loss 1.5926 | lr 2.67e-04 | grad 1.59 | tok/s 4579
step   1750 | loss 1.5734 | lr 2.67e-04 | grad 1.34 | tok/s 4562
step   1760 | loss 1.4575 | lr 2.67e-04 | grad 1.38 | tok/s 4479
step   1770 | loss 1.5106 | lr 2.67e-04 | grad 1.21 | tok/s 4627
step   1780 | loss 1.4500 | lr 2.67e-04 | grad 1.89 | tok/s 4575
step   1790 | loss 1.6012 | lr 2.67e-04 | grad 1.38 | tok/s 4490
step   1800 | loss 1.4818 | lr 2.67e-04 | grad 2.50 | tok/s 4410
step   1810 | loss 1.5296 | lr 2.67e-04 | grad 4.47 | tok/s 4550
step   1820 | loss 1.5089 | lr 2.67e-04 | grad 3.08 | tok/s 4639
step   1830 | loss 1.5202 | lr 2.67e-04 | grad 0.98 | tok/s 4427
step   1840 | loss 1.3400 | lr 2.67e-04 | grad 1.71 | tok/s 4711
step   1850 | loss 1.3905 | lr 2.67e-04 | grad 1.66 | tok/s 4467
step   1860 | loss 1.4233 | lr 2.67e-04 | grad 0.97 | tok/s 4575
step   1870 | loss 1.3127 | lr 2.67e-04 | grad 1.42 | tok/s 4404
step   1880 | loss 1.5334 | lr 2.67e-04 | grad 1.35 | tok/s 4269
step   1890 | loss 1.3968 | lr 2.67e-04 | grad 1.45 | tok/s 4569
step   1900 | loss 1.4805 | lr 2.67e-04 | grad 1.60 | tok/s 4327
step   1910 | loss 1.4044 | lr 2.67e-04 | grad 1.27 | tok/s 4745
step   1920 | loss 1.4666 | lr 2.67e-04 | grad 2.00 | tok/s 4367
step   1930 | loss 1.4605 | lr 2.67e-04 | grad 1.73 | tok/s 4709
step   1940 | loss 1.9029 | lr 2.67e-04 | grad 2.05 | tok/s 4699
step   1950 | loss 1.5687 | lr 2.67e-04 | grad 2.19 | tok/s 4743
step   1960 | loss 1.5686 | lr 2.67e-04 | grad 1.24 | tok/s 4623
step   1970 | loss 1.5829 | lr 2.67e-04 | grad 1.58 | tok/s 4429
step   1980 | loss 1.6473 | lr 2.67e-04 | grad 1.70 | tok/s 4498
step   1990 | loss 1.5122 | lr 2.67e-04 | grad 1.41 | tok/s 4573
step   2000 | loss 1.1165 | lr 2.67e-04 | grad 2.02 | tok/s 4739
  >>> saved checkpoint: checkpoint_step_002000_loss_1.1165.pt
step   2010 | loss 1.0438 | lr 2.67e-04 | grad 1.96 | tok/s 1848
step   2020 | loss 1.3758 | lr 2.67e-04 | grad 1.54 | tok/s 4809

Training complete! Final step: 2023
