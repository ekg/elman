Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_11/level1_100m_20260202_125213
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 1,189,787,904 parameters
Using schedule-free AdamW (lr=0.0003375150068317606)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 6.6622 | lr 3.38e-04 | grad 7.19 | tok/s 3831
step     20 | loss 2.9168 | lr 3.38e-04 | grad 5.50 | tok/s 5970
step     30 | loss 2.7378 | lr 3.38e-04 | grad 3.50 | tok/s 6008
step     40 | loss 2.5769 | lr 3.38e-04 | grad 3.00 | tok/s 5753
step     50 | loss 3.3988 | lr 3.38e-04 | grad 19.38 | tok/s 5828
step     60 | loss 2.3288 | lr 3.38e-04 | grad 3.61 | tok/s 5996
step     70 | loss 2.3290 | lr 3.38e-04 | grad 3.97 | tok/s 6045
step     80 | loss 5.1895 | lr 3.38e-04 | grad 15.62 | tok/s 6091
step     90 | loss 4.3277 | lr 3.38e-04 | grad 3.22 | tok/s 6186
step    100 | loss 3.7575 | lr 3.38e-04 | grad 3.41 | tok/s 6176
step    110 | loss 3.3925 | lr 3.38e-04 | grad 14.44 | tok/s 6170
step    120 | loss 3.1410 | lr 3.38e-04 | grad 11.75 | tok/s 6145
step    130 | loss 3.0146 | lr 3.38e-04 | grad 8.31 | tok/s 6145
step    140 | loss 2.6501 | lr 3.38e-04 | grad 3.28 | tok/s 6126
step    150 | loss 2.8221 | lr 3.38e-04 | grad 9.94 | tok/s 6127
step    160 | loss 2.4323 | lr 3.38e-04 | grad 8.44 | tok/s 6111
step    170 | loss 2.5634 | lr 3.38e-04 | grad 6.69 | tok/s 6112
step    180 | loss 2.3569 | lr 3.38e-04 | grad 10.44 | tok/s 6105
step    190 | loss 2.5191 | lr 3.38e-04 | grad 7.66 | tok/s 6110
step    200 | loss 2.3426 | lr 3.38e-04 | grad 4.12 | tok/s 6108
step    210 | loss 2.3534 | lr 3.38e-04 | grad 4.84 | tok/s 6112
step    220 | loss 2.6270 | lr 3.38e-04 | grad 3.77 | tok/s 6044
step    230 | loss 2.9502 | lr 3.38e-04 | grad 3.11 | tok/s 5962
step    240 | loss 2.5631 | lr 3.38e-04 | grad 3.45 | tok/s 5671
step    250 | loss 2.3457 | lr 3.38e-04 | grad 2.47 | tok/s 5814
step    260 | loss 2.0649 | lr 3.38e-04 | grad 2.97 | tok/s 6006
step    270 | loss 2.3785 | lr 3.38e-04 | grad 2.81 | tok/s 5924
step    280 | loss 2.5205 | lr 3.38e-04 | grad 3.00 | tok/s 5816
step    290 | loss 2.0633 | lr 3.38e-04 | grad 2.30 | tok/s 6126
step    300 | loss 1.0455 | lr 3.38e-04 | grad 3.23 | tok/s 6115
step    310 | loss 2.7329 | lr 3.38e-04 | grad 3.00 | tok/s 6008
step    320 | loss 2.3951 | lr 3.38e-04 | grad 4.03 | tok/s 5888
step    330 | loss 2.1964 | lr 3.38e-04 | grad 2.02 | tok/s 5697
step    340 | loss 2.4497 | lr 3.38e-04 | grad 2.55 | tok/s 5779
step    350 | loss 2.2170 | lr 3.38e-04 | grad 2.78 | tok/s 5923
step    360 | loss 1.9135 | lr 3.38e-04 | grad 4.28 | tok/s 6058
step    370 | loss 2.1202 | lr 3.38e-04 | grad 2.75 | tok/s 5497
step    380 | loss 2.0200 | lr 3.38e-04 | grad 1.78 | tok/s 5844
step    390 | loss 1.8177 | lr 3.38e-04 | grad 1.95 | tok/s 6112
step    400 | loss 1.8199 | lr 3.38e-04 | grad 2.30 | tok/s 6055
step    410 | loss 1.6926 | lr 3.38e-04 | grad 1.55 | tok/s 5916
step    420 | loss 2.0625 | lr 3.38e-04 | grad 2.75 | tok/s 5650
step    430 | loss 2.3366 | lr 3.38e-04 | grad 2.58 | tok/s 6008
step    440 | loss 2.3476 | lr 3.38e-04 | grad 2.67 | tok/s 5682
step    450 | loss 2.4289 | lr 3.38e-04 | grad 1.86 | tok/s 5886
step    460 | loss 1.9698 | lr 3.38e-04 | grad 3.12 | tok/s 5752
step    470 | loss 2.0616 | lr 3.38e-04 | grad 2.61 | tok/s 5937
step    480 | loss 2.3940 | lr 3.38e-04 | grad 3.41 | tok/s 5939
step    490 | loss 2.0227 | lr 3.38e-04 | grad 1.98 | tok/s 5606
step    500 | loss 1.9462 | lr 3.38e-04 | grad 2.55 | tok/s 5991
step    510 | loss 1.9239 | lr 3.38e-04 | grad 1.95 | tok/s 6073
step    520 | loss 1.9172 | lr 3.38e-04 | grad 1.54 | tok/s 6063
step    530 | loss 2.0935 | lr 3.38e-04 | grad 1.52 | tok/s 5823
step    540 | loss 1.9231 | lr 3.38e-04 | grad 2.33 | tok/s 5829
step    550 | loss 1.7487 | lr 3.38e-04 | grad 1.80 | tok/s 5708
step    560 | loss 1.9194 | lr 3.38e-04 | grad 1.93 | tok/s 5551
step    570 | loss 1.8762 | lr 3.38e-04 | grad 1.87 | tok/s 5710
step    580 | loss 1.7627 | lr 3.38e-04 | grad 1.59 | tok/s 5689
step    590 | loss 2.0542 | lr 3.38e-04 | grad 1.89 | tok/s 5839
step    600 | loss 1.9773 | lr 3.38e-04 | grad 1.45 | tok/s 5645
step    610 | loss 1.8069 | lr 3.38e-04 | grad 1.77 | tok/s 5934
step    620 | loss 1.7063 | lr 3.38e-04 | grad 1.27 | tok/s 5630
step    630 | loss 1.8173 | lr 3.38e-04 | grad 2.28 | tok/s 5659
step    640 | loss 2.0077 | lr 3.38e-04 | grad 1.21 | tok/s 5813
step    650 | loss 1.8268 | lr 3.38e-04 | grad 2.42 | tok/s 5842
step    660 | loss 1.8703 | lr 3.38e-04 | grad 1.37 | tok/s 5872
step    670 | loss 2.0711 | lr 3.38e-04 | grad 1.88 | tok/s 5925
step    680 | loss 1.8538 | lr 3.38e-04 | grad 1.53 | tok/s 5804
step    690 | loss 2.0074 | lr 3.38e-04 | grad 2.12 | tok/s 6011
step    700 | loss 1.6312 | lr 3.38e-04 | grad 1.88 | tok/s 6108
step    710 | loss 1.7412 | lr 3.38e-04 | grad 1.80 | tok/s 5701
step    720 | loss 1.5962 | lr 3.38e-04 | grad 2.05 | tok/s 5627
step    730 | loss 1.5120 | lr 3.38e-04 | grad 2.23 | tok/s 6121
step    740 | loss 1.6635 | lr 3.38e-04 | grad 1.59 | tok/s 6017
step    750 | loss 1.3945 | lr 3.38e-04 | grad 1.39 | tok/s 6104
step    760 | loss 1.2952 | lr 3.38e-04 | grad 1.53 | tok/s 6107
step    770 | loss 1.2445 | lr 3.38e-04 | grad 1.41 | tok/s 6110
step    780 | loss 1.2055 | lr 3.38e-04 | grad 1.29 | tok/s 6109
step    790 | loss 1.2935 | lr 3.38e-04 | grad 1.98 | tok/s 5933
step    800 | loss 1.9867 | lr 3.38e-04 | grad 2.75 | tok/s 5901
step    810 | loss 1.7778 | lr 3.38e-04 | grad 1.30 | tok/s 5885
step    820 | loss 1.8281 | lr 3.38e-04 | grad 2.42 | tok/s 5654
step    830 | loss 1.7015 | lr 3.38e-04 | grad 1.38 | tok/s 6069
step    840 | loss 1.5522 | lr 3.38e-04 | grad 1.10 | tok/s 6113
step    850 | loss 1.6864 | lr 3.38e-04 | grad 1.91 | tok/s 6103
step    860 | loss 1.6244 | lr 3.38e-04 | grad 2.77 | tok/s 6030
step    870 | loss 1.6332 | lr 3.38e-04 | grad 1.56 | tok/s 5810
step    880 | loss 1.8034 | lr 3.38e-04 | grad 1.58 | tok/s 5814
step    890 | loss 1.7835 | lr 3.38e-04 | grad 2.05 | tok/s 5928
step    900 | loss 1.6596 | lr 3.38e-04 | grad 1.36 | tok/s 5926
step    910 | loss 1.5221 | lr 3.38e-04 | grad 1.76 | tok/s 5786
step    920 | loss 1.6650 | lr 3.38e-04 | grad 2.31 | tok/s 6026
step    930 | loss 1.7175 | lr 3.38e-04 | grad 1.98 | tok/s 5754
step    940 | loss 1.5297 | lr 3.38e-04 | grad 1.37 | tok/s 6068
step    950 | loss 1.6058 | lr 3.38e-04 | grad 1.79 | tok/s 6097
step    960 | loss 1.4504 | lr 3.38e-04 | grad 1.59 | tok/s 6109
step    970 | loss 1.8172 | lr 3.38e-04 | grad 1.90 | tok/s 5736
step    980 | loss 1.7148 | lr 3.38e-04 | grad 1.38 | tok/s 5896
step    990 | loss 1.5818 | lr 3.38e-04 | grad 1.32 | tok/s 5977
step   1000 | loss 1.9624 | lr 3.38e-04 | grad 6.09 | tok/s 5758
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9624.pt
step   1010 | loss 1.8335 | lr 3.38e-04 | grad 1.52 | tok/s 2253
step   1020 | loss 1.5551 | lr 3.38e-04 | grad 2.00 | tok/s 5786
step   1030 | loss 1.5636 | lr 3.38e-04 | grad 2.12 | tok/s 6150
step   1040 | loss 1.5602 | lr 3.38e-04 | grad 1.34 | tok/s 5602
step   1050 | loss 1.7915 | lr 3.38e-04 | grad 1.69 | tok/s 6082
step   1060 | loss 1.8342 | lr 3.38e-04 | grad 1.57 | tok/s 6106
step   1070 | loss 1.6479 | lr 3.38e-04 | grad 1.27 | tok/s 5787
step   1080 | loss 1.3898 | lr 3.38e-04 | grad 1.52 | tok/s 5643
step   1090 | loss 1.2332 | lr 3.38e-04 | grad 1.58 | tok/s 5967
step   1100 | loss 1.5965 | lr 3.38e-04 | grad 1.24 | tok/s 6067
step   1110 | loss 1.4267 | lr 3.38e-04 | grad 1.26 | tok/s 6133
step   1120 | loss 1.3954 | lr 3.38e-04 | grad 1.23 | tok/s 6136
step   1130 | loss 1.3258 | lr 3.38e-04 | grad 1.27 | tok/s 6136
step   1140 | loss 1.3849 | lr 3.38e-04 | grad 1.43 | tok/s 6129
step   1150 | loss 1.2954 | lr 3.38e-04 | grad 1.29 | tok/s 6131
step   1160 | loss 1.2967 | lr 3.38e-04 | grad 1.27 | tok/s 6107
step   1170 | loss 1.4172 | lr 3.38e-04 | grad 1.45 | tok/s 6126
step   1180 | loss 1.3171 | lr 3.38e-04 | grad 1.09 | tok/s 6112
step   1190 | loss 1.2655 | lr 3.38e-04 | grad 1.49 | tok/s 6139
step   1200 | loss 1.3325 | lr 3.38e-04 | grad 1.07 | tok/s 6123
step   1210 | loss 1.3659 | lr 3.38e-04 | grad 1.30 | tok/s 6121
step   1220 | loss 1.2991 | lr 3.38e-04 | grad 1.39 | tok/s 6129
step   1230 | loss 1.3306 | lr 3.38e-04 | grad 1.25 | tok/s 6123
step   1240 | loss 1.9064 | lr 3.38e-04 | grad 1.52 | tok/s 5884
step   1250 | loss 1.6960 | lr 3.38e-04 | grad 1.79 | tok/s 5665
step   1260 | loss 1.4431 | lr 3.38e-04 | grad 1.41 | tok/s 5930
step   1270 | loss 1.8051 | lr 3.38e-04 | grad 1.50 | tok/s 5657
step   1280 | loss 1.5106 | lr 3.38e-04 | grad 1.27 | tok/s 5959
step   1290 | loss 1.5824 | lr 3.38e-04 | grad 2.23 | tok/s 5996
step   1300 | loss 1.5062 | lr 3.38e-04 | grad 1.44 | tok/s 5801
step   1310 | loss 1.6178 | lr 3.38e-04 | grad 1.11 | tok/s 5975
step   1320 | loss 1.8182 | lr 3.38e-04 | grad 4.78 | tok/s 6087
step   1330 | loss 1.3304 | lr 3.38e-04 | grad 1.67 | tok/s 5781
step   1340 | loss 1.8669 | lr 3.38e-04 | grad 2.08 | tok/s 5568
step   1350 | loss 1.6725 | lr 3.38e-04 | grad 1.48 | tok/s 5901
step   1360 | loss 1.4693 | lr 3.38e-04 | grad 1.95 | tok/s 5807
step   1370 | loss 1.7256 | lr 3.38e-04 | grad 1.52 | tok/s 5784
step   1380 | loss 1.5973 | lr 3.38e-04 | grad 1.12 | tok/s 5723
step   1390 | loss 1.5344 | lr 3.38e-04 | grad 1.60 | tok/s 5702
step   1400 | loss 1.4329 | lr 3.38e-04 | grad 1.27 | tok/s 5846
step   1410 | loss 1.6989 | lr 3.38e-04 | grad 3.75 | tok/s 5708
step   1420 | loss 1.5353 | lr 3.38e-04 | grad 1.20 | tok/s 5933
step   1430 | loss 1.3157 | lr 3.38e-04 | grad 1.39 | tok/s 5949
step   1440 | loss 1.1644 | lr 3.38e-04 | grad 1.35 | tok/s 6110
step   1450 | loss 1.6423 | lr 3.38e-04 | grad 1.76 | tok/s 5789
step   1460 | loss 1.6274 | lr 3.38e-04 | grad 1.17 | tok/s 5902
step   1470 | loss 1.7337 | lr 3.38e-04 | grad 2.31 | tok/s 5992
step   1480 | loss 1.8771 | lr 3.38e-04 | grad 1.87 | tok/s 6085
step   1490 | loss 1.4527 | lr 3.38e-04 | grad 1.00 | tok/s 6110
step   1500 | loss 1.5403 | lr 3.38e-04 | grad 1.58 | tok/s 6044
step   1510 | loss 1.4953 | lr 3.38e-04 | grad 1.13 | tok/s 6037
step   1520 | loss 1.4953 | lr 3.38e-04 | grad 1.44 | tok/s 6012
step   1530 | loss 1.5761 | lr 3.38e-04 | grad 1.50 | tok/s 5753
step   1540 | loss 1.4939 | lr 3.38e-04 | grad 1.35 | tok/s 5974
step   1550 | loss 1.5449 | lr 3.38e-04 | grad 4.53 | tok/s 5901
step   1560 | loss 1.4530 | lr 3.38e-04 | grad 0.96 | tok/s 5977
step   1570 | loss 1.6024 | lr 3.38e-04 | grad 2.12 | tok/s 5986
step   1580 | loss 1.7796 | lr 3.38e-04 | grad 1.16 | tok/s 5883
step   1590 | loss 1.3095 | lr 3.38e-04 | grad 1.02 | tok/s 5996
step   1600 | loss 0.9311 | lr 3.38e-04 | grad 1.41 | tok/s 6072
step   1610 | loss 1.3877 | lr 3.38e-04 | grad 1.95 | tok/s 5475
step   1620 | loss 1.6230 | lr 3.38e-04 | grad 1.34 | tok/s 5953
step   1630 | loss 1.3128 | lr 3.38e-04 | grad 1.61 | tok/s 6056
step   1640 | loss 1.6121 | lr 3.38e-04 | grad 1.75 | tok/s 5498
step   1650 | loss 1.5296 | lr 3.38e-04 | grad 1.07 | tok/s 5801
step   1660 | loss 1.3182 | lr 3.38e-04 | grad 1.55 | tok/s 5980
step   1670 | loss 1.8987 | lr 3.38e-04 | grad 2.05 | tok/s 5707
step   1680 | loss 1.5114 | lr 3.38e-04 | grad 1.70 | tok/s 5818
step   1690 | loss 1.5348 | lr 3.38e-04 | grad 1.72 | tok/s 5991
step   1700 | loss 1.4717 | lr 3.38e-04 | grad 2.05 | tok/s 5764
step   1710 | loss 1.6132 | lr 3.38e-04 | grad 2.06 | tok/s 5987
step   1720 | loss 1.4607 | lr 3.38e-04 | grad 2.72 | tok/s 6111
step   1730 | loss 1.3197 | lr 3.38e-04 | grad 1.41 | tok/s 6082
step   1740 | loss 1.6052 | lr 3.38e-04 | grad 2.20 | tok/s 5883
step   1750 | loss 1.5652 | lr 3.38e-04 | grad 1.53 | tok/s 5801
step   1760 | loss 1.5807 | lr 3.38e-04 | grad 1.19 | tok/s 5989
step   1770 | loss 1.5128 | lr 3.38e-04 | grad 1.50 | tok/s 5797
step   1780 | loss 1.4992 | lr 3.38e-04 | grad 1.30 | tok/s 5903
step   1790 | loss 1.4724 | lr 3.38e-04 | grad 1.38 | tok/s 5932
step   1800 | loss 1.5860 | lr 3.38e-04 | grad 1.38 | tok/s 5842
step   1810 | loss 1.5228 | lr 3.38e-04 | grad 1.21 | tok/s 5717
step   1820 | loss 1.4933 | lr 3.38e-04 | grad 2.22 | tok/s 5994
step   1830 | loss 1.6047 | lr 3.38e-04 | grad 0.97 | tok/s 5759
step   1840 | loss 1.4472 | lr 3.38e-04 | grad 1.33 | tok/s 5962
step   1850 | loss 1.3138 | lr 3.38e-04 | grad 1.19 | tok/s 5991
step   1860 | loss 1.4874 | lr 3.38e-04 | grad 1.32 | tok/s 5694
step   1870 | loss 1.2578 | lr 3.38e-04 | grad 1.20 | tok/s 5804
step   1880 | loss 1.5295 | lr 3.38e-04 | grad 1.52 | tok/s 5478
step   1890 | loss 1.4600 | lr 3.38e-04 | grad 1.15 | tok/s 5958
step   1900 | loss 1.4603 | lr 3.38e-04 | grad 1.84 | tok/s 5552
step   1910 | loss 1.4608 | lr 3.38e-04 | grad 1.34 | tok/s 5975
step   1920 | loss 1.4290 | lr 3.38e-04 | grad 1.62 | tok/s 5877
step   1930 | loss 1.4793 | lr 3.38e-04 | grad 1.42 | tok/s 5839
step   1940 | loss 1.8794 | lr 3.38e-04 | grad 2.06 | tok/s 6061
step   1950 | loss 1.6028 | lr 3.38e-04 | grad 1.79 | tok/s 6113
step   1960 | loss 1.6303 | lr 3.38e-04 | grad 1.91 | tok/s 5975
step   1970 | loss 1.5874 | lr 3.38e-04 | grad 1.92 | tok/s 5762
step   1980 | loss 1.4340 | lr 3.38e-04 | grad 1.41 | tok/s 5909
step   1990 | loss 1.7536 | lr 3.38e-04 | grad 1.46 | tok/s 5947
step   2000 | loss 1.3549 | lr 3.38e-04 | grad 1.68 | tok/s 5958
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3549.pt
step   2010 | loss 1.3383 | lr 3.38e-04 | grad 1.38 | tok/s 2412
step   2020 | loss 1.0724 | lr 3.38e-04 | grad 1.48 | tok/s 6238
step   2030 | loss 1.3015 | lr 3.38e-04 | grad 1.20 | tok/s 6207
step   2040 | loss 1.4126 | lr 3.38e-04 | grad 1.82 | tok/s 5936
step   2050 | loss 1.6938 | lr 3.38e-04 | grad 1.20 | tok/s 5836
step   2060 | loss 1.9379 | lr 3.38e-04 | grad 4.66 | tok/s 5860
step   2070 | loss 2.0685 | lr 3.38e-04 | grad 2.52 | tok/s 6158
step   2080 | loss 1.6286 | lr 3.38e-04 | grad 2.23 | tok/s 6007
step   2090 | loss 1.3947 | lr 3.38e-04 | grad 1.45 | tok/s 6084
step   2100 | loss 1.5112 | lr 3.38e-04 | grad 1.12 | tok/s 5726
step   2110 | loss 0.7836 | lr 3.38e-04 | grad 0.84 | tok/s 6198
step   2120 | loss 1.4910 | lr 3.38e-04 | grad 1.48 | tok/s 5788
step   2130 | loss 1.4460 | lr 3.38e-04 | grad 1.63 | tok/s 6058
step   2140 | loss 1.2967 | lr 3.38e-04 | grad 1.41 | tok/s 6137
step   2150 | loss 1.2382 | lr 3.38e-04 | grad 1.09 | tok/s 6112
step   2160 | loss 1.2301 | lr 3.38e-04 | grad 1.29 | tok/s 6112
step   2170 | loss 1.2415 | lr 3.38e-04 | grad 1.22 | tok/s 6124
step   2180 | loss 1.2546 | lr 3.38e-04 | grad 1.36 | tok/s 6111
step   2190 | loss 1.1922 | lr 3.38e-04 | grad 0.94 | tok/s 6127
step   2200 | loss 1.1833 | lr 3.38e-04 | grad 1.38 | tok/s 6124
step   2210 | loss 1.1584 | lr 3.38e-04 | grad 1.18 | tok/s 6116
step   2220 | loss 1.4812 | lr 3.38e-04 | grad 1.33 | tok/s 6005
step   2230 | loss 1.3983 | lr 3.38e-04 | grad 1.64 | tok/s 5921
step   2240 | loss 1.5533 | lr 3.38e-04 | grad 1.81 | tok/s 6123
step   2250 | loss 1.6553 | lr 3.38e-04 | grad 1.14 | tok/s 5919
step   2260 | loss 2.0642 | lr 3.38e-04 | grad 1.26 | tok/s 6054
step   2270 | loss 1.4271 | lr 3.38e-04 | grad 1.48 | tok/s 6108
step   2280 | loss 1.6230 | lr 3.38e-04 | grad 1.15 | tok/s 5839
step   2290 | loss 1.5513 | lr 3.38e-04 | grad 1.79 | tok/s 5940
step   2300 | loss 1.5312 | lr 3.38e-04 | grad 3.48 | tok/s 5820
step   2310 | loss 1.9722 | lr 3.38e-04 | grad 3.48 | tok/s 5795
step   2320 | loss 1.4955 | lr 3.38e-04 | grad 1.36 | tok/s 5667
step   2330 | loss 1.5852 | lr 3.38e-04 | grad 1.54 | tok/s 5817
step   2340 | loss 1.3986 | lr 3.38e-04 | grad 1.31 | tok/s 5981
step   2350 | loss 1.2685 | lr 3.38e-04 | grad 1.16 | tok/s 6063
step   2360 | loss 1.6810 | lr 3.38e-04 | grad 1.63 | tok/s 6046
step   2370 | loss 1.4602 | lr 3.38e-04 | grad 2.17 | tok/s 6126
step   2380 | loss 1.1584 | lr 3.38e-04 | grad 0.93 | tok/s 6115
step   2390 | loss 1.0637 | lr 3.38e-04 | grad 1.67 | tok/s 6121
step   2400 | loss 1.2893 | lr 3.38e-04 | grad 1.57 | tok/s 5840
step   2410 | loss 1.5554 | lr 3.38e-04 | grad 1.52 | tok/s 5553
step   2420 | loss 1.2815 | lr 3.38e-04 | grad 1.16 | tok/s 6098
step   2430 | loss 1.4862 | lr 3.38e-04 | grad 1.27 | tok/s 5883
step   2440 | loss 1.5005 | lr 3.38e-04 | grad 1.62 | tok/s 5886
step   2450 | loss 1.1216 | lr 3.38e-04 | grad 1.18 | tok/s 6133
step   2460 | loss 1.2556 | lr 3.38e-04 | grad 1.28 | tok/s 6049
step   2470 | loss 1.3289 | lr 3.38e-04 | grad 1.41 | tok/s 6013
step   2480 | loss 1.4792 | lr 3.38e-04 | grad 1.19 | tok/s 5842
step   2490 | loss 1.4327 | lr 3.38e-04 | grad 1.23 | tok/s 6056
step   2500 | loss 1.1230 | lr 3.38e-04 | grad 2.05 | tok/s 6126
step   2510 | loss 1.6336 | lr 3.38e-04 | grad 1.52 | tok/s 6020
step   2520 | loss 1.3450 | lr 3.38e-04 | grad 1.17 | tok/s 5874
step   2530 | loss 1.3992 | lr 3.38e-04 | grad 1.12 | tok/s 5944
step   2540 | loss 1.1806 | lr 3.38e-04 | grad 1.24 | tok/s 6044
step   2550 | loss 1.5825 | lr 3.38e-04 | grad 1.36 | tok/s 5590
step   2560 | loss 1.3278 | lr 3.38e-04 | grad 1.15 | tok/s 5738
step   2570 | loss 1.3699 | lr 3.38e-04 | grad 1.77 | tok/s 5948
step   2580 | loss 1.4915 | lr 3.38e-04 | grad 1.02 | tok/s 5594
step   2590 | loss 1.7572 | lr 3.38e-04 | grad 2.28 | tok/s 5922
step   2600 | loss 1.3600 | lr 3.38e-04 | grad 2.05 | tok/s 5967
step   2610 | loss 1.5611 | lr 3.38e-04 | grad 2.20 | tok/s 6012
step   2620 | loss 1.4161 | lr 3.38e-04 | grad 2.72 | tok/s 6076

Training complete! Final step: 2623
