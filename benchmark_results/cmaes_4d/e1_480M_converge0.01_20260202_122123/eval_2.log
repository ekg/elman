Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_2/level1_100m_20260202_122130
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 940,148,736 parameters
Using schedule-free AdamW (lr=0.00020923128618320058)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 5.3737 | lr 2.09e-04 | grad 11.81 | tok/s 4177
step     20 | loss 2.8596 | lr 2.09e-04 | grad 3.19 | tok/s 6841
step     30 | loss 2.5946 | lr 2.09e-04 | grad 3.09 | tok/s 6906
step     40 | loss 2.4368 | lr 2.09e-04 | grad 2.33 | tok/s 6600
step     50 | loss 3.1871 | lr 2.09e-04 | grad 6.53 | tok/s 6698
step     60 | loss 2.1815 | lr 2.09e-04 | grad 2.03 | tok/s 6907
step     70 | loss 2.1377 | lr 2.09e-04 | grad 3.09 | tok/s 6984
step     80 | loss 5.0624 | lr 2.09e-04 | grad 27.38 | tok/s 7018
step     90 | loss 4.5182 | lr 2.09e-04 | grad 3.48 | tok/s 7129
step    100 | loss 3.9565 | lr 2.09e-04 | grad 2.84 | tok/s 7121
step    110 | loss 3.2523 | lr 2.09e-04 | grad 6.91 | tok/s 7111
step    120 | loss 2.8588 | lr 2.09e-04 | grad 13.00 | tok/s 7098
step    130 | loss 2.7158 | lr 2.09e-04 | grad 5.06 | tok/s 7098
step    140 | loss 2.4398 | lr 2.09e-04 | grad 3.42 | tok/s 7098
step    150 | loss 2.5365 | lr 2.09e-04 | grad 6.88 | tok/s 7094
step    160 | loss 2.2597 | lr 2.09e-04 | grad 4.00 | tok/s 7088
step    170 | loss 2.3244 | lr 2.09e-04 | grad 7.31 | tok/s 7091
step    180 | loss 2.1932 | lr 2.09e-04 | grad 6.75 | tok/s 7092
step    190 | loss 2.2827 | lr 2.09e-04 | grad 6.53 | tok/s 7088
step    200 | loss 2.1038 | lr 2.09e-04 | grad 2.67 | tok/s 7084
step    210 | loss 2.1343 | lr 2.09e-04 | grad 4.44 | tok/s 7085
step    220 | loss 2.3827 | lr 2.09e-04 | grad 2.20 | tok/s 6996
step    230 | loss 2.3701 | lr 2.09e-04 | grad 1.92 | tok/s 6919
step    240 | loss 2.3667 | lr 2.09e-04 | grad 2.58 | tok/s 6567
step    250 | loss 2.2010 | lr 2.09e-04 | grad 1.59 | tok/s 6751
step    260 | loss 1.8440 | lr 2.09e-04 | grad 2.41 | tok/s 6972
step    270 | loss 2.2133 | lr 2.09e-04 | grad 2.05 | tok/s 6880
step    280 | loss 2.3915 | lr 2.09e-04 | grad 2.42 | tok/s 6742
step    290 | loss 1.8571 | lr 2.09e-04 | grad 2.59 | tok/s 7087
step    300 | loss 0.7718 | lr 2.09e-04 | grad 1.86 | tok/s 7089
step    310 | loss 2.6666 | lr 2.09e-04 | grad 2.64 | tok/s 6977
step    320 | loss 2.1975 | lr 2.09e-04 | grad 3.95 | tok/s 6828
step    330 | loss 2.0762 | lr 2.09e-04 | grad 1.67 | tok/s 6594
step    340 | loss 2.3176 | lr 2.09e-04 | grad 1.76 | tok/s 6702
step    350 | loss 2.0289 | lr 2.09e-04 | grad 1.88 | tok/s 6874
step    360 | loss 1.5613 | lr 2.09e-04 | grad 6.00 | tok/s 7026
step    370 | loss 1.9774 | lr 2.09e-04 | grad 1.85 | tok/s 6368
step    380 | loss 1.8706 | lr 2.09e-04 | grad 1.84 | tok/s 6785
step    390 | loss 1.6361 | lr 2.09e-04 | grad 1.56 | tok/s 7090
step    400 | loss 1.6245 | lr 2.09e-04 | grad 1.77 | tok/s 7024
step    410 | loss 1.4436 | lr 2.09e-04 | grad 1.48 | tok/s 6870
step    420 | loss 1.9079 | lr 2.09e-04 | grad 3.03 | tok/s 6558
step    430 | loss 2.1830 | lr 2.09e-04 | grad 2.02 | tok/s 6978
step    440 | loss 2.2025 | lr 2.09e-04 | grad 2.44 | tok/s 6593
step    450 | loss 2.1650 | lr 2.09e-04 | grad 1.55 | tok/s 6829
step    460 | loss 1.8319 | lr 2.09e-04 | grad 2.06 | tok/s 6683
step    470 | loss 1.8845 | lr 2.09e-04 | grad 1.84 | tok/s 6894
step    480 | loss 2.2700 | lr 2.09e-04 | grad 4.12 | tok/s 6891
step    490 | loss 1.8515 | lr 2.09e-04 | grad 1.69 | tok/s 6517
step    500 | loss 1.7589 | lr 2.09e-04 | grad 2.70 | tok/s 6958
step    510 | loss 1.7656 | lr 2.09e-04 | grad 1.50 | tok/s 7061
step    520 | loss 1.7664 | lr 2.09e-04 | grad 1.66 | tok/s 7039
step    530 | loss 1.9316 | lr 2.09e-04 | grad 1.52 | tok/s 6765
step    540 | loss 1.7910 | lr 2.09e-04 | grad 1.73 | tok/s 6772
step    550 | loss 1.6246 | lr 2.09e-04 | grad 1.94 | tok/s 6626
step    560 | loss 1.7653 | lr 2.09e-04 | grad 1.80 | tok/s 6455
step    570 | loss 1.7033 | lr 2.09e-04 | grad 2.31 | tok/s 6631
step    580 | loss 1.6124 | lr 2.09e-04 | grad 1.76 | tok/s 6613
step    590 | loss 1.8998 | lr 2.09e-04 | grad 1.94 | tok/s 6783
step    600 | loss 1.8746 | lr 2.09e-04 | grad 1.52 | tok/s 6552
step    610 | loss 1.6902 | lr 2.09e-04 | grad 1.55 | tok/s 6887
step    620 | loss 1.5933 | lr 2.09e-04 | grad 1.64 | tok/s 6523
step    630 | loss 1.7157 | lr 2.09e-04 | grad 2.70 | tok/s 6579
step    640 | loss 1.8567 | lr 2.09e-04 | grad 1.57 | tok/s 6755
step    650 | loss 1.7009 | lr 2.09e-04 | grad 1.77 | tok/s 6788
step    660 | loss 1.7393 | lr 2.09e-04 | grad 1.27 | tok/s 6821
step    670 | loss 1.9073 | lr 2.09e-04 | grad 2.08 | tok/s 6864
step    680 | loss 1.7537 | lr 2.09e-04 | grad 1.65 | tok/s 6729
step    690 | loss 1.8735 | lr 2.09e-04 | grad 2.41 | tok/s 6960
step    700 | loss 1.4708 | lr 2.09e-04 | grad 1.84 | tok/s 7097
step    710 | loss 1.6247 | lr 2.09e-04 | grad 1.57 | tok/s 6628
step    720 | loss 1.4940 | lr 2.09e-04 | grad 2.28 | tok/s 6523
step    730 | loss 1.3973 | lr 2.09e-04 | grad 2.11 | tok/s 7086
step    740 | loss 1.5688 | lr 2.09e-04 | grad 1.87 | tok/s 6988
step    750 | loss 1.3078 | lr 2.09e-04 | grad 1.67 | tok/s 7098
step    760 | loss 1.2050 | lr 2.09e-04 | grad 1.47 | tok/s 7102
step    770 | loss 1.1524 | lr 2.09e-04 | grad 1.34 | tok/s 7099
step    780 | loss 1.1075 | lr 2.09e-04 | grad 1.44 | tok/s 7099
step    790 | loss 1.2107 | lr 2.09e-04 | grad 2.39 | tok/s 6880
step    800 | loss 1.8536 | lr 2.09e-04 | grad 3.64 | tok/s 6851
step    810 | loss 1.7157 | lr 2.09e-04 | grad 1.41 | tok/s 6821
step    820 | loss 1.7269 | lr 2.09e-04 | grad 2.67 | tok/s 6547
step    830 | loss 1.6105 | lr 2.09e-04 | grad 1.66 | tok/s 7028
step    840 | loss 1.4479 | lr 2.09e-04 | grad 1.42 | tok/s 7096
step    850 | loss 1.5866 | lr 2.09e-04 | grad 1.45 | tok/s 7061
step    860 | loss 1.5192 | lr 2.09e-04 | grad 3.00 | tok/s 6983
step    870 | loss 1.5368 | lr 2.09e-04 | grad 1.77 | tok/s 6726
step    880 | loss 1.7170 | lr 2.09e-04 | grad 1.73 | tok/s 6757
step    890 | loss 1.7061 | lr 2.09e-04 | grad 2.20 | tok/s 6848
step    900 | loss 1.5770 | lr 2.09e-04 | grad 1.70 | tok/s 6858
step    910 | loss 1.4531 | lr 2.09e-04 | grad 2.45 | tok/s 6711
step    920 | loss 1.5777 | lr 2.09e-04 | grad 2.83 | tok/s 6978
step    930 | loss 1.6202 | lr 2.09e-04 | grad 2.42 | tok/s 6663
step    940 | loss 1.4457 | lr 2.09e-04 | grad 1.36 | tok/s 7032
step    950 | loss 1.5135 | lr 2.09e-04 | grad 1.55 | tok/s 7060
step    960 | loss 1.3557 | lr 2.09e-04 | grad 1.73 | tok/s 7066
step    970 | loss 1.7350 | lr 2.09e-04 | grad 2.42 | tok/s 6647
step    980 | loss 1.6435 | lr 2.09e-04 | grad 1.59 | tok/s 6833
step    990 | loss 1.5045 | lr 2.09e-04 | grad 1.38 | tok/s 6957
step   1000 | loss 1.8838 | lr 2.09e-04 | grad 6.62 | tok/s 6667
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8838.pt
step   1010 | loss 1.7325 | lr 2.09e-04 | grad 3.58 | tok/s 2725
step   1020 | loss 1.5565 | lr 2.09e-04 | grad 4.81 | tok/s 6498
step   1030 | loss 1.4317 | lr 2.09e-04 | grad 1.51 | tok/s 7022
step   1040 | loss 1.4889 | lr 2.09e-04 | grad 1.70 | tok/s 6520
step   1050 | loss 1.7049 | lr 2.09e-04 | grad 2.75 | tok/s 6967
step   1060 | loss 1.7594 | lr 2.09e-04 | grad 3.02 | tok/s 6999
step   1070 | loss 1.5773 | lr 2.09e-04 | grad 1.67 | tok/s 6680
step   1080 | loss 1.4001 | lr 2.09e-04 | grad 1.38 | tok/s 6514
step   1090 | loss 1.1283 | lr 2.09e-04 | grad 1.55 | tok/s 6973
step   1100 | loss 1.5307 | lr 2.09e-04 | grad 1.47 | tok/s 6914
step   1110 | loss 1.3898 | lr 2.09e-04 | grad 1.70 | tok/s 7105
step   1120 | loss 1.3614 | lr 2.09e-04 | grad 1.52 | tok/s 7105
step   1130 | loss 1.2799 | lr 2.09e-04 | grad 1.45 | tok/s 7108
step   1140 | loss 1.3320 | lr 2.09e-04 | grad 1.35 | tok/s 7110
step   1150 | loss 1.2722 | lr 2.09e-04 | grad 1.22 | tok/s 7106
step   1160 | loss 1.2548 | lr 2.09e-04 | grad 1.39 | tok/s 7101
step   1170 | loss 1.3479 | lr 2.09e-04 | grad 1.60 | tok/s 7102
step   1180 | loss 1.2890 | lr 2.09e-04 | grad 1.32 | tok/s 7105
step   1190 | loss 1.2257 | lr 2.09e-04 | grad 1.70 | tok/s 7098
step   1200 | loss 1.2767 | lr 2.09e-04 | grad 1.38 | tok/s 7102
step   1210 | loss 1.3119 | lr 2.09e-04 | grad 1.48 | tok/s 7104
step   1220 | loss 1.2627 | lr 2.09e-04 | grad 1.14 | tok/s 7101
step   1230 | loss 1.2928 | lr 2.09e-04 | grad 1.27 | tok/s 7105
step   1240 | loss 1.6575 | lr 2.09e-04 | grad 4.41 | tok/s 6848
step   1250 | loss 1.6377 | lr 2.09e-04 | grad 3.19 | tok/s 6630
step   1260 | loss 1.4335 | lr 2.09e-04 | grad 1.47 | tok/s 6831
step   1270 | loss 1.7314 | lr 2.09e-04 | grad 2.34 | tok/s 6519
step   1280 | loss 1.5006 | lr 2.09e-04 | grad 1.52 | tok/s 6927
step   1290 | loss 1.5074 | lr 2.09e-04 | grad 1.48 | tok/s 6964
step   1300 | loss 1.5002 | lr 2.09e-04 | grad 1.68 | tok/s 6732
step   1310 | loss 1.5138 | lr 2.09e-04 | grad 2.72 | tok/s 6943
step   1320 | loss 1.7017 | lr 2.09e-04 | grad 1.51 | tok/s 7071
step   1330 | loss 1.3562 | lr 2.09e-04 | grad 2.06 | tok/s 6758
step   1340 | loss 1.7476 | lr 2.09e-04 | grad 2.44 | tok/s 6519
step   1350 | loss 1.6496 | lr 2.09e-04 | grad 1.95 | tok/s 6725
step   1360 | loss 1.4134 | lr 2.09e-04 | grad 1.73 | tok/s 6705
step   1370 | loss 1.6553 | lr 2.09e-04 | grad 1.85 | tok/s 6888
step   1380 | loss 1.5349 | lr 2.09e-04 | grad 1.77 | tok/s 6481
step   1390 | loss 1.4270 | lr 2.09e-04 | grad 2.47 | tok/s 6668
step   1400 | loss 1.4338 | lr 2.09e-04 | grad 3.11 | tok/s 6776
step   1410 | loss 1.5460 | lr 2.09e-04 | grad 1.46 | tok/s 6605
step   1420 | loss 1.6181 | lr 2.09e-04 | grad 1.80 | tok/s 6935
step   1430 | loss 1.2934 | lr 2.09e-04 | grad 1.65 | tok/s 6747
step   1440 | loss 1.1305 | lr 2.09e-04 | grad 1.51 | tok/s 7094
step   1450 | loss 1.4900 | lr 2.09e-04 | grad 2.17 | tok/s 6889
step   1460 | loss 1.5785 | lr 2.09e-04 | grad 1.62 | tok/s 6673
step   1470 | loss 1.5704 | lr 2.09e-04 | grad 3.69 | tok/s 6961
step   1480 | loss 1.8092 | lr 2.09e-04 | grad 3.11 | tok/s 7094
step   1490 | loss 1.5126 | lr 2.09e-04 | grad 1.63 | tok/s 7062
step   1500 | loss 1.3624 | lr 2.09e-04 | grad 5.00 | tok/s 7005
step   1510 | loss 1.5459 | lr 2.09e-04 | grad 1.43 | tok/s 7078
step   1520 | loss 1.4567 | lr 2.09e-04 | grad 1.65 | tok/s 6856
step   1530 | loss 1.5020 | lr 2.09e-04 | grad 4.09 | tok/s 6785
step   1540 | loss 1.4936 | lr 2.09e-04 | grad 1.20 | tok/s 6838
step   1550 | loss 1.3493 | lr 2.09e-04 | grad 2.44 | tok/s 6992
step   1560 | loss 1.5520 | lr 2.09e-04 | grad 1.75 | tok/s 6730
step   1570 | loss 1.4298 | lr 2.09e-04 | grad 1.65 | tok/s 6921
step   1580 | loss 1.7617 | lr 2.09e-04 | grad 3.02 | tok/s 7059
step   1590 | loss 1.3345 | lr 2.09e-04 | grad 1.28 | tok/s 6685
step   1600 | loss 0.8505 | lr 2.09e-04 | grad 1.39 | tok/s 7086
step   1610 | loss 1.3091 | lr 2.09e-04 | grad 1.57 | tok/s 6344
step   1620 | loss 1.5879 | lr 2.09e-04 | grad 2.31 | tok/s 6786
step   1630 | loss 1.2609 | lr 2.09e-04 | grad 3.08 | tok/s 7073
step   1640 | loss 1.4915 | lr 2.09e-04 | grad 1.47 | tok/s 6453
step   1650 | loss 1.5633 | lr 2.09e-04 | grad 1.45 | tok/s 6547
step   1660 | loss 1.2262 | lr 2.09e-04 | grad 1.52 | tok/s 7019
step   1670 | loss 1.8213 | lr 2.09e-04 | grad 2.27 | tok/s 6540
step   1680 | loss 1.4998 | lr 2.09e-04 | grad 3.72 | tok/s 6675
step   1690 | loss 1.4626 | lr 2.09e-04 | grad 2.58 | tok/s 6966
step   1700 | loss 1.4247 | lr 2.09e-04 | grad 1.43 | tok/s 6646
step   1710 | loss 1.4749 | lr 2.09e-04 | grad 1.77 | tok/s 6928
step   1720 | loss 1.4499 | lr 2.09e-04 | grad 1.98 | tok/s 7093
step   1730 | loss 1.2113 | lr 2.09e-04 | grad 2.09 | tok/s 7090
step   1740 | loss 1.5011 | lr 2.09e-04 | grad 1.69 | tok/s 6789
step   1750 | loss 1.5468 | lr 2.09e-04 | grad 1.88 | tok/s 6808
step   1760 | loss 1.5730 | lr 2.09e-04 | grad 1.28 | tok/s 6819
step   1770 | loss 1.4882 | lr 2.09e-04 | grad 2.80 | tok/s 6734
step   1780 | loss 1.4317 | lr 2.09e-04 | grad 2.48 | tok/s 6855
step   1790 | loss 1.4520 | lr 2.09e-04 | grad 1.45 | tok/s 6826
step   1800 | loss 1.6058 | lr 2.09e-04 | grad 1.30 | tok/s 6678
step   1810 | loss 1.4335 | lr 2.09e-04 | grad 1.50 | tok/s 6633
step   1820 | loss 1.4682 | lr 2.09e-04 | grad 1.48 | tok/s 6958
step   1830 | loss 1.5964 | lr 2.09e-04 | grad 8.50 | tok/s 6819
step   1840 | loss 1.4062 | lr 2.09e-04 | grad 1.48 | tok/s 6704
step   1850 | loss 1.2703 | lr 2.09e-04 | grad 1.95 | tok/s 6937
step   1860 | loss 1.4453 | lr 2.09e-04 | grad 1.29 | tok/s 6621
step   1870 | loss 1.2030 | lr 2.09e-04 | grad 1.76 | tok/s 6882
step   1880 | loss 1.4554 | lr 2.09e-04 | grad 1.35 | tok/s 6305
step   1890 | loss 1.4548 | lr 2.09e-04 | grad 1.34 | tok/s 6826
step   1900 | loss 1.3971 | lr 2.09e-04 | grad 1.95 | tok/s 6527
step   1910 | loss 1.4342 | lr 2.09e-04 | grad 1.30 | tok/s 6761
step   1920 | loss 1.3911 | lr 2.09e-04 | grad 1.36 | tok/s 6929
step   1930 | loss 1.4548 | lr 2.09e-04 | grad 1.72 | tok/s 6678
step   1940 | loss 1.7353 | lr 2.09e-04 | grad 3.69 | tok/s 6963
step   1950 | loss 1.6319 | lr 2.09e-04 | grad 2.42 | tok/s 7089
step   1960 | loss 1.4712 | lr 2.09e-04 | grad 3.72 | tok/s 6920
step   1970 | loss 1.5548 | lr 2.09e-04 | grad 1.64 | tok/s 6869
step   1980 | loss 1.3742 | lr 2.09e-04 | grad 1.37 | tok/s 6734
step   1990 | loss 1.6395 | lr 2.09e-04 | grad 1.56 | tok/s 6772
step   2000 | loss 1.4634 | lr 2.09e-04 | grad 1.34 | tok/s 6860
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4634.pt
step   2010 | loss 1.2226 | lr 2.09e-04 | grad 1.45 | tok/s 2884
step   2020 | loss 1.1092 | lr 2.09e-04 | grad 0.68 | tok/s 7143
step   2030 | loss 1.2564 | lr 2.09e-04 | grad 1.40 | tok/s 7117
step   2040 | loss 1.2220 | lr 2.09e-04 | grad 1.74 | tok/s 6969
step   2050 | loss 1.5487 | lr 2.09e-04 | grad 1.74 | tok/s 6572
step   2060 | loss 1.6546 | lr 2.09e-04 | grad 1.84 | tok/s 6807
step   2070 | loss 2.1957 | lr 2.09e-04 | grad 2.81 | tok/s 7034
step   2080 | loss 1.7399 | lr 2.09e-04 | grad 2.78 | tok/s 7061
step   2090 | loss 1.3938 | lr 2.09e-04 | grad 1.91 | tok/s 6879
step   2100 | loss 1.5556 | lr 2.09e-04 | grad 11.81 | tok/s 6753
step   2110 | loss 0.9326 | lr 2.09e-04 | grad 1.71 | tok/s 6982
step   2120 | loss 1.0758 | lr 2.09e-04 | grad 2.75 | tok/s 6973
step   2130 | loss 1.5466 | lr 2.09e-04 | grad 1.31 | tok/s 6756
step   2140 | loss 1.2908 | lr 2.09e-04 | grad 1.62 | tok/s 7098
step   2150 | loss 1.1983 | lr 2.09e-04 | grad 1.22 | tok/s 7095
step   2160 | loss 1.2427 | lr 2.09e-04 | grad 1.20 | tok/s 7095
step   2170 | loss 1.1986 | lr 2.09e-04 | grad 1.59 | tok/s 7099
step   2180 | loss 1.2106 | lr 2.09e-04 | grad 1.34 | tok/s 7094
step   2190 | loss 1.2016 | lr 2.09e-04 | grad 1.31 | tok/s 7096
step   2200 | loss 1.1570 | lr 2.09e-04 | grad 1.28 | tok/s 7093
step   2210 | loss 1.1468 | lr 2.09e-04 | grad 1.13 | tok/s 7095
step   2220 | loss 1.3555 | lr 2.09e-04 | grad 1.41 | tok/s 6969
step   2230 | loss 1.3269 | lr 2.09e-04 | grad 1.41 | tok/s 7084
step   2240 | loss 1.5169 | lr 2.09e-04 | grad 2.33 | tok/s 6844
step   2250 | loss 1.5524 | lr 2.09e-04 | grad 1.55 | tok/s 6912
step   2260 | loss 1.9244 | lr 2.09e-04 | grad 2.23 | tok/s 7026
step   2270 | loss 1.4917 | lr 2.09e-04 | grad 1.83 | tok/s 7008
step   2280 | loss 1.3178 | lr 2.09e-04 | grad 2.09 | tok/s 6842
step   2290 | loss 1.7730 | lr 2.09e-04 | grad 2.62 | tok/s 6990
step   2300 | loss 1.4082 | lr 2.09e-04 | grad 1.34 | tok/s 6639
step   2310 | loss 1.6470 | lr 2.09e-04 | grad 3.78 | tok/s 6659
step   2320 | loss 1.8225 | lr 2.09e-04 | grad 1.69 | tok/s 6782
step   2330 | loss 1.4248 | lr 2.09e-04 | grad 4.66 | tok/s 6599
step   2340 | loss 1.3530 | lr 2.09e-04 | grad 3.69 | tok/s 6932
step   2350 | loss 1.3197 | lr 2.09e-04 | grad 1.75 | tok/s 7020
step   2360 | loss 1.4740 | lr 2.09e-04 | grad 3.00 | tok/s 6941
step   2370 | loss 1.4918 | lr 2.09e-04 | grad 1.97 | tok/s 7096
step   2380 | loss 1.1861 | lr 2.09e-04 | grad 1.34 | tok/s 7079
step   2390 | loss 1.1166 | lr 2.09e-04 | grad 1.40 | tok/s 7086
step   2400 | loss 1.1140 | lr 2.09e-04 | grad 1.55 | tok/s 6864
step   2410 | loss 1.4686 | lr 2.09e-04 | grad 3.50 | tok/s 6643
step   2420 | loss 1.4065 | lr 2.09e-04 | grad 1.52 | tok/s 6757
step   2430 | loss 1.2381 | lr 2.09e-04 | grad 3.03 | tok/s 6941
step   2440 | loss 1.4642 | lr 2.09e-04 | grad 1.82 | tok/s 6763
step   2450 | loss 1.3173 | lr 2.09e-04 | grad 1.59 | tok/s 7022
step   2460 | loss 1.1535 | lr 2.09e-04 | grad 1.83 | tok/s 7000
step   2470 | loss 1.2336 | lr 2.09e-04 | grad 1.23 | tok/s 7095
step   2480 | loss 1.3265 | lr 2.09e-04 | grad 1.48 | tok/s 6713
step   2490 | loss 1.5465 | lr 2.09e-04 | grad 1.87 | tok/s 6935
step   2500 | loss 1.1527 | lr 2.09e-04 | grad 2.88 | tok/s 7089
step   2510 | loss 1.4059 | lr 2.09e-04 | grad 4.09 | tok/s 7063
step   2520 | loss 1.3562 | lr 2.09e-04 | grad 1.38 | tok/s 6831
step   2530 | loss 1.3981 | lr 2.09e-04 | grad 2.34 | tok/s 6780
step   2540 | loss 1.1921 | lr 2.09e-04 | grad 1.41 | tok/s 7048
step   2550 | loss 1.4166 | lr 2.09e-04 | grad 5.34 | tok/s 6616
step   2560 | loss 1.3940 | lr 2.09e-04 | grad 1.36 | tok/s 6894
step   2570 | loss 1.3573 | lr 2.09e-04 | grad 1.70 | tok/s 6456
step   2580 | loss 1.3236 | lr 2.09e-04 | grad 1.72 | tok/s 6824
step   2590 | loss 1.5582 | lr 2.09e-04 | grad 3.08 | tok/s 6549
step   2600 | loss 1.3432 | lr 2.09e-04 | grad 2.02 | tok/s 7046
step   2610 | loss 1.5365 | lr 2.09e-04 | grad 1.64 | tok/s 6799
step   2620 | loss 1.4244 | lr 2.09e-04 | grad 1.38 | tok/s 7019
step   2630 | loss 1.4708 | lr 2.09e-04 | grad 1.78 | tok/s 6826
step   2640 | loss 1.5094 | lr 2.09e-04 | grad 2.44 | tok/s 7089
step   2650 | loss 1.3223 | lr 2.09e-04 | grad 1.91 | tok/s 6855
step   2660 | loss 1.3964 | lr 2.09e-04 | grad 2.27 | tok/s 6578
step   2670 | loss 1.6391 | lr 2.09e-04 | grad 4.38 | tok/s 6719
step   2680 | loss 1.3057 | lr 2.09e-04 | grad 1.21 | tok/s 7033
step   2690 | loss 1.4390 | lr 2.09e-04 | grad 1.49 | tok/s 6831
step   2700 | loss 1.5143 | lr 2.09e-04 | grad 2.44 | tok/s 6737
step   2710 | loss 1.3293 | lr 2.09e-04 | grad 1.52 | tok/s 6455
step   2720 | loss 1.1958 | lr 2.09e-04 | grad 1.56 | tok/s 6938
step   2730 | loss 1.6145 | lr 2.09e-04 | grad 6.09 | tok/s 6862
step   2740 | loss 1.6049 | lr 2.09e-04 | grad 1.68 | tok/s 7026
step   2750 | loss 1.3224 | lr 2.09e-04 | grad 1.62 | tok/s 6528
step   2760 | loss 1.5210 | lr 2.09e-04 | grad 2.19 | tok/s 6742
step   2770 | loss 1.1125 | lr 2.09e-04 | grad 1.33 | tok/s 7092
step   2780 | loss 1.6623 | lr 2.09e-04 | grad 4.47 | tok/s 6605
step   2790 | loss 1.4048 | lr 2.09e-04 | grad 1.19 | tok/s 6795
step   2800 | loss 1.2516 | lr 2.09e-04 | grad 1.66 | tok/s 6773
step   2810 | loss 1.3922 | lr 2.09e-04 | grad 1.25 | tok/s 6525
step   2820 | loss 1.1614 | lr 2.09e-04 | grad 2.06 | tok/s 6965
step   2830 | loss 0.8026 | lr 2.09e-04 | grad 2.30 | tok/s 7089
step   2840 | loss 1.6870 | lr 2.09e-04 | grad 1.71 | tok/s 6858
step   2850 | loss 1.6671 | lr 2.09e-04 | grad 1.26 | tok/s 6825
step   2860 | loss 1.3650 | lr 2.09e-04 | grad 1.48 | tok/s 6684
step   2870 | loss 1.4832 | lr 2.09e-04 | grad 2.67 | tok/s 6904
step   2880 | loss 1.2545 | lr 2.09e-04 | grad 1.73 | tok/s 7092
step   2890 | loss 1.4444 | lr 2.09e-04 | grad 1.41 | tok/s 6816
step   2900 | loss 1.4268 | lr 2.09e-04 | grad 1.27 | tok/s 6817
step   2910 | loss 1.4921 | lr 2.09e-04 | grad 1.80 | tok/s 6613
step   2920 | loss 1.5742 | lr 2.09e-04 | grad 1.54 | tok/s 6921
step   2930 | loss 1.2832 | lr 2.09e-04 | grad 1.80 | tok/s 6365
step   2940 | loss 1.3618 | lr 2.09e-04 | grad 1.81 | tok/s 6846
step   2950 | loss 1.3185 | lr 2.09e-04 | grad 2.27 | tok/s 6912
step   2960 | loss 1.3113 | lr 2.09e-04 | grad 1.81 | tok/s 6959
step   2970 | loss 1.7164 | lr 2.09e-04 | grad 1.39 | tok/s 6736
step   2980 | loss 1.9148 | lr 2.09e-04 | grad 2.00 | tok/s 6907
step   2990 | loss 1.3622 | lr 2.09e-04 | grad 3.31 | tok/s 6929
step   3000 | loss 1.2758 | lr 2.09e-04 | grad 4.16 | tok/s 6868
  >>> saved checkpoint: checkpoint_step_003000_loss_1.2758.pt
step   3010 | loss 1.2631 | lr 2.09e-04 | grad 2.00 | tok/s 2884
step   3020 | loss 1.4766 | lr 2.09e-04 | grad 2.06 | tok/s 6798
step   3030 | loss 1.4184 | lr 2.09e-04 | grad 1.51 | tok/s 6911

Training complete! Final step: 3033
