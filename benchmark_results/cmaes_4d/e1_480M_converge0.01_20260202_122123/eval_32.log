Using device: cuda
Output directory: benchmark_results/cmaes_4d/e1_480M_converge0.01_20260202_122123/eval_32/level1_100m_20260202_135258
Auto r_h_mode: spectral_norm (level 1 has full W_h)
Model: Level 1, 764,832,768 parameters
Using schedule-free AdamW (lr=0.00046117818727529984)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 5.5364 | lr 4.61e-04 | grad 9.69 | tok/s 4165
step     20 | loss 2.7322 | lr 4.61e-04 | grad 3.38 | tok/s 6726
step     30 | loss 2.6225 | lr 4.61e-04 | grad 5.31 | tok/s 6805
step     40 | loss 2.5226 | lr 4.61e-04 | grad 2.58 | tok/s 6524
step     50 | loss 3.0746 | lr 4.61e-04 | grad 3.80 | tok/s 6630
step     60 | loss 2.2720 | lr 4.61e-04 | grad 2.88 | tok/s 6849
step     70 | loss 2.2440 | lr 4.61e-04 | grad 3.30 | tok/s 6942
step     80 | loss 5.3543 | lr 4.61e-04 | grad 8.25 | tok/s 6980
step     90 | loss 4.5144 | lr 4.61e-04 | grad 2.69 | tok/s 7099
step    100 | loss 3.9421 | lr 4.61e-04 | grad 3.36 | tok/s 7091
step    110 | loss 3.4176 | lr 4.61e-04 | grad 10.81 | tok/s 7097
step    120 | loss 3.1767 | lr 4.61e-04 | grad 11.69 | tok/s 7097
step    130 | loss 3.1242 | lr 4.61e-04 | grad 5.03 | tok/s 7094
step    140 | loss 2.6750 | lr 4.61e-04 | grad 3.64 | tok/s 7099
step    150 | loss 2.8225 | lr 4.61e-04 | grad 7.31 | tok/s 7098
step    160 | loss 2.4350 | lr 4.61e-04 | grad 6.19 | tok/s 7098
step    170 | loss 2.5798 | lr 4.61e-04 | grad 6.66 | tok/s 7092
step    180 | loss 2.3910 | lr 4.61e-04 | grad 8.19 | tok/s 7094
step    190 | loss 2.5380 | lr 4.61e-04 | grad 9.12 | tok/s 7096
step    200 | loss 2.3053 | lr 4.61e-04 | grad 3.30 | tok/s 7094
step    210 | loss 2.3832 | lr 4.61e-04 | grad 5.34 | tok/s 7097
step    220 | loss 2.5691 | lr 4.61e-04 | grad 2.28 | tok/s 7005
step    230 | loss 2.8816 | lr 4.61e-04 | grad 3.48 | tok/s 6921
step    240 | loss 2.4677 | lr 4.61e-04 | grad 2.61 | tok/s 6574
step    250 | loss 2.2991 | lr 4.61e-04 | grad 2.05 | tok/s 6762
step    260 | loss 1.9779 | lr 4.61e-04 | grad 2.33 | tok/s 6976
step    270 | loss 2.3157 | lr 4.61e-04 | grad 1.88 | tok/s 6878
step    280 | loss 2.4794 | lr 4.61e-04 | grad 2.44 | tok/s 6754
step    290 | loss 1.9833 | lr 4.61e-04 | grad 2.45 | tok/s 7100
step    300 | loss 0.9713 | lr 4.61e-04 | grad 3.70 | tok/s 7100
step    310 | loss 2.6846 | lr 4.61e-04 | grad 3.02 | tok/s 6982
step    320 | loss 2.3364 | lr 4.61e-04 | grad 3.31 | tok/s 6840
step    330 | loss 2.1388 | lr 4.61e-04 | grad 1.65 | tok/s 6604
step    340 | loss 2.4096 | lr 4.61e-04 | grad 1.80 | tok/s 6707
step    350 | loss 2.1357 | lr 4.61e-04 | grad 1.82 | tok/s 6878
step    360 | loss 1.8218 | lr 4.61e-04 | grad 2.86 | tok/s 7032
step    370 | loss 2.0507 | lr 4.61e-04 | grad 1.80 | tok/s 6370
step    380 | loss 1.9634 | lr 4.61e-04 | grad 1.70 | tok/s 6790
step    390 | loss 1.7372 | lr 4.61e-04 | grad 1.38 | tok/s 7092
step    400 | loss 1.7591 | lr 4.61e-04 | grad 1.86 | tok/s 7030
step    410 | loss 1.6144 | lr 4.61e-04 | grad 1.21 | tok/s 6874
step    420 | loss 1.9995 | lr 4.61e-04 | grad 2.47 | tok/s 6561
step    430 | loss 2.2675 | lr 4.61e-04 | grad 1.81 | tok/s 6980
step    440 | loss 2.3309 | lr 4.61e-04 | grad 2.06 | tok/s 6599
step    450 | loss 2.3504 | lr 4.61e-04 | grad 1.44 | tok/s 6829
step    460 | loss 1.9253 | lr 4.61e-04 | grad 2.56 | tok/s 6689
step    470 | loss 2.0087 | lr 4.61e-04 | grad 2.03 | tok/s 6887
step    480 | loss 2.3565 | lr 4.61e-04 | grad 3.00 | tok/s 6894
step    490 | loss 1.9616 | lr 4.61e-04 | grad 1.60 | tok/s 6516
step    500 | loss 1.8931 | lr 4.61e-04 | grad 2.16 | tok/s 6959
step    510 | loss 1.8545 | lr 4.61e-04 | grad 1.25 | tok/s 7054
step    520 | loss 1.8611 | lr 4.61e-04 | grad 1.49 | tok/s 7035
step    530 | loss 2.0394 | lr 4.61e-04 | grad 1.05 | tok/s 6768
step    540 | loss 1.8791 | lr 4.61e-04 | grad 1.50 | tok/s 6774
step    550 | loss 1.7083 | lr 4.61e-04 | grad 1.55 | tok/s 6627
step    560 | loss 1.8726 | lr 4.61e-04 | grad 1.49 | tok/s 6455
step    570 | loss 1.8273 | lr 4.61e-04 | grad 1.45 | tok/s 6629
step    580 | loss 1.7033 | lr 4.61e-04 | grad 1.48 | tok/s 6611
step    590 | loss 2.0065 | lr 4.61e-04 | grad 1.70 | tok/s 6779
step    600 | loss 1.9645 | lr 4.61e-04 | grad 1.14 | tok/s 6544
step    610 | loss 1.7651 | lr 4.61e-04 | grad 1.26 | tok/s 6880
step    620 | loss 1.6671 | lr 4.61e-04 | grad 1.09 | tok/s 6518
step    630 | loss 1.7789 | lr 4.61e-04 | grad 1.98 | tok/s 6572
step    640 | loss 1.9602 | lr 4.61e-04 | grad 1.08 | tok/s 6752
step    650 | loss 1.7896 | lr 4.61e-04 | grad 1.57 | tok/s 6786
step    660 | loss 1.8218 | lr 4.61e-04 | grad 0.93 | tok/s 6816
step    670 | loss 2.0303 | lr 4.61e-04 | grad 1.56 | tok/s 6860
step    680 | loss 1.8254 | lr 4.61e-04 | grad 1.30 | tok/s 6723
step    690 | loss 1.9768 | lr 4.61e-04 | grad 1.89 | tok/s 6958
step    700 | loss 1.5720 | lr 4.61e-04 | grad 1.48 | tok/s 7093
step    710 | loss 1.7159 | lr 4.61e-04 | grad 1.40 | tok/s 6619
step    720 | loss 1.5708 | lr 4.61e-04 | grad 1.53 | tok/s 6523
step    730 | loss 1.4670 | lr 4.61e-04 | grad 1.66 | tok/s 7076
step    740 | loss 1.6285 | lr 4.61e-04 | grad 1.25 | tok/s 6990
step    750 | loss 1.3618 | lr 4.61e-04 | grad 1.05 | tok/s 7094
step    760 | loss 1.2649 | lr 4.61e-04 | grad 1.15 | tok/s 7092
step    770 | loss 1.2152 | lr 4.61e-04 | grad 0.94 | tok/s 7093
step    780 | loss 1.1744 | lr 4.61e-04 | grad 0.86 | tok/s 7095
step    790 | loss 1.2659 | lr 4.61e-04 | grad 1.52 | tok/s 6876
step    800 | loss 1.9686 | lr 4.61e-04 | grad 2.25 | tok/s 6851
step    810 | loss 1.7589 | lr 4.61e-04 | grad 1.05 | tok/s 6812
step    820 | loss 1.8021 | lr 4.61e-04 | grad 2.14 | tok/s 6541
step    830 | loss 1.6927 | lr 4.61e-04 | grad 1.19 | tok/s 7024
step    840 | loss 1.5242 | lr 4.61e-04 | grad 0.96 | tok/s 7094
step    850 | loss 1.6340 | lr 4.61e-04 | grad 1.31 | tok/s 7057
step    860 | loss 1.5932 | lr 4.61e-04 | grad 2.39 | tok/s 6979
step    870 | loss 1.6063 | lr 4.61e-04 | grad 1.45 | tok/s 6720
step    880 | loss 1.7743 | lr 4.61e-04 | grad 1.09 | tok/s 6754
step    890 | loss 1.7590 | lr 4.61e-04 | grad 1.62 | tok/s 6849
step    900 | loss 1.6437 | lr 4.61e-04 | grad 1.12 | tok/s 6855
step    910 | loss 1.5075 | lr 4.61e-04 | grad 1.48 | tok/s 6711
step    920 | loss 1.6406 | lr 4.61e-04 | grad 1.83 | tok/s 6977
step    930 | loss 1.6924 | lr 4.61e-04 | grad 1.54 | tok/s 6665
step    940 | loss 1.5049 | lr 4.61e-04 | grad 0.94 | tok/s 7026
step    950 | loss 1.5836 | lr 4.61e-04 | grad 1.32 | tok/s 7057
step    960 | loss 1.4218 | lr 4.61e-04 | grad 1.40 | tok/s 7062
step    970 | loss 1.7921 | lr 4.61e-04 | grad 1.65 | tok/s 6644
step    980 | loss 1.6964 | lr 4.61e-04 | grad 1.06 | tok/s 6826
step    990 | loss 1.5529 | lr 4.61e-04 | grad 1.02 | tok/s 6944
step   1000 | loss 1.9514 | lr 4.61e-04 | grad 5.50 | tok/s 6664
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9514.pt
step   1010 | loss 1.8027 | lr 4.61e-04 | grad 1.70 | tok/s 3560
step   1020 | loss 1.7147 | lr 4.61e-04 | grad 1.15 | tok/s 6498
step   1030 | loss 1.4960 | lr 4.61e-04 | grad 1.21 | tok/s 6877
step   1040 | loss 1.5763 | lr 4.61e-04 | grad 1.43 | tok/s 6835
step   1050 | loss 1.6836 | lr 4.61e-04 | grad 1.62 | tok/s 6645
step   1060 | loss 1.7688 | lr 4.61e-04 | grad 1.08 | tok/s 6939
step   1070 | loss 1.7625 | lr 4.61e-04 | grad 1.34 | tok/s 6802
step   1080 | loss 1.4715 | lr 4.61e-04 | grad 1.40 | tok/s 6393
step   1090 | loss 1.0872 | lr 4.61e-04 | grad 2.84 | tok/s 7087
step   1100 | loss 1.5832 | lr 4.61e-04 | grad 1.19 | tok/s 6811
step   1110 | loss 1.4661 | lr 4.61e-04 | grad 1.30 | tok/s 7116
step   1120 | loss 1.4006 | lr 4.61e-04 | grad 1.38 | tok/s 7114
step   1130 | loss 1.3348 | lr 4.61e-04 | grad 1.34 | tok/s 7116
step   1140 | loss 1.3389 | lr 4.61e-04 | grad 1.20 | tok/s 7115
step   1150 | loss 1.3377 | lr 4.61e-04 | grad 1.17 | tok/s 7113
step   1160 | loss 1.2651 | lr 4.61e-04 | grad 1.20 | tok/s 7114
step   1170 | loss 1.3359 | lr 4.61e-04 | grad 1.27 | tok/s 7105
step   1180 | loss 1.3646 | lr 4.61e-04 | grad 1.47 | tok/s 7103
step   1190 | loss 1.2750 | lr 4.61e-04 | grad 0.97 | tok/s 7101
step   1200 | loss 1.2897 | lr 4.61e-04 | grad 0.97 | tok/s 7101
step   1210 | loss 1.3181 | lr 4.61e-04 | grad 1.13 | tok/s 7098
step   1220 | loss 1.3245 | lr 4.61e-04 | grad 1.20 | tok/s 7100
step   1230 | loss 1.3193 | lr 4.61e-04 | grad 0.95 | tok/s 7102
step   1240 | loss 1.5244 | lr 4.61e-04 | grad 5.34 | tok/s 6946
step   1250 | loss 1.8078 | lr 4.61e-04 | grad 0.84 | tok/s 6751
step   1260 | loss 1.3901 | lr 4.61e-04 | grad 1.10 | tok/s 6650
step   1270 | loss 1.7827 | lr 4.61e-04 | grad 1.36 | tok/s 6568
step   1280 | loss 1.6285 | lr 4.61e-04 | grad 1.54 | tok/s 6957
step   1290 | loss 1.5578 | lr 4.61e-04 | grad 1.23 | tok/s 6845
step   1300 | loss 1.5633 | lr 4.61e-04 | grad 1.80 | tok/s 6732
step   1310 | loss 1.5082 | lr 4.61e-04 | grad 1.99 | tok/s 7065
step   1320 | loss 1.6732 | lr 4.61e-04 | grad 2.33 | tok/s 6978
step   1330 | loss 1.4920 | lr 4.61e-04 | grad 0.96 | tok/s 6983
step   1340 | loss 1.6965 | lr 4.61e-04 | grad 0.94 | tok/s 6467
step   1350 | loss 1.7802 | lr 4.61e-04 | grad 1.77 | tok/s 6593
step   1360 | loss 1.5375 | lr 4.61e-04 | grad 0.76 | tok/s 6892
step   1370 | loss 1.6406 | lr 4.61e-04 | grad 3.03 | tok/s 6757
step   1380 | loss 1.6332 | lr 4.61e-04 | grad 1.49 | tok/s 6495
step   1390 | loss 1.4723 | lr 4.61e-04 | grad 1.09 | tok/s 6742
step   1400 | loss 1.4389 | lr 4.61e-04 | grad 0.91 | tok/s 6804
step   1410 | loss 1.6293 | lr 4.61e-04 | grad 1.81 | tok/s 6663
step   1420 | loss 1.6816 | lr 4.61e-04 | grad 1.21 | tok/s 6664
step   1430 | loss 1.3959 | lr 4.61e-04 | grad 1.02 | tok/s 6755
step   1440 | loss 1.1739 | lr 4.61e-04 | grad 1.23 | tok/s 7102
step   1450 | loss 1.3455 | lr 4.61e-04 | grad 4.16 | tok/s 7000
step   1460 | loss 1.7266 | lr 4.61e-04 | grad 3.17 | tok/s 6621
step   1470 | loss 1.4863 | lr 4.61e-04 | grad 1.23 | tok/s 7040
step   1480 | loss 1.9420 | lr 4.61e-04 | grad 1.54 | tok/s 6960
step   1490 | loss 1.6447 | lr 4.61e-04 | grad 2.89 | tok/s 7068
step   1500 | loss 1.3286 | lr 4.61e-04 | grad 1.18 | tok/s 7099
step   1510 | loss 1.6398 | lr 4.61e-04 | grad 1.04 | tok/s 7001
step   1520 | loss 1.4733 | lr 4.61e-04 | grad 1.21 | tok/s 6862
step   1530 | loss 1.4211 | lr 4.61e-04 | grad 1.18 | tok/s 6881
step   1540 | loss 1.6826 | lr 4.61e-04 | grad 1.20 | tok/s 6742
step   1550 | loss 1.3136 | lr 4.61e-04 | grad 1.06 | tok/s 7045
step   1560 | loss 1.6446 | lr 4.61e-04 | grad 1.27 | tok/s 6674
step   1570 | loss 1.4052 | lr 4.61e-04 | grad 1.66 | tok/s 7029
step   1580 | loss 1.7824 | lr 4.61e-04 | grad 2.00 | tok/s 6998
step   1590 | loss 1.6113 | lr 4.61e-04 | grad 0.89 | tok/s 6660
step   1600 | loss 0.9114 | lr 4.61e-04 | grad 0.50 | tok/s 7118
step   1610 | loss 1.2208 | lr 4.61e-04 | grad 0.93 | tok/s 6577
step   1620 | loss 1.4765 | lr 4.61e-04 | grad 1.40 | tok/s 6732
step   1630 | loss 1.4761 | lr 4.61e-04 | grad 1.49 | tok/s 6894
step   1640 | loss 1.4997 | lr 4.61e-04 | grad 2.45 | tok/s 6677
step   1650 | loss 1.5902 | lr 4.61e-04 | grad 1.47 | tok/s 6318
step   1660 | loss 1.3307 | lr 4.61e-04 | grad 0.78 | tok/s 7101
step   1670 | loss 1.7179 | lr 4.61e-04 | grad 4.75 | tok/s 6746
step   1680 | loss 1.5899 | lr 4.61e-04 | grad 1.13 | tok/s 6597
step   1690 | loss 1.4760 | lr 4.61e-04 | grad 1.48 | tok/s 6849
step   1700 | loss 1.5605 | lr 4.61e-04 | grad 1.02 | tok/s 6653
step   1710 | loss 1.5372 | lr 4.61e-04 | grad 1.16 | tok/s 6877
step   1720 | loss 1.5760 | lr 4.61e-04 | grad 1.47 | tok/s 7091
step   1730 | loss 1.3182 | lr 4.61e-04 | grad 2.09 | tok/s 7095
step   1740 | loss 1.4879 | lr 4.61e-04 | grad 1.42 | tok/s 6793
step   1750 | loss 1.5566 | lr 4.61e-04 | grad 1.24 | tok/s 6923
step   1760 | loss 1.6064 | lr 4.61e-04 | grad 1.23 | tok/s 6828
step   1770 | loss 1.4595 | lr 4.61e-04 | grad 0.98 | tok/s 6704
step   1780 | loss 1.5168 | lr 4.61e-04 | grad 1.27 | tok/s 6867
step   1790 | loss 1.4556 | lr 4.61e-04 | grad 0.99 | tok/s 6816
step   1800 | loss 1.5922 | lr 4.61e-04 | grad 1.24 | tok/s 6663
step   1810 | loss 1.5420 | lr 4.61e-04 | grad 2.12 | tok/s 6752
step   1820 | loss 1.5158 | lr 4.61e-04 | grad 2.98 | tok/s 6808
step   1830 | loss 1.5048 | lr 4.61e-04 | grad 1.12 | tok/s 6932
step   1840 | loss 1.4871 | lr 4.61e-04 | grad 0.73 | tok/s 6608
step   1850 | loss 1.3583 | lr 4.61e-04 | grad 0.88 | tok/s 7069
step   1860 | loss 1.3846 | lr 4.61e-04 | grad 1.13 | tok/s 6613
step   1870 | loss 1.4146 | lr 4.61e-04 | grad 0.62 | tok/s 6964
step   1880 | loss 1.3469 | lr 4.61e-04 | grad 1.27 | tok/s 6301
step   1890 | loss 1.5598 | lr 4.61e-04 | grad 0.91 | tok/s 6656
step   1900 | loss 1.3831 | lr 4.61e-04 | grad 0.88 | tok/s 6741
step   1910 | loss 1.5053 | lr 4.61e-04 | grad 1.09 | tok/s 6566
step   1920 | loss 1.3865 | lr 4.61e-04 | grad 1.01 | tok/s 7030
step   1930 | loss 1.5021 | lr 4.61e-04 | grad 0.96 | tok/s 6604
step   1940 | loss 1.4967 | lr 4.61e-04 | grad 1.47 | tok/s 6975
step   1950 | loss 1.9383 | lr 4.61e-04 | grad 2.06 | tok/s 7098
step   1960 | loss 1.5816 | lr 4.61e-04 | grad 1.66 | tok/s 7101
step   1970 | loss 1.6282 | lr 4.61e-04 | grad 1.90 | tok/s 6867
step   1980 | loss 1.5330 | lr 4.61e-04 | grad 0.90 | tok/s 6679
step   1990 | loss 1.6738 | lr 4.61e-04 | grad 0.95 | tok/s 6740
step   2000 | loss 1.5208 | lr 4.61e-04 | grad 1.13 | tok/s 6843
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5208.pt
step   2010 | loss 1.2596 | lr 4.61e-04 | grad 1.10 | tok/s 3734
step   2020 | loss 1.3533 | lr 4.61e-04 | grad 1.26 | tok/s 6945
step   2030 | loss 1.0633 | lr 4.61e-04 | grad 1.10 | tok/s 7133
step   2040 | loss 1.2744 | lr 4.61e-04 | grad 0.84 | tok/s 7120
step   2050 | loss 1.4016 | lr 4.61e-04 | grad 1.55 | tok/s 6829
step   2060 | loss 1.6924 | lr 4.61e-04 | grad 1.10 | tok/s 6721
step   2070 | loss 1.9264 | lr 4.61e-04 | grad 3.25 | tok/s 6751
step   2080 | loss 2.0123 | lr 4.61e-04 | grad 2.19 | tok/s 7104
step   2090 | loss 1.6216 | lr 4.61e-04 | grad 2.08 | tok/s 6919
step   2100 | loss 1.3908 | lr 4.61e-04 | grad 1.36 | tok/s 7026
step   2110 | loss 1.5044 | lr 4.61e-04 | grad 0.99 | tok/s 6625
step   2120 | loss 0.7694 | lr 4.61e-04 | grad 0.79 | tok/s 7132
step   2130 | loss 1.4807 | lr 4.61e-04 | grad 1.23 | tok/s 6708
step   2140 | loss 1.4372 | lr 4.61e-04 | grad 1.26 | tok/s 7016
step   2150 | loss 1.2904 | lr 4.61e-04 | grad 1.17 | tok/s 7101
step   2160 | loss 1.2238 | lr 4.61e-04 | grad 0.97 | tok/s 7104
step   2170 | loss 1.2224 | lr 4.61e-04 | grad 1.01 | tok/s 7099
step   2180 | loss 1.2295 | lr 4.61e-04 | grad 1.06 | tok/s 7101
step   2190 | loss 1.2516 | lr 4.61e-04 | grad 1.19 | tok/s 7102
step   2200 | loss 1.1865 | lr 4.61e-04 | grad 0.81 | tok/s 7098
step   2210 | loss 1.1755 | lr 4.61e-04 | grad 0.93 | tok/s 7099
step   2220 | loss 1.1502 | lr 4.61e-04 | grad 0.77 | tok/s 7101
step   2230 | loss 1.4729 | lr 4.61e-04 | grad 1.04 | tok/s 6966
step   2240 | loss 1.3907 | lr 4.61e-04 | grad 1.44 | tok/s 6845
step   2250 | loss 1.5240 | lr 4.61e-04 | grad 1.70 | tok/s 7096
step   2260 | loss 1.6421 | lr 4.61e-04 | grad 0.91 | tok/s 6856
step   2270 | loss 2.0308 | lr 4.61e-04 | grad 1.17 | tok/s 7015
step   2280 | loss 1.4206 | lr 4.61e-04 | grad 1.18 | tok/s 7094
step   2290 | loss 1.6244 | lr 4.61e-04 | grad 1.02 | tok/s 6769
step   2300 | loss 1.5796 | lr 4.61e-04 | grad 1.50 | tok/s 6896
step   2310 | loss 1.5141 | lr 4.61e-04 | grad 2.45 | tok/s 6743
step   2320 | loss 1.9589 | lr 4.61e-04 | grad 2.88 | tok/s 6716
step   2330 | loss 1.4700 | lr 4.61e-04 | grad 1.16 | tok/s 6557
step   2340 | loss 1.5451 | lr 4.61e-04 | grad 1.30 | tok/s 6755
step   2350 | loss 1.3852 | lr 4.61e-04 | grad 1.06 | tok/s 6953
step   2360 | loss 1.2555 | lr 4.61e-04 | grad 1.02 | tok/s 7039
step   2370 | loss 1.6784 | lr 4.61e-04 | grad 1.31 | tok/s 7009
step   2380 | loss 1.4486 | lr 4.61e-04 | grad 1.88 | tok/s 7100
step   2390 | loss 1.1491 | lr 4.61e-04 | grad 0.73 | tok/s 7085
step   2400 | loss 1.0619 | lr 4.61e-04 | grad 1.41 | tok/s 7098
step   2410 | loss 1.2834 | lr 4.61e-04 | grad 1.35 | tok/s 6771
step   2420 | loss 1.5544 | lr 4.61e-04 | grad 1.20 | tok/s 6445
step   2430 | loss 1.2777 | lr 4.61e-04 | grad 1.04 | tok/s 7063
step   2440 | loss 1.4830 | lr 4.61e-04 | grad 0.98 | tok/s 6830
step   2450 | loss 1.4913 | lr 4.61e-04 | grad 1.33 | tok/s 6814
step   2460 | loss 1.1254 | lr 4.61e-04 | grad 0.94 | tok/s 7100
step   2470 | loss 1.2496 | lr 4.61e-04 | grad 1.07 | tok/s 7000
step   2480 | loss 1.3226 | lr 4.61e-04 | grad 1.28 | tok/s 6984
step   2490 | loss 1.4746 | lr 4.61e-04 | grad 1.05 | tok/s 6767
step   2500 | loss 1.4410 | lr 4.61e-04 | grad 1.26 | tok/s 7005
step   2510 | loss 1.0948 | lr 4.61e-04 | grad 1.78 | tok/s 7098
step   2520 | loss 1.6320 | lr 4.61e-04 | grad 1.30 | tok/s 6977
step   2530 | loss 1.3402 | lr 4.61e-04 | grad 0.96 | tok/s 6788
step   2540 | loss 1.3993 | lr 4.61e-04 | grad 0.90 | tok/s 6890
step   2550 | loss 1.1753 | lr 4.61e-04 | grad 1.05 | tok/s 7025
step   2560 | loss 1.5778 | lr 4.61e-04 | grad 1.18 | tok/s 6494
step   2570 | loss 1.3193 | lr 4.61e-04 | grad 0.99 | tok/s 6664
step   2580 | loss 1.3678 | lr 4.61e-04 | grad 1.34 | tok/s 6899
step   2590 | loss 1.4861 | lr 4.61e-04 | grad 0.85 | tok/s 6479
step   2600 | loss 1.7105 | lr 4.61e-04 | grad 1.48 | tok/s 6857
step   2610 | loss 1.3368 | lr 4.61e-04 | grad 1.77 | tok/s 6916
step   2620 | loss 1.5596 | lr 4.61e-04 | grad 1.74 | tok/s 6954
step   2630 | loss 1.4078 | lr 4.61e-04 | grad 2.25 | tok/s 7047
step   2640 | loss 1.5875 | lr 4.61e-04 | grad 0.88 | tok/s 6844
step   2650 | loss 1.4589 | lr 4.61e-04 | grad 1.08 | tok/s 6940
step   2660 | loss 1.3201 | lr 4.61e-04 | grad 1.02 | tok/s 6814
step   2670 | loss 1.4892 | lr 4.61e-04 | grad 1.03 | tok/s 6643
step   2680 | loss 1.6744 | lr 4.61e-04 | grad 0.71 | tok/s 6818
step   2690 | loss 1.3279 | lr 4.61e-04 | grad 1.52 | tok/s 7049
step   2700 | loss 1.4675 | lr 4.61e-04 | grad 1.12 | tok/s 6695
step   2710 | loss 1.5052 | lr 4.61e-04 | grad 1.20 | tok/s 6669
step   2720 | loss 1.3919 | lr 4.61e-04 | grad 1.27 | tok/s 6578
step   2730 | loss 1.1877 | lr 4.61e-04 | grad 1.51 | tok/s 7032
step   2740 | loss 1.8002 | lr 4.61e-04 | grad 1.38 | tok/s 6933
step   2750 | loss 1.5551 | lr 4.61e-04 | grad 3.31 | tok/s 6930
step   2760 | loss 1.3706 | lr 4.61e-04 | grad 1.23 | tok/s 6415
step   2770 | loss 1.4183 | lr 4.61e-04 | grad 0.85 | tok/s 6975
step   2780 | loss 1.2378 | lr 4.61e-04 | grad 1.15 | tok/s 7010
step   2790 | loss 1.9752 | lr 4.61e-04 | grad 1.64 | tok/s 6397
step   2800 | loss 1.1727 | lr 4.61e-04 | grad 0.99 | tok/s 7101
step   2810 | loss 1.3800 | lr 4.61e-04 | grad 0.99 | tok/s 6541
step   2820 | loss 1.4724 | lr 4.61e-04 | grad 2.02 | tok/s 6648
step   2830 | loss 1.0309 | lr 4.61e-04 | grad 3.08 | tok/s 7098
step   2840 | loss 1.1752 | lr 4.61e-04 | grad 2.88 | tok/s 6952
step   2850 | loss 1.7640 | lr 4.61e-04 | grad 3.39 | tok/s 6804
step   2860 | loss 1.5968 | lr 4.61e-04 | grad 1.42 | tok/s 6736
step   2870 | loss 1.4675 | lr 4.61e-04 | grad 1.79 | tok/s 6874
step   2880 | loss 1.3510 | lr 4.61e-04 | grad 1.10 | tok/s 7024
step   2890 | loss 1.4443 | lr 4.61e-04 | grad 1.38 | tok/s 6938
step   2900 | loss 1.4618 | lr 4.61e-04 | grad 2.66 | tok/s 6943
step   2910 | loss 1.4201 | lr 4.61e-04 | grad 1.15 | tok/s 6641
step   2920 | loss 1.7137 | lr 4.61e-04 | grad 2.92 | tok/s 6826
step   2930 | loss 1.3977 | lr 4.61e-04 | grad 1.02 | tok/s 6704
step   2940 | loss 1.3112 | lr 4.61e-04 | grad 0.97 | tok/s 6464
step   2950 | loss 1.3200 | lr 4.61e-04 | grad 1.20 | tok/s 6962
step   2960 | loss 1.3448 | lr 4.61e-04 | grad 1.10 | tok/s 6937
step   2970 | loss 1.6802 | lr 4.61e-04 | grad 2.83 | tok/s 6721
step   2980 | loss 2.0731 | lr 4.61e-04 | grad 5.69 | tok/s 6952
step   2990 | loss 1.5520 | lr 4.61e-04 | grad 1.02 | tok/s 6946
step   3000 | loss 1.3828 | lr 4.61e-04 | grad 1.09 | tok/s 6766
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3828.pt
step   3010 | loss 1.3771 | lr 4.61e-04 | grad 1.21 | tok/s 3662
step   3020 | loss 1.4337 | lr 4.61e-04 | grad 1.03 | tok/s 6914
step   3030 | loss 1.4759 | lr 4.61e-04 | grad 0.95 | tok/s 6635
step   3040 | loss 1.4507 | lr 4.61e-04 | grad 0.83 | tok/s 7067
step   3050 | loss 1.3724 | lr 4.61e-04 | grad 1.77 | tok/s 6883

Training complete! Final step: 3057
