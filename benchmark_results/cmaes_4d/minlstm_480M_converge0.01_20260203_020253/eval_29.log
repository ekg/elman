Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_29/levelminlstm_100m_20260203_033415
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 940,167,168 parameters
Using schedule-free AdamW (lr=0.00038935434480308996)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 6.6380 | lr 3.89e-04 | grad 9.56 | tok/s 7453
step     20 | loss 3.3376 | lr 3.89e-04 | grad 6.69 | tok/s 7744
step     30 | loss 3.1199 | lr 3.89e-04 | grad 16.62 | tok/s 7771
step     40 | loss 3.0441 | lr 3.89e-04 | grad 3.27 | tok/s 7401
step     50 | loss 2.9356 | lr 3.89e-04 | grad 13.81 | tok/s 7373
step     60 | loss 3.6141 | lr 3.89e-04 | grad 2.44 | tok/s 7831
step     70 | loss 2.7894 | lr 3.89e-04 | grad 5.34 | tok/s 7845
step     80 | loss 4.7084 | lr 3.89e-04 | grad 13.50 | tok/s 7814
step     90 | loss 5.4958 | lr 3.89e-04 | grad 11.56 | tok/s 8032
step    100 | loss 4.1162 | lr 3.89e-04 | grad 6.16 | tok/s 8025
step    110 | loss 3.9178 | lr 3.89e-04 | grad 9.81 | tok/s 8023
step    120 | loss 3.8499 | lr 3.89e-04 | grad 6.75 | tok/s 8022
step    130 | loss 3.9769 | lr 3.89e-04 | grad 4.78 | tok/s 8016
step    140 | loss 3.4493 | lr 3.89e-04 | grad 9.38 | tok/s 8015
step    150 | loss 3.6421 | lr 3.89e-04 | grad 10.38 | tok/s 8006
step    160 | loss 3.4871 | lr 3.89e-04 | grad 5.12 | tok/s 8005
step    170 | loss 3.2996 | lr 3.89e-04 | grad 5.66 | tok/s 8001
step    180 | loss 3.3924 | lr 3.89e-04 | grad 4.38 | tok/s 7999
step    190 | loss 3.2683 | lr 3.89e-04 | grad 5.97 | tok/s 7988
step    200 | loss 3.2647 | lr 3.89e-04 | grad 3.44 | tok/s 7988
step    210 | loss 3.0504 | lr 3.89e-04 | grad 4.81 | tok/s 7984
step    220 | loss 2.9747 | lr 3.89e-04 | grad 6.75 | tok/s 7879
step    230 | loss 3.1507 | lr 3.89e-04 | grad 8.75 | tok/s 7890
step    240 | loss 3.3135 | lr 3.89e-04 | grad 3.64 | tok/s 7465
step    250 | loss 2.7386 | lr 3.89e-04 | grad 2.52 | tok/s 7527
step    260 | loss 2.5565 | lr 3.89e-04 | grad 3.19 | tok/s 7900
step    270 | loss 2.6576 | lr 3.89e-04 | grad 5.50 | tok/s 7612
step    280 | loss 2.6141 | lr 3.89e-04 | grad 5.97 | tok/s 7811
step    290 | loss 2.9778 | lr 3.89e-04 | grad 2.83 | tok/s 7777
step    300 | loss 2.1893 | lr 3.89e-04 | grad 2.80 | tok/s 8003
step    310 | loss 2.6898 | lr 3.89e-04 | grad 3.59 | tok/s 7855
step    320 | loss 2.8274 | lr 3.89e-04 | grad 2.64 | tok/s 7974
step    330 | loss 2.5355 | lr 3.89e-04 | grad 3.06 | tok/s 7206
step    340 | loss 2.7289 | lr 3.89e-04 | grad 1.61 | tok/s 7650
step    350 | loss 2.5500 | lr 3.89e-04 | grad 3.03 | tok/s 7572
step    360 | loss 2.9938 | lr 3.89e-04 | grad 2.16 | tok/s 7971
step    370 | loss 2.6046 | lr 3.89e-04 | grad 3.14 | tok/s 7326
step    380 | loss 2.4157 | lr 3.89e-04 | grad 1.91 | tok/s 7488
step    390 | loss 2.3081 | lr 3.89e-04 | grad 1.83 | tok/s 7890
step    400 | loss 2.2683 | lr 3.89e-04 | grad 3.53 | tok/s 7902
step    410 | loss 2.3691 | lr 3.89e-04 | grad 2.33 | tok/s 7973
step    420 | loss 2.2036 | lr 3.89e-04 | grad 2.05 | tok/s 7290
step    430 | loss 2.6285 | lr 3.89e-04 | grad 4.22 | tok/s 7762
step    440 | loss 2.7371 | lr 3.89e-04 | grad 2.55 | tok/s 7626
step    450 | loss 2.8232 | lr 3.89e-04 | grad 9.06 | tok/s 7637
step    460 | loss 2.4888 | lr 3.89e-04 | grad 2.14 | tok/s 7480
step    470 | loss 2.4718 | lr 3.89e-04 | grad 1.81 | tok/s 7611
step    480 | loss 2.4831 | lr 3.89e-04 | grad 2.19 | tok/s 7762
step    490 | loss 2.8677 | lr 3.89e-04 | grad 2.25 | tok/s 7663
step    500 | loss 2.2396 | lr 3.89e-04 | grad 3.16 | tok/s 7552
step    510 | loss 2.4094 | lr 3.89e-04 | grad 2.02 | tok/s 7874
step    520 | loss 2.4020 | lr 3.89e-04 | grad 2.12 | tok/s 7911
step    530 | loss 2.5240 | lr 3.89e-04 | grad 2.25 | tok/s 7828
step    540 | loss 2.3120 | lr 3.89e-04 | grad 1.74 | tok/s 7626
step    550 | loss 2.1542 | lr 3.89e-04 | grad 2.38 | tok/s 7624
step    560 | loss 2.2344 | lr 3.89e-04 | grad 5.06 | tok/s 6975
step    570 | loss 2.2887 | lr 3.89e-04 | grad 1.98 | tok/s 7626
step    580 | loss 2.2006 | lr 3.89e-04 | grad 1.95 | tok/s 7477
step    590 | loss 2.1425 | lr 3.89e-04 | grad 1.49 | tok/s 7385
step    600 | loss 2.6020 | lr 3.89e-04 | grad 2.27 | tok/s 7565
step    610 | loss 2.2476 | lr 3.89e-04 | grad 2.62 | tok/s 7575
step    620 | loss 2.0982 | lr 3.89e-04 | grad 2.94 | tok/s 7530
step    630 | loss 2.1930 | lr 3.89e-04 | grad 3.58 | tok/s 7379
step    640 | loss 2.4230 | lr 3.89e-04 | grad 3.45 | tok/s 7545
step    650 | loss 2.3143 | lr 3.89e-04 | grad 2.69 | tok/s 7618
step    660 | loss 2.2580 | lr 3.89e-04 | grad 2.42 | tok/s 7744
step    670 | loss 2.1794 | lr 3.89e-04 | grad 2.11 | tok/s 7557
step    680 | loss 2.6406 | lr 3.89e-04 | grad 2.00 | tok/s 7873
step    690 | loss 2.4056 | lr 3.89e-04 | grad 1.94 | tok/s 7472
step    700 | loss 2.5277 | lr 3.89e-04 | grad 2.31 | tok/s 7971
step    710 | loss 2.3433 | lr 3.89e-04 | grad 2.27 | tok/s 7755
step    720 | loss 1.9738 | lr 3.89e-04 | grad 2.12 | tok/s 7042
step    730 | loss 2.3310 | lr 3.89e-04 | grad 2.39 | tok/s 7979
step    740 | loss 2.1332 | lr 3.89e-04 | grad 1.77 | tok/s 7846
step    750 | loss 2.2084 | lr 3.89e-04 | grad 2.22 | tok/s 7984
step    760 | loss 2.0097 | lr 3.89e-04 | grad 1.72 | tok/s 7983
step    770 | loss 2.0624 | lr 3.89e-04 | grad 2.36 | tok/s 7987
step    780 | loss 2.0044 | lr 3.89e-04 | grad 2.30 | tok/s 7985
step    790 | loss 1.9367 | lr 3.89e-04 | grad 1.62 | tok/s 7989
step    800 | loss 2.2155 | lr 3.89e-04 | grad 3.11 | tok/s 7468
step    810 | loss 2.4227 | lr 3.89e-04 | grad 2.34 | tok/s 7669
step    820 | loss 2.1914 | lr 3.89e-04 | grad 2.50 | tok/s 7572
step    830 | loss 2.2241 | lr 3.89e-04 | grad 2.06 | tok/s 7710
step    840 | loss 2.3551 | lr 3.89e-04 | grad 2.77 | tok/s 7980
step    850 | loss 2.3470 | lr 3.89e-04 | grad 6.66 | tok/s 7949
step    860 | loss 2.2397 | lr 3.89e-04 | grad 2.31 | tok/s 7915
step    870 | loss 2.1571 | lr 3.89e-04 | grad 1.45 | tok/s 7637
step    880 | loss 2.2236 | lr 3.89e-04 | grad 2.70 | tok/s 7579
step    890 | loss 2.2600 | lr 3.89e-04 | grad 1.23 | tok/s 7727
step    900 | loss 2.1965 | lr 3.89e-04 | grad 2.23 | tok/s 7752
step    910 | loss 1.9820 | lr 3.89e-04 | grad 1.58 | tok/s 7538
step    920 | loss 2.2880 | lr 3.89e-04 | grad 2.31 | tok/s 7894
step    930 | loss 2.1459 | lr 3.89e-04 | grad 2.27 | tok/s 7596
step    940 | loss 2.2158 | lr 3.89e-04 | grad 2.52 | tok/s 7693
step    950 | loss 2.2725 | lr 3.89e-04 | grad 2.03 | tok/s 7954
step    960 | loss 2.1360 | lr 3.89e-04 | grad 2.03 | tok/s 7993
step    970 | loss 2.1695 | lr 3.89e-04 | grad 1.48 | tok/s 7639
step    980 | loss 2.2967 | lr 3.89e-04 | grad 1.30 | tok/s 7689
step    990 | loss 2.0646 | lr 3.89e-04 | grad 1.77 | tok/s 7667
step   1000 | loss 2.1525 | lr 3.89e-04 | grad 1.90 | tok/s 7581
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1525.pt
step   1010 | loss 2.1765 | lr 3.89e-04 | grad 3.31 | tok/s 2625
step   1020 | loss 2.1028 | lr 3.89e-04 | grad 2.11 | tok/s 7347
step   1030 | loss 2.0286 | lr 3.89e-04 | grad 2.06 | tok/s 7770
step   1040 | loss 2.0612 | lr 3.89e-04 | grad 2.30 | tok/s 7719
step   1050 | loss 2.1067 | lr 3.89e-04 | grad 2.53 | tok/s 7500
step   1060 | loss 2.2890 | lr 3.89e-04 | grad 2.38 | tok/s 7824
step   1070 | loss 2.3859 | lr 3.89e-04 | grad 2.38 | tok/s 7672
step   1080 | loss 1.9312 | lr 3.89e-04 | grad 2.06 | tok/s 7214
step   1090 | loss 1.5884 | lr 3.89e-04 | grad 2.05 | tok/s 8006
step   1100 | loss 2.0809 | lr 3.89e-04 | grad 1.83 | tok/s 7670
step   1110 | loss 2.0562 | lr 3.89e-04 | grad 2.22 | tok/s 8002
step   1120 | loss 2.0333 | lr 3.89e-04 | grad 2.44 | tok/s 8006
step   1130 | loss 1.9728 | lr 3.89e-04 | grad 2.34 | tok/s 8002
step   1140 | loss 1.9448 | lr 3.89e-04 | grad 2.64 | tok/s 8008
step   1150 | loss 1.9429 | lr 3.89e-04 | grad 1.91 | tok/s 8004
step   1160 | loss 1.8378 | lr 3.89e-04 | grad 1.99 | tok/s 8004
step   1170 | loss 1.9311 | lr 3.89e-04 | grad 1.98 | tok/s 8000
step   1180 | loss 1.9959 | lr 3.89e-04 | grad 2.11 | tok/s 7992
step   1190 | loss 1.9211 | lr 3.89e-04 | grad 1.76 | tok/s 7990
step   1200 | loss 1.9192 | lr 3.89e-04 | grad 1.88 | tok/s 7990
step   1210 | loss 1.8902 | lr 3.89e-04 | grad 2.50 | tok/s 7991
step   1220 | loss 1.8625 | lr 3.89e-04 | grad 2.80 | tok/s 7990
step   1230 | loss 1.8820 | lr 3.89e-04 | grad 2.06 | tok/s 7990
step   1240 | loss 2.0373 | lr 3.89e-04 | grad 5.62 | tok/s 7817
step   1250 | loss 2.2618 | lr 3.89e-04 | grad 1.70 | tok/s 7601
step   1260 | loss 1.7594 | lr 3.89e-04 | grad 1.78 | tok/s 7498
step   1270 | loss 2.1602 | lr 3.89e-04 | grad 1.83 | tok/s 7390
step   1280 | loss 2.1667 | lr 3.89e-04 | grad 1.96 | tok/s 7831
step   1290 | loss 2.0379 | lr 3.89e-04 | grad 2.47 | tok/s 7703
step   1300 | loss 2.0228 | lr 3.89e-04 | grad 3.14 | tok/s 7585
step   1310 | loss 2.0243 | lr 3.89e-04 | grad 3.20 | tok/s 7952
step   1320 | loss 2.1922 | lr 3.89e-04 | grad 2.75 | tok/s 7853
step   1330 | loss 2.0512 | lr 3.89e-04 | grad 2.55 | tok/s 7859
step   1340 | loss 2.1445 | lr 3.89e-04 | grad 1.07 | tok/s 7282
step   1350 | loss 2.1783 | lr 3.89e-04 | grad 2.53 | tok/s 7426
step   1360 | loss 2.0880 | lr 3.89e-04 | grad 2.59 | tok/s 7756
step   1370 | loss 2.0885 | lr 3.89e-04 | grad 2.55 | tok/s 7608
step   1380 | loss 2.0874 | lr 3.89e-04 | grad 1.67 | tok/s 7318
step   1390 | loss 1.9053 | lr 3.89e-04 | grad 2.23 | tok/s 7590
step   1400 | loss 2.0599 | lr 3.89e-04 | grad 2.17 | tok/s 7657
step   1410 | loss 2.1687 | lr 3.89e-04 | grad 2.66 | tok/s 7507
step   1420 | loss 2.0919 | lr 3.89e-04 | grad 2.02 | tok/s 7509
step   1430 | loss 1.9701 | lr 3.89e-04 | grad 2.03 | tok/s 7607
step   1440 | loss 1.8766 | lr 3.89e-04 | grad 1.84 | tok/s 7984
step   1450 | loss 1.9437 | lr 3.89e-04 | grad 5.84 | tok/s 7884
step   1460 | loss 2.1698 | lr 3.89e-04 | grad 4.38 | tok/s 7458
step   1470 | loss 2.0388 | lr 3.89e-04 | grad 2.12 | tok/s 7916
step   1480 | loss 2.7144 | lr 3.89e-04 | grad 2.42 | tok/s 7833
step   1490 | loss 2.3492 | lr 3.89e-04 | grad 3.23 | tok/s 7952
step   1500 | loss 1.9484 | lr 3.89e-04 | grad 2.77 | tok/s 7987
step   1510 | loss 2.1423 | lr 3.89e-04 | grad 2.44 | tok/s 7879
step   1520 | loss 1.9613 | lr 3.89e-04 | grad 2.16 | tok/s 7722
step   1530 | loss 1.9088 | lr 3.89e-04 | grad 1.60 | tok/s 7740
step   1540 | loss 2.1445 | lr 3.89e-04 | grad 1.59 | tok/s 7597
step   1550 | loss 1.8866 | lr 3.89e-04 | grad 1.86 | tok/s 7924
step   1560 | loss 2.0372 | lr 3.89e-04 | grad 2.14 | tok/s 7514
step   1570 | loss 1.9947 | lr 3.89e-04 | grad 2.06 | tok/s 7902
step   1580 | loss 2.6112 | lr 3.89e-04 | grad 2.33 | tok/s 7872
step   1590 | loss 2.1219 | lr 3.89e-04 | grad 3.12 | tok/s 7496
step   1600 | loss 1.3909 | lr 3.89e-04 | grad 1.00 | tok/s 8039
step   1610 | loss 1.6462 | lr 3.89e-04 | grad 1.69 | tok/s 7405
step   1620 | loss 2.0703 | lr 3.89e-04 | grad 2.34 | tok/s 7577
step   1630 | loss 2.0179 | lr 3.89e-04 | grad 3.39 | tok/s 7753
step   1640 | loss 1.9375 | lr 3.89e-04 | grad 2.83 | tok/s 7517
step   1650 | loss 2.0281 | lr 3.89e-04 | grad 2.41 | tok/s 7114
step   1660 | loss 1.9672 | lr 3.89e-04 | grad 2.06 | tok/s 7982
step   1670 | loss 2.2370 | lr 3.89e-04 | grad 4.78 | tok/s 7590
step   1680 | loss 1.9896 | lr 3.89e-04 | grad 2.19 | tok/s 7425
step   1690 | loss 2.0682 | lr 3.89e-04 | grad 2.42 | tok/s 7705
step   1700 | loss 2.0587 | lr 3.89e-04 | grad 1.52 | tok/s 7486
step   1710 | loss 2.0556 | lr 3.89e-04 | grad 2.14 | tok/s 7732
step   1720 | loss 2.2118 | lr 3.89e-04 | grad 2.06 | tok/s 7990
step   1730 | loss 2.2445 | lr 3.89e-04 | grad 2.16 | tok/s 7990
step   1740 | loss 2.0633 | lr 3.89e-04 | grad 1.86 | tok/s 7647
step   1750 | loss 2.0119 | lr 3.89e-04 | grad 1.65 | tok/s 7789
step   1760 | loss 2.0391 | lr 3.89e-04 | grad 1.78 | tok/s 7684
step   1770 | loss 1.8952 | lr 3.89e-04 | grad 1.45 | tok/s 7550
step   1780 | loss 1.9491 | lr 3.89e-04 | grad 2.38 | tok/s 7728
step   1790 | loss 1.9187 | lr 3.89e-04 | grad 1.65 | tok/s 7667
step   1800 | loss 2.0116 | lr 3.89e-04 | grad 1.54 | tok/s 7498
step   1810 | loss 2.0378 | lr 3.89e-04 | grad 2.67 | tok/s 7589
step   1820 | loss 2.0206 | lr 3.89e-04 | grad 3.02 | tok/s 7660
step   1830 | loss 1.9263 | lr 3.89e-04 | grad 1.33 | tok/s 7793
step   1840 | loss 1.9323 | lr 3.89e-04 | grad 2.86 | tok/s 7437
step   1850 | loss 2.0445 | lr 3.89e-04 | grad 1.87 | tok/s 7947
step   1860 | loss 1.7935 | lr 3.89e-04 | grad 2.06 | tok/s 7432
step   1870 | loss 1.9404 | lr 3.89e-04 | grad 2.27 | tok/s 7830
step   1880 | loss 1.7999 | lr 3.89e-04 | grad 1.63 | tok/s 7095
step   1890 | loss 2.0493 | lr 3.89e-04 | grad 1.91 | tok/s 7492
step   1900 | loss 1.8366 | lr 3.89e-04 | grad 1.76 | tok/s 7582
step   1910 | loss 1.9180 | lr 3.89e-04 | grad 3.28 | tok/s 7391
step   1920 | loss 1.8843 | lr 3.89e-04 | grad 1.69 | tok/s 7897
step   1930 | loss 1.9160 | lr 3.89e-04 | grad 1.72 | tok/s 7429
step   1940 | loss 1.9181 | lr 3.89e-04 | grad 1.91 | tok/s 7843
step   1950 | loss 2.6994 | lr 3.89e-04 | grad 2.38 | tok/s 7986
step   1960 | loss 2.3288 | lr 3.89e-04 | grad 3.20 | tok/s 7983
step   1970 | loss 2.1913 | lr 3.89e-04 | grad 2.56 | tok/s 7726
step   1980 | loss 1.9970 | lr 3.89e-04 | grad 1.92 | tok/s 7523
step   1990 | loss 2.1394 | lr 3.89e-04 | grad 1.87 | tok/s 7588
step   2000 | loss 1.9213 | lr 3.89e-04 | grad 1.89 | tok/s 7693
  >>> saved checkpoint: checkpoint_step_002000_loss_1.9213.pt
step   2010 | loss 1.8312 | lr 3.89e-04 | grad 1.95 | tok/s 2718
step   2020 | loss 1.8496 | lr 3.89e-04 | grad 1.41 | tok/s 7956
step   2030 | loss 1.5708 | lr 3.89e-04 | grad 1.77 | tok/s 8078
step   2040 | loss 2.0315 | lr 3.89e-04 | grad 2.42 | tok/s 8034
step   2050 | loss 2.0111 | lr 3.89e-04 | grad 1.68 | tok/s 7256
step   2060 | loss 2.1195 | lr 3.89e-04 | grad 1.80 | tok/s 7853
step   2070 | loss 2.4317 | lr 3.89e-04 | grad 3.42 | tok/s 7765
step   2080 | loss 2.2967 | lr 3.89e-04 | grad 1.80 | tok/s 7993
step   2090 | loss 1.9937 | lr 3.89e-04 | grad 1.95 | tok/s 7725
step   2100 | loss 2.0838 | lr 3.89e-04 | grad 1.49 | tok/s 7651
step   2110 | loss 1.6347 | lr 3.89e-04 | grad 2.44 | tok/s 7846
step   2120 | loss 1.2991 | lr 3.89e-04 | grad 2.34 | tok/s 7977
step   2130 | loss 2.0858 | lr 3.89e-04 | grad 1.66 | tok/s 7556
step   2140 | loss 1.7987 | lr 3.89e-04 | grad 2.23 | tok/s 7990
step   2150 | loss 1.7510 | lr 3.89e-04 | grad 2.11 | tok/s 7998
step   2160 | loss 1.7482 | lr 3.89e-04 | grad 2.27 | tok/s 7990
step   2170 | loss 1.7334 | lr 3.89e-04 | grad 2.23 | tok/s 7991
step   2180 | loss 1.7146 | lr 3.89e-04 | grad 2.28 | tok/s 7991
step   2190 | loss 1.7058 | lr 3.89e-04 | grad 2.27 | tok/s 7991
step   2200 | loss 1.6593 | lr 3.89e-04 | grad 2.38 | tok/s 7989
step   2210 | loss 1.6520 | lr 3.89e-04 | grad 2.62 | tok/s 7990
step   2220 | loss 1.7534 | lr 3.89e-04 | grad 2.58 | tok/s 7919
step   2230 | loss 1.8909 | lr 3.89e-04 | grad 1.68 | tok/s 7906
step   2240 | loss 2.1126 | lr 3.89e-04 | grad 2.06 | tok/s 7705
step   2250 | loss 2.2711 | lr 3.89e-04 | grad 3.64 | tok/s 7823
step   2260 | loss 2.2453 | lr 3.89e-04 | grad 3.78 | tok/s 7871
step   2270 | loss 2.1330 | lr 3.89e-04 | grad 2.23 | tok/s 7895
step   2280 | loss 1.9296 | lr 3.89e-04 | grad 1.77 | tok/s 7990
step   2290 | loss 2.4467 | lr 3.89e-04 | grad 3.42 | tok/s 7592
step   2300 | loss 2.1152 | lr 3.89e-04 | grad 2.16 | tok/s 7487
step   2310 | loss 2.0680 | lr 3.89e-04 | grad 2.97 | tok/s 7718
step   2320 | loss 2.2036 | lr 3.89e-04 | grad 1.58 | tok/s 7432
step   2330 | loss 1.8483 | lr 3.89e-04 | grad 2.64 | tok/s 7490
step   2340 | loss 1.9455 | lr 3.89e-04 | grad 2.27 | tok/s 7824
step   2350 | loss 1.9259 | lr 3.89e-04 | grad 2.27 | tok/s 7823
step   2360 | loss 1.9350 | lr 3.89e-04 | grad 2.81 | tok/s 7821
step   2370 | loss 2.2041 | lr 3.89e-04 | grad 2.11 | tok/s 7987
step   2380 | loss 1.9053 | lr 3.89e-04 | grad 2.39 | tok/s 7975
step   2390 | loss 1.7224 | lr 3.89e-04 | grad 1.89 | tok/s 7987
step   2400 | loss 1.5377 | lr 3.89e-04 | grad 2.06 | tok/s 7920
step   2410 | loss 1.8504 | lr 3.89e-04 | grad 2.89 | tok/s 7439
step   2420 | loss 1.9466 | lr 3.89e-04 | grad 1.87 | tok/s 7533
step   2430 | loss 1.7245 | lr 3.89e-04 | grad 1.45 | tok/s 7862
step   2440 | loss 1.9006 | lr 3.89e-04 | grad 2.06 | tok/s 7564
step   2450 | loss 1.8963 | lr 3.89e-04 | grad 1.12 | tok/s 7906
step   2460 | loss 1.6949 | lr 3.89e-04 | grad 2.22 | tok/s 7939
step   2470 | loss 1.8165 | lr 3.89e-04 | grad 2.58 | tok/s 7943
step   2480 | loss 1.7598 | lr 3.89e-04 | grad 1.99 | tok/s 7558
step   2490 | loss 1.9688 | lr 3.89e-04 | grad 2.97 | tok/s 7808
step   2500 | loss 2.0931 | lr 3.89e-04 | grad 1.58 | tok/s 7986
step   2510 | loss 1.9993 | lr 3.89e-04 | grad 1.65 | tok/s 7989
step   2520 | loss 1.9283 | lr 3.89e-04 | grad 1.79 | tok/s 7772
step   2530 | loss 1.7610 | lr 3.89e-04 | grad 1.80 | tok/s 7520
step   2540 | loss 1.8607 | lr 3.89e-04 | grad 1.86 | tok/s 7938
step   2550 | loss 1.6596 | lr 3.89e-04 | grad 2.95 | tok/s 7454
step   2560 | loss 2.1447 | lr 3.89e-04 | grad 1.59 | tok/s 7750
step   2570 | loss 1.7597 | lr 3.89e-04 | grad 2.47 | tok/s 7277
step   2580 | loss 1.9138 | lr 3.89e-04 | grad 1.92 | tok/s 7870
step   2590 | loss 1.9489 | lr 3.89e-04 | grad 2.08 | tok/s 7199
step   2600 | loss 2.2816 | lr 3.89e-04 | grad 3.36 | tok/s 7934
step   2610 | loss 1.9955 | lr 3.89e-04 | grad 1.65 | tok/s 7653
step   2620 | loss 1.9118 | lr 3.89e-04 | grad 1.61 | tok/s 7898
step   2630 | loss 1.9533 | lr 3.89e-04 | grad 2.19 | tok/s 7755
step   2640 | loss 2.0712 | lr 3.89e-04 | grad 1.56 | tok/s 7905
step   2650 | loss 1.9164 | lr 3.89e-04 | grad 2.02 | tok/s 7727
step   2660 | loss 1.7819 | lr 3.89e-04 | grad 2.17 | tok/s 7554
step   2670 | loss 1.9896 | lr 3.89e-04 | grad 3.42 | tok/s 7430
step   2680 | loss 1.9810 | lr 3.89e-04 | grad 1.70 | tok/s 7920
step   2690 | loss 1.8964 | lr 3.89e-04 | grad 2.12 | tok/s 7804
step   2700 | loss 2.1213 | lr 3.89e-04 | grad 4.22 | tok/s 7582
step   2710 | loss 1.8360 | lr 3.89e-04 | grad 2.27 | tok/s 7232
step   2720 | loss 1.8386 | lr 3.89e-04 | grad 1.93 | tok/s 7767
step   2730 | loss 1.8367 | lr 3.89e-04 | grad 2.08 | tok/s 7725
step   2740 | loss 2.2950 | lr 3.89e-04 | grad 1.96 | tok/s 7916
step   2750 | loss 1.7843 | lr 3.89e-04 | grad 1.60 | tok/s 7544
step   2760 | loss 1.9355 | lr 3.89e-04 | grad 2.08 | tok/s 7412
step   2770 | loss 1.7320 | lr 3.89e-04 | grad 2.05 | tok/s 7978
step   2780 | loss 2.0858 | lr 3.89e-04 | grad 4.19 | tok/s 7574
step   2790 | loss 1.9654 | lr 3.89e-04 | grad 1.89 | tok/s 7511
step   2800 | loss 1.6901 | lr 3.89e-04 | grad 1.27 | tok/s 7758
step   2810 | loss 1.7974 | lr 3.89e-04 | grad 1.77 | tok/s 7284
step   2820 | loss 1.8889 | lr 3.89e-04 | grad 3.27 | tok/s 7777
step   2830 | loss 1.6815 | lr 3.89e-04 | grad 1.66 | tok/s 7987
step   2840 | loss 2.1624 | lr 3.89e-04 | grad 2.17 | tok/s 7796
step   2850 | loss 2.1340 | lr 3.89e-04 | grad 1.88 | tok/s 7632
step   2860 | loss 1.7546 | lr 3.89e-04 | grad 1.50 | tok/s 7597
step   2870 | loss 1.9052 | lr 3.89e-04 | grad 1.52 | tok/s 7705
step   2880 | loss 1.9490 | lr 3.89e-04 | grad 1.58 | tok/s 7974
step   2890 | loss 1.8809 | lr 3.89e-04 | grad 2.36 | tok/s 7674
step   2900 | loss 1.9220 | lr 3.89e-04 | grad 1.50 | tok/s 7833
step   2910 | loss 1.8075 | lr 3.89e-04 | grad 1.74 | tok/s 7354
step   2920 | loss 2.0731 | lr 3.89e-04 | grad 1.79 | tok/s 7742
step   2930 | loss 1.6842 | lr 3.89e-04 | grad 1.76 | tok/s 7384
step   2940 | loss 1.7753 | lr 3.89e-04 | grad 1.50 | tok/s 7503
step   2950 | loss 1.8101 | lr 3.89e-04 | grad 2.27 | tok/s 7874
step   2960 | loss 1.7427 | lr 3.89e-04 | grad 1.84 | tok/s 7743
step   2970 | loss 2.1101 | lr 3.89e-04 | grad 2.81 | tok/s 7655
step   2980 | loss 2.4917 | lr 3.89e-04 | grad 1.74 | tok/s 7778
step   2990 | loss 1.8331 | lr 3.89e-04 | grad 3.47 | tok/s 7925
step   3000 | loss 1.8282 | lr 3.89e-04 | grad 2.45 | tok/s 7553
  >>> saved checkpoint: checkpoint_step_003000_loss_1.8282.pt
step   3010 | loss 1.7575 | lr 3.89e-04 | grad 1.95 | tok/s 2409
step   3020 | loss 1.8656 | lr 3.89e-04 | grad 1.52 | tok/s 7513
step   3030 | loss 1.8312 | lr 3.89e-04 | grad 1.90 | tok/s 7981
step   3040 | loss 1.8308 | lr 3.89e-04 | grad 2.59 | tok/s 7772
step   3050 | loss 1.8916 | lr 3.89e-04 | grad 2.09 | tok/s 7901
step   3060 | loss 1.8744 | lr 3.89e-04 | grad 1.81 | tok/s 8010
step   3070 | loss 1.8352 | lr 3.89e-04 | grad 2.64 | tok/s 7719
step   3080 | loss 1.7736 | lr 3.89e-04 | grad 2.67 | tok/s 7670
step   3090 | loss 1.9709 | lr 3.89e-04 | grad 2.28 | tok/s 7837
step   3100 | loss 1.5774 | lr 3.89e-04 | grad 1.95 | tok/s 8010
step   3110 | loss 1.6381 | lr 3.89e-04 | grad 1.31 | tok/s 7798
step   3120 | loss 1.9032 | lr 3.89e-04 | grad 1.98 | tok/s 7781
step   3130 | loss 1.6882 | lr 3.89e-04 | grad 1.84 | tok/s 7999
step   3140 | loss 1.7021 | lr 3.89e-04 | grad 2.09 | tok/s 7838
step   3150 | loss 1.9952 | lr 3.89e-04 | grad 2.77 | tok/s 7485
step   3160 | loss 1.7172 | lr 3.89e-04 | grad 2.45 | tok/s 7488
step   3170 | loss 1.7261 | lr 3.89e-04 | grad 1.80 | tok/s 7735
step   3180 | loss 1.3485 | lr 3.89e-04 | grad 6.81 | tok/s 8046
step   3190 | loss 2.0927 | lr 3.89e-04 | grad 2.11 | tok/s 7706
step   3200 | loss 2.2150 | lr 3.89e-04 | grad 2.95 | tok/s 7764
step   3210 | loss 2.5202 | lr 3.89e-04 | grad 3.28 | tok/s 7984
step   3220 | loss 2.2504 | lr 3.89e-04 | grad 2.25 | tok/s 7977
step   3230 | loss 2.1501 | lr 3.89e-04 | grad 2.45 | tok/s 7981
step   3240 | loss 2.0989 | lr 3.89e-04 | grad 2.72 | tok/s 7975
step   3250 | loss 2.0269 | lr 3.89e-04 | grad 2.52 | tok/s 7983
step   3260 | loss 1.9977 | lr 3.89e-04 | grad 2.62 | tok/s 7980
step   3270 | loss 1.9220 | lr 3.89e-04 | grad 2.89 | tok/s 7974
step   3280 | loss 1.9214 | lr 3.89e-04 | grad 3.16 | tok/s 7978
step   3290 | loss 1.8906 | lr 3.89e-04 | grad 2.55 | tok/s 7975
step   3300 | loss 1.8654 | lr 3.89e-04 | grad 2.19 | tok/s 7977
step   3310 | loss 1.8502 | lr 3.89e-04 | grad 2.55 | tok/s 7970
step   3320 | loss 2.0771 | lr 3.89e-04 | grad 2.62 | tok/s 7489
step   3330 | loss 1.8996 | lr 3.89e-04 | grad 2.03 | tok/s 7864
step   3340 | loss 1.8118 | lr 3.89e-04 | grad 3.02 | tok/s 7539
step   3350 | loss 1.8504 | lr 3.89e-04 | grad 1.93 | tok/s 7408
step   3360 | loss 1.8829 | lr 3.89e-04 | grad 1.97 | tok/s 7596
step   3370 | loss 1.8657 | lr 3.89e-04 | grad 2.11 | tok/s 7667
step   3380 | loss 1.6993 | lr 3.89e-04 | grad 2.20 | tok/s 7754
step   3390 | loss 1.6016 | lr 3.89e-04 | grad 2.33 | tok/s 7973
step   3400 | loss 1.6780 | lr 3.89e-04 | grad 1.79 | tok/s 7885

Training complete! Final step: 3408
