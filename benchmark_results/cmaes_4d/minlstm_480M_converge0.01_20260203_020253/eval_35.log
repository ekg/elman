Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_35/levelminlstm_100m_20260203_040443
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 669,301,760 parameters
Using schedule-free AdamW (lr=0.0008952844112117612)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 8.4258 | lr 8.95e-04 | grad 26.12 | tok/s 10260
step     20 | loss 3.1406 | lr 8.95e-04 | grad 3.30 | tok/s 11000
step     30 | loss 3.1262 | lr 8.95e-04 | grad 7.72 | tok/s 11041
step     40 | loss 3.0852 | lr 8.95e-04 | grad 4.62 | tok/s 10519
step     50 | loss 2.9638 | lr 8.95e-04 | grad 16.38 | tok/s 10467
step     60 | loss 3.6000 | lr 8.95e-04 | grad 3.52 | tok/s 11118
step     70 | loss 2.6940 | lr 8.95e-04 | grad 4.00 | tok/s 11117
step     80 | loss 4.4483 | lr 8.95e-04 | grad 30.38 | tok/s 11045
step     90 | loss 4.9318 | lr 8.95e-04 | grad 11.12 | tok/s 11338
step    100 | loss 3.9206 | lr 8.95e-04 | grad 7.62 | tok/s 11338
step    110 | loss 3.6609 | lr 8.95e-04 | grad 6.59 | tok/s 11301
step    120 | loss 3.6049 | lr 8.95e-04 | grad 5.81 | tok/s 11275
step    130 | loss 3.6303 | lr 8.95e-04 | grad 4.47 | tok/s 11260
step    140 | loss 3.0264 | lr 8.95e-04 | grad 8.69 | tok/s 11241
step    150 | loss 3.2079 | lr 8.95e-04 | grad 8.06 | tok/s 11214
step    160 | loss 3.0138 | lr 8.95e-04 | grad 8.50 | tok/s 11196
step    170 | loss 2.7798 | lr 8.95e-04 | grad 10.50 | tok/s 11154
step    180 | loss 2.7775 | lr 8.95e-04 | grad 4.25 | tok/s 11152
step    190 | loss 2.7542 | lr 8.95e-04 | grad 6.44 | tok/s 11143
step    200 | loss 2.5822 | lr 8.95e-04 | grad 2.56 | tok/s 11136
step    210 | loss 2.4145 | lr 8.95e-04 | grad 4.62 | tok/s 11134
step    220 | loss 2.5250 | lr 8.95e-04 | grad 9.62 | tok/s 10978
step    230 | loss 3.1275 | lr 8.95e-04 | grad 8.12 | tok/s 10996
step    240 | loss 3.0494 | lr 8.95e-04 | grad 3.38 | tok/s 10399
step    250 | loss 2.6315 | lr 8.95e-04 | grad 3.39 | tok/s 10477
step    260 | loss 2.4146 | lr 8.95e-04 | grad 4.28 | tok/s 10991
step    270 | loss 2.4878 | lr 8.95e-04 | grad 5.19 | tok/s 10588
step    280 | loss 2.4916 | lr 8.95e-04 | grad 7.69 | tok/s 10863
step    290 | loss 2.7106 | lr 8.95e-04 | grad 3.66 | tok/s 10815
step    300 | loss 1.6375 | lr 8.95e-04 | grad 2.78 | tok/s 11127
step    310 | loss 2.3009 | lr 8.95e-04 | grad 3.53 | tok/s 10921
step    320 | loss 2.6034 | lr 8.95e-04 | grad 3.39 | tok/s 11072
step    330 | loss 2.3834 | lr 8.95e-04 | grad 3.39 | tok/s 10022
step    340 | loss 2.5495 | lr 8.95e-04 | grad 2.08 | tok/s 10623
step    350 | loss 2.3996 | lr 8.95e-04 | grad 3.25 | tok/s 10498
step    360 | loss 2.4923 | lr 8.95e-04 | grad 2.72 | tok/s 11060
step    370 | loss 2.3567 | lr 8.95e-04 | grad 2.08 | tok/s 10181
step    380 | loss 2.2202 | lr 8.95e-04 | grad 2.53 | tok/s 10411
step    390 | loss 2.0942 | lr 8.95e-04 | grad 4.47 | tok/s 10955
step    400 | loss 2.0203 | lr 8.95e-04 | grad 3.44 | tok/s 10954
step    410 | loss 2.1141 | lr 8.95e-04 | grad 4.97 | tok/s 11049
step    420 | loss 2.0022 | lr 8.95e-04 | grad 2.58 | tok/s 10106
step    430 | loss 2.3923 | lr 8.95e-04 | grad 2.84 | tok/s 10758
step    440 | loss 2.5210 | lr 8.95e-04 | grad 2.62 | tok/s 10566
step    450 | loss 2.4451 | lr 8.95e-04 | grad 11.25 | tok/s 10585
step    460 | loss 2.2788 | lr 8.95e-04 | grad 1.74 | tok/s 10361
step    470 | loss 2.1924 | lr 8.95e-04 | grad 1.87 | tok/s 10543
step    480 | loss 2.2211 | lr 8.95e-04 | grad 2.31 | tok/s 10745
step    490 | loss 2.5485 | lr 8.95e-04 | grad 2.42 | tok/s 10635
step    500 | loss 1.9857 | lr 8.95e-04 | grad 2.38 | tok/s 10468
step    510 | loss 2.1620 | lr 8.95e-04 | grad 2.69 | tok/s 10907
step    520 | loss 2.1282 | lr 8.95e-04 | grad 1.28 | tok/s 10961
step    530 | loss 2.2371 | lr 8.95e-04 | grad 2.17 | tok/s 10844
step    540 | loss 2.0931 | lr 8.95e-04 | grad 1.63 | tok/s 10570
step    550 | loss 1.8778 | lr 8.95e-04 | grad 2.27 | tok/s 10555
step    560 | loss 1.9845 | lr 8.95e-04 | grad 5.31 | tok/s 9660
step    570 | loss 1.9980 | lr 8.95e-04 | grad 2.38 | tok/s 10560
step    580 | loss 1.9302 | lr 8.95e-04 | grad 2.30 | tok/s 10359
step    590 | loss 1.8489 | lr 8.95e-04 | grad 2.52 | tok/s 10234
step    600 | loss 2.2890 | lr 8.95e-04 | grad 2.16 | tok/s 10489
step    610 | loss 1.9918 | lr 8.95e-04 | grad 2.19 | tok/s 10493
step    620 | loss 1.8231 | lr 8.95e-04 | grad 2.89 | tok/s 10426
step    630 | loss 1.9131 | lr 8.95e-04 | grad 3.25 | tok/s 10212
step    640 | loss 2.0801 | lr 8.95e-04 | grad 3.69 | tok/s 10462
step    650 | loss 1.9987 | lr 8.95e-04 | grad 2.34 | tok/s 10564
step    660 | loss 1.9710 | lr 8.95e-04 | grad 2.31 | tok/s 10714
step    670 | loss 1.8594 | lr 8.95e-04 | grad 2.00 | tok/s 10454
step    680 | loss 2.2757 | lr 8.95e-04 | grad 1.89 | tok/s 10894
step    690 | loss 2.0828 | lr 8.95e-04 | grad 1.92 | tok/s 10340
step    700 | loss 2.0539 | lr 8.95e-04 | grad 2.64 | tok/s 11046
step    710 | loss 1.9091 | lr 8.95e-04 | grad 2.27 | tok/s 10738
step    720 | loss 1.6842 | lr 8.95e-04 | grad 2.20 | tok/s 9759
step    730 | loss 1.8618 | lr 8.95e-04 | grad 2.53 | tok/s 11046
step    740 | loss 1.7346 | lr 8.95e-04 | grad 1.43 | tok/s 10865
step    750 | loss 1.6892 | lr 8.95e-04 | grad 2.20 | tok/s 11057
step    760 | loss 1.4920 | lr 8.95e-04 | grad 2.14 | tok/s 11059
step    770 | loss 1.4351 | lr 8.95e-04 | grad 1.84 | tok/s 11042
step    780 | loss 1.3732 | lr 8.95e-04 | grad 2.11 | tok/s 11034
step    790 | loss 1.2971 | lr 8.95e-04 | grad 1.91 | tok/s 11046
step    800 | loss 1.8480 | lr 8.95e-04 | grad 2.36 | tok/s 10339
step    810 | loss 2.0706 | lr 8.95e-04 | grad 2.27 | tok/s 10618
step    820 | loss 1.8999 | lr 8.95e-04 | grad 2.69 | tok/s 10480
step    830 | loss 1.8924 | lr 8.95e-04 | grad 2.02 | tok/s 10676
step    840 | loss 1.8391 | lr 8.95e-04 | grad 2.48 | tok/s 11039
step    850 | loss 1.9384 | lr 8.95e-04 | grad 7.19 | tok/s 10997
step    860 | loss 1.7597 | lr 8.95e-04 | grad 2.02 | tok/s 10964
step    870 | loss 1.7656 | lr 8.95e-04 | grad 1.28 | tok/s 10574
step    880 | loss 1.8118 | lr 8.95e-04 | grad 2.42 | tok/s 10512
step    890 | loss 1.9135 | lr 8.95e-04 | grad 1.05 | tok/s 10696
step    900 | loss 1.8414 | lr 8.95e-04 | grad 1.84 | tok/s 10731
step    910 | loss 1.6423 | lr 8.95e-04 | grad 2.09 | tok/s 10436
step    920 | loss 1.7518 | lr 8.95e-04 | grad 1.75 | tok/s 10924
step    930 | loss 1.7515 | lr 8.95e-04 | grad 1.84 | tok/s 10512
step    940 | loss 1.8001 | lr 8.95e-04 | grad 1.80 | tok/s 10648
step    950 | loss 1.7385 | lr 8.95e-04 | grad 1.62 | tok/s 10997
step    960 | loss 1.5643 | lr 8.95e-04 | grad 1.48 | tok/s 11030
step    970 | loss 1.7666 | lr 8.95e-04 | grad 1.15 | tok/s 10561
step    980 | loss 1.9083 | lr 8.95e-04 | grad 1.26 | tok/s 10633
step    990 | loss 1.6702 | lr 8.95e-04 | grad 1.73 | tok/s 10616
step   1000 | loss 1.8208 | lr 8.95e-04 | grad 2.02 | tok/s 10494
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8208.pt
step   1010 | loss 1.8728 | lr 8.95e-04 | grad 2.02 | tok/s 3671
step   1020 | loss 1.7852 | lr 8.95e-04 | grad 2.05 | tok/s 10291
step   1030 | loss 1.6640 | lr 8.95e-04 | grad 1.85 | tok/s 10689
step   1040 | loss 1.6785 | lr 8.95e-04 | grad 1.85 | tok/s 11035
step   1050 | loss 1.7509 | lr 8.95e-04 | grad 2.22 | tok/s 10206
step   1060 | loss 1.9212 | lr 8.95e-04 | grad 2.22 | tok/s 10976
step   1070 | loss 1.9053 | lr 8.95e-04 | grad 1.31 | tok/s 10933
step   1080 | loss 1.5672 | lr 8.95e-04 | grad 1.34 | tok/s 9944
step   1090 | loss 1.2837 | lr 8.95e-04 | grad 0.66 | tok/s 10958
step   1100 | loss 1.5983 | lr 8.95e-04 | grad 2.62 | tok/s 10633
step   1110 | loss 1.6554 | lr 8.95e-04 | grad 1.40 | tok/s 11135
step   1120 | loss 1.5531 | lr 8.95e-04 | grad 2.50 | tok/s 11119
step   1130 | loss 1.4995 | lr 8.95e-04 | grad 1.91 | tok/s 11122
step   1140 | loss 1.4710 | lr 8.95e-04 | grad 1.76 | tok/s 11112
step   1150 | loss 1.4756 | lr 8.95e-04 | grad 1.77 | tok/s 11093
step   1160 | loss 1.3911 | lr 8.95e-04 | grad 2.33 | tok/s 11086
step   1170 | loss 1.4114 | lr 8.95e-04 | grad 2.23 | tok/s 11099
step   1180 | loss 1.5308 | lr 8.95e-04 | grad 1.11 | tok/s 11091
step   1190 | loss 1.3992 | lr 8.95e-04 | grad 2.03 | tok/s 11083
step   1200 | loss 1.3828 | lr 8.95e-04 | grad 1.80 | tok/s 11081
step   1210 | loss 1.4389 | lr 8.95e-04 | grad 2.27 | tok/s 11076
step   1220 | loss 1.4408 | lr 8.95e-04 | grad 2.00 | tok/s 11083
step   1230 | loss 1.4214 | lr 8.95e-04 | grad 1.73 | tok/s 11068
step   1240 | loss 1.3726 | lr 8.95e-04 | grad 1.78 | tok/s 11072
step   1250 | loss 2.0537 | lr 8.95e-04 | grad 2.14 | tok/s 10489
step   1260 | loss 1.5199 | lr 8.95e-04 | grad 3.62 | tok/s 10420
step   1270 | loss 1.7894 | lr 8.95e-04 | grad 3.81 | tok/s 10372
step   1280 | loss 1.7568 | lr 8.95e-04 | grad 1.76 | tok/s 10660
step   1290 | loss 1.6166 | lr 8.95e-04 | grad 1.92 | tok/s 10600
step   1300 | loss 1.6819 | lr 8.95e-04 | grad 1.54 | tok/s 10677
step   1310 | loss 1.6424 | lr 8.95e-04 | grad 2.34 | tok/s 10843
step   1320 | loss 1.7238 | lr 8.95e-04 | grad 1.46 | tok/s 10880
step   1330 | loss 1.7323 | lr 8.95e-04 | grad 2.12 | tok/s 10884
step   1340 | loss 1.6465 | lr 8.95e-04 | grad 4.78 | tok/s 10393
step   1350 | loss 1.8503 | lr 8.95e-04 | grad 1.67 | tok/s 10055
step   1360 | loss 1.6867 | lr 8.95e-04 | grad 2.09 | tok/s 10668
step   1370 | loss 1.5424 | lr 8.95e-04 | grad 1.31 | tok/s 10548
step   1380 | loss 1.8411 | lr 8.95e-04 | grad 1.19 | tok/s 10140
step   1390 | loss 1.6721 | lr 8.95e-04 | grad 1.59 | tok/s 10749
step   1400 | loss 1.5370 | lr 8.95e-04 | grad 1.25 | tok/s 10374
step   1410 | loss 1.5865 | lr 8.95e-04 | grad 1.71 | tok/s 10410
step   1420 | loss 1.8506 | lr 8.95e-04 | grad 2.80 | tok/s 10437
step   1430 | loss 1.5389 | lr 8.95e-04 | grad 1.35 | tok/s 10617
step   1440 | loss 1.3201 | lr 8.95e-04 | grad 1.80 | tok/s 10950
step   1450 | loss 1.3014 | lr 8.95e-04 | grad 2.91 | tok/s 11030
step   1460 | loss 1.7779 | lr 8.95e-04 | grad 2.02 | tok/s 10423
step   1470 | loss 1.6789 | lr 8.95e-04 | grad 1.38 | tok/s 10794
step   1480 | loss 2.1080 | lr 8.95e-04 | grad 2.72 | tok/s 10859
step   1490 | loss 1.8424 | lr 8.95e-04 | grad 1.80 | tok/s 11028
step   1500 | loss 1.5546 | lr 8.95e-04 | grad 1.33 | tok/s 11068
step   1510 | loss 1.6513 | lr 8.95e-04 | grad 2.05 | tok/s 10922
step   1520 | loss 1.5717 | lr 8.95e-04 | grad 2.81 | tok/s 10699
step   1530 | loss 1.5519 | lr 8.95e-04 | grad 1.58 | tok/s 10951
step   1540 | loss 1.7200 | lr 8.95e-04 | grad 1.50 | tok/s 10304
step   1550 | loss 1.4684 | lr 8.95e-04 | grad 1.77 | tok/s 10986
step   1560 | loss 1.6638 | lr 8.95e-04 | grad 2.05 | tok/s 10419
step   1570 | loss 1.4844 | lr 8.95e-04 | grad 1.73 | tok/s 11055
step   1580 | loss 1.9547 | lr 8.95e-04 | grad 3.44 | tok/s 10799
step   1590 | loss 1.8620 | lr 8.95e-04 | grad 1.49 | tok/s 10383
step   1600 | loss 1.1471 | lr 8.95e-04 | grad 0.88 | tok/s 11104
step   1610 | loss 1.1290 | lr 8.95e-04 | grad 1.87 | tok/s 10767
step   1620 | loss 1.5574 | lr 8.95e-04 | grad 1.84 | tok/s 10046
step   1630 | loss 1.5981 | lr 8.95e-04 | grad 1.36 | tok/s 10750
step   1640 | loss 1.4709 | lr 8.95e-04 | grad 1.48 | tok/s 10504
step   1650 | loss 1.6082 | lr 8.95e-04 | grad 1.20 | tok/s 10080
step   1660 | loss 1.5867 | lr 8.95e-04 | grad 1.21 | tok/s 10746
step   1670 | loss 1.5791 | lr 8.95e-04 | grad 4.25 | tok/s 10728
step   1680 | loss 1.8201 | lr 8.95e-04 | grad 1.47 | tok/s 10302
step   1690 | loss 1.6184 | lr 8.95e-04 | grad 2.75 | tok/s 10491
step   1700 | loss 1.6177 | lr 8.95e-04 | grad 1.80 | tok/s 10703
step   1710 | loss 1.6175 | lr 8.95e-04 | grad 1.48 | tok/s 10507
step   1720 | loss 1.6762 | lr 8.95e-04 | grad 1.74 | tok/s 10945
step   1730 | loss 1.5113 | lr 8.95e-04 | grad 2.25 | tok/s 11061
step   1740 | loss 1.5456 | lr 8.95e-04 | grad 1.09 | tok/s 10782
step   1750 | loss 1.6792 | lr 8.95e-04 | grad 1.46 | tok/s 10597
step   1760 | loss 1.6282 | lr 8.95e-04 | grad 1.70 | tok/s 10652
step   1770 | loss 1.5323 | lr 8.95e-04 | grad 1.27 | tok/s 10460
step   1780 | loss 1.6073 | lr 8.95e-04 | grad 1.54 | tok/s 10854
step   1790 | loss 1.5154 | lr 8.95e-04 | grad 1.30 | tok/s 10591
step   1800 | loss 1.7055 | lr 8.95e-04 | grad 1.13 | tok/s 10687
step   1810 | loss 1.5445 | lr 8.95e-04 | grad 1.62 | tok/s 10293
step   1820 | loss 1.6308 | lr 8.95e-04 | grad 2.95 | tok/s 10445
step   1830 | loss 1.5800 | lr 8.95e-04 | grad 1.48 | tok/s 10851
step   1840 | loss 1.6206 | lr 8.95e-04 | grad 1.84 | tok/s 10426
step   1850 | loss 1.4697 | lr 8.95e-04 | grad 1.50 | tok/s 10885
step   1860 | loss 1.4562 | lr 8.95e-04 | grad 1.79 | tok/s 10535
step   1870 | loss 1.5046 | lr 8.95e-04 | grad 1.50 | tok/s 10574
step   1880 | loss 1.3547 | lr 8.95e-04 | grad 1.27 | tok/s 10369
step   1890 | loss 1.6125 | lr 8.95e-04 | grad 1.52 | tok/s 9863
step   1900 | loss 1.4907 | lr 8.95e-04 | grad 1.62 | tok/s 10642
step   1910 | loss 1.5364 | lr 8.95e-04 | grad 1.37 | tok/s 10085
step   1920 | loss 1.5225 | lr 8.95e-04 | grad 1.77 | tok/s 11034
step   1930 | loss 1.5457 | lr 8.95e-04 | grad 1.41 | tok/s 10365
step   1940 | loss 1.5409 | lr 8.95e-04 | grad 1.80 | tok/s 10792
step   1950 | loss 2.1026 | lr 8.95e-04 | grad 2.44 | tok/s 10947
step   1960 | loss 1.8258 | lr 8.95e-04 | grad 2.55 | tok/s 11066
step   1970 | loss 1.6802 | lr 8.95e-04 | grad 1.75 | tok/s 10795
step   1980 | loss 1.6495 | lr 8.95e-04 | grad 1.54 | tok/s 10316
step   1990 | loss 1.7123 | lr 8.95e-04 | grad 8.25 | tok/s 10518
step   2000 | loss 1.5798 | lr 8.95e-04 | grad 1.35 | tok/s 10659
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5798.pt
step   2010 | loss 1.3581 | lr 8.95e-04 | grad 1.62 | tok/s 3921
step   2020 | loss 1.4329 | lr 8.95e-04 | grad 1.84 | tok/s 10939
step   2030 | loss 1.1437 | lr 8.95e-04 | grad 1.37 | tok/s 11307
step   2040 | loss 1.4690 | lr 8.95e-04 | grad 1.32 | tok/s 11188
step   2050 | loss 1.4962 | lr 8.95e-04 | grad 1.66 | tok/s 10715
step   2060 | loss 1.7566 | lr 8.95e-04 | grad 1.19 | tok/s 10564
step   2070 | loss 2.0293 | lr 8.95e-04 | grad 5.38 | tok/s 10605
step   2080 | loss 2.3004 | lr 8.95e-04 | grad 3.89 | tok/s 11151
step   2090 | loss 1.6306 | lr 8.95e-04 | grad 2.25 | tok/s 10855
step   2100 | loss 1.4885 | lr 8.95e-04 | grad 1.20 | tok/s 11016
step   2110 | loss 1.5552 | lr 8.95e-04 | grad 1.25 | tok/s 10404
step   2120 | loss 0.8264 | lr 8.95e-04 | grad 0.88 | tok/s 11301
step   2130 | loss 1.5152 | lr 8.95e-04 | grad 1.61 | tok/s 10520
step   2140 | loss 1.5067 | lr 8.95e-04 | grad 1.74 | tok/s 10985
step   2150 | loss 1.3719 | lr 8.95e-04 | grad 1.45 | tok/s 11107
step   2160 | loss 1.3094 | lr 8.95e-04 | grad 1.52 | tok/s 11111
step   2170 | loss 1.3107 | lr 8.95e-04 | grad 1.62 | tok/s 11100
step   2180 | loss 1.3045 | lr 8.95e-04 | grad 1.53 | tok/s 11096
step   2190 | loss 1.3285 | lr 8.95e-04 | grad 1.23 | tok/s 11098
step   2200 | loss 1.2655 | lr 8.95e-04 | grad 1.45 | tok/s 11104
step   2210 | loss 1.2443 | lr 8.95e-04 | grad 1.37 | tok/s 11085
step   2220 | loss 1.2237 | lr 8.95e-04 | grad 1.31 | tok/s 11078
step   2230 | loss 1.5393 | lr 8.95e-04 | grad 1.64 | tok/s 10883
step   2240 | loss 1.4707 | lr 8.95e-04 | grad 1.94 | tok/s 10698
step   2250 | loss 1.6894 | lr 8.95e-04 | grad 2.30 | tok/s 11097
step   2260 | loss 1.6950 | lr 8.95e-04 | grad 1.47 | tok/s 10712
step   2270 | loss 2.0737 | lr 8.95e-04 | grad 1.49 | tok/s 10974
step   2280 | loss 1.5126 | lr 8.95e-04 | grad 1.34 | tok/s 11064
step   2290 | loss 1.6592 | lr 8.95e-04 | grad 1.25 | tok/s 10558
step   2300 | loss 1.8430 | lr 8.95e-04 | grad 2.02 | tok/s 10768
step   2310 | loss 1.6022 | lr 8.95e-04 | grad 2.16 | tok/s 10539
step   2320 | loss 1.9645 | lr 8.95e-04 | grad 2.03 | tok/s 10500
step   2330 | loss 1.5009 | lr 8.95e-04 | grad 1.55 | tok/s 10251
step   2340 | loss 1.6250 | lr 8.95e-04 | grad 1.70 | tok/s 10561
step   2350 | loss 1.4524 | lr 8.95e-04 | grad 1.36 | tok/s 10855
step   2360 | loss 1.3510 | lr 8.95e-04 | grad 1.36 | tok/s 10998
step   2370 | loss 1.7294 | lr 8.95e-04 | grad 1.74 | tok/s 10953
step   2380 | loss 1.6028 | lr 8.95e-04 | grad 1.98 | tok/s 11086
step   2390 | loss 1.2112 | lr 8.95e-04 | grad 1.04 | tok/s 11059
step   2400 | loss 1.1315 | lr 8.95e-04 | grad 1.52 | tok/s 11075
step   2410 | loss 1.3201 | lr 8.95e-04 | grad 1.49 | tok/s 10607
step   2420 | loss 1.5703 | lr 8.95e-04 | grad 1.60 | tok/s 10078
step   2430 | loss 1.3612 | lr 8.95e-04 | grad 1.25 | tok/s 11033
step   2440 | loss 1.5324 | lr 8.95e-04 | grad 1.20 | tok/s 10655
step   2450 | loss 1.5542 | lr 8.95e-04 | grad 1.66 | tok/s 10625
step   2460 | loss 1.2256 | lr 8.95e-04 | grad 1.28 | tok/s 11098
step   2470 | loss 1.3236 | lr 8.95e-04 | grad 1.56 | tok/s 10914
step   2480 | loss 1.3730 | lr 8.95e-04 | grad 1.25 | tok/s 10892
step   2490 | loss 1.5177 | lr 8.95e-04 | grad 1.02 | tok/s 10574
step   2500 | loss 1.5337 | lr 8.95e-04 | grad 1.95 | tok/s 10923
step   2510 | loss 1.2509 | lr 8.95e-04 | grad 1.53 | tok/s 11077
step   2520 | loss 1.6756 | lr 8.95e-04 | grad 1.62 | tok/s 10889
step   2530 | loss 1.3855 | lr 8.95e-04 | grad 1.38 | tok/s 10602
step   2540 | loss 1.4892 | lr 8.95e-04 | grad 1.07 | tok/s 10766
step   2550 | loss 1.2564 | lr 8.95e-04 | grad 1.21 | tok/s 10963
step   2560 | loss 1.6215 | lr 8.95e-04 | grad 1.64 | tok/s 10164
step   2570 | loss 1.3918 | lr 8.95e-04 | grad 1.27 | tok/s 10423
step   2580 | loss 1.4306 | lr 8.95e-04 | grad 1.74 | tok/s 10790
step   2590 | loss 1.5374 | lr 8.95e-04 | grad 0.98 | tok/s 10139
step   2600 | loss 1.8577 | lr 8.95e-04 | grad 4.81 | tok/s 10717
step   2610 | loss 1.5075 | lr 8.95e-04 | grad 1.82 | tok/s 10783
step   2620 | loss 1.6267 | lr 8.95e-04 | grad 1.82 | tok/s 10861
step   2630 | loss 1.4673 | lr 8.95e-04 | grad 2.12 | tok/s 11007
step   2640 | loss 1.5936 | lr 8.95e-04 | grad 1.32 | tok/s 10697
step   2650 | loss 1.5765 | lr 8.95e-04 | grad 1.16 | tok/s 10829
step   2660 | loss 1.4051 | lr 8.95e-04 | grad 1.33 | tok/s 10650
step   2670 | loss 1.5318 | lr 8.95e-04 | grad 0.97 | tok/s 10397
step   2680 | loss 1.7326 | lr 8.95e-04 | grad 0.97 | tok/s 10670
step   2690 | loss 1.4052 | lr 8.95e-04 | grad 1.49 | tok/s 11026
step   2700 | loss 1.5057 | lr 8.95e-04 | grad 1.55 | tok/s 10472
step   2710 | loss 1.6378 | lr 8.95e-04 | grad 1.68 | tok/s 10454
step   2720 | loss 1.4395 | lr 8.95e-04 | grad 1.48 | tok/s 10304
step   2730 | loss 1.2473 | lr 8.95e-04 | grad 1.63 | tok/s 11001
step   2740 | loss 1.8842 | lr 8.95e-04 | grad 1.69 | tok/s 10856
step   2750 | loss 1.5912 | lr 8.95e-04 | grad 3.12 | tok/s 10858
step   2760 | loss 1.4151 | lr 8.95e-04 | grad 1.55 | tok/s 10073
step   2770 | loss 1.4750 | lr 8.95e-04 | grad 2.48 | tok/s 10923
step   2780 | loss 1.3147 | lr 8.95e-04 | grad 1.45 | tok/s 10972
step   2790 | loss 1.9336 | lr 8.95e-04 | grad 1.29 | tok/s 10037
step   2800 | loss 1.2290 | lr 8.95e-04 | grad 1.40 | tok/s 11104
step   2810 | loss 1.4181 | lr 8.95e-04 | grad 1.35 | tok/s 10248
step   2820 | loss 1.5009 | lr 8.95e-04 | grad 2.39 | tok/s 10430
step   2830 | loss 1.1609 | lr 8.95e-04 | grad 2.58 | tok/s 11114
step   2840 | loss 1.2216 | lr 8.95e-04 | grad 3.33 | tok/s 10859
step   2850 | loss 1.8430 | lr 8.95e-04 | grad 2.94 | tok/s 10680
step   2860 | loss 1.6633 | lr 8.95e-04 | grad 1.62 | tok/s 10571
step   2870 | loss 1.5144 | lr 8.95e-04 | grad 2.11 | tok/s 10771
step   2880 | loss 1.4560 | lr 8.95e-04 | grad 1.39 | tok/s 10994
step   2890 | loss 1.5048 | lr 8.95e-04 | grad 1.32 | tok/s 10878
step   2900 | loss 1.5194 | lr 8.95e-04 | grad 2.09 | tok/s 10878
step   2910 | loss 1.4540 | lr 8.95e-04 | grad 1.34 | tok/s 10429
step   2920 | loss 1.7583 | lr 8.95e-04 | grad 2.45 | tok/s 10726
step   2930 | loss 1.4530 | lr 8.95e-04 | grad 1.37 | tok/s 10536
step   2940 | loss 1.3474 | lr 8.95e-04 | grad 1.38 | tok/s 10146
step   2950 | loss 1.3739 | lr 8.95e-04 | grad 1.50 | tok/s 10916
step   2960 | loss 1.3855 | lr 8.95e-04 | grad 1.48 | tok/s 10884
step   2970 | loss 1.6982 | lr 8.95e-04 | grad 3.03 | tok/s 10558
step   2980 | loss 2.1299 | lr 8.95e-04 | grad 7.94 | tok/s 10914
step   2990 | loss 1.5765 | lr 8.95e-04 | grad 1.31 | tok/s 10909
step   3000 | loss 1.4374 | lr 8.95e-04 | grad 1.26 | tok/s 10630
  >>> saved checkpoint: checkpoint_step_003000_loss_1.4374.pt
step   3010 | loss 1.4867 | lr 8.95e-04 | grad 1.28 | tok/s 3588
step   3020 | loss 1.5053 | lr 8.95e-04 | grad 1.30 | tok/s 10548
step   3030 | loss 1.5131 | lr 8.95e-04 | grad 1.52 | tok/s 10952
step   3040 | loss 1.4726 | lr 8.95e-04 | grad 1.35 | tok/s 11208
step   3050 | loss 1.4126 | lr 8.95e-04 | grad 1.35 | tok/s 10901
step   3060 | loss 1.5922 | lr 8.95e-04 | grad 1.99 | tok/s 11063
step   3070 | loss 1.4861 | lr 8.95e-04 | grad 1.50 | tok/s 11255
step   3080 | loss 1.4798 | lr 8.95e-04 | grad 1.31 | tok/s 10841
step   3090 | loss 1.5259 | lr 8.95e-04 | grad 3.34 | tok/s 10627
step   3100 | loss 1.4216 | lr 8.95e-04 | grad 0.90 | tok/s 11115
step   3110 | loss 1.0150 | lr 8.95e-04 | grad 1.06 | tok/s 11228
step   3120 | loss 1.3471 | lr 8.95e-04 | grad 2.09 | tok/s 10733
step   3130 | loss 1.4126 | lr 8.95e-04 | grad 1.03 | tok/s 11107
step   3140 | loss 1.2026 | lr 8.95e-04 | grad 1.15 | tok/s 11212
step   3150 | loss 1.3588 | lr 8.95e-04 | grad 1.56 | tok/s 10865
step   3160 | loss 1.6326 | lr 8.95e-04 | grad 1.52 | tok/s 10585
step   3170 | loss 1.4428 | lr 8.95e-04 | grad 3.39 | tok/s 10387
step   3180 | loss 1.2599 | lr 8.95e-04 | grad 1.93 | tok/s 11046
step   3190 | loss 1.1115 | lr 8.95e-04 | grad 2.70 | tok/s 11269
step   3200 | loss 1.6611 | lr 8.95e-04 | grad 1.48 | tok/s 10813
step   3210 | loss 2.0973 | lr 8.95e-04 | grad 1.75 | tok/s 10888
step   3220 | loss 2.1568 | lr 8.95e-04 | grad 1.33 | tok/s 11189
step   3230 | loss 1.9084 | lr 8.95e-04 | grad 2.09 | tok/s 11202
step   3240 | loss 1.7719 | lr 8.95e-04 | grad 2.03 | tok/s 11187
step   3250 | loss 1.6991 | lr 8.95e-04 | grad 2.39 | tok/s 11180
step   3260 | loss 1.6162 | lr 8.95e-04 | grad 2.08 | tok/s 11184
step   3270 | loss 1.5771 | lr 8.95e-04 | grad 2.42 | tok/s 11191
step   3280 | loss 1.5147 | lr 8.95e-04 | grad 2.05 | tok/s 11183
step   3290 | loss 1.4653 | lr 8.95e-04 | grad 1.60 | tok/s 11191
step   3300 | loss 1.4708 | lr 8.95e-04 | grad 2.08 | tok/s 11185
step   3310 | loss 1.4338 | lr 8.95e-04 | grad 1.63 | tok/s 11158
step   3320 | loss 1.4645 | lr 8.95e-04 | grad 2.12 | tok/s 11175
step   3330 | loss 1.7375 | lr 8.95e-04 | grad 2.20 | tok/s 10495
step   3340 | loss 1.4901 | lr 8.95e-04 | grad 1.26 | tok/s 11020
step   3350 | loss 1.4461 | lr 8.95e-04 | grad 1.48 | tok/s 10569
step   3360 | loss 1.4541 | lr 8.95e-04 | grad 1.12 | tok/s 10350
step   3370 | loss 1.5970 | lr 8.95e-04 | grad 1.55 | tok/s 10646
step   3380 | loss 1.5323 | lr 8.95e-04 | grad 1.07 | tok/s 10546
step   3390 | loss 1.2702 | lr 8.95e-04 | grad 0.96 | tok/s 11127
step   3400 | loss 1.1386 | lr 8.95e-04 | grad 1.26 | tok/s 11180
step   3410 | loss 1.3914 | lr 8.95e-04 | grad 1.35 | tok/s 10824
step   3420 | loss 1.5010 | lr 8.95e-04 | grad 1.52 | tok/s 10604
step   3430 | loss 1.5343 | lr 8.95e-04 | grad 1.10 | tok/s 10512
step   3440 | loss 1.3863 | lr 8.95e-04 | grad 1.70 | tok/s 11041
step   3450 | loss 1.6574 | lr 8.95e-04 | grad 8.75 | tok/s 10931
step   3460 | loss 1.4081 | lr 8.95e-04 | grad 1.63 | tok/s 10924
step   3470 | loss 1.4999 | lr 8.95e-04 | grad 1.18 | tok/s 10960
step   3480 | loss 1.3634 | lr 8.95e-04 | grad 0.98 | tok/s 10420
step   3490 | loss 1.2405 | lr 8.95e-04 | grad 1.20 | tok/s 11093
step   3500 | loss 1.3710 | lr 8.95e-04 | grad 1.55 | tok/s 10564
step   3510 | loss 1.4916 | lr 8.95e-04 | grad 1.46 | tok/s 10819
step   3520 | loss 1.6022 | lr 8.95e-04 | grad 1.76 | tok/s 10876
step   3530 | loss 1.6561 | lr 8.95e-04 | grad 1.51 | tok/s 11013
step   3540 | loss 1.7748 | lr 8.95e-04 | grad 6.81 | tok/s 10407
step   3550 | loss 1.7286 | lr 8.95e-04 | grad 1.11 | tok/s 10041
step   3560 | loss 1.4606 | lr 8.95e-04 | grad 1.14 | tok/s 10292
step   3570 | loss 1.3051 | lr 8.95e-04 | grad 1.53 | tok/s 10918
step   3580 | loss 1.1806 | lr 8.95e-04 | grad 0.86 | tok/s 10905
step   3590 | loss 1.4228 | lr 8.95e-04 | grad 0.96 | tok/s 10253
step   3600 | loss 1.5831 | lr 8.95e-04 | grad 1.90 | tok/s 10434
step   3610 | loss 1.3595 | lr 8.95e-04 | grad 1.80 | tok/s 11182
step   3620 | loss 1.4664 | lr 8.95e-04 | grad 1.64 | tok/s 10725
step   3630 | loss 1.6127 | lr 8.95e-04 | grad 2.44 | tok/s 10804
step   3640 | loss 1.4689 | lr 8.95e-04 | grad 1.10 | tok/s 10265
step   3650 | loss 1.3791 | lr 8.95e-04 | grad 1.21 | tok/s 10288
step   3660 | loss 1.5202 | lr 8.95e-04 | grad 2.06 | tok/s 10742
step   3670 | loss 1.2349 | lr 8.95e-04 | grad 1.05 | tok/s 10897
step   3680 | loss 1.3678 | lr 8.95e-04 | grad 1.09 | tok/s 10403
step   3690 | loss 1.4918 | lr 8.95e-04 | grad 1.27 | tok/s 10778
step   3700 | loss 1.3661 | lr 8.95e-04 | grad 1.19 | tok/s 10422
step   3710 | loss 1.5721 | lr 8.95e-04 | grad 1.58 | tok/s 10952
step   3720 | loss 1.4405 | lr 8.95e-04 | grad 1.32 | tok/s 10425
step   3730 | loss 1.4786 | lr 8.95e-04 | grad 1.17 | tok/s 11108
step   3740 | loss 1.5278 | lr 8.95e-04 | grad 3.95 | tok/s 10761
step   3750 | loss 1.4324 | lr 8.95e-04 | grad 1.13 | tok/s 11034
step   3760 | loss 1.3306 | lr 8.95e-04 | grad 1.69 | tok/s 11175
step   3770 | loss 1.2976 | lr 8.95e-04 | grad 1.38 | tok/s 11176
step   3780 | loss 1.2675 | lr 8.95e-04 | grad 1.80 | tok/s 11184
step   3790 | loss 1.2708 | lr 8.95e-04 | grad 1.23 | tok/s 11167
step   3800 | loss 1.2655 | lr 8.95e-04 | grad 1.39 | tok/s 11165
step   3810 | loss 1.2135 | lr 8.95e-04 | grad 1.64 | tok/s 11182
step   3820 | loss 1.2332 | lr 8.95e-04 | grad 1.47 | tok/s 11181
step   3830 | loss 1.2052 | lr 8.95e-04 | grad 1.36 | tok/s 11164
step   3840 | loss 1.1963 | lr 8.95e-04 | grad 1.52 | tok/s 11169
step   3850 | loss 1.2596 | lr 8.95e-04 | grad 1.41 | tok/s 11098
step   3860 | loss 1.4834 | lr 8.95e-04 | grad 1.54 | tok/s 10561
step   3870 | loss 1.6731 | lr 8.95e-04 | grad 1.60 | tok/s 10699
step   3880 | loss 1.4574 | lr 8.95e-04 | grad 1.13 | tok/s 10252
step   3890 | loss 1.4707 | lr 8.95e-04 | grad 1.72 | tok/s 10855
step   3900 | loss 1.5541 | lr 8.95e-04 | grad 0.86 | tok/s 10904
step   3910 | loss 1.4178 | lr 8.95e-04 | grad 1.88 | tok/s 10458
step   3920 | loss 1.5949 | lr 8.95e-04 | grad 0.96 | tok/s 10761
step   3930 | loss 1.5421 | lr 8.95e-04 | grad 1.45 | tok/s 10875
step   3940 | loss 1.4470 | lr 8.95e-04 | grad 1.12 | tok/s 10601
step   3950 | loss 1.4590 | lr 8.95e-04 | grad 2.08 | tok/s 10504
step   3960 | loss 1.4916 | lr 8.95e-04 | grad 1.14 | tok/s 11182
step   3970 | loss 1.5800 | lr 8.95e-04 | grad 2.53 | tok/s 10492
step   3980 | loss 1.4141 | lr 8.95e-04 | grad 2.12 | tok/s 11155
step   3990 | loss 0.9762 | lr 8.95e-04 | grad 1.30 | tok/s 11298
step   4000 | loss 1.1386 | lr 8.95e-04 | grad 1.12 | tok/s 11184
  >>> saved checkpoint: checkpoint_step_004000_loss_1.1386.pt
step   4010 | loss 1.4253 | lr 8.95e-04 | grad 1.10 | tok/s 3486
step   4020 | loss 1.3827 | lr 8.95e-04 | grad 1.24 | tok/s 10472
step   4030 | loss 1.4483 | lr 8.95e-04 | grad 2.97 | tok/s 11119
step   4040 | loss 1.3913 | lr 8.95e-04 | grad 1.29 | tok/s 10357
step   4050 | loss 2.2800 | lr 8.95e-04 | grad 1.40 | tok/s 10903
step   4060 | loss 1.5428 | lr 8.95e-04 | grad 1.02 | tok/s 10670
step   4070 | loss 1.4106 | lr 8.95e-04 | grad 1.70 | tok/s 11018
step   4080 | loss 1.3550 | lr 8.95e-04 | grad 1.09 | tok/s 10868
step   4090 | loss 1.4024 | lr 8.95e-04 | grad 2.38 | tok/s 10770
step   4100 | loss 1.4647 | lr 8.95e-04 | grad 4.22 | tok/s 11265
step   4110 | loss 1.6055 | lr 8.95e-04 | grad 1.29 | tok/s 10690
step   4120 | loss 1.5536 | lr 8.95e-04 | grad 1.30 | tok/s 10608
step   4130 | loss 0.9170 | lr 8.95e-04 | grad 1.12 | tok/s 11381
step   4140 | loss 1.4236 | lr 8.95e-04 | grad 0.96 | tok/s 10129
step   4150 | loss 1.2330 | lr 8.95e-04 | grad 1.32 | tok/s 10703
step   4160 | loss 1.0532 | lr 8.95e-04 | grad 1.53 | tok/s 11052
step   4170 | loss 1.7953 | lr 8.95e-04 | grad 3.06 | tok/s 11157
step   4180 | loss 1.7878 | lr 8.95e-04 | grad 2.20 | tok/s 11156
step   4190 | loss 1.3945 | lr 8.95e-04 | grad 3.05 | tok/s 11262
step   4200 | loss 1.4814 | lr 8.95e-04 | grad 1.16 | tok/s 10769
step   4210 | loss 1.3571 | lr 8.95e-04 | grad 1.26 | tok/s 10553
step   4220 | loss 2.1956 | lr 8.95e-04 | grad 2.97 | tok/s 10924
step   4230 | loss 2.0275 | lr 8.95e-04 | grad 1.29 | tok/s 10814
step   4240 | loss 1.5343 | lr 8.95e-04 | grad 1.27 | tok/s 10665
step   4250 | loss 1.3700 | lr 8.95e-04 | grad 1.27 | tok/s 10427
step   4260 | loss 1.4671 | lr 8.95e-04 | grad 0.97 | tok/s 11175
step   4270 | loss 1.6221 | lr 8.95e-04 | grad 1.38 | tok/s 10702
step   4280 | loss 1.5791 | lr 8.95e-04 | grad 1.60 | tok/s 10621
step   4290 | loss 1.7659 | lr 8.95e-04 | grad 1.61 | tok/s 11189
step   4300 | loss 1.4476 | lr 8.95e-04 | grad 1.30 | tok/s 10697
step   4310 | loss 1.5285 | lr 8.95e-04 | grad 1.16 | tok/s 11026
step   4320 | loss 1.4409 | lr 8.95e-04 | grad 1.16 | tok/s 10708
step   4330 | loss 1.4550 | lr 8.95e-04 | grad 1.08 | tok/s 11055
step   4340 | loss 1.1695 | lr 8.95e-04 | grad 1.16 | tok/s 11256
step   4350 | loss 1.1319 | lr 8.95e-04 | grad 1.52 | tok/s 11136
step   4360 | loss 1.4829 | lr 8.95e-04 | grad 1.14 | tok/s 10669
step   4370 | loss 1.5203 | lr 8.95e-04 | grad 1.18 | tok/s 10790
step   4380 | loss 1.3079 | lr 8.95e-04 | grad 1.16 | tok/s 11160
step   4390 | loss 1.2091 | lr 8.95e-04 | grad 0.82 | tok/s 11212
step   4400 | loss 1.4200 | lr 8.95e-04 | grad 1.75 | tok/s 11209
step   4410 | loss 1.5519 | lr 8.95e-04 | grad 2.42 | tok/s 10635
step   4420 | loss 1.3036 | lr 8.95e-04 | grad 1.94 | tok/s 10688
step   4430 | loss 1.5798 | lr 8.95e-04 | grad 1.91 | tok/s 10370
step   4440 | loss 1.4162 | lr 8.95e-04 | grad 1.23 | tok/s 10321
step   4450 | loss 1.8417 | lr 8.95e-04 | grad 1.54 | tok/s 10963
step   4460 | loss 1.2799 | lr 8.95e-04 | grad 1.02 | tok/s 11079
step   4470 | loss 1.3985 | lr 8.95e-04 | grad 1.27 | tok/s 10661
step   4480 | loss 1.5296 | lr 8.95e-04 | grad 0.96 | tok/s 10519
step   4490 | loss 1.3274 | lr 8.95e-04 | grad 1.22 | tok/s 10847
step   4500 | loss 1.6238 | lr 8.95e-04 | grad 1.45 | tok/s 11033
step   4510 | loss 1.4958 | lr 8.95e-04 | grad 1.27 | tok/s 10971
step   4520 | loss 1.6685 | lr 8.95e-04 | grad 1.23 | tok/s 10466
step   4530 | loss 1.6755 | lr 8.95e-04 | grad 1.70 | tok/s 10355
step   4540 | loss 1.5047 | lr 8.95e-04 | grad 1.30 | tok/s 10432
step   4550 | loss 1.4128 | lr 8.95e-04 | grad 1.29 | tok/s 11086
step   4560 | loss 1.2302 | lr 8.95e-04 | grad 1.91 | tok/s 11098
step   4570 | loss 1.3172 | lr 8.95e-04 | grad 0.91 | tok/s 9941
step   4580 | loss 2.1177 | lr 8.95e-04 | grad 1.49 | tok/s 10664
step   4590 | loss 1.3664 | lr 8.95e-04 | grad 1.16 | tok/s 10576
step   4600 | loss 1.6399 | lr 8.95e-04 | grad 1.03 | tok/s 10750
step   4610 | loss 1.2536 | lr 8.95e-04 | grad 1.26 | tok/s 11074
step   4620 | loss 1.4661 | lr 8.95e-04 | grad 1.12 | tok/s 11200
step   4630 | loss 1.3764 | lr 8.95e-04 | grad 1.48 | tok/s 11194
step   4640 | loss 1.3651 | lr 8.95e-04 | grad 1.26 | tok/s 11204
step   4650 | loss 1.3462 | lr 8.95e-04 | grad 1.34 | tok/s 11189
step   4660 | loss 1.3114 | lr 8.95e-04 | grad 1.19 | tok/s 11195
step   4670 | loss 1.2924 | lr 8.95e-04 | grad 1.66 | tok/s 11202
step   4680 | loss 1.2896 | lr 8.95e-04 | grad 1.02 | tok/s 11195
step   4690 | loss 1.2667 | lr 8.95e-04 | grad 1.47 | tok/s 11174
step   4700 | loss 1.3056 | lr 8.95e-04 | grad 1.44 | tok/s 11180
step   4710 | loss 1.2028 | lr 8.95e-04 | grad 1.41 | tok/s 11181
step   4720 | loss 1.2077 | lr 8.95e-04 | grad 1.49 | tok/s 11198
step   4730 | loss 1.2506 | lr 8.95e-04 | grad 1.68 | tok/s 11188
step   4740 | loss 1.2347 | lr 8.95e-04 | grad 1.18 | tok/s 11183
step   4750 | loss 1.1880 | lr 8.95e-04 | grad 1.20 | tok/s 11186

Training complete! Final step: 4759
