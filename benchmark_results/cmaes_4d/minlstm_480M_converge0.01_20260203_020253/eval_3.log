Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_3/levelminlstm_100m_20260203_020300
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 133,109,760 parameters
Using schedule-free AdamW (lr=0.00014650000641689155)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 3.6945 | lr 1.47e-04 | grad 2.11 | tok/s 26546
step     20 | loss 3.3020 | lr 1.47e-04 | grad 9.12 | tok/s 29050
step     30 | loss 3.9225 | lr 1.47e-04 | grad 3.52 | tok/s 30261
step     40 | loss 3.3373 | lr 1.47e-04 | grad 1.78 | tok/s 30241
step     50 | loss 3.0290 | lr 1.47e-04 | grad 2.08 | tok/s 30204
step     60 | loss 3.0099 | lr 1.47e-04 | grad 1.45 | tok/s 29496
step     70 | loss 2.7942 | lr 1.47e-04 | grad 2.97 | tok/s 29145
step     80 | loss 3.1618 | lr 1.47e-04 | grad 2.95 | tok/s 29809
step     90 | loss 2.9046 | lr 1.47e-04 | grad 4.53 | tok/s 28692
step    100 | loss 2.5518 | lr 1.47e-04 | grad 1.85 | tok/s 28878
step    110 | loss 2.6107 | lr 1.47e-04 | grad 2.33 | tok/s 28868
step    120 | loss 2.6649 | lr 1.47e-04 | grad 1.98 | tok/s 28707
step    130 | loss 2.6551 | lr 1.47e-04 | grad 1.65 | tok/s 29219
step    140 | loss 2.4331 | lr 1.47e-04 | grad 2.75 | tok/s 28272
step    150 | loss 2.4076 | lr 1.47e-04 | grad 1.94 | tok/s 28274
step    160 | loss 2.3375 | lr 1.47e-04 | grad 2.92 | tok/s 28241
step    170 | loss 2.4698 | lr 1.47e-04 | grad 3.06 | tok/s 28971
step    180 | loss 2.4326 | lr 1.47e-04 | grad 1.59 | tok/s 28448
step    190 | loss 2.2890 | lr 1.47e-04 | grad 1.84 | tok/s 29896
step    200 | loss 2.1757 | lr 1.47e-04 | grad 2.59 | tok/s 29534
step    210 | loss 2.4161 | lr 1.47e-04 | grad 2.53 | tok/s 29078
step    220 | loss 2.3264 | lr 1.47e-04 | grad 1.89 | tok/s 29197
step    230 | loss 2.2611 | lr 1.47e-04 | grad 2.23 | tok/s 29035
step    240 | loss 2.3267 | lr 1.47e-04 | grad 2.22 | tok/s 29348
step    250 | loss 2.2414 | lr 1.47e-04 | grad 2.03 | tok/s 28720
step    260 | loss 2.2944 | lr 1.47e-04 | grad 1.70 | tok/s 28329
step    270 | loss 2.2949 | lr 1.47e-04 | grad 1.78 | tok/s 28711
step    280 | loss 2.0007 | lr 1.47e-04 | grad 2.14 | tok/s 28964
step    290 | loss 2.0771 | lr 1.47e-04 | grad 1.84 | tok/s 30034
step    300 | loss 2.0386 | lr 1.47e-04 | grad 2.05 | tok/s 30030
step    310 | loss 1.9887 | lr 1.47e-04 | grad 2.05 | tok/s 30027
step    320 | loss 2.1118 | lr 1.47e-04 | grad 1.95 | tok/s 28432
step    330 | loss 2.1546 | lr 1.47e-04 | grad 2.09 | tok/s 29162
step    340 | loss 2.2251 | lr 1.47e-04 | grad 2.33 | tok/s 28569
step    350 | loss 2.1158 | lr 1.47e-04 | grad 1.44 | tok/s 28364
step    360 | loss 2.1543 | lr 1.47e-04 | grad 2.44 | tok/s 28553
step    370 | loss 2.0685 | lr 1.47e-04 | grad 1.84 | tok/s 29275
step    380 | loss 2.4026 | lr 1.47e-04 | grad 2.03 | tok/s 29841
step    390 | loss 2.0493 | lr 1.47e-04 | grad 2.97 | tok/s 29110
step    400 | loss 2.2779 | lr 1.47e-04 | grad 3.03 | tok/s 28980
step    410 | loss 1.8735 | lr 1.47e-04 | grad 2.83 | tok/s 28886
step    420 | loss 2.1205 | lr 1.47e-04 | grad 2.78 | tok/s 28290
step    430 | loss 2.1449 | lr 1.47e-04 | grad 2.12 | tok/s 28633
step    440 | loss 2.2700 | lr 1.47e-04 | grad 2.98 | tok/s 29486
step    450 | loss 2.0039 | lr 1.47e-04 | grad 2.00 | tok/s 28780
step    460 | loss 2.0711 | lr 1.47e-04 | grad 2.11 | tok/s 28726
step    470 | loss 1.9947 | lr 1.47e-04 | grad 2.72 | tok/s 28858
step    480 | loss 1.9705 | lr 1.47e-04 | grad 2.08 | tok/s 27776
step    490 | loss 2.2956 | lr 1.47e-04 | grad 4.56 | tok/s 29282
step    500 | loss 2.3361 | lr 1.47e-04 | grad 2.38 | tok/s 28890
step    510 | loss 1.7892 | lr 1.47e-04 | grad 5.81 | tok/s 29446
step    520 | loss 2.0964 | lr 1.47e-04 | grad 2.34 | tok/s 28985
step    530 | loss 2.4293 | lr 1.47e-04 | grad 3.55 | tok/s 29302
step    540 | loss 1.7892 | lr 1.47e-04 | grad 3.23 | tok/s 29406
step    550 | loss 1.8132 | lr 1.47e-04 | grad 3.06 | tok/s 30042
step    560 | loss 1.7952 | lr 1.47e-04 | grad 2.78 | tok/s 29902
step    570 | loss 2.2902 | lr 1.47e-04 | grad 4.88 | tok/s 29499
step    580 | loss 2.3122 | lr 1.47e-04 | grad 3.36 | tok/s 29095
step    590 | loss 2.0902 | lr 1.47e-04 | grad 1.98 | tok/s 28540
step    600 | loss 2.1338 | lr 1.47e-04 | grad 4.19 | tok/s 29790
step    610 | loss 1.8648 | lr 1.47e-04 | grad 3.22 | tok/s 28960
step    620 | loss 1.9318 | lr 1.47e-04 | grad 2.22 | tok/s 29357
step    630 | loss 2.1057 | lr 1.47e-04 | grad 3.19 | tok/s 29479
step    640 | loss 2.0081 | lr 1.47e-04 | grad 2.75 | tok/s 29368
step    650 | loss 1.9919 | lr 1.47e-04 | grad 1.87 | tok/s 28361
step    660 | loss 2.2516 | lr 1.47e-04 | grad 2.66 | tok/s 29024
step    670 | loss 2.0542 | lr 1.47e-04 | grad 2.91 | tok/s 28965
step    680 | loss 2.1557 | lr 1.47e-04 | grad 5.72 | tok/s 28944
step    690 | loss 2.1229 | lr 1.47e-04 | grad 2.22 | tok/s 28892
step    700 | loss 2.0195 | lr 1.47e-04 | grad 2.94 | tok/s 28546
step    710 | loss 1.9565 | lr 1.47e-04 | grad 4.12 | tok/s 28627
step    720 | loss 2.1590 | lr 1.47e-04 | grad 2.38 | tok/s 29054
step    730 | loss 2.0409 | lr 1.47e-04 | grad 2.03 | tok/s 29220
step    740 | loss 1.9557 | lr 1.47e-04 | grad 4.22 | tok/s 28305
step    750 | loss 2.1371 | lr 1.47e-04 | grad 2.11 | tok/s 29105
step    760 | loss 1.9040 | lr 1.47e-04 | grad 2.20 | tok/s 28952
step    770 | loss 1.9394 | lr 1.47e-04 | grad 2.91 | tok/s 29215
step    780 | loss 2.0019 | lr 1.47e-04 | grad 3.12 | tok/s 29233
step    790 | loss 1.9061 | lr 1.47e-04 | grad 2.59 | tok/s 29610
step    800 | loss 1.8779 | lr 1.47e-04 | grad 2.34 | tok/s 28670
step    810 | loss 2.3254 | lr 1.47e-04 | grad 4.69 | tok/s 29559
step    820 | loss 2.5473 | lr 1.47e-04 | grad 3.08 | tok/s 30036
step    830 | loss 2.3535 | lr 1.47e-04 | grad 2.94 | tok/s 30046
step    840 | loss 2.2365 | lr 1.47e-04 | grad 3.62 | tok/s 29470
step    850 | loss 1.9304 | lr 1.47e-04 | grad 2.34 | tok/s 28269
step    860 | loss 1.8522 | lr 1.47e-04 | grad 1.93 | tok/s 29306
step    870 | loss 1.9786 | lr 1.47e-04 | grad 2.53 | tok/s 29085
step    880 | loss 1.8936 | lr 1.47e-04 | grad 2.56 | tok/s 28965
step    890 | loss 2.0859 | lr 1.47e-04 | grad 2.45 | tok/s 28885
step    900 | loss 1.9640 | lr 1.47e-04 | grad 2.48 | tok/s 28304
step    910 | loss 1.8836 | lr 1.47e-04 | grad 2.23 | tok/s 28641
step    920 | loss 1.9213 | lr 1.47e-04 | grad 3.09 | tok/s 28227
step    930 | loss 1.9651 | lr 1.47e-04 | grad 2.78 | tok/s 28526
step    940 | loss 1.9384 | lr 1.47e-04 | grad 3.92 | tok/s 28943
step    950 | loss 1.8852 | lr 1.47e-04 | grad 4.03 | tok/s 29954
step    960 | loss 1.7617 | lr 1.47e-04 | grad 2.72 | tok/s 30047
step    970 | loss 1.7710 | lr 1.47e-04 | grad 2.28 | tok/s 29568
step    980 | loss 2.0338 | lr 1.47e-04 | grad 2.62 | tok/s 28588
step    990 | loss 1.9991 | lr 1.47e-04 | grad 3.06 | tok/s 28752
step   1000 | loss 2.1349 | lr 1.47e-04 | grad 4.69 | tok/s 28951
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1349.pt
step   1010 | loss 1.8020 | lr 1.47e-04 | grad 2.56 | tok/s 24698
step   1020 | loss 2.0542 | lr 1.47e-04 | grad 2.53 | tok/s 28414
step   1030 | loss 1.8725 | lr 1.47e-04 | grad 2.86 | tok/s 29069
step   1040 | loss 1.8447 | lr 1.47e-04 | grad 2.39 | tok/s 28701
step   1050 | loss 2.1074 | lr 1.47e-04 | grad 2.70 | tok/s 29122
step   1060 | loss 2.2118 | lr 1.47e-04 | grad 5.41 | tok/s 29018
step   1070 | loss 2.3007 | lr 1.47e-04 | grad 3.31 | tok/s 28764
step   1080 | loss 2.1553 | lr 1.47e-04 | grad 2.61 | tok/s 28958
step   1090 | loss 1.9257 | lr 1.47e-04 | grad 3.58 | tok/s 29273
step   1100 | loss 1.8917 | lr 1.47e-04 | grad 4.16 | tok/s 29288
step   1110 | loss 2.0414 | lr 1.47e-04 | grad 4.19 | tok/s 29537
step   1120 | loss 1.9350 | lr 1.47e-04 | grad 2.84 | tok/s 28379
step   1130 | loss 1.8501 | lr 1.47e-04 | grad 2.86 | tok/s 28846
step   1140 | loss 2.1115 | lr 1.47e-04 | grad 1.99 | tok/s 28290
step   1150 | loss 1.8257 | lr 1.47e-04 | grad 3.80 | tok/s 28719
step   1160 | loss 1.8822 | lr 1.47e-04 | grad 2.56 | tok/s 28958
step   1170 | loss 1.8504 | lr 1.47e-04 | grad 2.70 | tok/s 30024
step   1180 | loss 1.8139 | lr 1.47e-04 | grad 3.27 | tok/s 30032
step   1190 | loss 1.7557 | lr 1.47e-04 | grad 2.83 | tok/s 30033
step   1200 | loss 1.7654 | lr 1.47e-04 | grad 3.06 | tok/s 30047
step   1210 | loss 1.8862 | lr 1.47e-04 | grad 5.88 | tok/s 29775
step   1220 | loss 1.6054 | lr 1.47e-04 | grad 2.78 | tok/s 28776
step   1230 | loss 1.8271 | lr 1.47e-04 | grad 3.50 | tok/s 28681
step   1240 | loss 1.8990 | lr 1.47e-04 | grad 3.02 | tok/s 28998
step   1250 | loss 2.0631 | lr 1.47e-04 | grad 4.78 | tok/s 29088
step   1260 | loss 2.0037 | lr 1.47e-04 | grad 2.83 | tok/s 29006
step   1270 | loss 1.9805 | lr 1.47e-04 | grad 3.00 | tok/s 28939
step   1280 | loss 1.8790 | lr 1.47e-04 | grad 4.91 | tok/s 28366
step   1290 | loss 1.9114 | lr 1.47e-04 | grad 3.17 | tok/s 28705
step   1300 | loss 1.9655 | lr 1.47e-04 | grad 2.67 | tok/s 28025
step   1310 | loss 1.9671 | lr 1.47e-04 | grad 3.19 | tok/s 29189
step   1320 | loss 1.8408 | lr 1.47e-04 | grad 4.75 | tok/s 29242
step   1330 | loss 1.7995 | lr 1.47e-04 | grad 2.64 | tok/s 28428
step   1340 | loss 1.9546 | lr 1.47e-04 | grad 3.20 | tok/s 29391
step   1350 | loss 1.8046 | lr 1.47e-04 | grad 3.42 | tok/s 28158
step   1360 | loss 2.0435 | lr 1.47e-04 | grad 2.69 | tok/s 28857
step   1370 | loss 1.8961 | lr 1.47e-04 | grad 2.97 | tok/s 28943
step   1380 | loss 1.9038 | lr 1.47e-04 | grad 4.19 | tok/s 29071
step   1390 | loss 2.0049 | lr 1.47e-04 | grad 3.88 | tok/s 29145
step   1400 | loss 1.8812 | lr 1.47e-04 | grad 4.50 | tok/s 28820
step   1410 | loss 1.8749 | lr 1.47e-04 | grad 2.25 | tok/s 28258
step   1420 | loss 1.7264 | lr 1.47e-04 | grad 2.53 | tok/s 27941
step   1430 | loss 1.7117 | lr 1.47e-04 | grad 3.09 | tok/s 29535
step   1440 | loss 1.8176 | lr 1.47e-04 | grad 2.97 | tok/s 28593
step   1450 | loss 1.9398 | lr 1.47e-04 | grad 3.44 | tok/s 28859
step   1460 | loss 1.8829 | lr 1.47e-04 | grad 2.53 | tok/s 28567
step   1470 | loss 1.8498 | lr 1.47e-04 | grad 2.52 | tok/s 29152
step   1480 | loss 1.9789 | lr 1.47e-04 | grad 3.11 | tok/s 28500
step   1490 | loss 1.9750 | lr 1.47e-04 | grad 2.30 | tok/s 28728
step   1500 | loss 2.0210 | lr 1.47e-04 | grad 3.73 | tok/s 29288
step   1510 | loss 1.8725 | lr 1.47e-04 | grad 2.95 | tok/s 29314
step   1520 | loss 1.8279 | lr 1.47e-04 | grad 3.12 | tok/s 28265
step   1530 | loss 1.8605 | lr 1.47e-04 | grad 3.17 | tok/s 29455
step   1540 | loss 2.1508 | lr 1.47e-04 | grad 4.72 | tok/s 29054
step   1550 | loss 1.9883 | lr 1.47e-04 | grad 2.77 | tok/s 28651
step   1560 | loss 1.9819 | lr 1.47e-04 | grad 5.94 | tok/s 29305
step   1570 | loss 1.8303 | lr 1.47e-04 | grad 2.38 | tok/s 28286
step   1580 | loss 1.7438 | lr 1.47e-04 | grad 3.30 | tok/s 28116
step   1590 | loss 1.8793 | lr 1.47e-04 | grad 2.77 | tok/s 29622
step   1600 | loss 1.7681 | lr 1.47e-04 | grad 2.75 | tok/s 29580
step   1610 | loss 1.8211 | lr 1.47e-04 | grad 2.75 | tok/s 28945
step   1620 | loss 1.8065 | lr 1.47e-04 | grad 2.50 | tok/s 28935
step   1630 | loss 1.7580 | lr 1.47e-04 | grad 4.31 | tok/s 28125
step   1640 | loss 1.8461 | lr 1.47e-04 | grad 3.61 | tok/s 28422
step   1650 | loss 1.8500 | lr 1.47e-04 | grad 2.78 | tok/s 28785
step   1660 | loss 2.2004 | lr 1.47e-04 | grad 3.06 | tok/s 29508
step   1670 | loss 1.9976 | lr 1.47e-04 | grad 3.91 | tok/s 28391
step   1680 | loss 1.9717 | lr 1.47e-04 | grad 3.47 | tok/s 29755
step   1690 | loss 1.9743 | lr 1.47e-04 | grad 4.28 | tok/s 28517
step   1700 | loss 1.8960 | lr 1.47e-04 | grad 3.03 | tok/s 28860
step   1710 | loss 1.9448 | lr 1.47e-04 | grad 4.53 | tok/s 28445
step   1720 | loss 1.9031 | lr 1.47e-04 | grad 2.62 | tok/s 28857
step   1730 | loss 1.8895 | lr 1.47e-04 | grad 3.11 | tok/s 28531
step   1740 | loss 2.0316 | lr 1.47e-04 | grad 3.58 | tok/s 28694
step   1750 | loss 1.8991 | lr 1.47e-04 | grad 2.88 | tok/s 28700
step   1760 | loss 1.8555 | lr 1.47e-04 | grad 3.00 | tok/s 27738
step   1770 | loss 1.9835 | lr 1.47e-04 | grad 3.22 | tok/s 28970
step   1780 | loss 1.8271 | lr 1.47e-04 | grad 4.53 | tok/s 28634
step   1790 | loss 1.8508 | lr 1.47e-04 | grad 3.02 | tok/s 29472
step   1800 | loss 1.7336 | lr 1.47e-04 | grad 2.91 | tok/s 28234
step   1810 | loss 1.7850 | lr 1.47e-04 | grad 2.94 | tok/s 28965
step   1820 | loss 1.8786 | lr 1.47e-04 | grad 4.22 | tok/s 28478
step   1830 | loss 1.9958 | lr 1.47e-04 | grad 3.39 | tok/s 28557
step   1840 | loss 1.8132 | lr 1.47e-04 | grad 4.09 | tok/s 28320
step   1850 | loss 1.8082 | lr 1.47e-04 | grad 3.19 | tok/s 28867
step   1860 | loss 1.8130 | lr 1.47e-04 | grad 3.41 | tok/s 29272
step   1870 | loss 1.9143 | lr 1.47e-04 | grad 5.12 | tok/s 29056
step   1880 | loss 1.9277 | lr 1.47e-04 | grad 2.50 | tok/s 28950
step   1890 | loss 1.8566 | lr 1.47e-04 | grad 6.59 | tok/s 28515
step   1900 | loss 1.8194 | lr 1.47e-04 | grad 3.31 | tok/s 29259
step   1910 | loss 1.7691 | lr 1.47e-04 | grad 3.36 | tok/s 29069
step   1920 | loss 1.7251 | lr 1.47e-04 | grad 3.30 | tok/s 29757
step   1930 | loss 1.7950 | lr 1.47e-04 | grad 3.83 | tok/s 29222
step   1940 | loss 2.1198 | lr 1.47e-04 | grad 13.12 | tok/s 29244
step   1950 | loss 1.8729 | lr 1.47e-04 | grad 8.25 | tok/s 28900
step   1960 | loss 1.7919 | lr 1.47e-04 | grad 4.53 | tok/s 28502
step   1970 | loss 1.9891 | lr 1.47e-04 | grad 4.12 | tok/s 29117
step   1980 | loss 1.8464 | lr 1.47e-04 | grad 3.69 | tok/s 29380
step   1990 | loss 1.6797 | lr 1.47e-04 | grad 3.38 | tok/s 29671
step   2000 | loss 1.5673 | lr 1.47e-04 | grad 3.45 | tok/s 30030
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5673.pt
step   2010 | loss 1.8059 | lr 1.47e-04 | grad 3.11 | tok/s 25490
step   2020 | loss 1.6467 | lr 1.47e-04 | grad 4.19 | tok/s 30041
step   2030 | loss 1.6458 | lr 1.47e-04 | grad 2.92 | tok/s 29689
step   2040 | loss 1.8385 | lr 1.47e-04 | grad 3.50 | tok/s 28482
step   2050 | loss 1.7781 | lr 1.47e-04 | grad 2.67 | tok/s 29567
step   2060 | loss 1.9802 | lr 1.47e-04 | grad 5.06 | tok/s 28580
step   2070 | loss 1.8171 | lr 1.47e-04 | grad 2.70 | tok/s 29081
step   2080 | loss 1.8070 | lr 1.47e-04 | grad 2.92 | tok/s 28397
step   2090 | loss 1.6490 | lr 1.47e-04 | grad 3.95 | tok/s 29469
step   2100 | loss 1.6236 | lr 1.47e-04 | grad 3.52 | tok/s 28953
step   2110 | loss 1.8484 | lr 1.47e-04 | grad 2.73 | tok/s 28609
step   2120 | loss 1.9534 | lr 1.47e-04 | grad 3.05 | tok/s 28646
step   2130 | loss 1.8542 | lr 1.47e-04 | grad 4.75 | tok/s 28516
step   2140 | loss 1.7693 | lr 1.47e-04 | grad 3.69 | tok/s 28587
step   2150 | loss 2.0071 | lr 1.47e-04 | grad 3.83 | tok/s 28902
step   2160 | loss 1.9494 | lr 1.47e-04 | grad 4.03 | tok/s 29505
step   2170 | loss 1.9229 | lr 1.47e-04 | grad 4.25 | tok/s 29275
step   2180 | loss 1.6159 | lr 1.47e-04 | grad 3.59 | tok/s 30027
step   2190 | loss 1.6482 | lr 1.47e-04 | grad 2.81 | tok/s 30024
step   2200 | loss 1.6385 | lr 1.47e-04 | grad 3.06 | tok/s 30027
step   2210 | loss 1.7943 | lr 1.47e-04 | grad 2.66 | tok/s 28902
step   2220 | loss 2.0418 | lr 1.47e-04 | grad 3.23 | tok/s 29993
step   2230 | loss 1.9793 | lr 1.47e-04 | grad 3.48 | tok/s 29481
step   2240 | loss 1.8752 | lr 1.47e-04 | grad 4.78 | tok/s 29037
step   2250 | loss 1.8110 | lr 1.47e-04 | grad 3.50 | tok/s 29270
step   2260 | loss 1.9292 | lr 1.47e-04 | grad 2.70 | tok/s 28370
step   2270 | loss 1.7697 | lr 1.47e-04 | grad 2.86 | tok/s 28708
step   2280 | loss 1.7511 | lr 1.47e-04 | grad 3.50 | tok/s 28425
step   2290 | loss 1.8242 | lr 1.47e-04 | grad 3.42 | tok/s 30027
step   2300 | loss 1.7899 | lr 1.47e-04 | grad 3.97 | tok/s 30030
step   2310 | loss 1.7453 | lr 1.47e-04 | grad 3.39 | tok/s 30029
step   2320 | loss 1.7692 | lr 1.47e-04 | grad 3.53 | tok/s 29142
step   2330 | loss 1.9725 | lr 1.47e-04 | grad 5.88 | tok/s 29038
step   2340 | loss 2.2962 | lr 1.47e-04 | grad 2.83 | tok/s 28901
step   2350 | loss 1.7966 | lr 1.47e-04 | grad 4.34 | tok/s 29505
step   2360 | loss 1.8001 | lr 1.47e-04 | grad 5.62 | tok/s 28591
step   2370 | loss 1.7108 | lr 1.47e-04 | grad 3.17 | tok/s 28346
step   2380 | loss 1.8707 | lr 1.47e-04 | grad 3.66 | tok/s 28466
step   2390 | loss 1.9194 | lr 1.47e-04 | grad 5.97 | tok/s 29042
step   2400 | loss 1.7474 | lr 1.47e-04 | grad 2.88 | tok/s 28165
step   2410 | loss 1.9653 | lr 1.47e-04 | grad 3.58 | tok/s 28958
step   2420 | loss 1.7476 | lr 1.47e-04 | grad 3.78 | tok/s 28861
step   2430 | loss 1.7322 | lr 1.47e-04 | grad 3.70 | tok/s 28873
step   2440 | loss 2.0415 | lr 1.47e-04 | grad 5.91 | tok/s 29915
step   2450 | loss 1.8986 | lr 1.47e-04 | grad 4.59 | tok/s 29297
step   2460 | loss 1.7186 | lr 1.47e-04 | grad 2.94 | tok/s 28832
step   2470 | loss 1.7200 | lr 1.47e-04 | grad 4.84 | tok/s 28842
step   2480 | loss 1.8831 | lr 1.47e-04 | grad 4.78 | tok/s 29098
step   2490 | loss 1.8269 | lr 1.47e-04 | grad 3.34 | tok/s 29510
step   2500 | loss 2.0146 | lr 1.47e-04 | grad 3.05 | tok/s 28256
step   2510 | loss 1.8186 | lr 1.47e-04 | grad 3.72 | tok/s 28546
step   2520 | loss 1.8170 | lr 1.47e-04 | grad 2.64 | tok/s 28233
step   2530 | loss 2.2617 | lr 1.47e-04 | grad 4.56 | tok/s 29381
step   2540 | loss 1.8635 | lr 1.47e-04 | grad 3.59 | tok/s 28751
step   2550 | loss 1.9309 | lr 1.47e-04 | grad 4.03 | tok/s 29402
step   2560 | loss 1.7177 | lr 1.47e-04 | grad 3.86 | tok/s 28573
step   2570 | loss 1.7627 | lr 1.47e-04 | grad 3.83 | tok/s 29048
step   2580 | loss 1.7790 | lr 1.47e-04 | grad 3.80 | tok/s 29462
step   2590 | loss 1.8404 | lr 1.47e-04 | grad 3.12 | tok/s 29819
step   2600 | loss 1.7433 | lr 1.47e-04 | grad 4.62 | tok/s 30010
step   2610 | loss 1.7640 | lr 1.47e-04 | grad 4.28 | tok/s 28457
step   2620 | loss 1.8523 | lr 1.47e-04 | grad 4.62 | tok/s 28490
step   2630 | loss 1.8720 | lr 1.47e-04 | grad 8.31 | tok/s 28586
step   2640 | loss 1.8533 | lr 1.47e-04 | grad 4.19 | tok/s 29757
step   2650 | loss 1.6591 | lr 1.47e-04 | grad 3.94 | tok/s 30033
step   2660 | loss 1.6523 | lr 1.47e-04 | grad 3.73 | tok/s 30029
step   2670 | loss 1.8041 | lr 1.47e-04 | grad 3.30 | tok/s 29388
step   2680 | loss 1.7467 | lr 1.47e-04 | grad 2.30 | tok/s 28264
step   2690 | loss 2.1827 | lr 1.47e-04 | grad 3.88 | tok/s 29375
step   2700 | loss 1.9948 | lr 1.47e-04 | grad 3.92 | tok/s 28915
step   2710 | loss 1.7219 | lr 1.47e-04 | grad 3.69 | tok/s 29372
step   2720 | loss 1.7878 | lr 1.47e-04 | grad 3.23 | tok/s 28643
step   2730 | loss 1.8904 | lr 1.47e-04 | grad 7.78 | tok/s 28894
step   2740 | loss 1.9407 | lr 1.47e-04 | grad 5.91 | tok/s 28459
step   2750 | loss 1.7299 | lr 1.47e-04 | grad 4.47 | tok/s 28835
step   2760 | loss 1.7402 | lr 1.47e-04 | grad 3.95 | tok/s 28824
step   2770 | loss 1.9855 | lr 1.47e-04 | grad 19.75 | tok/s 28924
step   2780 | loss 2.0814 | lr 1.47e-04 | grad 4.59 | tok/s 28864
step   2790 | loss 1.7833 | lr 1.47e-04 | grad 3.39 | tok/s 29545
step   2800 | loss 2.0844 | lr 1.47e-04 | grad 3.17 | tok/s 29534
step   2810 | loss 1.8615 | lr 1.47e-04 | grad 5.41 | tok/s 29310
step   2820 | loss 1.6419 | lr 1.47e-04 | grad 3.67 | tok/s 28735
step   2830 | loss 1.7850 | lr 1.47e-04 | grad 3.42 | tok/s 29043
step   2840 | loss 1.8067 | lr 1.47e-04 | grad 6.41 | tok/s 29230
step   2850 | loss 2.3418 | lr 1.47e-04 | grad 3.78 | tok/s 29462
step   2860 | loss 1.9093 | lr 1.47e-04 | grad 4.59 | tok/s 29023
step   2870 | loss 1.8080 | lr 1.47e-04 | grad 6.12 | tok/s 28560
step   2880 | loss 1.6793 | lr 1.47e-04 | grad 3.48 | tok/s 28455
step   2890 | loss 1.9043 | lr 1.47e-04 | grad 3.44 | tok/s 29301
step   2900 | loss 1.9026 | lr 1.47e-04 | grad 3.09 | tok/s 29081
step   2910 | loss 1.7309 | lr 1.47e-04 | grad 3.19 | tok/s 28291
step   2920 | loss 1.8814 | lr 1.47e-04 | grad 4.09 | tok/s 28989
step   2930 | loss 1.9300 | lr 1.47e-04 | grad 6.81 | tok/s 28617
step   2940 | loss 1.8831 | lr 1.47e-04 | grad 4.25 | tok/s 28359
step   2950 | loss 1.7220 | lr 1.47e-04 | grad 4.44 | tok/s 29026
step   2960 | loss 1.8593 | lr 1.47e-04 | grad 4.16 | tok/s 29366
step   2970 | loss 1.6379 | lr 1.47e-04 | grad 3.44 | tok/s 29119
step   2980 | loss 1.8836 | lr 1.47e-04 | grad 3.28 | tok/s 29089
step   2990 | loss 1.7437 | lr 1.47e-04 | grad 3.81 | tok/s 29198
step   3000 | loss 1.8266 | lr 1.47e-04 | grad 4.56 | tok/s 28770
  >>> saved checkpoint: checkpoint_step_003000_loss_1.8266.pt
step   3010 | loss 1.7443 | lr 1.47e-04 | grad 3.05 | tok/s 24584
step   3020 | loss 1.8310 | lr 1.47e-04 | grad 4.75 | tok/s 28780
step   3030 | loss 1.6981 | lr 1.47e-04 | grad 3.39 | tok/s 28464
step   3040 | loss 1.7257 | lr 1.47e-04 | grad 4.06 | tok/s 28908
step   3050 | loss 1.7577 | lr 1.47e-04 | grad 3.78 | tok/s 29281
step   3060 | loss 1.6924 | lr 1.47e-04 | grad 4.53 | tok/s 29181
step   3070 | loss 1.7368 | lr 1.47e-04 | grad 3.66 | tok/s 28534
step   3080 | loss 1.6893 | lr 1.47e-04 | grad 3.52 | tok/s 29267
step   3090 | loss 1.7299 | lr 1.47e-04 | grad 4.88 | tok/s 28271
step   3100 | loss 2.2929 | lr 1.47e-04 | grad 5.53 | tok/s 29548
step   3110 | loss 2.2344 | lr 1.47e-04 | grad 4.19 | tok/s 30019
step   3120 | loss 2.1205 | lr 1.47e-04 | grad 4.25 | tok/s 29991
step   3130 | loss 1.8773 | lr 1.47e-04 | grad 4.12 | tok/s 28103
step   3140 | loss 2.4761 | lr 1.47e-04 | grad 4.50 | tok/s 29281
step   3150 | loss 1.8401 | lr 1.47e-04 | grad 5.09 | tok/s 28939
step   3160 | loss 1.8627 | lr 1.47e-04 | grad 4.44 | tok/s 28908
step   3170 | loss 1.7338 | lr 1.47e-04 | grad 3.53 | tok/s 28912
step   3180 | loss 2.3151 | lr 1.47e-04 | grad 3.75 | tok/s 29715
step   3190 | loss 1.8471 | lr 1.47e-04 | grad 3.70 | tok/s 29403
step   3200 | loss 1.8893 | lr 1.47e-04 | grad 3.28 | tok/s 28628
step   3210 | loss 1.7834 | lr 1.47e-04 | grad 3.70 | tok/s 27907
step   3220 | loss 1.7981 | lr 1.47e-04 | grad 2.72 | tok/s 27656
step   3230 | loss 1.6634 | lr 1.47e-04 | grad 3.39 | tok/s 28895
step   3240 | loss 2.1022 | lr 1.47e-04 | grad 3.33 | tok/s 29476
step   3250 | loss 1.7497 | lr 1.47e-04 | grad 4.19 | tok/s 28880
step   3260 | loss 2.0114 | lr 1.47e-04 | grad 4.59 | tok/s 29849
step   3270 | loss 1.8865 | lr 1.47e-04 | grad 3.97 | tok/s 28609
step   3280 | loss 1.8253 | lr 1.47e-04 | grad 3.22 | tok/s 28845

Training complete! Final step: 3284
