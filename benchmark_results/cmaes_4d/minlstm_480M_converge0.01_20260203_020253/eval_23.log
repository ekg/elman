Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_23/levelminlstm_100m_20260203_030359
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 161,018,880 parameters
Using schedule-free AdamW (lr=0.00019341431558599935)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 3.9676 | lr 1.93e-04 | grad 2.80 | tok/s 24316
step     20 | loss 3.3790 | lr 1.93e-04 | grad 9.56 | tok/s 26077
step     30 | loss 3.8335 | lr 1.93e-04 | grad 3.56 | tok/s 27178
step     40 | loss 3.2076 | lr 1.93e-04 | grad 3.12 | tok/s 27152
step     50 | loss 2.9193 | lr 1.93e-04 | grad 1.95 | tok/s 27119
step     60 | loss 2.9528 | lr 1.93e-04 | grad 1.43 | tok/s 26489
step     70 | loss 2.7298 | lr 1.93e-04 | grad 2.98 | tok/s 26179
step     80 | loss 2.9831 | lr 1.93e-04 | grad 2.38 | tok/s 26799
step     90 | loss 2.8129 | lr 1.93e-04 | grad 3.44 | tok/s 25759
step    100 | loss 2.4502 | lr 1.93e-04 | grad 2.27 | tok/s 25952
step    110 | loss 2.5246 | lr 1.93e-04 | grad 1.94 | tok/s 25954
step    120 | loss 2.5693 | lr 1.93e-04 | grad 2.05 | tok/s 25811
step    130 | loss 2.5520 | lr 1.93e-04 | grad 1.84 | tok/s 26267
step    140 | loss 2.3462 | lr 1.93e-04 | grad 2.58 | tok/s 25431
step    150 | loss 2.3173 | lr 1.93e-04 | grad 1.98 | tok/s 25459
step    160 | loss 2.2543 | lr 1.93e-04 | grad 2.64 | tok/s 25418
step    170 | loss 2.3696 | lr 1.93e-04 | grad 2.25 | tok/s 26053
step    180 | loss 2.3269 | lr 1.93e-04 | grad 1.80 | tok/s 25611
step    190 | loss 2.1913 | lr 1.93e-04 | grad 1.98 | tok/s 26945
step    200 | loss 2.0513 | lr 1.93e-04 | grad 2.39 | tok/s 26619
step    210 | loss 2.3224 | lr 1.93e-04 | grad 2.19 | tok/s 26195
step    220 | loss 2.2421 | lr 1.93e-04 | grad 1.80 | tok/s 26305
step    230 | loss 2.1656 | lr 1.93e-04 | grad 2.06 | tok/s 26156
step    240 | loss 2.2181 | lr 1.93e-04 | grad 1.90 | tok/s 26430
step    250 | loss 2.1560 | lr 1.93e-04 | grad 2.08 | tok/s 25878
step    260 | loss 2.1958 | lr 1.93e-04 | grad 1.58 | tok/s 25528
step    270 | loss 2.2023 | lr 1.93e-04 | grad 1.62 | tok/s 25872
step    280 | loss 1.9046 | lr 1.93e-04 | grad 2.44 | tok/s 26106
step    290 | loss 1.9523 | lr 1.93e-04 | grad 1.76 | tok/s 27055
step    300 | loss 1.9063 | lr 1.93e-04 | grad 2.00 | tok/s 27052
step    310 | loss 1.8585 | lr 1.93e-04 | grad 2.52 | tok/s 27041
step    320 | loss 2.0201 | lr 1.93e-04 | grad 1.74 | tok/s 25609
step    330 | loss 2.0527 | lr 1.93e-04 | grad 1.86 | tok/s 26268
step    340 | loss 2.1253 | lr 1.93e-04 | grad 2.23 | tok/s 25737
step    350 | loss 2.0153 | lr 1.93e-04 | grad 1.59 | tok/s 25536
step    360 | loss 2.0503 | lr 1.93e-04 | grad 2.67 | tok/s 25716
step    370 | loss 1.9598 | lr 1.93e-04 | grad 1.84 | tok/s 26365
step    380 | loss 2.2856 | lr 1.93e-04 | grad 2.38 | tok/s 26875
step    390 | loss 1.9432 | lr 1.93e-04 | grad 2.77 | tok/s 26187
step    400 | loss 2.1735 | lr 1.93e-04 | grad 2.58 | tok/s 26089
step    410 | loss 1.7709 | lr 1.93e-04 | grad 2.39 | tok/s 26031
step    420 | loss 2.0202 | lr 1.93e-04 | grad 2.42 | tok/s 25466
step    430 | loss 2.0418 | lr 1.93e-04 | grad 1.91 | tok/s 25770
step    440 | loss 2.1424 | lr 1.93e-04 | grad 2.45 | tok/s 26553
step    450 | loss 1.9125 | lr 1.93e-04 | grad 1.90 | tok/s 25903
step    460 | loss 1.9765 | lr 1.93e-04 | grad 2.17 | tok/s 25860
step    470 | loss 1.8886 | lr 1.93e-04 | grad 2.39 | tok/s 25987
step    480 | loss 1.8687 | lr 1.93e-04 | grad 1.95 | tok/s 25014
step    490 | loss 2.1682 | lr 1.93e-04 | grad 3.88 | tok/s 26379
step    500 | loss 2.1881 | lr 1.93e-04 | grad 2.02 | tok/s 26017
step    510 | loss 1.6949 | lr 1.93e-04 | grad 8.25 | tok/s 26516
step    520 | loss 1.9792 | lr 1.93e-04 | grad 2.33 | tok/s 26107
step    530 | loss 2.3065 | lr 1.93e-04 | grad 2.75 | tok/s 26384
step    540 | loss 1.6809 | lr 1.93e-04 | grad 2.92 | tok/s 26484
step    550 | loss 1.6977 | lr 1.93e-04 | grad 2.61 | tok/s 27044
step    560 | loss 1.6759 | lr 1.93e-04 | grad 2.52 | tok/s 26919
step    570 | loss 2.1821 | lr 1.93e-04 | grad 4.53 | tok/s 26555
step    580 | loss 2.1887 | lr 1.93e-04 | grad 3.19 | tok/s 26190
step    590 | loss 1.9978 | lr 1.93e-04 | grad 1.84 | tok/s 25707
step    600 | loss 2.0179 | lr 1.93e-04 | grad 3.09 | tok/s 26822
step    610 | loss 1.7445 | lr 1.93e-04 | grad 2.45 | tok/s 26072
step    620 | loss 1.8127 | lr 1.93e-04 | grad 2.12 | tok/s 26434
step    630 | loss 1.9573 | lr 1.93e-04 | grad 2.98 | tok/s 26536
step    640 | loss 1.8860 | lr 1.93e-04 | grad 2.34 | tok/s 26453
step    650 | loss 1.8727 | lr 1.93e-04 | grad 1.89 | tok/s 25561
step    660 | loss 2.1110 | lr 1.93e-04 | grad 2.44 | tok/s 26145
step    670 | loss 1.9385 | lr 1.93e-04 | grad 2.28 | tok/s 26098
step    680 | loss 2.0388 | lr 1.93e-04 | grad 5.25 | tok/s 26065
step    690 | loss 1.9989 | lr 1.93e-04 | grad 1.98 | tok/s 26013
step    700 | loss 1.8960 | lr 1.93e-04 | grad 2.94 | tok/s 25705
step    710 | loss 1.8328 | lr 1.93e-04 | grad 3.27 | tok/s 25818
step    720 | loss 2.0232 | lr 1.93e-04 | grad 2.23 | tok/s 26177
step    730 | loss 1.9370 | lr 1.93e-04 | grad 1.99 | tok/s 26326
step    740 | loss 1.8578 | lr 1.93e-04 | grad 3.98 | tok/s 25510
step    750 | loss 2.0513 | lr 1.93e-04 | grad 1.56 | tok/s 26236
step    760 | loss 1.7918 | lr 1.93e-04 | grad 2.05 | tok/s 26063
step    770 | loss 1.8442 | lr 1.93e-04 | grad 2.66 | tok/s 26294
step    780 | loss 1.8846 | lr 1.93e-04 | grad 2.53 | tok/s 26321
step    790 | loss 1.7422 | lr 1.93e-04 | grad 2.25 | tok/s 26670
step    800 | loss 1.7688 | lr 1.93e-04 | grad 2.14 | tok/s 25818
step    810 | loss 2.2166 | lr 1.93e-04 | grad 3.55 | tok/s 26635
step    820 | loss 2.3765 | lr 1.93e-04 | grad 2.88 | tok/s 27060
step    830 | loss 2.1748 | lr 1.93e-04 | grad 2.66 | tok/s 27055
step    840 | loss 2.0886 | lr 1.93e-04 | grad 3.02 | tok/s 26531
step    850 | loss 1.8184 | lr 1.93e-04 | grad 2.02 | tok/s 25478
step    860 | loss 1.7226 | lr 1.93e-04 | grad 2.02 | tok/s 26413
step    870 | loss 1.8639 | lr 1.93e-04 | grad 2.31 | tok/s 26221
step    880 | loss 1.7740 | lr 1.93e-04 | grad 2.31 | tok/s 26078
step    890 | loss 1.9883 | lr 1.93e-04 | grad 2.25 | tok/s 25997
step    900 | loss 1.8529 | lr 1.93e-04 | grad 2.17 | tok/s 25465
step    910 | loss 1.7800 | lr 1.93e-04 | grad 2.06 | tok/s 25766
step    920 | loss 1.8199 | lr 1.93e-04 | grad 2.56 | tok/s 25406
step    930 | loss 1.8466 | lr 1.93e-04 | grad 2.28 | tok/s 25675
step    940 | loss 1.8327 | lr 1.93e-04 | grad 3.52 | tok/s 26063
step    950 | loss 1.7545 | lr 1.93e-04 | grad 2.17 | tok/s 26946
step    960 | loss 1.6363 | lr 1.93e-04 | grad 2.45 | tok/s 27027
step    970 | loss 1.6455 | lr 1.93e-04 | grad 1.91 | tok/s 26599
step    980 | loss 1.9227 | lr 1.93e-04 | grad 2.16 | tok/s 25758
step    990 | loss 1.8804 | lr 1.93e-04 | grad 2.67 | tok/s 25872
step   1000 | loss 2.0055 | lr 1.93e-04 | grad 4.09 | tok/s 26058
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0055.pt
step   1010 | loss 1.6766 | lr 1.93e-04 | grad 2.53 | tok/s 22204
step   1020 | loss 1.9419 | lr 1.93e-04 | grad 1.95 | tok/s 25572
step   1030 | loss 1.7616 | lr 1.93e-04 | grad 2.50 | tok/s 26168
step   1040 | loss 1.7452 | lr 1.93e-04 | grad 1.94 | tok/s 25841
step   1050 | loss 1.9578 | lr 1.93e-04 | grad 2.48 | tok/s 26241
step   1060 | loss 2.0764 | lr 1.93e-04 | grad 4.88 | tok/s 26142
step   1070 | loss 2.1869 | lr 1.93e-04 | grad 2.88 | tok/s 25906
step   1080 | loss 2.0399 | lr 1.93e-04 | grad 2.25 | tok/s 26075
step   1090 | loss 1.8250 | lr 1.93e-04 | grad 2.81 | tok/s 26375
step   1100 | loss 1.7676 | lr 1.93e-04 | grad 3.88 | tok/s 26390
step   1110 | loss 1.8791 | lr 1.93e-04 | grad 3.17 | tok/s 26596
step   1120 | loss 1.8412 | lr 1.93e-04 | grad 2.75 | tok/s 25546
step   1130 | loss 1.7430 | lr 1.93e-04 | grad 2.92 | tok/s 25983
step   1140 | loss 2.0050 | lr 1.93e-04 | grad 1.88 | tok/s 25476
step   1150 | loss 1.7272 | lr 1.93e-04 | grad 2.88 | tok/s 25876
step   1160 | loss 1.7922 | lr 1.93e-04 | grad 2.59 | tok/s 26073
step   1170 | loss 1.7591 | lr 1.93e-04 | grad 2.48 | tok/s 27049
step   1180 | loss 1.7112 | lr 1.93e-04 | grad 2.36 | tok/s 27057
step   1190 | loss 1.6424 | lr 1.93e-04 | grad 2.41 | tok/s 27056
step   1200 | loss 1.6411 | lr 1.93e-04 | grad 2.70 | tok/s 27051
step   1210 | loss 1.7651 | lr 1.93e-04 | grad 5.34 | tok/s 26809
step   1220 | loss 1.5150 | lr 1.93e-04 | grad 2.42 | tok/s 25934
step   1230 | loss 1.7233 | lr 1.93e-04 | grad 2.88 | tok/s 25834
step   1240 | loss 1.7899 | lr 1.93e-04 | grad 2.36 | tok/s 26108
step   1250 | loss 1.9465 | lr 1.93e-04 | grad 3.95 | tok/s 26179
step   1260 | loss 1.8929 | lr 1.93e-04 | grad 2.44 | tok/s 26111
step   1270 | loss 1.8697 | lr 1.93e-04 | grad 2.58 | tok/s 26069
step   1280 | loss 1.7696 | lr 1.93e-04 | grad 4.31 | tok/s 25545
step   1290 | loss 1.8088 | lr 1.93e-04 | grad 2.73 | tok/s 25848
step   1300 | loss 1.8440 | lr 1.93e-04 | grad 2.23 | tok/s 25247
step   1310 | loss 1.8526 | lr 1.93e-04 | grad 3.00 | tok/s 26304
step   1320 | loss 1.7205 | lr 1.93e-04 | grad 3.89 | tok/s 26359
step   1330 | loss 1.6826 | lr 1.93e-04 | grad 2.20 | tok/s 25603
step   1340 | loss 1.8162 | lr 1.93e-04 | grad 2.39 | tok/s 26485
step   1350 | loss 1.6964 | lr 1.93e-04 | grad 2.39 | tok/s 25377
step   1360 | loss 1.9167 | lr 1.93e-04 | grad 2.31 | tok/s 25998
step   1370 | loss 1.7736 | lr 1.93e-04 | grad 2.80 | tok/s 26080
step   1380 | loss 1.7850 | lr 1.93e-04 | grad 3.09 | tok/s 26185
step   1390 | loss 1.8669 | lr 1.93e-04 | grad 3.14 | tok/s 26250
step   1400 | loss 1.7774 | lr 1.93e-04 | grad 4.22 | tok/s 25976
step   1410 | loss 1.7592 | lr 1.93e-04 | grad 1.88 | tok/s 25471
step   1420 | loss 1.6277 | lr 1.93e-04 | grad 2.48 | tok/s 25178
step   1430 | loss 1.5834 | lr 1.93e-04 | grad 2.75 | tok/s 26601
step   1440 | loss 1.7017 | lr 1.93e-04 | grad 2.69 | tok/s 25763
step   1450 | loss 1.8346 | lr 1.93e-04 | grad 2.88 | tok/s 25998
step   1460 | loss 1.7677 | lr 1.93e-04 | grad 2.33 | tok/s 25770
step   1470 | loss 1.7210 | lr 1.93e-04 | grad 2.17 | tok/s 26265
step   1480 | loss 1.8754 | lr 1.93e-04 | grad 2.31 | tok/s 25660
step   1490 | loss 1.8672 | lr 1.93e-04 | grad 2.16 | tok/s 25861
step   1500 | loss 1.8803 | lr 1.93e-04 | grad 3.08 | tok/s 26359
step   1510 | loss 1.7511 | lr 1.93e-04 | grad 2.61 | tok/s 26383
step   1520 | loss 1.7083 | lr 1.93e-04 | grad 2.70 | tok/s 25449
step   1530 | loss 1.7257 | lr 1.93e-04 | grad 2.75 | tok/s 26516
step   1540 | loss 2.0430 | lr 1.93e-04 | grad 14.81 | tok/s 26166
step   1550 | loss 1.8746 | lr 1.93e-04 | grad 2.30 | tok/s 25795
step   1560 | loss 1.8377 | lr 1.93e-04 | grad 5.19 | tok/s 26393
step   1570 | loss 1.7280 | lr 1.93e-04 | grad 2.27 | tok/s 25492
step   1580 | loss 1.6347 | lr 1.93e-04 | grad 2.64 | tok/s 25320
step   1590 | loss 1.7692 | lr 1.93e-04 | grad 2.25 | tok/s 26680
step   1600 | loss 1.6738 | lr 1.93e-04 | grad 2.22 | tok/s 26653
step   1610 | loss 1.7193 | lr 1.93e-04 | grad 3.28 | tok/s 26056
step   1620 | loss 1.6918 | lr 1.93e-04 | grad 2.16 | tok/s 26060
step   1630 | loss 1.6455 | lr 1.93e-04 | grad 3.58 | tok/s 25327
step   1640 | loss 1.7413 | lr 1.93e-04 | grad 2.97 | tok/s 25589
step   1650 | loss 1.7223 | lr 1.93e-04 | grad 1.88 | tok/s 25907
step   1660 | loss 2.0520 | lr 1.93e-04 | grad 2.62 | tok/s 26569
step   1670 | loss 1.8812 | lr 1.93e-04 | grad 3.42 | tok/s 25561
step   1680 | loss 1.8091 | lr 1.93e-04 | grad 3.09 | tok/s 26774
step   1690 | loss 1.8452 | lr 1.93e-04 | grad 3.41 | tok/s 25679
step   1700 | loss 1.7578 | lr 1.93e-04 | grad 2.64 | tok/s 26019
step   1710 | loss 1.8339 | lr 1.93e-04 | grad 3.64 | tok/s 25640
step   1720 | loss 1.7914 | lr 1.93e-04 | grad 2.05 | tok/s 26010
step   1730 | loss 1.7669 | lr 1.93e-04 | grad 2.48 | tok/s 25700
step   1740 | loss 1.9122 | lr 1.93e-04 | grad 2.59 | tok/s 25864
step   1750 | loss 1.7724 | lr 1.93e-04 | grad 2.42 | tok/s 25865
step   1760 | loss 1.7509 | lr 1.93e-04 | grad 2.23 | tok/s 24997
step   1770 | loss 1.8605 | lr 1.93e-04 | grad 2.81 | tok/s 26113
step   1780 | loss 1.7171 | lr 1.93e-04 | grad 3.62 | tok/s 25789
step   1790 | loss 1.7374 | lr 1.93e-04 | grad 2.38 | tok/s 26548
step   1800 | loss 1.6287 | lr 1.93e-04 | grad 2.48 | tok/s 25433
step   1810 | loss 1.6658 | lr 1.93e-04 | grad 2.06 | tok/s 26082
step   1820 | loss 1.7598 | lr 1.93e-04 | grad 3.58 | tok/s 25652
step   1830 | loss 1.8867 | lr 1.93e-04 | grad 2.84 | tok/s 25724
step   1840 | loss 1.7066 | lr 1.93e-04 | grad 3.48 | tok/s 25500
step   1850 | loss 1.6816 | lr 1.93e-04 | grad 3.16 | tok/s 26001
step   1860 | loss 1.6923 | lr 1.93e-04 | grad 2.81 | tok/s 26359
step   1870 | loss 1.7931 | lr 1.93e-04 | grad 4.41 | tok/s 26165
step   1880 | loss 1.7964 | lr 1.93e-04 | grad 3.09 | tok/s 26079
step   1890 | loss 1.7493 | lr 1.93e-04 | grad 5.25 | tok/s 25678
step   1900 | loss 1.6861 | lr 1.93e-04 | grad 2.59 | tok/s 26350
step   1910 | loss 1.6463 | lr 1.93e-04 | grad 2.91 | tok/s 26175
step   1920 | loss 1.6217 | lr 1.93e-04 | grad 2.73 | tok/s 26796
step   1930 | loss 1.7149 | lr 1.93e-04 | grad 3.17 | tok/s 26315
step   1940 | loss 1.9726 | lr 1.93e-04 | grad 9.62 | tok/s 26335
step   1950 | loss 1.6789 | lr 1.93e-04 | grad 7.28 | tok/s 26038
step   1960 | loss 1.6795 | lr 1.93e-04 | grad 3.56 | tok/s 25672
step   1970 | loss 1.8445 | lr 1.93e-04 | grad 3.17 | tok/s 26225
step   1980 | loss 1.7145 | lr 1.93e-04 | grad 2.98 | tok/s 26471
step   1990 | loss 1.5438 | lr 1.93e-04 | grad 2.69 | tok/s 26742
step   2000 | loss 1.4336 | lr 1.93e-04 | grad 3.00 | tok/s 27052
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4336.pt
step   2010 | loss 1.6758 | lr 1.93e-04 | grad 2.72 | tok/s 22927
step   2020 | loss 1.5268 | lr 1.93e-04 | grad 3.20 | tok/s 27042
step   2030 | loss 1.5299 | lr 1.93e-04 | grad 2.56 | tok/s 26737
step   2040 | loss 1.7470 | lr 1.93e-04 | grad 2.75 | tok/s 25657
step   2050 | loss 1.6805 | lr 1.93e-04 | grad 2.23 | tok/s 26638
step   2060 | loss 1.8493 | lr 1.93e-04 | grad 4.53 | tok/s 25754
step   2070 | loss 1.6929 | lr 1.93e-04 | grad 2.12 | tok/s 26212
step   2080 | loss 1.6978 | lr 1.93e-04 | grad 2.27 | tok/s 25598
step   2090 | loss 1.5230 | lr 1.93e-04 | grad 3.19 | tok/s 26562
step   2100 | loss 1.4934 | lr 1.93e-04 | grad 2.47 | tok/s 26089
step   2110 | loss 1.7419 | lr 1.93e-04 | grad 2.34 | tok/s 25770
step   2120 | loss 1.8343 | lr 1.93e-04 | grad 3.28 | tok/s 25814
step   2130 | loss 1.7491 | lr 1.93e-04 | grad 4.66 | tok/s 25705
step   2140 | loss 1.6633 | lr 1.93e-04 | grad 2.56 | tok/s 25754
step   2150 | loss 1.8812 | lr 1.93e-04 | grad 2.88 | tok/s 26039
step   2160 | loss 1.8171 | lr 1.93e-04 | grad 3.91 | tok/s 26586
step   2170 | loss 1.7854 | lr 1.93e-04 | grad 3.25 | tok/s 26365
step   2180 | loss 1.4828 | lr 1.93e-04 | grad 2.83 | tok/s 27025
step   2190 | loss 1.5126 | lr 1.93e-04 | grad 2.47 | tok/s 27036
step   2200 | loss 1.4994 | lr 1.93e-04 | grad 2.52 | tok/s 27029
step   2210 | loss 1.6751 | lr 1.93e-04 | grad 2.25 | tok/s 26040
step   2220 | loss 1.8473 | lr 1.93e-04 | grad 3.58 | tok/s 27014
step   2230 | loss 1.8160 | lr 1.93e-04 | grad 2.86 | tok/s 26559
step   2240 | loss 1.7576 | lr 1.93e-04 | grad 3.80 | tok/s 26154
step   2250 | loss 1.6807 | lr 1.93e-04 | grad 2.92 | tok/s 26370
step   2260 | loss 1.8236 | lr 1.93e-04 | grad 2.38 | tok/s 25554
step   2270 | loss 1.6525 | lr 1.93e-04 | grad 2.53 | tok/s 25858
step   2280 | loss 1.6513 | lr 1.93e-04 | grad 2.83 | tok/s 25583
step   2290 | loss 1.7145 | lr 1.93e-04 | grad 2.75 | tok/s 27038
step   2300 | loss 1.6691 | lr 1.93e-04 | grad 2.89 | tok/s 27040
step   2310 | loss 1.6133 | lr 1.93e-04 | grad 2.66 | tok/s 27037
step   2320 | loss 1.6591 | lr 1.93e-04 | grad 2.34 | tok/s 26259
step   2330 | loss 1.8538 | lr 1.93e-04 | grad 5.03 | tok/s 26169
step   2340 | loss 2.1518 | lr 1.93e-04 | grad 2.14 | tok/s 26045
step   2350 | loss 1.6681 | lr 1.93e-04 | grad 3.47 | tok/s 26594
step   2360 | loss 1.6826 | lr 1.93e-04 | grad 3.69 | tok/s 25756
step   2370 | loss 1.5947 | lr 1.93e-04 | grad 2.38 | tok/s 25529
step   2380 | loss 1.7561 | lr 1.93e-04 | grad 2.78 | tok/s 25644
step   2390 | loss 1.7891 | lr 1.93e-04 | grad 4.84 | tok/s 26175
step   2400 | loss 1.6468 | lr 1.93e-04 | grad 2.38 | tok/s 25383
step   2410 | loss 1.8348 | lr 1.93e-04 | grad 2.41 | tok/s 26082
step   2420 | loss 1.6193 | lr 1.93e-04 | grad 2.62 | tok/s 25991
step   2430 | loss 1.6228 | lr 1.93e-04 | grad 2.98 | tok/s 26037
step   2440 | loss 1.8166 | lr 1.93e-04 | grad 4.66 | tok/s 26923
step   2450 | loss 1.6667 | lr 1.93e-04 | grad 3.25 | tok/s 26383
step   2460 | loss 1.6016 | lr 1.93e-04 | grad 2.48 | tok/s 25954
step   2470 | loss 1.6034 | lr 1.93e-04 | grad 4.19 | tok/s 25970
step   2480 | loss 1.7633 | lr 1.93e-04 | grad 3.89 | tok/s 26194
step   2490 | loss 1.6911 | lr 1.93e-04 | grad 2.70 | tok/s 26564
step   2500 | loss 1.9081 | lr 1.93e-04 | grad 2.19 | tok/s 25438
step   2510 | loss 1.7102 | lr 1.93e-04 | grad 2.83 | tok/s 25697
step   2520 | loss 1.7034 | lr 1.93e-04 | grad 2.42 | tok/s 25424
step   2530 | loss 2.1458 | lr 1.93e-04 | grad 3.44 | tok/s 26455
step   2540 | loss 1.7506 | lr 1.93e-04 | grad 2.83 | tok/s 25884
step   2550 | loss 1.8081 | lr 1.93e-04 | grad 3.02 | tok/s 26480
step   2560 | loss 1.6043 | lr 1.93e-04 | grad 2.86 | tok/s 25741
step   2570 | loss 1.6567 | lr 1.93e-04 | grad 2.52 | tok/s 26145
step   2580 | loss 1.6584 | lr 1.93e-04 | grad 2.69 | tok/s 26525
step   2590 | loss 1.7194 | lr 1.93e-04 | grad 2.69 | tok/s 26851
step   2600 | loss 1.6131 | lr 1.93e-04 | grad 3.45 | tok/s 27018
step   2610 | loss 1.6671 | lr 1.93e-04 | grad 3.72 | tok/s 25624
step   2620 | loss 1.7429 | lr 1.93e-04 | grad 3.84 | tok/s 25642
step   2630 | loss 1.7725 | lr 1.93e-04 | grad 6.91 | tok/s 25741
step   2640 | loss 1.7489 | lr 1.93e-04 | grad 2.83 | tok/s 26791
step   2650 | loss 1.5362 | lr 1.93e-04 | grad 2.97 | tok/s 27032
step   2660 | loss 1.5329 | lr 1.93e-04 | grad 2.38 | tok/s 27018
step   2670 | loss 1.6752 | lr 1.93e-04 | grad 2.88 | tok/s 26467
step   2680 | loss 1.6410 | lr 1.93e-04 | grad 2.03 | tok/s 25477
step   2690 | loss 2.0100 | lr 1.93e-04 | grad 2.94 | tok/s 26473
step   2700 | loss 1.8926 | lr 1.93e-04 | grad 3.23 | tok/s 26046
step   2710 | loss 1.5895 | lr 1.93e-04 | grad 2.95 | tok/s 26449
step   2720 | loss 1.6682 | lr 1.93e-04 | grad 2.41 | tok/s 25798
step   2730 | loss 1.7721 | lr 1.93e-04 | grad 7.09 | tok/s 26027
step   2740 | loss 1.8287 | lr 1.93e-04 | grad 4.97 | tok/s 25643
step   2750 | loss 1.6277 | lr 1.93e-04 | grad 3.23 | tok/s 25956
step   2760 | loss 1.6166 | lr 1.93e-04 | grad 2.92 | tok/s 25964
step   2770 | loss 1.8463 | lr 1.93e-04 | grad 12.94 | tok/s 26038
step   2780 | loss 1.9343 | lr 1.93e-04 | grad 4.22 | tok/s 25990
step   2790 | loss 1.6584 | lr 1.93e-04 | grad 2.88 | tok/s 26605
step   2800 | loss 1.9232 | lr 1.93e-04 | grad 2.53 | tok/s 26599
step   2810 | loss 1.7584 | lr 1.93e-04 | grad 4.22 | tok/s 26392
step   2820 | loss 1.5341 | lr 1.93e-04 | grad 3.20 | tok/s 25888
step   2830 | loss 1.6550 | lr 1.93e-04 | grad 2.41 | tok/s 26147
step   2840 | loss 1.6783 | lr 1.93e-04 | grad 4.59 | tok/s 26331
step   2850 | loss 2.1728 | lr 1.93e-04 | grad 2.64 | tok/s 26533
step   2860 | loss 1.7939 | lr 1.93e-04 | grad 3.66 | tok/s 26146
step   2870 | loss 1.6984 | lr 1.93e-04 | grad 4.91 | tok/s 25724
step   2880 | loss 1.5764 | lr 1.93e-04 | grad 2.30 | tok/s 25625
step   2890 | loss 1.7518 | lr 1.93e-04 | grad 2.47 | tok/s 26388
step   2900 | loss 1.7915 | lr 1.93e-04 | grad 2.59 | tok/s 26194
step   2910 | loss 1.6275 | lr 1.93e-04 | grad 2.11 | tok/s 25485
step   2920 | loss 1.7425 | lr 1.93e-04 | grad 3.17 | tok/s 26133
step   2930 | loss 1.8190 | lr 1.93e-04 | grad 5.88 | tok/s 25763
step   2940 | loss 1.7546 | lr 1.93e-04 | grad 3.34 | tok/s 25549
step   2950 | loss 1.6169 | lr 1.93e-04 | grad 3.17 | tok/s 26135

Training complete! Final step: 2958
