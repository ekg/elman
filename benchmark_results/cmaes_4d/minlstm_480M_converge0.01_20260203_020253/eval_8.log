Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_8/levelminlstm_100m_20260203_020300
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 1,325,639,424 parameters
Using schedule-free AdamW (lr=0.00021190959695285602)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 7.4938 | lr 2.12e-04 | grad 4.19 | tok/s 5525
step     20 | loss 3.3607 | lr 2.12e-04 | grad 3.25 | tok/s 5696
step     30 | loss 3.1218 | lr 2.12e-04 | grad 5.56 | tok/s 5733
step     40 | loss 2.9933 | lr 2.12e-04 | grad 4.25 | tok/s 5375
step     50 | loss 3.2082 | lr 2.12e-04 | grad 14.06 | tok/s 5451
step     60 | loss 3.5403 | lr 2.12e-04 | grad 3.20 | tok/s 5731
step     70 | loss 2.7968 | lr 2.12e-04 | grad 3.55 | tok/s 5726
step     80 | loss 5.2612 | lr 2.12e-04 | grad 12.56 | tok/s 5700
step     90 | loss 5.3177 | lr 2.12e-04 | grad 13.38 | tok/s 5854
step    100 | loss 4.2453 | lr 2.12e-04 | grad 5.22 | tok/s 5850
step    110 | loss 4.0433 | lr 2.12e-04 | grad 15.69 | tok/s 5850
step    120 | loss 3.9196 | lr 2.12e-04 | grad 7.25 | tok/s 5841
step    130 | loss 3.8719 | lr 2.12e-04 | grad 3.19 | tok/s 5830
step    140 | loss 3.4637 | lr 2.12e-04 | grad 2.19 | tok/s 5829
step    150 | loss 3.7736 | lr 2.12e-04 | grad 7.88 | tok/s 5826
step    160 | loss 3.4528 | lr 2.12e-04 | grad 3.03 | tok/s 5821
step    170 | loss 3.3962 | lr 2.12e-04 | grad 4.50 | tok/s 5813
step    180 | loss 3.4050 | lr 2.12e-04 | grad 10.38 | tok/s 5813
step    190 | loss 3.3595 | lr 2.12e-04 | grad 8.25 | tok/s 5814
step    200 | loss 3.2525 | lr 2.12e-04 | grad 12.00 | tok/s 5813
step    210 | loss 3.1694 | lr 2.12e-04 | grad 3.28 | tok/s 5813
step    220 | loss 3.1118 | lr 2.12e-04 | grad 2.73 | tok/s 5740
step    230 | loss 3.3238 | lr 2.12e-04 | grad 7.69 | tok/s 5742
step    240 | loss 3.1877 | lr 2.12e-04 | grad 4.25 | tok/s 5430
step    250 | loss 2.7194 | lr 2.12e-04 | grad 3.95 | tok/s 5475
step    260 | loss 2.5116 | lr 2.12e-04 | grad 2.64 | tok/s 5751
step    270 | loss 2.7027 | lr 2.12e-04 | grad 5.47 | tok/s 5536
step    280 | loss 2.7380 | lr 2.12e-04 | grad 5.25 | tok/s 5680
step    290 | loss 2.9502 | lr 2.12e-04 | grad 3.38 | tok/s 5653
step    300 | loss 2.2086 | lr 2.12e-04 | grad 2.62 | tok/s 5813
step    310 | loss 2.8265 | lr 2.12e-04 | grad 3.41 | tok/s 5709
step    320 | loss 2.8806 | lr 2.12e-04 | grad 4.50 | tok/s 5789
step    330 | loss 2.5480 | lr 2.12e-04 | grad 4.19 | tok/s 5245
step    340 | loss 2.8309 | lr 2.12e-04 | grad 6.06 | tok/s 5565
step    350 | loss 2.4850 | lr 2.12e-04 | grad 3.64 | tok/s 5497
step    360 | loss 3.1171 | lr 2.12e-04 | grad 3.17 | tok/s 5789
step    370 | loss 2.6052 | lr 2.12e-04 | grad 2.22 | tok/s 5265
step    380 | loss 2.4013 | lr 2.12e-04 | grad 2.91 | tok/s 5449
step    390 | loss 2.3479 | lr 2.12e-04 | grad 4.06 | tok/s 5777
step    400 | loss 2.3216 | lr 2.12e-04 | grad 1.73 | tok/s 5740
step    410 | loss 2.3944 | lr 2.12e-04 | grad 3.17 | tok/s 5785
step    420 | loss 2.2578 | lr 2.12e-04 | grad 2.75 | tok/s 5290
step    430 | loss 2.7753 | lr 2.12e-04 | grad 4.62 | tok/s 5636
step    440 | loss 2.6722 | lr 2.12e-04 | grad 2.94 | tok/s 5533
step    450 | loss 2.8660 | lr 2.12e-04 | grad 5.97 | tok/s 5437
step    460 | loss 2.5176 | lr 2.12e-04 | grad 3.00 | tok/s 5492
step    470 | loss 2.5364 | lr 2.12e-04 | grad 1.67 | tok/s 5572
step    480 | loss 2.6496 | lr 2.12e-04 | grad 5.66 | tok/s 5633
step    490 | loss 2.8174 | lr 2.12e-04 | grad 3.09 | tok/s 5539
step    500 | loss 2.2889 | lr 2.12e-04 | grad 1.98 | tok/s 5498
step    510 | loss 2.4663 | lr 2.12e-04 | grad 3.14 | tok/s 5721
step    520 | loss 2.4465 | lr 2.12e-04 | grad 3.86 | tok/s 5740
step    530 | loss 2.5748 | lr 2.12e-04 | grad 3.20 | tok/s 5680
step    540 | loss 2.3267 | lr 2.12e-04 | grad 3.66 | tok/s 5442
step    550 | loss 2.2215 | lr 2.12e-04 | grad 2.84 | tok/s 5605
step    560 | loss 2.2749 | lr 2.12e-04 | grad 4.72 | tok/s 5017
step    570 | loss 2.3022 | lr 2.12e-04 | grad 5.94 | tok/s 5524
step    580 | loss 2.2625 | lr 2.12e-04 | grad 3.42 | tok/s 5416
step    590 | loss 2.3388 | lr 2.12e-04 | grad 5.53 | tok/s 5415
step    600 | loss 2.4993 | lr 2.12e-04 | grad 2.39 | tok/s 5373
step    610 | loss 2.3351 | lr 2.12e-04 | grad 2.28 | tok/s 5610
step    620 | loss 2.1225 | lr 2.12e-04 | grad 3.48 | tok/s 5408
step    630 | loss 2.2894 | lr 2.12e-04 | grad 2.92 | tok/s 5400
step    640 | loss 2.4193 | lr 2.12e-04 | grad 3.86 | tok/s 5475
step    650 | loss 2.3307 | lr 2.12e-04 | grad 2.92 | tok/s 5525
step    660 | loss 2.3284 | lr 2.12e-04 | grad 2.34 | tok/s 5534
step    670 | loss 2.3686 | lr 2.12e-04 | grad 7.62 | tok/s 5557
step    680 | loss 2.5504 | lr 2.12e-04 | grad 3.64 | tok/s 5617
step    690 | loss 2.4923 | lr 2.12e-04 | grad 2.44 | tok/s 5498
step    700 | loss 2.6164 | lr 2.12e-04 | grad 3.09 | tok/s 5776
step    710 | loss 2.3633 | lr 2.12e-04 | grad 2.48 | tok/s 5498
step    720 | loss 2.0619 | lr 2.12e-04 | grad 2.98 | tok/s 5233
step    730 | loss 2.3545 | lr 2.12e-04 | grad 3.02 | tok/s 5785
step    740 | loss 2.2012 | lr 2.12e-04 | grad 3.31 | tok/s 5688
step    750 | loss 2.2464 | lr 2.12e-04 | grad 2.89 | tok/s 5786
step    760 | loss 2.0737 | lr 2.12e-04 | grad 3.02 | tok/s 5782
step    770 | loss 2.1220 | lr 2.12e-04 | grad 2.61 | tok/s 5785
step    780 | loss 2.0857 | lr 2.12e-04 | grad 3.06 | tok/s 5775
step    790 | loss 2.0367 | lr 2.12e-04 | grad 2.92 | tok/s 5786
step    800 | loss 2.3056 | lr 2.12e-04 | grad 2.16 | tok/s 5410
step    810 | loss 2.4437 | lr 2.12e-04 | grad 3.05 | tok/s 5554
step    820 | loss 2.2484 | lr 2.12e-04 | grad 2.39 | tok/s 5461
step    830 | loss 2.2721 | lr 2.12e-04 | grad 2.97 | tok/s 5605
step    840 | loss 2.4084 | lr 2.12e-04 | grad 2.75 | tok/s 5776
step    850 | loss 2.3851 | lr 2.12e-04 | grad 6.94 | tok/s 5752
step    860 | loss 2.3105 | lr 2.12e-04 | grad 3.25 | tok/s 5721
step    870 | loss 2.1698 | lr 2.12e-04 | grad 2.50 | tok/s 5493
step    880 | loss 2.3563 | lr 2.12e-04 | grad 3.22 | tok/s 5519
step    890 | loss 2.2665 | lr 2.12e-04 | grad 3.58 | tok/s 5590
step    900 | loss 2.2037 | lr 2.12e-04 | grad 2.77 | tok/s 5611
step    910 | loss 2.0000 | lr 2.12e-04 | grad 2.09 | tok/s 5453
step    920 | loss 2.4122 | lr 2.12e-04 | grad 3.55 | tok/s 5727
step    930 | loss 2.2279 | lr 2.12e-04 | grad 3.86 | tok/s 5440
step    940 | loss 2.2127 | lr 2.12e-04 | grad 1.95 | tok/s 5628
step    950 | loss 2.3754 | lr 2.12e-04 | grad 3.02 | tok/s 5750
step    960 | loss 2.2121 | lr 2.12e-04 | grad 3.16 | tok/s 5788
step    970 | loss 2.2277 | lr 2.12e-04 | grad 3.58 | tok/s 5515
step    980 | loss 2.3267 | lr 2.12e-04 | grad 3.03 | tok/s 5582
step    990 | loss 2.1236 | lr 2.12e-04 | grad 3.48 | tok/s 5541
step   1000 | loss 2.2061 | lr 2.12e-04 | grad 2.31 | tok/s 5473
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2061.pt
step   1010 | loss 2.2349 | lr 2.12e-04 | grad 2.72 | tok/s 1683
step   1020 | loss 2.0956 | lr 2.12e-04 | grad 2.31 | tok/s 5577
step   1030 | loss 2.1137 | lr 2.12e-04 | grad 3.20 | tok/s 5757
step   1040 | loss 2.1470 | lr 2.12e-04 | grad 2.91 | tok/s 5312
step   1050 | loss 2.3436 | lr 2.12e-04 | grad 2.22 | tok/s 5726
step   1060 | loss 2.4729 | lr 2.12e-04 | grad 1.83 | tok/s 5690
step   1070 | loss 1.9718 | lr 2.12e-04 | grad 2.38 | tok/s 5179
step   1080 | loss 1.7658 | lr 2.12e-04 | grad 1.87 | tok/s 5703
step   1090 | loss 1.9911 | lr 2.12e-04 | grad 3.69 | tok/s 5535
step   1100 | loss 2.1423 | lr 2.12e-04 | grad 2.27 | tok/s 5813
step   1110 | loss 2.1031 | lr 2.12e-04 | grad 3.59 | tok/s 5813
step   1120 | loss 2.0484 | lr 2.12e-04 | grad 2.17 | tok/s 5805
step   1130 | loss 2.0146 | lr 2.12e-04 | grad 2.83 | tok/s 5795
step   1140 | loss 2.0142 | lr 2.12e-04 | grad 3.00 | tok/s 5787
step   1150 | loss 1.9231 | lr 2.12e-04 | grad 3.11 | tok/s 5788
step   1160 | loss 1.9741 | lr 2.12e-04 | grad 2.48 | tok/s 5777
step   1170 | loss 2.0900 | lr 2.12e-04 | grad 2.81 | tok/s 5783
step   1180 | loss 2.0122 | lr 2.12e-04 | grad 2.73 | tok/s 5784
step   1190 | loss 1.9948 | lr 2.12e-04 | grad 3.19 | tok/s 5783
step   1200 | loss 1.9760 | lr 2.12e-04 | grad 3.22 | tok/s 5779
step   1210 | loss 1.9500 | lr 2.12e-04 | grad 3.25 | tok/s 5784
step   1220 | loss 1.9566 | lr 2.12e-04 | grad 2.50 | tok/s 5785
step   1230 | loss 1.9089 | lr 2.12e-04 | grad 2.70 | tok/s 5776
step   1240 | loss 2.4164 | lr 2.12e-04 | grad 3.42 | tok/s 5474
step   1250 | loss 1.8767 | lr 2.12e-04 | grad 2.83 | tok/s 5432
step   1260 | loss 2.1398 | lr 2.12e-04 | grad 3.44 | tok/s 5405
step   1270 | loss 2.2237 | lr 2.12e-04 | grad 3.16 | tok/s 5563
step   1280 | loss 2.0499 | lr 2.12e-04 | grad 2.53 | tok/s 5531
step   1290 | loss 2.1087 | lr 2.12e-04 | grad 3.09 | tok/s 5572
step   1300 | loss 2.0618 | lr 2.12e-04 | grad 2.78 | tok/s 5661
step   1310 | loss 2.1857 | lr 2.12e-04 | grad 2.62 | tok/s 5673
step   1320 | loss 2.2634 | lr 2.12e-04 | grad 3.64 | tok/s 5685
step   1330 | loss 2.1189 | lr 2.12e-04 | grad 6.47 | tok/s 5433
step   1340 | loss 2.2173 | lr 2.12e-04 | grad 2.55 | tok/s 5253
step   1350 | loss 2.1411 | lr 2.12e-04 | grad 3.03 | tok/s 5568
step   1360 | loss 1.9797 | lr 2.12e-04 | grad 1.62 | tok/s 5508
step   1370 | loss 2.2787 | lr 2.12e-04 | grad 1.90 | tok/s 5293
step   1380 | loss 2.0749 | lr 2.12e-04 | grad 2.83 | tok/s 5613
step   1390 | loss 2.0548 | lr 2.12e-04 | grad 3.19 | tok/s 5419
step   1400 | loss 2.0672 | lr 2.12e-04 | grad 2.97 | tok/s 5438
step   1410 | loss 2.2792 | lr 2.12e-04 | grad 3.12 | tok/s 5446
step   1420 | loss 2.0721 | lr 2.12e-04 | grad 2.81 | tok/s 5541
step   1430 | loss 1.9323 | lr 2.12e-04 | grad 2.41 | tok/s 5717
step   1440 | loss 1.9291 | lr 2.12e-04 | grad 3.77 | tok/s 5753
step   1450 | loss 2.2043 | lr 2.12e-04 | grad 3.73 | tok/s 5432
step   1460 | loss 2.1702 | lr 2.12e-04 | grad 2.31 | tok/s 5632
step   1470 | loss 2.6287 | lr 2.12e-04 | grad 3.45 | tok/s 5665
step   1480 | loss 2.4808 | lr 2.12e-04 | grad 2.64 | tok/s 5748
step   1490 | loss 2.1358 | lr 2.12e-04 | grad 2.20 | tok/s 5772
step   1500 | loss 2.1710 | lr 2.12e-04 | grad 2.77 | tok/s 5698
step   1510 | loss 2.0054 | lr 2.12e-04 | grad 3.00 | tok/s 5580
step   1520 | loss 2.0069 | lr 2.12e-04 | grad 2.22 | tok/s 5714
step   1530 | loss 2.1561 | lr 2.12e-04 | grad 2.38 | tok/s 5379
step   1540 | loss 1.9783 | lr 2.12e-04 | grad 2.77 | tok/s 5727
step   1550 | loss 2.0882 | lr 2.12e-04 | grad 3.55 | tok/s 5431
step   1560 | loss 2.0197 | lr 2.12e-04 | grad 2.56 | tok/s 5769
step   1570 | loss 2.5190 | lr 2.12e-04 | grad 3.38 | tok/s 5630
step   1580 | loss 2.3723 | lr 2.12e-04 | grad 2.84 | tok/s 5417
step   1590 | loss 1.7444 | lr 2.12e-04 | grad 1.51 | tok/s 5787
step   1600 | loss 1.5023 | lr 2.12e-04 | grad 2.48 | tok/s 5609
step   1610 | loss 2.0122 | lr 2.12e-04 | grad 2.95 | tok/s 5253
step   1620 | loss 2.1564 | lr 2.12e-04 | grad 3.66 | tok/s 5612
step   1630 | loss 1.9558 | lr 2.12e-04 | grad 2.31 | tok/s 5485
step   1640 | loss 2.0208 | lr 2.12e-04 | grad 2.45 | tok/s 5264
step   1650 | loss 2.1171 | lr 2.12e-04 | grad 3.67 | tok/s 5608
step   1660 | loss 2.1393 | lr 2.12e-04 | grad 3.52 | tok/s 5593
step   1670 | loss 2.1956 | lr 2.12e-04 | grad 2.25 | tok/s 5370
step   1680 | loss 2.1016 | lr 2.12e-04 | grad 3.16 | tok/s 5470
step   1690 | loss 2.1892 | lr 2.12e-04 | grad 3.00 | tok/s 5585
step   1700 | loss 2.0916 | lr 2.12e-04 | grad 2.39 | tok/s 5481
step   1710 | loss 2.2086 | lr 2.12e-04 | grad 2.00 | tok/s 5713
step   1720 | loss 2.3022 | lr 2.12e-04 | grad 2.92 | tok/s 5769
step   1730 | loss 2.1668 | lr 2.12e-04 | grad 1.95 | tok/s 5625
step   1740 | loss 2.1325 | lr 2.12e-04 | grad 1.89 | tok/s 5527
step   1750 | loss 2.0281 | lr 2.12e-04 | grad 2.23 | tok/s 5552
step   1760 | loss 1.9547 | lr 2.12e-04 | grad 1.78 | tok/s 5451
step   1770 | loss 2.0306 | lr 2.12e-04 | grad 2.22 | tok/s 5669
step   1780 | loss 1.9683 | lr 2.12e-04 | grad 2.61 | tok/s 5523
step   1790 | loss 2.1283 | lr 2.12e-04 | grad 3.56 | tok/s 5569
step   1800 | loss 2.0137 | lr 2.12e-04 | grad 2.77 | tok/s 5375
step   1810 | loss 2.0819 | lr 2.12e-04 | grad 3.47 | tok/s 5442
step   1820 | loss 2.0190 | lr 2.12e-04 | grad 2.83 | tok/s 5665
step   1830 | loss 2.0601 | lr 2.12e-04 | grad 3.88 | tok/s 5434
step   1840 | loss 2.0788 | lr 2.12e-04 | grad 3.02 | tok/s 5683
step   1850 | loss 1.8809 | lr 2.12e-04 | grad 2.88 | tok/s 5502
step   1860 | loss 2.0092 | lr 2.12e-04 | grad 2.11 | tok/s 5517
step   1870 | loss 1.8862 | lr 2.12e-04 | grad 2.70 | tok/s 5409
step   1880 | loss 2.0599 | lr 2.12e-04 | grad 2.64 | tok/s 5143
step   1890 | loss 1.9075 | lr 2.12e-04 | grad 2.80 | tok/s 5559
step   1900 | loss 1.9591 | lr 2.12e-04 | grad 2.83 | tok/s 5270
step   1910 | loss 1.9834 | lr 2.12e-04 | grad 3.59 | tok/s 5773
step   1920 | loss 1.9490 | lr 2.12e-04 | grad 3.08 | tok/s 5416
step   1930 | loss 1.9420 | lr 2.12e-04 | grad 2.14 | tok/s 5633
step   1940 | loss 2.7263 | lr 2.12e-04 | grad 2.61 | tok/s 5712
step   1950 | loss 2.5119 | lr 2.12e-04 | grad 2.45 | tok/s 5772
step   1960 | loss 2.2471 | lr 2.12e-04 | grad 2.61 | tok/s 5634
step   1970 | loss 2.1239 | lr 2.12e-04 | grad 2.33 | tok/s 5391
step   1980 | loss 2.1994 | lr 2.12e-04 | grad 7.62 | tok/s 5491
step   1990 | loss 1.9705 | lr 2.12e-04 | grad 2.25 | tok/s 5561
step   2000 | loss 1.5834 | lr 2.12e-04 | grad 1.55 | tok/s 5759
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5834.pt
step   2010 | loss 1.3935 | lr 2.12e-04 | grad 2.48 | tok/s 1893
step   2020 | loss 2.2131 | lr 2.12e-04 | grad 2.80 | tok/s 5845
step   2030 | loss 2.0177 | lr 2.12e-04 | grad 2.84 | tok/s 5622
step   2040 | loss 2.1575 | lr 2.12e-04 | grad 2.52 | tok/s 5469
step   2050 | loss 2.2133 | lr 2.12e-04 | grad 3.38 | tok/s 5519
step   2060 | loss 2.5522 | lr 2.12e-04 | grad 3.39 | tok/s 5813
step   2070 | loss 2.1742 | lr 2.12e-04 | grad 3.92 | tok/s 5696
step   2080 | loss 2.1506 | lr 2.12e-04 | grad 2.97 | tok/s 5710
step   2090 | loss 2.1072 | lr 2.12e-04 | grad 3.11 | tok/s 5412
step   2100 | loss 1.3180 | lr 2.12e-04 | grad 1.76 | tok/s 5844
step   2110 | loss 1.8474 | lr 2.12e-04 | grad 2.39 | tok/s 5586
step   2120 | loss 1.9629 | lr 2.12e-04 | grad 2.41 | tok/s 5639
step   2130 | loss 1.8576 | lr 2.12e-04 | grad 3.16 | tok/s 5791
step   2140 | loss 1.7977 | lr 2.12e-04 | grad 3.03 | tok/s 5784
step   2150 | loss 1.8315 | lr 2.12e-04 | grad 2.25 | tok/s 5787
step   2160 | loss 1.7925 | lr 2.12e-04 | grad 3.47 | tok/s 5777
step   2170 | loss 1.7752 | lr 2.12e-04 | grad 2.95 | tok/s 5776
step   2180 | loss 1.7745 | lr 2.12e-04 | grad 3.56 | tok/s 5780
step   2190 | loss 1.7432 | lr 2.12e-04 | grad 3.41 | tok/s 5774
step   2200 | loss 1.7166 | lr 2.12e-04 | grad 3.03 | tok/s 5773
step   2210 | loss 1.9131 | lr 2.12e-04 | grad 4.28 | tok/s 5672
step   2220 | loss 1.9043 | lr 2.12e-04 | grad 1.58 | tok/s 5570
step   2230 | loss 2.4050 | lr 2.12e-04 | grad 4.12 | tok/s 5771
step   2240 | loss 2.1965 | lr 2.12e-04 | grad 2.14 | tok/s 5578
step   2250 | loss 2.4579 | lr 2.12e-04 | grad 1.98 | tok/s 5722
step   2260 | loss 2.0331 | lr 2.12e-04 | grad 3.41 | tok/s 5772
step   2270 | loss 2.1590 | lr 2.12e-04 | grad 4.28 | tok/s 5576
step   2280 | loss 2.5359 | lr 2.12e-04 | grad 3.94 | tok/s 5668
step   2290 | loss 2.0507 | lr 2.12e-04 | grad 2.66 | tok/s 5452
step   2300 | loss 2.2752 | lr 2.12e-04 | grad 2.84 | tok/s 5435
step   2310 | loss 2.0394 | lr 2.12e-04 | grad 2.84 | tok/s 5466
step   2320 | loss 2.0017 | lr 2.12e-04 | grad 2.80 | tok/s 5390
step   2330 | loss 1.9503 | lr 2.12e-04 | grad 3.25 | tok/s 5630
step   2340 | loss 1.9762 | lr 2.12e-04 | grad 2.47 | tok/s 5773
step   2350 | loss 2.1475 | lr 2.12e-04 | grad 2.70 | tok/s 5653
step   2360 | loss 2.2325 | lr 2.12e-04 | grad 2.19 | tok/s 5776
step   2370 | loss 1.8202 | lr 2.12e-04 | grad 2.50 | tok/s 5770
step   2380 | loss 1.7686 | lr 2.12e-04 | grad 3.20 | tok/s 5773
step   2390 | loss 1.6791 | lr 2.12e-04 | grad 2.05 | tok/s 5544
step   2400 | loss 1.9694 | lr 2.12e-04 | grad 1.91 | tok/s 5339
step   2410 | loss 1.9177 | lr 2.12e-04 | grad 1.48 | tok/s 5645
step   2420 | loss 1.9305 | lr 2.12e-04 | grad 1.49 | tok/s 5592
step   2430 | loss 1.9770 | lr 2.12e-04 | grad 2.38 | tok/s 5574
step   2440 | loss 1.8800 | lr 2.12e-04 | grad 2.12 | tok/s 5737
step   2450 | loss 1.8123 | lr 2.12e-04 | grad 3.12 | tok/s 5703
step   2460 | loss 1.9058 | lr 2.12e-04 | grad 1.98 | tok/s 5728

Training complete! Final step: 2469
