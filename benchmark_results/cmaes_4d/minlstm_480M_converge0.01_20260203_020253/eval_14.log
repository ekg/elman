Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_14/levelminlstm_100m_20260203_023335
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 198,663,168 parameters
Using schedule-free AdamW (lr=0.0006773243750831238)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.8869 | lr 6.77e-04 | grad 3.17 | tok/s 20541
step     20 | loss 4.0051 | lr 6.77e-04 | grad 5.97 | tok/s 22630
step     30 | loss 4.0211 | lr 6.77e-04 | grad 3.50 | tok/s 23463
step     40 | loss 3.3903 | lr 6.77e-04 | grad 3.17 | tok/s 23442
step     50 | loss 3.1087 | lr 6.77e-04 | grad 3.88 | tok/s 23413
step     60 | loss 3.2176 | lr 6.77e-04 | grad 3.83 | tok/s 22729
step     70 | loss 2.8536 | lr 6.77e-04 | grad 2.92 | tok/s 22534
step     80 | loss 3.0346 | lr 6.77e-04 | grad 3.08 | tok/s 23069
step     90 | loss 2.8998 | lr 6.77e-04 | grad 2.62 | tok/s 22399
step    100 | loss 2.5158 | lr 6.77e-04 | grad 2.44 | tok/s 22447
step    110 | loss 2.6044 | lr 6.77e-04 | grad 2.02 | tok/s 22230
step    120 | loss 2.7446 | lr 6.77e-04 | grad 3.16 | tok/s 22457
step    130 | loss 2.4912 | lr 6.77e-04 | grad 2.25 | tok/s 22661
step    140 | loss 2.3647 | lr 6.77e-04 | grad 1.41 | tok/s 21878
step    150 | loss 2.3574 | lr 6.77e-04 | grad 2.31 | tok/s 21838
step    160 | loss 2.2799 | lr 6.77e-04 | grad 1.73 | tok/s 21975
step    170 | loss 2.3900 | lr 6.77e-04 | grad 2.23 | tok/s 22345
step    180 | loss 2.3718 | lr 6.77e-04 | grad 1.95 | tok/s 22346
step    190 | loss 2.1742 | lr 6.77e-04 | grad 2.66 | tok/s 23212
step    200 | loss 2.1226 | lr 6.77e-04 | grad 1.52 | tok/s 22932
step    210 | loss 2.2624 | lr 6.77e-04 | grad 1.41 | tok/s 22572
step    220 | loss 2.2576 | lr 6.77e-04 | grad 1.70 | tok/s 22607
step    230 | loss 2.1312 | lr 6.77e-04 | grad 1.61 | tok/s 22499
step    240 | loss 2.1812 | lr 6.77e-04 | grad 1.83 | tok/s 22842
step    250 | loss 2.1758 | lr 6.77e-04 | grad 2.23 | tok/s 22242
step    260 | loss 2.1026 | lr 6.77e-04 | grad 2.42 | tok/s 22048
step    270 | loss 2.1519 | lr 6.77e-04 | grad 1.98 | tok/s 22224
step    280 | loss 1.8659 | lr 6.77e-04 | grad 1.63 | tok/s 22540
step    290 | loss 1.8596 | lr 6.77e-04 | grad 1.59 | tok/s 23290
step    300 | loss 1.8450 | lr 6.77e-04 | grad 2.22 | tok/s 23302
step    310 | loss 1.7716 | lr 6.77e-04 | grad 1.97 | tok/s 23303
step    320 | loss 2.0541 | lr 6.77e-04 | grad 1.86 | tok/s 22027
step    330 | loss 1.9700 | lr 6.77e-04 | grad 1.95 | tok/s 22621
step    340 | loss 2.0755 | lr 6.77e-04 | grad 1.82 | tok/s 22206
step    350 | loss 1.9394 | lr 6.77e-04 | grad 1.43 | tok/s 21899
step    360 | loss 1.9590 | lr 6.77e-04 | grad 1.44 | tok/s 22314
step    370 | loss 1.9735 | lr 6.77e-04 | grad 2.05 | tok/s 22679
step    380 | loss 2.1326 | lr 6.77e-04 | grad 1.41 | tok/s 23118
step    390 | loss 1.8890 | lr 6.77e-04 | grad 1.27 | tok/s 22472
step    400 | loss 2.0522 | lr 6.77e-04 | grad 1.67 | tok/s 22669
step    410 | loss 1.6963 | lr 6.77e-04 | grad 1.78 | tok/s 22374
step    420 | loss 1.9599 | lr 6.77e-04 | grad 1.34 | tok/s 21843
step    430 | loss 1.9529 | lr 6.77e-04 | grad 1.73 | tok/s 22401
step    440 | loss 2.0084 | lr 6.77e-04 | grad 1.63 | tok/s 22719
step    450 | loss 1.8215 | lr 6.77e-04 | grad 1.52 | tok/s 22432
step    460 | loss 1.8956 | lr 6.77e-04 | grad 1.56 | tok/s 22158
step    470 | loss 1.7549 | lr 6.77e-04 | grad 1.02 | tok/s 22299
step    480 | loss 1.7841 | lr 6.77e-04 | grad 1.65 | tok/s 21860
step    490 | loss 2.1337 | lr 6.77e-04 | grad 2.20 | tok/s 22740
step    500 | loss 1.9981 | lr 6.77e-04 | grad 0.75 | tok/s 22425
step    510 | loss 1.5499 | lr 6.77e-04 | grad 1.14 | tok/s 22877
step    520 | loss 2.0129 | lr 6.77e-04 | grad 4.72 | tok/s 22425
step    530 | loss 2.0668 | lr 6.77e-04 | grad 1.27 | tok/s 22684
step    540 | loss 1.5771 | lr 6.77e-04 | grad 1.40 | tok/s 22947
step    550 | loss 1.5675 | lr 6.77e-04 | grad 1.64 | tok/s 23298
step    560 | loss 1.5581 | lr 6.77e-04 | grad 1.30 | tok/s 23188
step    570 | loss 2.1061 | lr 6.77e-04 | grad 1.74 | tok/s 22828
step    580 | loss 2.0616 | lr 6.77e-04 | grad 1.48 | tok/s 22588
step    590 | loss 1.8899 | lr 6.77e-04 | grad 1.52 | tok/s 22160
step    600 | loss 1.8647 | lr 6.77e-04 | grad 1.40 | tok/s 23175
step    610 | loss 1.6009 | lr 6.77e-04 | grad 1.28 | tok/s 22478
step    620 | loss 1.6854 | lr 6.77e-04 | grad 1.61 | tok/s 22787
step    630 | loss 1.8297 | lr 6.77e-04 | grad 2.00 | tok/s 22867
step    640 | loss 1.7134 | lr 6.77e-04 | grad 1.47 | tok/s 22726
step    650 | loss 1.7601 | lr 6.77e-04 | grad 1.16 | tok/s 21798
step    660 | loss 1.9391 | lr 6.77e-04 | grad 1.19 | tok/s 22800
step    670 | loss 1.7934 | lr 6.77e-04 | grad 1.00 | tok/s 22355
step    680 | loss 1.8792 | lr 6.77e-04 | grad 0.94 | tok/s 22361
step    690 | loss 1.8626 | lr 6.77e-04 | grad 1.16 | tok/s 22560
step    700 | loss 1.8157 | lr 6.77e-04 | grad 1.11 | tok/s 22004
step    710 | loss 1.6449 | lr 6.77e-04 | grad 1.65 | tok/s 22486
step    720 | loss 1.8454 | lr 6.77e-04 | grad 1.41 | tok/s 22469
step    730 | loss 1.7710 | lr 6.77e-04 | grad 1.05 | tok/s 22633
step    740 | loss 1.7324 | lr 6.77e-04 | grad 1.12 | tok/s 22139
step    750 | loss 1.9440 | lr 6.77e-04 | grad 1.30 | tok/s 22634
step    760 | loss 1.6414 | lr 6.77e-04 | grad 1.38 | tok/s 22339
step    770 | loss 1.7503 | lr 6.77e-04 | grad 2.28 | tok/s 22807
step    780 | loss 1.6780 | lr 6.77e-04 | grad 0.96 | tok/s 22699
step    790 | loss 1.5400 | lr 6.77e-04 | grad 1.26 | tok/s 22971
step    800 | loss 1.6095 | lr 6.77e-04 | grad 1.64 | tok/s 22261
step    810 | loss 2.1822 | lr 6.77e-04 | grad 2.23 | tok/s 22957
step    820 | loss 2.1480 | lr 6.77e-04 | grad 1.61 | tok/s 23325
step    830 | loss 1.9354 | lr 6.77e-04 | grad 2.11 | tok/s 23316
step    840 | loss 1.8833 | lr 6.77e-04 | grad 1.59 | tok/s 22780
step    850 | loss 1.6693 | lr 6.77e-04 | grad 1.50 | tok/s 22012
step    860 | loss 1.5680 | lr 6.77e-04 | grad 1.21 | tok/s 22814
step    870 | loss 1.7179 | lr 6.77e-04 | grad 1.03 | tok/s 22490
step    880 | loss 1.6152 | lr 6.77e-04 | grad 1.04 | tok/s 22510
step    890 | loss 1.9589 | lr 6.77e-04 | grad 2.73 | tok/s 22357
step    900 | loss 1.6021 | lr 6.77e-04 | grad 0.91 | tok/s 22101
step    910 | loss 1.6383 | lr 6.77e-04 | grad 1.09 | tok/s 22188
step    920 | loss 1.7054 | lr 6.77e-04 | grad 1.48 | tok/s 22016
step    930 | loss 1.6382 | lr 6.77e-04 | grad 1.27 | tok/s 22090
step    940 | loss 1.7396 | lr 6.77e-04 | grad 1.23 | tok/s 22479
step    950 | loss 1.5242 | lr 6.77e-04 | grad 1.13 | tok/s 23323
step    960 | loss 1.4584 | lr 6.77e-04 | grad 1.39 | tok/s 23320
step    970 | loss 1.5023 | lr 6.77e-04 | grad 1.50 | tok/s 22799
step    980 | loss 1.7563 | lr 6.77e-04 | grad 0.90 | tok/s 22378
step    990 | loss 1.6890 | lr 6.77e-04 | grad 0.87 | tok/s 21925
step   1000 | loss 1.8010 | lr 6.77e-04 | grad 1.08 | tok/s 22874
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8010.pt
step   1010 | loss 1.5228 | lr 6.77e-04 | grad 1.27 | tok/s 18220
step   1020 | loss 1.7950 | lr 6.77e-04 | grad 1.00 | tok/s 22146
step   1030 | loss 1.6624 | lr 6.77e-04 | grad 1.23 | tok/s 22555
step   1040 | loss 1.5539 | lr 6.77e-04 | grad 0.71 | tok/s 22079
step   1050 | loss 1.8495 | lr 6.77e-04 | grad 2.77 | tok/s 23007
step   1060 | loss 1.9961 | lr 6.77e-04 | grad 1.62 | tok/s 22539
step   1070 | loss 1.8014 | lr 6.77e-04 | grad 1.47 | tok/s 22258
step   1080 | loss 1.8383 | lr 6.77e-04 | grad 1.39 | tok/s 22385
step   1090 | loss 1.5379 | lr 6.77e-04 | grad 1.08 | tok/s 22936
step   1100 | loss 1.6988 | lr 6.77e-04 | grad 2.22 | tok/s 22731
step   1110 | loss 1.6938 | lr 6.77e-04 | grad 0.98 | tok/s 22659
step   1120 | loss 1.6596 | lr 6.77e-04 | grad 1.09 | tok/s 22176
step   1130 | loss 1.6590 | lr 6.77e-04 | grad 1.62 | tok/s 22475
step   1140 | loss 1.7646 | lr 6.77e-04 | grad 1.30 | tok/s 21926
step   1150 | loss 1.6549 | lr 6.77e-04 | grad 0.71 | tok/s 22155
step   1160 | loss 1.6099 | lr 6.77e-04 | grad 1.39 | tok/s 22749
step   1170 | loss 1.5766 | lr 6.77e-04 | grad 1.35 | tok/s 23320
step   1180 | loss 1.5075 | lr 6.77e-04 | grad 1.43 | tok/s 23317
step   1190 | loss 1.4444 | lr 6.77e-04 | grad 1.09 | tok/s 23321
step   1200 | loss 1.4442 | lr 6.77e-04 | grad 1.65 | tok/s 23316
step   1210 | loss 1.6101 | lr 6.77e-04 | grad 1.03 | tok/s 22940
step   1220 | loss 1.4029 | lr 6.77e-04 | grad 0.85 | tok/s 22271
step   1230 | loss 1.6028 | lr 6.77e-04 | grad 1.53 | tok/s 22411
step   1240 | loss 1.6052 | lr 6.77e-04 | grad 0.99 | tok/s 22566
step   1250 | loss 1.7741 | lr 6.77e-04 | grad 2.56 | tok/s 22694
step   1260 | loss 1.6493 | lr 6.77e-04 | grad 0.78 | tok/s 22535
step   1270 | loss 1.6690 | lr 6.77e-04 | grad 0.87 | tok/s 22064
step   1280 | loss 1.5962 | lr 6.77e-04 | grad 1.14 | tok/s 22326
step   1290 | loss 1.7242 | lr 6.77e-04 | grad 1.14 | tok/s 22309
step   1300 | loss 1.5830 | lr 6.77e-04 | grad 0.84 | tok/s 21665
step   1310 | loss 1.7151 | lr 6.77e-04 | grad 1.72 | tok/s 22578
step   1320 | loss 1.5315 | lr 6.77e-04 | grad 1.44 | tok/s 22799
step   1330 | loss 1.4883 | lr 6.77e-04 | grad 1.23 | tok/s 22299
step   1340 | loss 1.5649 | lr 6.77e-04 | grad 1.02 | tok/s 22596
step   1350 | loss 1.5209 | lr 6.77e-04 | grad 0.77 | tok/s 22083
step   1360 | loss 1.7196 | lr 6.77e-04 | grad 1.05 | tok/s 22493
step   1370 | loss 1.5863 | lr 6.77e-04 | grad 1.06 | tok/s 22259
step   1380 | loss 1.5620 | lr 6.77e-04 | grad 0.77 | tok/s 22849
step   1390 | loss 1.6690 | lr 6.77e-04 | grad 1.51 | tok/s 22550
step   1400 | loss 1.6239 | lr 6.77e-04 | grad 0.77 | tok/s 22148
step   1410 | loss 1.5923 | lr 6.77e-04 | grad 0.85 | tok/s 21758
step   1420 | loss 1.4640 | lr 6.77e-04 | grad 1.07 | tok/s 21896
step   1430 | loss 1.3497 | lr 6.77e-04 | grad 1.11 | tok/s 23263
step   1440 | loss 1.5856 | lr 6.77e-04 | grad 0.93 | tok/s 21877
step   1450 | loss 1.6550 | lr 6.77e-04 | grad 0.88 | tok/s 22380
step   1460 | loss 1.5821 | lr 6.77e-04 | grad 1.06 | tok/s 22269
step   1470 | loss 1.5260 | lr 6.77e-04 | grad 0.92 | tok/s 22588
step   1480 | loss 1.6872 | lr 6.77e-04 | grad 1.14 | tok/s 22170
step   1490 | loss 1.7207 | lr 6.77e-04 | grad 1.01 | tok/s 22440
step   1500 | loss 1.6203 | lr 6.77e-04 | grad 0.93 | tok/s 22684
step   1510 | loss 1.5715 | lr 6.77e-04 | grad 0.73 | tok/s 22842
step   1520 | loss 1.5057 | lr 6.77e-04 | grad 0.65 | tok/s 22171
step   1530 | loss 1.4527 | lr 6.77e-04 | grad 0.89 | tok/s 22887
step   1540 | loss 2.1220 | lr 6.77e-04 | grad 0.96 | tok/s 22468
step   1550 | loss 1.5714 | lr 6.77e-04 | grad 0.72 | tok/s 21962
step   1560 | loss 1.6026 | lr 6.77e-04 | grad 1.06 | tok/s 22859
step   1570 | loss 1.5399 | lr 6.77e-04 | grad 0.77 | tok/s 21984
step   1580 | loss 1.5486 | lr 6.77e-04 | grad 2.23 | tok/s 22070
step   1590 | loss 1.4213 | lr 6.77e-04 | grad 2.30 | tok/s 23086
step   1600 | loss 1.5872 | lr 6.77e-04 | grad 0.70 | tok/s 22677
step   1610 | loss 1.5699 | lr 6.77e-04 | grad 0.94 | tok/s 22606
step   1620 | loss 1.4980 | lr 6.77e-04 | grad 0.67 | tok/s 22222
step   1630 | loss 1.5299 | lr 6.77e-04 | grad 0.69 | tok/s 21958
step   1640 | loss 1.5531 | lr 6.77e-04 | grad 1.09 | tok/s 22200
step   1650 | loss 1.5339 | lr 6.77e-04 | grad 1.38 | tok/s 22515
step   1660 | loss 1.9663 | lr 6.77e-04 | grad 0.96 | tok/s 22928
step   1670 | loss 1.5257 | lr 6.77e-04 | grad 1.14 | tok/s 22135
step   1680 | loss 1.6312 | lr 6.77e-04 | grad 2.17 | tok/s 23115
step   1690 | loss 1.6042 | lr 6.77e-04 | grad 1.03 | tok/s 22009
step   1700 | loss 1.5474 | lr 6.77e-04 | grad 1.08 | tok/s 22234
step   1710 | loss 1.6407 | lr 6.77e-04 | grad 1.25 | tok/s 22214
step   1720 | loss 1.6076 | lr 6.77e-04 | grad 2.84 | tok/s 22693
step   1730 | loss 1.5423 | lr 6.77e-04 | grad 0.97 | tok/s 22070
step   1740 | loss 1.6654 | lr 6.77e-04 | grad 0.75 | tok/s 21990
step   1750 | loss 1.6257 | lr 6.77e-04 | grad 1.09 | tok/s 22401
step   1760 | loss 1.5418 | lr 6.77e-04 | grad 1.05 | tok/s 21880
step   1770 | loss 1.6856 | lr 6.77e-04 | grad 0.89 | tok/s 22331
step   1780 | loss 1.4889 | lr 6.77e-04 | grad 0.95 | tok/s 22530
step   1790 | loss 1.5577 | lr 6.77e-04 | grad 0.75 | tok/s 22623
step   1800 | loss 1.4560 | lr 6.77e-04 | grad 0.92 | tok/s 22155
step   1810 | loss 1.5287 | lr 6.77e-04 | grad 0.86 | tok/s 22187
step   1820 | loss 1.4913 | lr 6.77e-04 | grad 0.78 | tok/s 22242
step   1830 | loss 1.7188 | lr 6.77e-04 | grad 1.09 | tok/s 22075
step   1840 | loss 1.5012 | lr 6.77e-04 | grad 0.72 | tok/s 22107
step   1850 | loss 1.4838 | lr 6.77e-04 | grad 1.01 | tok/s 22680
step   1860 | loss 1.5126 | lr 6.77e-04 | grad 0.95 | tok/s 22729
step   1870 | loss 1.5791 | lr 6.77e-04 | grad 0.57 | tok/s 22210
step   1880 | loss 1.5887 | lr 6.77e-04 | grad 0.94 | tok/s 22731
step   1890 | loss 1.6481 | lr 6.77e-04 | grad 1.20 | tok/s 22261
step   1900 | loss 1.4174 | lr 6.77e-04 | grad 1.06 | tok/s 22700
step   1910 | loss 1.4541 | lr 6.77e-04 | grad 0.89 | tok/s 22549
step   1920 | loss 1.4888 | lr 6.77e-04 | grad 0.77 | tok/s 23052
step   1930 | loss 1.5638 | lr 6.77e-04 | grad 1.00 | tok/s 22670
step   1940 | loss 1.6396 | lr 6.77e-04 | grad 3.67 | tok/s 22936
step   1950 | loss 1.4577 | lr 6.77e-04 | grad 0.82 | tok/s 22296
step   1960 | loss 1.5872 | lr 6.77e-04 | grad 1.40 | tok/s 22328
step   1970 | loss 1.5290 | lr 6.77e-04 | grad 0.94 | tok/s 22529
step   1980 | loss 1.5041 | lr 6.77e-04 | grad 0.82 | tok/s 22684
step   1990 | loss 1.3005 | lr 6.77e-04 | grad 1.23 | tok/s 23278
step   2000 | loss 1.2640 | lr 6.77e-04 | grad 0.71 | tok/s 23323
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2640.pt
step   2010 | loss 1.4527 | lr 6.77e-04 | grad 1.30 | tok/s 19177
step   2020 | loss 1.3386 | lr 6.77e-04 | grad 1.12 | tok/s 23327
step   2030 | loss 1.3852 | lr 6.77e-04 | grad 0.84 | tok/s 22827
step   2040 | loss 1.6205 | lr 6.77e-04 | grad 1.00 | tok/s 22233
step   2050 | loss 1.5092 | lr 6.77e-04 | grad 0.88 | tok/s 22654
step   2060 | loss 1.6090 | lr 6.77e-04 | grad 0.85 | tok/s 22488
step   2070 | loss 1.5115 | lr 6.77e-04 | grad 0.97 | tok/s 22459
step   2080 | loss 1.4570 | lr 6.77e-04 | grad 1.84 | tok/s 22215
step   2090 | loss 1.4163 | lr 6.77e-04 | grad 1.27 | tok/s 22728
step   2100 | loss 1.2350 | lr 6.77e-04 | grad 0.98 | tok/s 22681
step   2110 | loss 1.6024 | lr 6.77e-04 | grad 0.84 | tok/s 22281
step   2120 | loss 1.5913 | lr 6.77e-04 | grad 0.81 | tok/s 22240
step   2130 | loss 1.5624 | lr 6.77e-04 | grad 0.81 | tok/s 21941
step   2140 | loss 1.5700 | lr 6.77e-04 | grad 1.46 | tok/s 22538
step   2150 | loss 1.6315 | lr 6.77e-04 | grad 0.87 | tok/s 22304
step   2160 | loss 1.6829 | lr 6.77e-04 | grad 0.73 | tok/s 23061
step   2170 | loss 1.4052 | lr 6.77e-04 | grad 1.04 | tok/s 22862
step   2180 | loss 1.3045 | lr 6.77e-04 | grad 0.85 | tok/s 23316
step   2190 | loss 1.2903 | lr 6.77e-04 | grad 1.07 | tok/s 23315
step   2200 | loss 1.2973 | lr 6.77e-04 | grad 0.86 | tok/s 23308
step   2210 | loss 1.5170 | lr 6.77e-04 | grad 1.30 | tok/s 22423
step   2220 | loss 1.7228 | lr 6.77e-04 | grad 1.57 | tok/s 23322
step   2230 | loss 1.5794 | lr 6.77e-04 | grad 1.16 | tok/s 22738
step   2240 | loss 1.4639 | lr 6.77e-04 | grad 0.82 | tok/s 22653
step   2250 | loss 1.5131 | lr 6.77e-04 | grad 1.13 | tok/s 22608
step   2260 | loss 1.4999 | lr 6.77e-04 | grad 1.04 | tok/s 21947
step   2270 | loss 1.4985 | lr 6.77e-04 | grad 0.96 | tok/s 22256
step   2280 | loss 1.5146 | lr 6.77e-04 | grad 1.01 | tok/s 22405
step   2290 | loss 1.5021 | lr 6.77e-04 | grad 1.23 | tok/s 23317
step   2300 | loss 1.4359 | lr 6.77e-04 | grad 0.94 | tok/s 23319
step   2310 | loss 1.4278 | lr 6.77e-04 | grad 0.89 | tok/s 23129
step   2320 | loss 1.5566 | lr 6.77e-04 | grad 1.38 | tok/s 22644
step   2330 | loss 1.6005 | lr 6.77e-04 | grad 1.31 | tok/s 22400
step   2340 | loss 1.7973 | lr 6.77e-04 | grad 0.79 | tok/s 22709
step   2350 | loss 1.4983 | lr 6.77e-04 | grad 0.88 | tok/s 22609
step   2360 | loss 1.4442 | lr 6.77e-04 | grad 0.75 | tok/s 22087
step   2370 | loss 1.4595 | lr 6.77e-04 | grad 0.72 | tok/s 22264
step   2380 | loss 1.5434 | lr 6.77e-04 | grad 0.92 | tok/s 22302
step   2390 | loss 1.5878 | lr 6.77e-04 | grad 0.90 | tok/s 22244
step   2400 | loss 1.5106 | lr 6.77e-04 | grad 6.25 | tok/s 21864
step   2410 | loss 1.6221 | lr 6.77e-04 | grad 2.28 | tok/s 22656
step   2420 | loss 1.4052 | lr 6.77e-04 | grad 0.91 | tok/s 22435
step   2430 | loss 1.4549 | lr 6.77e-04 | grad 0.82 | tok/s 22607
step   2440 | loss 1.5340 | lr 6.77e-04 | grad 1.74 | tok/s 23308
step   2450 | loss 1.4110 | lr 6.77e-04 | grad 0.82 | tok/s 22757
step   2460 | loss 1.4048 | lr 6.77e-04 | grad 0.69 | tok/s 21909
step   2470 | loss 1.4677 | lr 6.77e-04 | grad 1.15 | tok/s 22865
step   2480 | loss 1.5029 | lr 6.77e-04 | grad 1.00 | tok/s 22473
step   2490 | loss 1.6558 | lr 6.77e-04 | grad 0.94 | tok/s 22744
step   2500 | loss 1.5430 | lr 6.77e-04 | grad 0.76 | tok/s 21926
step   2510 | loss 1.5342 | lr 6.77e-04 | grad 0.62 | tok/s 22136
step   2520 | loss 1.6314 | lr 6.77e-04 | grad 2.81 | tok/s 22155
step   2530 | loss 1.7631 | lr 6.77e-04 | grad 0.75 | tok/s 22817
step   2540 | loss 1.4587 | lr 6.77e-04 | grad 1.18 | tok/s 22133
step   2550 | loss 1.6592 | lr 6.77e-04 | grad 1.05 | tok/s 22842

Training complete! Final step: 2550
