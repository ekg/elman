Using device: cuda
Output directory: benchmark_results/cmaes_4d/minlstm_480M_converge0.01_20260203_020253/eval_27/levelminlstm_100m_20260203_033417
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 166,969,088 parameters
Using schedule-free AdamW (lr=0.0003522591995781556)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.2936 | lr 3.52e-04 | grad 3.17 | tok/s 23084
step     20 | loss 3.8020 | lr 3.52e-04 | grad 7.59 | tok/s 25308
step     30 | loss 3.7393 | lr 3.52e-04 | grad 4.88 | tok/s 26248
step     40 | loss 3.2741 | lr 3.52e-04 | grad 3.97 | tok/s 26231
step     50 | loss 2.9741 | lr 3.52e-04 | grad 2.45 | tok/s 26205
step     60 | loss 3.0935 | lr 3.52e-04 | grad 4.03 | tok/s 25455
step     70 | loss 2.8090 | lr 3.52e-04 | grad 2.22 | tok/s 25237
step     80 | loss 2.9639 | lr 3.52e-04 | grad 2.64 | tok/s 25833
step     90 | loss 2.8442 | lr 3.52e-04 | grad 2.42 | tok/s 25082
step    100 | loss 2.4576 | lr 3.52e-04 | grad 2.59 | tok/s 25136
step    110 | loss 2.5624 | lr 3.52e-04 | grad 2.19 | tok/s 24889
step    120 | loss 2.7140 | lr 3.52e-04 | grad 3.14 | tok/s 25155
step    130 | loss 2.4503 | lr 3.52e-04 | grad 2.25 | tok/s 25389
step    140 | loss 2.3368 | lr 3.52e-04 | grad 2.09 | tok/s 24516
step    150 | loss 2.3187 | lr 3.52e-04 | grad 1.70 | tok/s 24457
step    160 | loss 2.2466 | lr 3.52e-04 | grad 1.42 | tok/s 24619
step    170 | loss 2.3495 | lr 3.52e-04 | grad 1.98 | tok/s 25039
step    180 | loss 2.3122 | lr 3.52e-04 | grad 2.06 | tok/s 25041
step    190 | loss 2.1287 | lr 3.52e-04 | grad 2.38 | tok/s 26020
step    200 | loss 2.0596 | lr 3.52e-04 | grad 1.54 | tok/s 25697
step    210 | loss 2.2347 | lr 3.52e-04 | grad 1.59 | tok/s 25296
step    220 | loss 2.2120 | lr 3.52e-04 | grad 1.58 | tok/s 25331
step    230 | loss 2.1004 | lr 3.52e-04 | grad 1.50 | tok/s 25219
step    240 | loss 2.1603 | lr 3.52e-04 | grad 1.79 | tok/s 25603
step    250 | loss 2.1644 | lr 3.52e-04 | grad 2.77 | tok/s 24927
step    260 | loss 2.0814 | lr 3.52e-04 | grad 2.50 | tok/s 24726
step    270 | loss 2.1304 | lr 3.52e-04 | grad 1.88 | tok/s 24887
step    280 | loss 1.8439 | lr 3.52e-04 | grad 1.49 | tok/s 25240
step    290 | loss 1.8405 | lr 3.52e-04 | grad 1.56 | tok/s 26088
step    300 | loss 1.8125 | lr 3.52e-04 | grad 2.14 | tok/s 26107
step    310 | loss 1.7578 | lr 3.52e-04 | grad 2.14 | tok/s 26098
step    320 | loss 2.0341 | lr 3.52e-04 | grad 1.80 | tok/s 24680
step    330 | loss 1.9596 | lr 3.52e-04 | grad 1.90 | tok/s 25349
step    340 | loss 2.0624 | lr 3.52e-04 | grad 1.98 | tok/s 24871
step    350 | loss 1.9279 | lr 3.52e-04 | grad 1.66 | tok/s 24550
step    360 | loss 1.9486 | lr 3.52e-04 | grad 1.94 | tok/s 25018
step    370 | loss 1.9593 | lr 3.52e-04 | grad 2.05 | tok/s 25429
step    380 | loss 2.1007 | lr 3.52e-04 | grad 1.48 | tok/s 25919
step    390 | loss 1.8826 | lr 3.52e-04 | grad 1.56 | tok/s 25192
step    400 | loss 2.0590 | lr 3.52e-04 | grad 1.81 | tok/s 25419
step    410 | loss 1.6945 | lr 3.52e-04 | grad 1.41 | tok/s 25080
step    420 | loss 1.9639 | lr 3.52e-04 | grad 1.43 | tok/s 24490
step    430 | loss 1.9546 | lr 3.52e-04 | grad 1.94 | tok/s 25112
step    440 | loss 2.0204 | lr 3.52e-04 | grad 1.55 | tok/s 25467
step    450 | loss 1.8296 | lr 3.52e-04 | grad 1.55 | tok/s 25146
step    460 | loss 1.8998 | lr 3.52e-04 | grad 1.89 | tok/s 24840
step    470 | loss 1.7582 | lr 3.52e-04 | grad 1.09 | tok/s 25014
step    480 | loss 1.7973 | lr 3.52e-04 | grad 1.46 | tok/s 24515
step    490 | loss 2.1494 | lr 3.52e-04 | grad 2.75 | tok/s 25500
step    500 | loss 1.9966 | lr 3.52e-04 | grad 1.06 | tok/s 25137
step    510 | loss 1.5680 | lr 3.52e-04 | grad 1.38 | tok/s 25640
step    520 | loss 2.0155 | lr 3.52e-04 | grad 5.22 | tok/s 25166
step    530 | loss 2.0746 | lr 3.52e-04 | grad 1.56 | tok/s 25442
step    540 | loss 1.5820 | lr 3.52e-04 | grad 1.82 | tok/s 25701
step    550 | loss 1.5756 | lr 3.52e-04 | grad 1.55 | tok/s 26123
step    560 | loss 1.5722 | lr 3.52e-04 | grad 1.48 | tok/s 26008
step    570 | loss 2.1027 | lr 3.52e-04 | grad 2.03 | tok/s 25606
step    580 | loss 2.0671 | lr 3.52e-04 | grad 1.65 | tok/s 25323
step    590 | loss 1.9038 | lr 3.52e-04 | grad 1.64 | tok/s 24814
step    600 | loss 1.8781 | lr 3.52e-04 | grad 1.59 | tok/s 25973
step    610 | loss 1.6200 | lr 3.52e-04 | grad 1.59 | tok/s 25197
step    620 | loss 1.6974 | lr 3.52e-04 | grad 1.83 | tok/s 25542
step    630 | loss 1.8390 | lr 3.52e-04 | grad 4.12 | tok/s 25635
step    640 | loss 1.7404 | lr 3.52e-04 | grad 1.54 | tok/s 25470
step    650 | loss 1.7863 | lr 3.52e-04 | grad 1.40 | tok/s 24438
step    660 | loss 1.9613 | lr 3.52e-04 | grad 1.65 | tok/s 25561
step    670 | loss 1.8181 | lr 3.52e-04 | grad 1.27 | tok/s 25069
step    680 | loss 1.9076 | lr 3.52e-04 | grad 1.42 | tok/s 25069
step    690 | loss 1.8833 | lr 3.52e-04 | grad 1.17 | tok/s 25290
step    700 | loss 1.8226 | lr 3.52e-04 | grad 1.31 | tok/s 24667
step    710 | loss 1.6888 | lr 3.52e-04 | grad 2.45 | tok/s 25206
step    720 | loss 1.8848 | lr 3.52e-04 | grad 1.74 | tok/s 25191
step    730 | loss 1.7964 | lr 3.52e-04 | grad 1.42 | tok/s 25366
step    740 | loss 1.7572 | lr 3.52e-04 | grad 1.37 | tok/s 24821
step    750 | loss 1.9466 | lr 3.52e-04 | grad 1.89 | tok/s 25360
step    760 | loss 1.6670 | lr 3.52e-04 | grad 2.14 | tok/s 25027
step    770 | loss 1.7813 | lr 3.52e-04 | grad 2.86 | tok/s 25570
step    780 | loss 1.7126 | lr 3.52e-04 | grad 1.23 | tok/s 25459
step    790 | loss 1.5692 | lr 3.52e-04 | grad 1.42 | tok/s 25777
step    800 | loss 1.6442 | lr 3.52e-04 | grad 3.31 | tok/s 24955
step    810 | loss 2.2032 | lr 3.52e-04 | grad 2.22 | tok/s 25725
step    820 | loss 2.1894 | lr 3.52e-04 | grad 2.17 | tok/s 26134
step    830 | loss 1.9913 | lr 3.52e-04 | grad 2.47 | tok/s 26132
step    840 | loss 1.9199 | lr 3.52e-04 | grad 1.86 | tok/s 25509
step    850 | loss 1.6977 | lr 3.52e-04 | grad 1.87 | tok/s 24641
step    860 | loss 1.6001 | lr 3.52e-04 | grad 1.55 | tok/s 25578
step    870 | loss 1.7453 | lr 3.52e-04 | grad 1.34 | tok/s 25204
step    880 | loss 1.6463 | lr 3.52e-04 | grad 1.47 | tok/s 25212
step    890 | loss 1.9920 | lr 3.52e-04 | grad 2.98 | tok/s 25047
step    900 | loss 1.6368 | lr 3.52e-04 | grad 1.05 | tok/s 24763
step    910 | loss 1.6707 | lr 3.52e-04 | grad 1.21 | tok/s 24857
step    920 | loss 1.7420 | lr 3.52e-04 | grad 1.65 | tok/s 24662
step    930 | loss 1.6809 | lr 3.52e-04 | grad 1.66 | tok/s 24759
step    940 | loss 1.7753 | lr 3.52e-04 | grad 1.76 | tok/s 25181
step    950 | loss 1.5564 | lr 3.52e-04 | grad 1.40 | tok/s 26129
step    960 | loss 1.4840 | lr 3.52e-04 | grad 1.56 | tok/s 26126
step    970 | loss 1.5303 | lr 3.52e-04 | grad 1.58 | tok/s 25546
step    980 | loss 1.8036 | lr 3.52e-04 | grad 1.24 | tok/s 25058
step    990 | loss 1.7352 | lr 3.52e-04 | grad 1.19 | tok/s 24570
step   1000 | loss 1.8506 | lr 3.52e-04 | grad 1.42 | tok/s 25630
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8506.pt
step   1010 | loss 1.5488 | lr 3.52e-04 | grad 1.27 | tok/s 20429
step   1020 | loss 1.8887 | lr 3.52e-04 | grad 1.17 | tok/s 24852
step   1030 | loss 1.6709 | lr 3.52e-04 | grad 1.52 | tok/s 25435
step   1040 | loss 1.6270 | lr 3.52e-04 | grad 2.05 | tok/s 24758
step   1050 | loss 1.8482 | lr 3.52e-04 | grad 3.17 | tok/s 25562
step   1060 | loss 1.9869 | lr 3.52e-04 | grad 3.64 | tok/s 25235
step   1070 | loss 1.9052 | lr 3.52e-04 | grad 1.34 | tok/s 25010
step   1080 | loss 1.9019 | lr 3.52e-04 | grad 1.41 | tok/s 25081
step   1090 | loss 1.6547 | lr 3.52e-04 | grad 2.83 | tok/s 25578
step   1100 | loss 1.6876 | lr 3.52e-04 | grad 1.52 | tok/s 25479
step   1110 | loss 1.7196 | lr 3.52e-04 | grad 1.74 | tok/s 25469
step   1120 | loss 1.7334 | lr 3.52e-04 | grad 1.37 | tok/s 24859
step   1130 | loss 1.6783 | lr 3.52e-04 | grad 2.61 | tok/s 25075
step   1140 | loss 1.8287 | lr 3.52e-04 | grad 1.12 | tok/s 24568
step   1150 | loss 1.6816 | lr 3.52e-04 | grad 1.37 | tok/s 24908
step   1160 | loss 1.6437 | lr 3.52e-04 | grad 1.57 | tok/s 25399
step   1170 | loss 1.6126 | lr 3.52e-04 | grad 1.63 | tok/s 26120
step   1180 | loss 1.5546 | lr 3.52e-04 | grad 1.38 | tok/s 26130
step   1190 | loss 1.4874 | lr 3.52e-04 | grad 1.45 | tok/s 26128
step   1200 | loss 1.4782 | lr 3.52e-04 | grad 1.48 | tok/s 26132
step   1210 | loss 1.6439 | lr 3.52e-04 | grad 1.44 | tok/s 25725
step   1220 | loss 1.4417 | lr 3.52e-04 | grad 2.09 | tok/s 25153
step   1230 | loss 1.6019 | lr 3.52e-04 | grad 1.40 | tok/s 24865
step   1240 | loss 1.6789 | lr 3.52e-04 | grad 2.06 | tok/s 25319
step   1250 | loss 1.7678 | lr 3.52e-04 | grad 1.51 | tok/s 25377
step   1260 | loss 1.7673 | lr 3.52e-04 | grad 1.63 | tok/s 25243
step   1270 | loss 1.7271 | lr 3.52e-04 | grad 1.14 | tok/s 24913
step   1280 | loss 1.6521 | lr 3.52e-04 | grad 2.17 | tok/s 24834
step   1290 | loss 1.7489 | lr 3.52e-04 | grad 2.52 | tok/s 25118
step   1300 | loss 1.6387 | lr 3.52e-04 | grad 1.72 | tok/s 24176
step   1310 | loss 1.7347 | lr 3.52e-04 | grad 1.66 | tok/s 25460
step   1320 | loss 1.5644 | lr 3.52e-04 | grad 1.44 | tok/s 25560
step   1330 | loss 1.5677 | lr 3.52e-04 | grad 1.77 | tok/s 24790
step   1340 | loss 1.6291 | lr 3.52e-04 | grad 1.15 | tok/s 25532
step   1350 | loss 1.6036 | lr 3.52e-04 | grad 1.64 | tok/s 24638
step   1360 | loss 1.7693 | lr 3.52e-04 | grad 1.66 | tok/s 25110
step   1370 | loss 1.6261 | lr 3.52e-04 | grad 1.40 | tok/s 25120
step   1380 | loss 1.6094 | lr 3.52e-04 | grad 1.45 | tok/s 25350
step   1390 | loss 1.7495 | lr 3.52e-04 | grad 1.26 | tok/s 25264
step   1400 | loss 1.6693 | lr 3.52e-04 | grad 1.05 | tok/s 24855
step   1410 | loss 1.6336 | lr 3.52e-04 | grad 1.09 | tok/s 24543
step   1420 | loss 1.5063 | lr 3.52e-04 | grad 1.09 | tok/s 24479
step   1430 | loss 1.4111 | lr 3.52e-04 | grad 1.37 | tok/s 25927
step   1440 | loss 1.5892 | lr 3.52e-04 | grad 1.16 | tok/s 24698
step   1450 | loss 1.7237 | lr 3.52e-04 | grad 1.06 | tok/s 25176
step   1460 | loss 1.6127 | lr 3.52e-04 | grad 1.16 | tok/s 24843
step   1470 | loss 1.5835 | lr 3.52e-04 | grad 1.15 | tok/s 25437
step   1480 | loss 1.7375 | lr 3.52e-04 | grad 1.46 | tok/s 24733
step   1490 | loss 1.7746 | lr 3.52e-04 | grad 1.55 | tok/s 24955
step   1500 | loss 1.6958 | lr 3.52e-04 | grad 1.12 | tok/s 25562
step   1510 | loss 1.6298 | lr 3.52e-04 | grad 2.36 | tok/s 25568
step   1520 | loss 1.5555 | lr 3.52e-04 | grad 1.27 | tok/s 24710
step   1530 | loss 1.5184 | lr 3.52e-04 | grad 1.11 | tok/s 25630
step   1540 | loss 2.1218 | lr 3.52e-04 | grad 7.12 | tok/s 25292
step   1550 | loss 1.6387 | lr 3.52e-04 | grad 1.05 | tok/s 24601
step   1560 | loss 1.6572 | lr 3.52e-04 | grad 1.27 | tok/s 25703
step   1570 | loss 1.5907 | lr 3.52e-04 | grad 1.52 | tok/s 24775
step   1580 | loss 1.5679 | lr 3.52e-04 | grad 2.25 | tok/s 24397
step   1590 | loss 1.5197 | lr 3.52e-04 | grad 0.75 | tok/s 25878
step   1600 | loss 1.6346 | lr 3.52e-04 | grad 1.51 | tok/s 25419
step   1610 | loss 1.6155 | lr 3.52e-04 | grad 1.41 | tok/s 25325
step   1620 | loss 1.5356 | lr 3.52e-04 | grad 1.08 | tok/s 24984
step   1630 | loss 1.5859 | lr 3.52e-04 | grad 2.14 | tok/s 24686
step   1640 | loss 1.5864 | lr 3.52e-04 | grad 1.91 | tok/s 24798
step   1650 | loss 1.5710 | lr 3.52e-04 | grad 1.66 | tok/s 25164
step   1660 | loss 2.0387 | lr 3.52e-04 | grad 3.00 | tok/s 25648
step   1670 | loss 1.5914 | lr 3.52e-04 | grad 1.41 | tok/s 24740
step   1680 | loss 1.6480 | lr 3.52e-04 | grad 2.16 | tok/s 25868
step   1690 | loss 1.7025 | lr 3.52e-04 | grad 1.42 | tok/s 24705
step   1700 | loss 1.6050 | lr 3.52e-04 | grad 1.58 | tok/s 25031
step   1710 | loss 1.6600 | lr 3.52e-04 | grad 1.32 | tok/s 24906
step   1720 | loss 1.6982 | lr 3.52e-04 | grad 2.11 | tok/s 25225
step   1730 | loss 1.6102 | lr 3.52e-04 | grad 2.14 | tok/s 24769
step   1740 | loss 1.7404 | lr 3.52e-04 | grad 1.16 | tok/s 24897
step   1750 | loss 1.6803 | lr 3.52e-04 | grad 1.67 | tok/s 24970
step   1760 | loss 1.5793 | lr 3.52e-04 | grad 1.21 | tok/s 24258
step   1770 | loss 1.7577 | lr 3.52e-04 | grad 2.81 | tok/s 25206
step   1780 | loss 1.5567 | lr 3.52e-04 | grad 1.42 | tok/s 25035
step   1790 | loss 1.6084 | lr 3.52e-04 | grad 1.05 | tok/s 25656
step   1800 | loss 1.5068 | lr 3.52e-04 | grad 1.95 | tok/s 24505
step   1810 | loss 1.5741 | lr 3.52e-04 | grad 2.25 | tok/s 25229
step   1820 | loss 1.5678 | lr 3.52e-04 | grad 1.82 | tok/s 24799
step   1830 | loss 1.7633 | lr 3.52e-04 | grad 1.08 | tok/s 24686
step   1840 | loss 1.5708 | lr 3.52e-04 | grad 1.07 | tok/s 24857
step   1850 | loss 1.5328 | lr 3.52e-04 | grad 1.48 | tok/s 25169
step   1860 | loss 1.5571 | lr 3.52e-04 | grad 1.47 | tok/s 25477
step   1870 | loss 1.6622 | lr 3.52e-04 | grad 1.23 | tok/s 25211
step   1880 | loss 1.6382 | lr 3.52e-04 | grad 1.26 | tok/s 25284
step   1890 | loss 1.6698 | lr 3.52e-04 | grad 2.00 | tok/s 24811
step   1900 | loss 1.5061 | lr 3.52e-04 | grad 1.85 | tok/s 25475
step   1910 | loss 1.5119 | lr 3.52e-04 | grad 1.14 | tok/s 25280
step   1920 | loss 1.5065 | lr 3.52e-04 | grad 1.48 | tok/s 25716
step   1930 | loss 1.5978 | lr 3.52e-04 | grad 1.18 | tok/s 25469
step   1940 | loss 1.7421 | lr 3.52e-04 | grad 4.03 | tok/s 25583
step   1950 | loss 1.4927 | lr 3.52e-04 | grad 1.49 | tok/s 24966
step   1960 | loss 1.6194 | lr 3.52e-04 | grad 2.78 | tok/s 25001
step   1970 | loss 1.6293 | lr 3.52e-04 | grad 1.45 | tok/s 25225
step   1980 | loss 1.5504 | lr 3.52e-04 | grad 1.44 | tok/s 25673
step   1990 | loss 1.3665 | lr 3.52e-04 | grad 1.41 | tok/s 25833
step   2000 | loss 1.2966 | lr 3.52e-04 | grad 1.15 | tok/s 26135
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2966.pt
step   2010 | loss 1.5488 | lr 3.52e-04 | grad 1.48 | tok/s 21521
step   2020 | loss 1.3829 | lr 3.52e-04 | grad 1.47 | tok/s 26127
step   2030 | loss 1.4317 | lr 3.52e-04 | grad 1.45 | tok/s 25758
step   2040 | loss 1.6423 | lr 3.52e-04 | grad 1.27 | tok/s 24724
step   2050 | loss 1.5728 | lr 3.52e-04 | grad 2.36 | tok/s 25683
step   2060 | loss 1.6854 | lr 3.52e-04 | grad 2.83 | tok/s 25108
step   2070 | loss 1.5638 | lr 3.52e-04 | grad 1.53 | tok/s 25223
step   2080 | loss 1.5775 | lr 3.52e-04 | grad 1.86 | tok/s 24698
step   2090 | loss 1.3613 | lr 3.52e-04 | grad 1.32 | tok/s 25649
step   2100 | loss 1.3223 | lr 3.52e-04 | grad 1.30 | tok/s 25393
step   2110 | loss 1.6340 | lr 3.52e-04 | grad 1.22 | tok/s 24936
step   2120 | loss 1.7175 | lr 3.52e-04 | grad 1.27 | tok/s 24958
step   2130 | loss 1.6073 | lr 3.52e-04 | grad 1.70 | tok/s 24784
step   2140 | loss 1.5383 | lr 3.52e-04 | grad 1.38 | tok/s 24920
step   2150 | loss 1.7499 | lr 3.52e-04 | grad 1.71 | tok/s 25218
step   2160 | loss 1.7400 | lr 3.52e-04 | grad 3.89 | tok/s 25700
step   2170 | loss 1.5323 | lr 3.52e-04 | grad 1.38 | tok/s 25493
step   2180 | loss 1.3404 | lr 3.52e-04 | grad 1.23 | tok/s 26138
step   2190 | loss 1.3660 | lr 3.52e-04 | grad 1.37 | tok/s 26135
step   2200 | loss 1.3258 | lr 3.52e-04 | grad 1.28 | tok/s 26127
step   2210 | loss 1.5473 | lr 3.52e-04 | grad 1.52 | tok/s 25173
step   2220 | loss 1.7152 | lr 3.52e-04 | grad 2.33 | tok/s 26112
step   2230 | loss 1.6230 | lr 3.52e-04 | grad 1.69 | tok/s 25606
step   2240 | loss 1.6086 | lr 3.52e-04 | grad 1.27 | tok/s 25341
step   2250 | loss 1.5294 | lr 3.52e-04 | grad 1.48 | tok/s 25316
step   2260 | loss 1.6683 | lr 3.52e-04 | grad 1.29 | tok/s 24628
step   2270 | loss 1.5076 | lr 3.52e-04 | grad 1.38 | tok/s 25077
step   2280 | loss 1.5499 | lr 3.52e-04 | grad 1.26 | tok/s 24885
step   2290 | loss 1.5645 | lr 3.52e-04 | grad 1.18 | tok/s 26133
step   2300 | loss 1.4953 | lr 3.52e-04 | grad 1.30 | tok/s 26131
step   2310 | loss 1.4420 | lr 3.52e-04 | grad 1.48 | tok/s 26126
step   2320 | loss 1.6174 | lr 3.52e-04 | grad 2.53 | tok/s 25254
step   2330 | loss 1.6290 | lr 3.52e-04 | grad 1.33 | tok/s 25299
step   2340 | loss 1.9441 | lr 3.52e-04 | grad 1.41 | tok/s 25231
step   2350 | loss 1.5130 | lr 3.52e-04 | grad 1.57 | tok/s 25686
step   2360 | loss 1.5582 | lr 3.52e-04 | grad 1.52 | tok/s 24806
step   2370 | loss 1.4676 | lr 3.52e-04 | grad 1.20 | tok/s 24756
step   2380 | loss 1.6026 | lr 3.52e-04 | grad 1.27 | tok/s 24774
step   2390 | loss 1.6611 | lr 3.52e-04 | grad 7.00 | tok/s 24997
step   2400 | loss 1.5037 | lr 3.52e-04 | grad 1.30 | tok/s 24520
step   2410 | loss 1.7088 | lr 3.52e-04 | grad 1.57 | tok/s 25471
step   2420 | loss 1.4919 | lr 3.52e-04 | grad 1.30 | tok/s 25017
step   2430 | loss 1.4914 | lr 3.52e-04 | grad 1.22 | tok/s 25308
step   2440 | loss 1.5951 | lr 3.52e-04 | grad 3.62 | tok/s 26032
step   2450 | loss 1.4895 | lr 3.52e-04 | grad 1.37 | tok/s 25500
step   2460 | loss 1.4647 | lr 3.52e-04 | grad 2.08 | tok/s 25055
step   2470 | loss 1.4817 | lr 3.52e-04 | grad 1.43 | tok/s 25126
step   2480 | loss 1.5915 | lr 3.52e-04 | grad 1.44 | tok/s 25341
step   2490 | loss 1.7257 | lr 3.52e-04 | grad 3.77 | tok/s 25622
step   2500 | loss 1.6210 | lr 3.52e-04 | grad 2.22 | tok/s 24546
step   2510 | loss 1.5893 | lr 3.52e-04 | grad 1.58 | tok/s 24637
step   2520 | loss 1.5364 | lr 3.52e-04 | grad 1.30 | tok/s 24888
step   2530 | loss 1.9863 | lr 3.52e-04 | grad 1.22 | tok/s 25483
step   2540 | loss 1.5788 | lr 3.52e-04 | grad 1.12 | tok/s 25073
step   2550 | loss 1.6816 | lr 3.52e-04 | grad 1.41 | tok/s 25456
step   2560 | loss 1.4773 | lr 3.52e-04 | grad 1.38 | tok/s 24856
step   2570 | loss 1.5788 | lr 3.52e-04 | grad 3.23 | tok/s 25435
step   2580 | loss 1.5106 | lr 3.52e-04 | grad 1.88 | tok/s 25525
step   2590 | loss 1.5236 | lr 3.52e-04 | grad 1.40 | tok/s 26076
step   2600 | loss 1.4393 | lr 3.52e-04 | grad 1.25 | tok/s 26124
step   2610 | loss 1.5709 | lr 3.52e-04 | grad 1.46 | tok/s 24651
step   2620 | loss 1.5733 | lr 3.52e-04 | grad 1.17 | tok/s 24646
step   2630 | loss 1.7495 | lr 3.52e-04 | grad 2.53 | tok/s 25153
step   2640 | loss 1.5240 | lr 3.52e-04 | grad 1.30 | tok/s 25889
step   2650 | loss 1.3956 | lr 3.52e-04 | grad 1.05 | tok/s 26134
step   2660 | loss 1.3997 | lr 3.52e-04 | grad 1.20 | tok/s 26128
step   2670 | loss 1.5169 | lr 3.52e-04 | grad 1.50 | tok/s 25415
step   2680 | loss 1.5274 | lr 3.52e-04 | grad 1.12 | tok/s 24477
step   2690 | loss 1.9420 | lr 3.52e-04 | grad 4.97 | tok/s 25859
step   2700 | loss 1.6285 | lr 3.52e-04 | grad 1.59 | tok/s 25156
step   2710 | loss 1.4108 | lr 3.52e-04 | grad 1.28 | tok/s 25360
step   2720 | loss 1.5615 | lr 3.52e-04 | grad 1.48 | tok/s 25037
step   2730 | loss 1.6258 | lr 3.52e-04 | grad 1.09 | tok/s 25024
step   2740 | loss 1.6784 | lr 3.52e-04 | grad 2.09 | tok/s 24966
step   2750 | loss 1.5001 | lr 3.52e-04 | grad 1.08 | tok/s 24972
step   2760 | loss 1.4470 | lr 3.52e-04 | grad 1.02 | tok/s 24891
step   2770 | loss 1.7073 | lr 3.52e-04 | grad 1.38 | tok/s 25137
step   2780 | loss 1.7791 | lr 3.52e-04 | grad 1.39 | tok/s 25279
step   2790 | loss 1.5703 | lr 3.52e-04 | grad 2.89 | tok/s 25883
step   2800 | loss 1.6731 | lr 3.52e-04 | grad 1.55 | tok/s 25529
step   2810 | loss 1.6096 | lr 3.52e-04 | grad 0.94 | tok/s 25483
step   2820 | loss 1.4467 | lr 3.52e-04 | grad 2.06 | tok/s 25116
step   2830 | loss 1.5044 | lr 3.52e-04 | grad 2.92 | tok/s 25278
step   2840 | loss 1.5133 | lr 3.52e-04 | grad 1.59 | tok/s 25310
step   2850 | loss 1.9574 | lr 3.52e-04 | grad 1.40 | tok/s 25565

Training complete! Final step: 2858
