Using device: cuda
Output directory: benchmark_results/cmaes_4d/fla-gdn_480M_converge0.01_20260203_055712/eval_5/levelfla-gdn_100m_20260203_055718
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 608,416,416 parameters
Using schedule-free AdamW (lr=0.00019024672608060258)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 5.4771 | lr 1.90e-04 | grad 51.25 | tok/s 382
step     20 | loss 4.5192 | lr 1.90e-04 | grad 28.38 | tok/s 19104
step     30 | loss 3.7542 | lr 1.90e-04 | grad 16.62 | tok/s 19059
step     40 | loss 3.0677 | lr 1.90e-04 | grad 22.38 | tok/s 18974
step     50 | loss 3.1445 | lr 1.90e-04 | grad 11.81 | tok/s 18982
step     60 | loss 2.6350 | lr 1.90e-04 | grad 13.00 | tok/s 18980
step     70 | loss 2.7894 | lr 1.90e-04 | grad 14.81 | tok/s 18994
step     80 | loss 2.3794 | lr 1.90e-04 | grad 8.25 | tok/s 18866
step     90 | loss 2.5951 | lr 1.90e-04 | grad 6.94 | tok/s 18831
step    100 | loss 2.3001 | lr 1.90e-04 | grad 5.62 | tok/s 18753
step    110 | loss 2.3347 | lr 1.90e-04 | grad 9.88 | tok/s 18750
step    120 | loss 2.7579 | lr 1.90e-04 | grad 9.44 | tok/s 18495
step    130 | loss 3.5491 | lr 1.90e-04 | grad 8.56 | tok/s 18102
step    140 | loss 2.7528 | lr 1.90e-04 | grad 6.56 | tok/s 17418
step    150 | loss 2.4306 | lr 1.90e-04 | grad 5.25 | tok/s 17741
step    160 | loss 2.0315 | lr 1.90e-04 | grad 6.06 | tok/s 18216
step    170 | loss 2.3082 | lr 1.90e-04 | grad 4.25 | tok/s 17786
step    180 | loss 2.6200 | lr 1.90e-04 | grad 13.25 | tok/s 17764
step    190 | loss 1.4222 | lr 1.90e-04 | grad 4.00 | tok/s 18510
step    200 | loss 0.8499 | lr 1.90e-04 | grad 7.28 | tok/s 18255
step    210 | loss 2.5637 | lr 1.90e-04 | grad 7.72 | tok/s 18247
step    220 | loss 2.1990 | lr 1.90e-04 | grad 4.62 | tok/s 17596
step    230 | loss 2.1116 | lr 1.90e-04 | grad 4.28 | tok/s 17173
step    240 | loss 2.3261 | lr 1.90e-04 | grad 3.84 | tok/s 17306
step    250 | loss 1.9857 | lr 1.90e-04 | grad 10.44 | tok/s 17720
step    260 | loss 1.4153 | lr 1.90e-04 | grad 40.00 | tok/s 18045
step    270 | loss 1.9130 | lr 1.90e-04 | grad 4.03 | tok/s 16290
step    280 | loss 1.8904 | lr 1.90e-04 | grad 4.12 | tok/s 17547
step    290 | loss 1.6033 | lr 1.90e-04 | grad 3.64 | tok/s 18136
step    300 | loss 1.6029 | lr 1.90e-04 | grad 4.72 | tok/s 17973
step    310 | loss 1.3765 | lr 1.90e-04 | grad 4.09 | tok/s 17524
step    320 | loss 1.9595 | lr 1.90e-04 | grad 8.69 | tok/s 16810
step    330 | loss 2.0334 | lr 1.90e-04 | grad 2.89 | tok/s 17623
step    340 | loss 2.2784 | lr 1.90e-04 | grad 7.75 | tok/s 17008
step    350 | loss 1.9475 | lr 1.90e-04 | grad 3.11 | tok/s 17234
step    360 | loss 1.6804 | lr 1.90e-04 | grad 4.03 | tok/s 17141
step    370 | loss 1.8795 | lr 1.90e-04 | grad 3.86 | tok/s 17398
step    380 | loss 2.3932 | lr 1.90e-04 | grad 5.88 | tok/s 17574
step    390 | loss 1.7486 | lr 1.90e-04 | grad 3.09 | tok/s 16604
step    400 | loss 1.7319 | lr 1.90e-04 | grad 3.52 | tok/s 17627
step    410 | loss 1.6629 | lr 1.90e-04 | grad 3.00 | tok/s 17895
step    420 | loss 1.6820 | lr 1.90e-04 | grad 3.06 | tok/s 17479
step    430 | loss 1.9272 | lr 1.90e-04 | grad 2.69 | tok/s 17402
step    440 | loss 1.7100 | lr 1.90e-04 | grad 3.25 | tok/s 17244
step    450 | loss 1.5795 | lr 1.90e-04 | grad 3.62 | tok/s 16535
step    460 | loss 1.7148 | lr 1.90e-04 | grad 4.22 | tok/s 16330
step    470 | loss 1.6114 | lr 1.90e-04 | grad 5.56 | tok/s 16966
step    480 | loss 1.5370 | lr 1.90e-04 | grad 2.91 | tok/s 16552
step    490 | loss 1.8936 | lr 1.90e-04 | grad 3.67 | tok/s 17149
step    500 | loss 1.7765 | lr 1.90e-04 | grad 2.69 | tok/s 16595
step    510 | loss 1.6006 | lr 1.90e-04 | grad 2.89 | tok/s 16878
step    520 | loss 1.5664 | lr 1.90e-04 | grad 2.75 | tok/s 16840
step    530 | loss 1.5956 | lr 1.90e-04 | grad 3.47 | tok/s 16586
step    540 | loss 1.8502 | lr 1.90e-04 | grad 6.88 | tok/s 16758
step    550 | loss 1.6103 | lr 1.90e-04 | grad 3.67 | tok/s 17158
step    560 | loss 1.6785 | lr 1.90e-04 | grad 3.81 | tok/s 17254
step    570 | loss 1.9247 | lr 1.90e-04 | grad 3.78 | tok/s 17219
step    580 | loss 1.6967 | lr 1.90e-04 | grad 4.16 | tok/s 16810
step    590 | loss 1.7873 | lr 1.90e-04 | grad 3.48 | tok/s 17565
step    600 | loss 1.4084 | lr 1.90e-04 | grad 3.16 | tok/s 17761
step    610 | loss 1.5810 | lr 1.90e-04 | grad 4.03 | tok/s 16321
step    620 | loss 1.4555 | lr 1.90e-04 | grad 3.58 | tok/s 16710
step    630 | loss 1.2868 | lr 1.90e-04 | grad 3.84 | tok/s 17746
step    640 | loss 1.4379 | lr 1.90e-04 | grad 2.59 | tok/s 17514
step    650 | loss 1.1902 | lr 1.90e-04 | grad 2.38 | tok/s 17757
step    660 | loss 1.0732 | lr 1.90e-04 | grad 2.09 | tok/s 17767
step    670 | loss 1.0449 | lr 1.90e-04 | grad 2.38 | tok/s 17784
step    680 | loss 0.9492 | lr 1.90e-04 | grad 2.38 | tok/s 17781
step    690 | loss 1.1500 | lr 1.90e-04 | grad 3.23 | tok/s 17043
step    700 | loss 1.8990 | lr 1.90e-04 | grad 7.28 | tok/s 17350
step    710 | loss 1.6281 | lr 1.90e-04 | grad 6.06 | tok/s 16814
step    720 | loss 1.6273 | lr 1.90e-04 | grad 10.25 | tok/s 16669
step    730 | loss 1.4630 | lr 1.90e-04 | grad 3.59 | tok/s 17569
step    740 | loss 1.4339 | lr 1.90e-04 | grad 5.66 | tok/s 17728
step    750 | loss 1.4790 | lr 1.90e-04 | grad 4.50 | tok/s 17467
step    760 | loss 1.4525 | lr 1.90e-04 | grad 3.25 | tok/s 17359
step    770 | loss 1.4762 | lr 1.90e-04 | grad 3.02 | tok/s 17029
step    780 | loss 1.6603 | lr 1.90e-04 | grad 3.34 | tok/s 16914
step    790 | loss 1.5949 | lr 1.90e-04 | grad 2.42 | tok/s 16964
step    800 | loss 1.5279 | lr 1.90e-04 | grad 2.30 | tok/s 16998
step    810 | loss 1.4139 | lr 1.90e-04 | grad 3.02 | tok/s 17144
step    820 | loss 1.5376 | lr 1.90e-04 | grad 4.28 | tok/s 17452
step    830 | loss 1.5037 | lr 1.90e-04 | grad 2.30 | tok/s 16505
step    840 | loss 1.3488 | lr 1.90e-04 | grad 2.50 | tok/s 17746
step    850 | loss 1.4621 | lr 1.90e-04 | grad 3.02 | tok/s 17660
step    860 | loss 1.3027 | lr 1.90e-04 | grad 2.64 | tok/s 17500
step    870 | loss 1.7363 | lr 1.90e-04 | grad 4.19 | tok/s 16701
step    880 | loss 1.5221 | lr 1.90e-04 | grad 2.92 | tok/s 16915
step    890 | loss 1.4389 | lr 1.90e-04 | grad 2.52 | tok/s 17419
step    900 | loss 1.8049 | lr 1.90e-04 | grad 7.41 | tok/s 16752
step    910 | loss 1.6118 | lr 1.90e-04 | grad 3.23 | tok/s 16839
step    920 | loss 1.5940 | lr 1.90e-04 | grad 2.42 | tok/s 15884
step    930 | loss 1.4286 | lr 1.90e-04 | grad 2.89 | tok/s 16901
step    940 | loss 1.4419 | lr 1.90e-04 | grad 10.94 | tok/s 17444
step    950 | loss 1.4681 | lr 1.90e-04 | grad 2.89 | tok/s 16187
step    960 | loss 1.6983 | lr 1.90e-04 | grad 4.81 | tok/s 17459
step    970 | loss 1.6649 | lr 1.90e-04 | grad 3.00 | tok/s 17574
step    980 | loss 1.4365 | lr 1.90e-04 | grad 2.16 | tok/s 16392
step    990 | loss 1.2105 | lr 1.90e-04 | grad 6.69 | tok/s 16590
step   1000 | loss 1.1780 | lr 1.90e-04 | grad 2.97 | tok/s 17025
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1780.pt
step   1010 | loss 1.4314 | lr 1.90e-04 | grad 2.44 | tok/s 6773
step   1020 | loss 1.3158 | lr 1.90e-04 | grad 2.86 | tok/s 17880
step   1030 | loss 1.2526 | lr 1.90e-04 | grad 2.55 | tok/s 17896
step   1040 | loss 1.2609 | lr 1.90e-04 | grad 2.53 | tok/s 17877
step   1050 | loss 1.2522 | lr 1.90e-04 | grad 2.23 | tok/s 17857
step   1060 | loss 1.1846 | lr 1.90e-04 | grad 2.27 | tok/s 17872
step   1070 | loss 1.2503 | lr 1.90e-04 | grad 2.52 | tok/s 17847
step   1080 | loss 1.2799 | lr 1.90e-04 | grad 3.23 | tok/s 17831
step   1090 | loss 1.1827 | lr 1.90e-04 | grad 2.36 | tok/s 17825
step   1100 | loss 1.2006 | lr 1.90e-04 | grad 2.20 | tok/s 17817
step   1110 | loss 1.2278 | lr 1.90e-04 | grad 2.50 | tok/s 17828
step   1120 | loss 1.2434 | lr 1.90e-04 | grad 2.31 | tok/s 17815
step   1130 | loss 1.2228 | lr 1.90e-04 | grad 2.11 | tok/s 17818
step   1140 | loss 1.3282 | lr 1.90e-04 | grad 14.44 | tok/s 17428
step   1150 | loss 1.6537 | lr 1.90e-04 | grad 2.69 | tok/s 16991
step   1160 | loss 1.2538 | lr 1.90e-04 | grad 2.59 | tok/s 16781
step   1170 | loss 1.6803 | lr 1.90e-04 | grad 2.69 | tok/s 16502
step   1180 | loss 1.5474 | lr 1.90e-04 | grad 2.39 | tok/s 17448
step   1190 | loss 1.4700 | lr 1.90e-04 | grad 3.38 | tok/s 17176
step   1200 | loss 1.4620 | lr 1.90e-04 | grad 3.89 | tok/s 16917
step   1210 | loss 1.4220 | lr 1.90e-04 | grad 4.06 | tok/s 17695
step   1220 | loss 1.5845 | lr 1.90e-04 | grad 4.84 | tok/s 17491
step   1230 | loss 1.3564 | lr 1.90e-04 | grad 2.80 | tok/s 17506
step   1240 | loss 1.5949 | lr 1.90e-04 | grad 2.42 | tok/s 16238
step   1250 | loss 1.6730 | lr 1.90e-04 | grad 3.78 | tok/s 16548
step   1260 | loss 1.4099 | lr 1.90e-04 | grad 2.11 | tok/s 17288
step   1270 | loss 1.4841 | lr 1.90e-04 | grad 7.66 | tok/s 16938
step   1280 | loss 1.5165 | lr 1.90e-04 | grad 3.62 | tok/s 16303
step   1290 | loss 1.3709 | lr 1.90e-04 | grad 2.58 | tok/s 16902
step   1300 | loss 1.3382 | lr 1.90e-04 | grad 2.47 | tok/s 17024
step   1310 | loss 1.4784 | lr 1.90e-04 | grad 4.25 | tok/s 16707
step   1320 | loss 1.5511 | lr 1.90e-04 | grad 2.88 | tok/s 16686
step   1330 | loss 1.2913 | lr 1.90e-04 | grad 2.62 | tok/s 16917
step   1340 | loss 1.1041 | lr 1.90e-04 | grad 2.42 | tok/s 17752
step   1350 | loss 1.2192 | lr 1.90e-04 | grad 9.06 | tok/s 17540
step   1360 | loss 1.5786 | lr 1.90e-04 | grad 6.19 | tok/s 16615
step   1370 | loss 1.3981 | lr 1.90e-04 | grad 2.30 | tok/s 17619
step   1380 | loss 1.8115 | lr 1.90e-04 | grad 5.00 | tok/s 17418
step   1390 | loss 1.5201 | lr 1.90e-04 | grad 5.56 | tok/s 17658
step   1400 | loss 1.2506 | lr 1.90e-04 | grad 2.42 | tok/s 17739
step   1410 | loss 1.4898 | lr 1.90e-04 | grad 2.56 | tok/s 17531
step   1420 | loss 1.4032 | lr 1.90e-04 | grad 2.67 | tok/s 17180
step   1430 | loss 1.3662 | lr 1.90e-04 | grad 2.33 | tok/s 17229
step   1440 | loss 1.5902 | lr 1.90e-04 | grad 2.69 | tok/s 16906
step   1450 | loss 1.2141 | lr 1.90e-04 | grad 2.69 | tok/s 17623
step   1460 | loss 1.5463 | lr 1.90e-04 | grad 2.66 | tok/s 16744
step   1470 | loss 1.2958 | lr 1.90e-04 | grad 4.09 | tok/s 17571
step   1480 | loss 1.6032 | lr 1.90e-04 | grad 4.78 | tok/s 17544
step   1490 | loss 1.4938 | lr 1.90e-04 | grad 2.36 | tok/s 16718
step   1500 | loss 0.8368 | lr 1.90e-04 | grad 2.06 | tok/s 17971
step   1510 | loss 1.1416 | lr 1.90e-04 | grad 2.28 | tok/s 16567
step   1520 | loss 1.3100 | lr 1.90e-04 | grad 3.33 | tok/s 16899
step   1530 | loss 1.3338 | lr 1.90e-04 | grad 2.73 | tok/s 17288
step   1540 | loss 1.3922 | lr 1.90e-04 | grad 5.53 | tok/s 16784
step   1550 | loss 1.5060 | lr 1.90e-04 | grad 5.47 | tok/s 15897
step   1560 | loss 1.2098 | lr 1.90e-04 | grad 2.05 | tok/s 17793
step   1570 | loss 1.5432 | lr 1.90e-04 | grad 9.06 | tok/s 16928
step   1580 | loss 1.4948 | lr 1.90e-04 | grad 2.91 | tok/s 16542
step   1590 | loss 1.3647 | lr 1.90e-04 | grad 4.62 | tok/s 17159
step   1600 | loss 1.4623 | lr 1.90e-04 | grad 2.42 | tok/s 16654
step   1610 | loss 1.3705 | lr 1.90e-04 | grad 2.69 | tok/s 17209
step   1620 | loss 1.4411 | lr 1.90e-04 | grad 3.52 | tok/s 17723
step   1630 | loss 1.1352 | lr 1.90e-04 | grad 4.19 | tok/s 17739
step   1640 | loss 1.3433 | lr 1.90e-04 | grad 3.00 | tok/s 16998
step   1650 | loss 1.4880 | lr 1.90e-04 | grad 2.56 | tok/s 17286
step   1660 | loss 1.5367 | lr 1.90e-04 | grad 2.80 | tok/s 17065
step   1670 | loss 1.3943 | lr 1.90e-04 | grad 2.20 | tok/s 16789
step   1680 | loss 1.4392 | lr 1.90e-04 | grad 3.17 | tok/s 17174
step   1690 | loss 1.3698 | lr 1.90e-04 | grad 2.38 | tok/s 17054
step   1700 | loss 1.5228 | lr 1.90e-04 | grad 2.73 | tok/s 16683
step   1710 | loss 1.4193 | lr 1.90e-04 | grad 3.66 | tok/s 16879
step   1720 | loss 1.4082 | lr 1.90e-04 | grad 6.09 | tok/s 17010
step   1730 | loss 1.3908 | lr 1.90e-04 | grad 2.80 | tok/s 17322
step   1740 | loss 1.3762 | lr 1.90e-04 | grad 2.09 | tok/s 16528
step   1750 | loss 1.2663 | lr 1.90e-04 | grad 2.28 | tok/s 17673
step   1760 | loss 1.3150 | lr 1.90e-04 | grad 2.72 | tok/s 16575
step   1770 | loss 1.2875 | lr 1.90e-04 | grad 1.84 | tok/s 17449
step   1780 | loss 1.2482 | lr 1.90e-04 | grad 3.12 | tok/s 15829
step   1790 | loss 1.5017 | lr 1.90e-04 | grad 2.09 | tok/s 16683
step   1800 | loss 1.3291 | lr 1.90e-04 | grad 2.16 | tok/s 16876
step   1810 | loss 1.4314 | lr 1.90e-04 | grad 2.22 | tok/s 16438
step   1820 | loss 1.3271 | lr 1.90e-04 | grad 2.33 | tok/s 17571
step   1830 | loss 1.4340 | lr 1.90e-04 | grad 2.38 | tok/s 16526
step   1840 | loss 1.4237 | lr 1.90e-04 | grad 3.52 | tok/s 17405
step   1850 | loss 1.8310 | lr 1.90e-04 | grad 4.81 | tok/s 17758
step   1860 | loss 1.4463 | lr 1.90e-04 | grad 4.38 | tok/s 17767
step   1870 | loss 1.4857 | lr 1.90e-04 | grad 6.28 | tok/s 17208
step   1880 | loss 1.4283 | lr 1.90e-04 | grad 2.03 | tok/s 16725
step   1890 | loss 1.5574 | lr 1.90e-04 | grad 2.33 | tok/s 16886
step   1900 | loss 1.4522 | lr 1.90e-04 | grad 2.41 | tok/s 17129
step   1910 | loss 0.9756 | lr 1.90e-04 | grad 2.06 | tok/s 17806
step   1920 | loss 1.2393 | lr 1.90e-04 | grad 2.12 | tok/s 17136
step   1930 | loss 1.1408 | lr 1.90e-04 | grad 2.75 | tok/s 17643
step   1940 | loss 1.1092 | lr 1.90e-04 | grad 2.45 | tok/s 17859
step   1950 | loss 1.1576 | lr 1.90e-04 | grad 2.52 | tok/s 17607
step   1960 | loss 1.4721 | lr 1.90e-04 | grad 2.92 | tok/s 16226
step   1970 | loss 1.6357 | lr 1.90e-04 | grad 2.55 | tok/s 17171
step   1980 | loss 2.0645 | lr 1.90e-04 | grad 5.06 | tok/s 17473
step   1990 | loss 1.6566 | lr 1.90e-04 | grad 3.41 | tok/s 17739
step   2000 | loss 1.3220 | lr 1.90e-04 | grad 2.52 | tok/s 17150
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3220.pt
step   2010 | loss 1.4338 | lr 1.90e-04 | grad 2.58 | tok/s 6358
step   2020 | loss 0.7720 | lr 1.90e-04 | grad 1.98 | tok/s 18218
step   2030 | loss 1.2505 | lr 1.90e-04 | grad 2.78 | tok/s 17133
step   2040 | loss 1.3891 | lr 1.90e-04 | grad 2.17 | tok/s 17516
step   2050 | loss 1.2446 | lr 1.90e-04 | grad 2.36 | tok/s 17880
step   2060 | loss 1.1499 | lr 1.90e-04 | grad 2.17 | tok/s 17879
step   2070 | loss 1.1802 | lr 1.90e-04 | grad 2.09 | tok/s 17911
step   2080 | loss 1.1518 | lr 1.90e-04 | grad 1.95 | tok/s 16462
step   2090 | loss 1.1843 | lr 1.90e-04 | grad 2.20 | tok/s 17877
step   2100 | loss 1.1222 | lr 1.90e-04 | grad 2.03 | tok/s 17868
step   2110 | loss 1.1178 | lr 1.90e-04 | grad 2.16 | tok/s 17850
step   2120 | loss 1.0798 | lr 1.90e-04 | grad 2.06 | tok/s 17829
step   2130 | loss 1.3788 | lr 1.90e-04 | grad 2.59 | tok/s 17507
step   2140 | loss 1.3149 | lr 1.90e-04 | grad 5.66 | tok/s 17229
step   2150 | loss 1.4654 | lr 1.90e-04 | grad 6.47 | tok/s 17848
step   2160 | loss 1.4978 | lr 1.90e-04 | grad 2.61 | tok/s 17254
step   2170 | loss 1.9239 | lr 1.90e-04 | grad 3.62 | tok/s 17691
step   2180 | loss 1.3375 | lr 1.90e-04 | grad 3.39 | tok/s 17830
step   2190 | loss 1.4559 | lr 1.90e-04 | grad 2.91 | tok/s 17015
step   2200 | loss 1.3301 | lr 1.90e-04 | grad 2.86 | tok/s 17484
step   2210 | loss 1.3536 | lr 1.90e-04 | grad 2.28 | tok/s 16942
step   2220 | loss 1.8686 | lr 1.90e-04 | grad 3.20 | tok/s 16805
step   2230 | loss 1.4119 | lr 1.90e-04 | grad 3.06 | tok/s 16632
step   2240 | loss 1.4041 | lr 1.90e-04 | grad 3.48 | tok/s 16792
step   2250 | loss 1.3132 | lr 1.90e-04 | grad 3.27 | tok/s 17420
step   2260 | loss 1.2081 | lr 1.90e-04 | grad 3.75 | tok/s 17646
step   2270 | loss 1.5038 | lr 1.90e-04 | grad 3.62 | tok/s 17601
step   2280 | loss 1.3692 | lr 1.90e-04 | grad 4.00 | tok/s 17804
step   2290 | loss 1.1031 | lr 1.90e-04 | grad 5.75 | tok/s 17778
step   2300 | loss 0.9719 | lr 1.90e-04 | grad 1.94 | tok/s 17785
step   2310 | loss 1.1810 | lr 1.90e-04 | grad 2.59 | tok/s 17110
step   2320 | loss 1.4210 | lr 1.90e-04 | grad 3.17 | tok/s 16372
step   2330 | loss 1.2418 | lr 1.90e-04 | grad 2.75 | tok/s 17486
step   2340 | loss 1.3154 | lr 1.90e-04 | grad 3.92 | tok/s 17230
step   2350 | loss 1.4240 | lr 1.90e-04 | grad 3.03 | tok/s 17169
step   2360 | loss 1.0686 | lr 1.90e-04 | grad 1.85 | tok/s 17711
step   2370 | loss 1.1711 | lr 1.90e-04 | grad 2.02 | tok/s 17568
step   2380 | loss 1.2098 | lr 1.90e-04 | grad 2.52 | tok/s 17519
step   2390 | loss 1.3904 | lr 1.90e-04 | grad 4.28 | tok/s 16979
