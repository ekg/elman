Using device: cuda
Output directory: benchmark_results/cmaes_4d/fla-gdn_480M_converge0.01_20260203_055712/eval_7/levelfla-gdn_100m_20260203_055719
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 608,416,416 parameters
Using schedule-free AdamW (lr=0.00018912218692801832)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 5.4276 | lr 1.89e-04 | grad 57.00 | tok/s 387
step     20 | loss 4.6574 | lr 1.89e-04 | grad 25.75 | tok/s 19260
step     30 | loss 3.7171 | lr 1.89e-04 | grad 24.00 | tok/s 19230
step     40 | loss 3.2512 | lr 1.89e-04 | grad 23.75 | tok/s 19128
step     50 | loss 3.0531 | lr 1.89e-04 | grad 19.50 | tok/s 19045
step     60 | loss 2.6457 | lr 1.89e-04 | grad 7.91 | tok/s 18969
step     70 | loss 2.6781 | lr 1.89e-04 | grad 14.94 | tok/s 18972
step     80 | loss 2.6082 | lr 1.89e-04 | grad 7.94 | tok/s 18948
step     90 | loss 2.5626 | lr 1.89e-04 | grad 7.16 | tok/s 18834
step    100 | loss 2.2384 | lr 1.89e-04 | grad 10.56 | tok/s 18862
step    110 | loss 2.3187 | lr 1.89e-04 | grad 5.78 | tok/s 18774
step    120 | loss 2.7435 | lr 1.89e-04 | grad 9.62 | tok/s 18509
step    130 | loss 3.5173 | lr 1.89e-04 | grad 16.12 | tok/s 18434
step    140 | loss 2.7457 | lr 1.89e-04 | grad 6.56 | tok/s 17124
step    150 | loss 2.5456 | lr 1.89e-04 | grad 4.88 | tok/s 17604
step    160 | loss 2.0495 | lr 1.89e-04 | grad 7.31 | tok/s 18486
step    170 | loss 2.3458 | lr 1.89e-04 | grad 5.50 | tok/s 17602
step    180 | loss 2.4701 | lr 1.89e-04 | grad 6.44 | tok/s 17849
step    190 | loss 1.7838 | lr 1.89e-04 | grad 4.50 | tok/s 18054
step    200 | loss 0.6812 | lr 1.89e-04 | grad 2.92 | tok/s 18431
step    210 | loss 2.4596 | lr 1.89e-04 | grad 6.88 | tok/s 18034
step    220 | loss 2.1390 | lr 1.89e-04 | grad 8.00 | tok/s 17799
step    230 | loss 2.1613 | lr 1.89e-04 | grad 4.31 | tok/s 16737
step    240 | loss 2.3741 | lr 1.89e-04 | grad 4.25 | tok/s 17243
step    250 | loss 2.0648 | lr 1.89e-04 | grad 6.78 | tok/s 17486
step    260 | loss 1.3128 | lr 1.89e-04 | grad 4.34 | tok/s 17967
step    270 | loss 2.0048 | lr 1.89e-04 | grad 3.75 | tok/s 16184
step    280 | loss 1.8943 | lr 1.89e-04 | grad 4.72 | tok/s 17264
step    290 | loss 1.6391 | lr 1.89e-04 | grad 4.06 | tok/s 17945
step    300 | loss 1.5609 | lr 1.89e-04 | grad 3.95 | tok/s 17793
step    310 | loss 1.4190 | lr 1.89e-04 | grad 3.19 | tok/s 17892
step    320 | loss 1.7189 | lr 1.89e-04 | grad 3.45 | tok/s 16115
step    330 | loss 2.2309 | lr 1.89e-04 | grad 3.45 | tok/s 17540
step    340 | loss 2.2046 | lr 1.89e-04 | grad 5.16 | tok/s 16756
step    350 | loss 2.0037 | lr 1.89e-04 | grad 3.89 | tok/s 17097
step    360 | loss 1.8370 | lr 1.89e-04 | grad 4.22 | tok/s 16775
step    370 | loss 1.7924 | lr 1.89e-04 | grad 3.75 | tok/s 17271
step    380 | loss 2.1919 | lr 1.89e-04 | grad 6.75 | tok/s 17300
step    390 | loss 1.9059 | lr 1.89e-04 | grad 3.16 | tok/s 16348
step    400 | loss 1.6612 | lr 1.89e-04 | grad 4.22 | tok/s 17378
step    410 | loss 1.7113 | lr 1.89e-04 | grad 3.05 | tok/s 17578
step    420 | loss 1.7046 | lr 1.89e-04 | grad 2.70 | tok/s 17545
step    430 | loss 1.9009 | lr 1.89e-04 | grad 3.38 | tok/s 16895
step    440 | loss 1.7467 | lr 1.89e-04 | grad 2.64 | tok/s 16841
step    450 | loss 1.5310 | lr 1.89e-04 | grad 2.61 | tok/s 16556
step    460 | loss 1.7264 | lr 1.89e-04 | grad 3.62 | tok/s 16029
step    470 | loss 1.6602 | lr 1.89e-04 | grad 2.81 | tok/s 16606
step    480 | loss 1.5110 | lr 1.89e-04 | grad 2.69 | tok/s 16296
step    490 | loss 1.8268 | lr 1.89e-04 | grad 7.69 | tok/s 16893
step    500 | loss 1.8177 | lr 1.89e-04 | grad 3.56 | tok/s 16151
step    510 | loss 1.6109 | lr 1.89e-04 | grad 3.05 | tok/s 17098
step    520 | loss 1.5421 | lr 1.89e-04 | grad 3.09 | tok/s 16270
step    530 | loss 1.6270 | lr 1.89e-04 | grad 3.33 | tok/s 16125
step    540 | loss 1.7718 | lr 1.89e-04 | grad 3.33 | tok/s 16725
step    550 | loss 1.6433 | lr 1.89e-04 | grad 3.41 | tok/s 16786
step    560 | loss 1.7155 | lr 1.89e-04 | grad 4.12 | tok/s 16987
step    570 | loss 1.8364 | lr 1.89e-04 | grad 3.88 | tok/s 16847
step    580 | loss 1.7661 | lr 1.89e-04 | grad 3.45 | tok/s 16599
step    590 | loss 1.8228 | lr 1.89e-04 | grad 3.62 | tok/s 17097
step    600 | loss 1.4647 | lr 1.89e-04 | grad 3.59 | tok/s 17489
step    610 | loss 1.5487 | lr 1.89e-04 | grad 2.62 | tok/s 16544
step    620 | loss 1.4083 | lr 1.89e-04 | grad 5.62 | tok/s 15899
step    630 | loss 1.3098 | lr 1.89e-04 | grad 2.39 | tok/s 17434
step    640 | loss 1.5308 | lr 1.89e-04 | grad 2.98 | tok/s 17192
step    650 | loss 1.1962 | lr 1.89e-04 | grad 2.72 | tok/s 17477
step    660 | loss 1.1055 | lr 1.89e-04 | grad 2.36 | tok/s 17426
step    670 | loss 1.0475 | lr 1.89e-04 | grad 2.31 | tok/s 17448
step    680 | loss 0.9733 | lr 1.89e-04 | grad 2.28 | tok/s 17454
step    690 | loss 1.0391 | lr 1.89e-04 | grad 3.23 | tok/s 17020
step    700 | loss 1.7144 | lr 1.89e-04 | grad 3.81 | tok/s 16678
step    710 | loss 1.7631 | lr 1.89e-04 | grad 2.62 | tok/s 16742
step    720 | loss 1.6216 | lr 1.89e-04 | grad 2.45 | tok/s 16077
step    730 | loss 1.5468 | lr 1.89e-04 | grad 3.34 | tok/s 17219
step    740 | loss 1.3695 | lr 1.89e-04 | grad 3.75 | tok/s 17399
step    750 | loss 1.5294 | lr 1.89e-04 | grad 2.94 | tok/s 17345
step    760 | loss 1.3697 | lr 1.89e-04 | grad 2.72 | tok/s 17211
step    770 | loss 1.5300 | lr 1.89e-04 | grad 3.02 | tok/s 16386
step    780 | loss 1.6419 | lr 1.89e-04 | grad 4.97 | tok/s 16741
step    790 | loss 1.6114 | lr 1.89e-04 | grad 3.28 | tok/s 16620
step    800 | loss 1.5367 | lr 1.89e-04 | grad 4.06 | tok/s 16815
step    810 | loss 1.3804 | lr 1.89e-04 | grad 4.34 | tok/s 16433
step    820 | loss 1.5379 | lr 1.89e-04 | grad 5.12 | tok/s 17080
step    830 | loss 1.5459 | lr 1.89e-04 | grad 2.67 | tok/s 16323
step    840 | loss 1.4069 | lr 1.89e-04 | grad 2.81 | tok/s 17200
step    850 | loss 1.4574 | lr 1.89e-04 | grad 3.20 | tok/s 17256
step    860 | loss 1.2573 | lr 1.89e-04 | grad 4.12 | tok/s 17322
step    870 | loss 1.6230 | lr 1.89e-04 | grad 4.88 | tok/s 16279
step    880 | loss 1.6353 | lr 1.89e-04 | grad 3.58 | tok/s 16926
step    890 | loss 1.4045 | lr 1.89e-04 | grad 2.69 | tok/s 16766
step    900 | loss 1.7325 | lr 1.89e-04 | grad 6.56 | tok/s 16299
step    910 | loss 1.6750 | lr 1.89e-04 | grad 3.67 | tok/s 16801
step    920 | loss 1.6237 | lr 1.89e-04 | grad 2.52 | tok/s 15713
step    930 | loss 1.4459 | lr 1.89e-04 | grad 4.28 | tok/s 16124
step    940 | loss 1.3957 | lr 1.89e-04 | grad 2.44 | tok/s 17103
step    950 | loss 1.4388 | lr 1.89e-04 | grad 2.73 | tok/s 15703
step    960 | loss 1.7159 | lr 1.89e-04 | grad 3.53 | tok/s 17125
step    970 | loss 1.6945 | lr 1.89e-04 | grad 5.25 | tok/s 17054
step    980 | loss 1.4769 | lr 1.89e-04 | grad 2.62 | tok/s 16287
step    990 | loss 1.2991 | lr 1.89e-04 | grad 2.58 | tok/s 15908
step   1000 | loss 1.0765 | lr 1.89e-04 | grad 2.30 | tok/s 16961
  >>> saved checkpoint: checkpoint_step_001000_loss_1.0765.pt
step   1010 | loss 1.4230 | lr 1.89e-04 | grad 2.58 | tok/s 5804
step   1020 | loss 1.3247 | lr 1.89e-04 | grad 2.81 | tok/s 17606
step   1030 | loss 1.2713 | lr 1.89e-04 | grad 2.25 | tok/s 17604
step   1040 | loss 1.2502 | lr 1.89e-04 | grad 2.64 | tok/s 17522
step   1050 | loss 1.2672 | lr 1.89e-04 | grad 2.22 | tok/s 17508
step   1060 | loss 1.1877 | lr 1.89e-04 | grad 2.34 | tok/s 17512
step   1070 | loss 1.2100 | lr 1.89e-04 | grad 2.59 | tok/s 17497
step   1080 | loss 1.3164 | lr 1.89e-04 | grad 2.05 | tok/s 17479
step   1090 | loss 1.1931 | lr 1.89e-04 | grad 2.77 | tok/s 17439
step   1100 | loss 1.1826 | lr 1.89e-04 | grad 2.59 | tok/s 17471
step   1110 | loss 1.2350 | lr 1.89e-04 | grad 2.47 | tok/s 17481
step   1120 | loss 1.2537 | lr 1.89e-04 | grad 2.66 | tok/s 17460
step   1130 | loss 1.2210 | lr 1.89e-04 | grad 2.14 | tok/s 17443
step   1140 | loss 1.1817 | lr 1.89e-04 | grad 1.91 | tok/s 17446
step   1150 | loss 1.7608 | lr 1.89e-04 | grad 3.97 | tok/s 16508
step   1160 | loss 1.3466 | lr 1.89e-04 | grad 4.62 | tok/s 16412
step   1170 | loss 1.6031 | lr 1.89e-04 | grad 6.25 | tok/s 16320
step   1180 | loss 1.5890 | lr 1.89e-04 | grad 2.83 | tok/s 16794
step   1190 | loss 1.4303 | lr 1.89e-04 | grad 2.39 | tok/s 16680
step   1200 | loss 1.4718 | lr 1.89e-04 | grad 2.92 | tok/s 16785
step   1210 | loss 1.4314 | lr 1.89e-04 | grad 3.14 | tok/s 17034
step   1220 | loss 1.5389 | lr 1.89e-04 | grad 2.86 | tok/s 17098
step   1230 | loss 1.5469 | lr 1.89e-04 | grad 2.91 | tok/s 17118
step   1240 | loss 1.4365 | lr 1.89e-04 | grad 11.38 | tok/s 16364
step   1250 | loss 1.6493 | lr 1.89e-04 | grad 3.05 | tok/s 15829
step   1260 | loss 1.4496 | lr 1.89e-04 | grad 3.05 | tok/s 16774
step   1270 | loss 1.3866 | lr 1.89e-04 | grad 2.08 | tok/s 16576
step   1280 | loss 1.5658 | lr 1.89e-04 | grad 3.19 | tok/s 15960
step   1290 | loss 1.4703 | lr 1.89e-04 | grad 2.02 | tok/s 16919
step   1300 | loss 1.3453 | lr 1.89e-04 | grad 2.36 | tok/s 16290
step   1310 | loss 1.3934 | lr 1.89e-04 | grad 3.67 | tok/s 16344
step   1320 | loss 1.5895 | lr 1.89e-04 | grad 6.59 | tok/s 16361
step   1330 | loss 1.3048 | lr 1.89e-04 | grad 2.39 | tok/s 16633
step   1340 | loss 1.1136 | lr 1.89e-04 | grad 2.48 | tok/s 17201
step   1350 | loss 1.1239 | lr 1.89e-04 | grad 5.38 | tok/s 17327
step   1360 | loss 1.5550 | lr 1.89e-04 | grad 2.62 | tok/s 16358
step   1370 | loss 1.4752 | lr 1.89e-04 | grad 2.44 | tok/s 16949
step   1380 | loss 1.7595 | lr 1.89e-04 | grad 5.41 | tok/s 17052
step   1390 | loss 1.5293 | lr 1.89e-04 | grad 2.23 | tok/s 17283
step   1400 | loss 1.3231 | lr 1.89e-04 | grad 2.31 | tok/s 17353
step   1410 | loss 1.4759 | lr 1.89e-04 | grad 2.52 | tok/s 17114
step   1420 | loss 1.3745 | lr 1.89e-04 | grad 4.75 | tok/s 16775
step   1430 | loss 1.3963 | lr 1.89e-04 | grad 2.66 | tok/s 17165
step   1440 | loss 1.5713 | lr 1.89e-04 | grad 2.94 | tok/s 16174
step   1450 | loss 1.2521 | lr 1.89e-04 | grad 3.11 | tok/s 17223
step   1460 | loss 1.5296 | lr 1.89e-04 | grad 3.02 | tok/s 16307
step   1470 | loss 1.2569 | lr 1.89e-04 | grad 2.77 | tok/s 17327
step   1480 | loss 1.6379 | lr 1.89e-04 | grad 5.06 | tok/s 16951
step   1490 | loss 1.5581 | lr 1.89e-04 | grad 3.20 | tok/s 16281
step   1500 | loss 0.9819 | lr 1.89e-04 | grad 2.59 | tok/s 17407
step   1510 | loss 0.9928 | lr 1.89e-04 | grad 2.95 | tok/s 16947
step   1520 | loss 1.3734 | lr 1.89e-04 | grad 3.41 | tok/s 15785
step   1530 | loss 1.3211 | lr 1.89e-04 | grad 2.89 | tok/s 16877
step   1540 | loss 1.2800 | lr 1.89e-04 | grad 2.56 | tok/s 16476
step   1550 | loss 1.4666 | lr 1.89e-04 | grad 3.62 | tok/s 15815
step   1560 | loss 1.3678 | lr 1.89e-04 | grad 2.12 | tok/s 16865
step   1570 | loss 1.3005 | lr 1.89e-04 | grad 8.75 | tok/s 16801
step   1580 | loss 1.6543 | lr 1.89e-04 | grad 2.41 | tok/s 16147
step   1590 | loss 1.4084 | lr 1.89e-04 | grad 5.09 | tok/s 16436
step   1600 | loss 1.4461 | lr 1.89e-04 | grad 2.78 | tok/s 16799
step   1610 | loss 1.3542 | lr 1.89e-04 | grad 2.58 | tok/s 16497
step   1620 | loss 1.4591 | lr 1.89e-04 | grad 3.59 | tok/s 17132
step   1630 | loss 1.1613 | lr 1.89e-04 | grad 3.48 | tok/s 17336
step   1640 | loss 1.2714 | lr 1.89e-04 | grad 3.31 | tok/s 16919
step   1650 | loss 1.5429 | lr 1.89e-04 | grad 3.05 | tok/s 16639
step   1660 | loss 1.4847 | lr 1.89e-04 | grad 2.69 | tok/s 16666
step   1670 | loss 1.3888 | lr 1.89e-04 | grad 2.95 | tok/s 16369
step   1680 | loss 1.4592 | lr 1.89e-04 | grad 2.36 | tok/s 17010
step   1690 | loss 1.3457 | lr 1.89e-04 | grad 2.05 | tok/s 16563
step   1700 | loss 1.5748 | lr 1.89e-04 | grad 2.94 | tok/s 16706
step   1710 | loss 1.3944 | lr 1.89e-04 | grad 2.61 | tok/s 16089
step   1720 | loss 1.4046 | lr 1.89e-04 | grad 6.59 | tok/s 16341
step   1730 | loss 1.3570 | lr 1.89e-04 | grad 2.78 | tok/s 17008
step   1740 | loss 1.4552 | lr 1.89e-04 | grad 3.33 | tok/s 16322
step   1750 | loss 1.2657 | lr 1.89e-04 | grad 2.23 | tok/s 17095
step   1760 | loss 1.2800 | lr 1.89e-04 | grad 2.86 | tok/s 16546
step   1770 | loss 1.3655 | lr 1.89e-04 | grad 4.28 | tok/s 16610
step   1780 | loss 1.1843 | lr 1.89e-04 | grad 2.70 | tok/s 16273
step   1790 | loss 1.4584 | lr 1.89e-04 | grad 2.19 | tok/s 15470
step   1800 | loss 1.3571 | lr 1.89e-04 | grad 3.12 | tok/s 16695
step   1810 | loss 1.4030 | lr 1.89e-04 | grad 3.20 | tok/s 15863
step   1820 | loss 1.3643 | lr 1.89e-04 | grad 2.53 | tok/s 17357
step   1830 | loss 1.4103 | lr 1.89e-04 | grad 2.80 | tok/s 16318
step   1840 | loss 1.3978 | lr 1.89e-04 | grad 2.52 | tok/s 16940
step   1850 | loss 1.8643 | lr 1.89e-04 | grad 4.72 | tok/s 17190
step   1860 | loss 1.4619 | lr 1.89e-04 | grad 5.56 | tok/s 17346
step   1870 | loss 1.4698 | lr 1.89e-04 | grad 3.05 | tok/s 16909
step   1880 | loss 1.4736 | lr 1.89e-04 | grad 2.55 | tok/s 16219
step   1890 | loss 1.5275 | lr 1.89e-04 | grad 12.25 | tok/s 16507
step   1900 | loss 1.4352 | lr 1.89e-04 | grad 2.36 | tok/s 16731
step   1910 | loss 1.1044 | lr 1.89e-04 | grad 2.08 | tok/s 17347
step   1920 | loss 1.1624 | lr 1.89e-04 | grad 2.39 | tok/s 16792
step   1930 | loss 1.2847 | lr 1.89e-04 | grad 5.22 | tok/s 17154
step   1940 | loss 0.9621 | lr 1.89e-04 | grad 3.16 | tok/s 17527
step   1950 | loss 1.1579 | lr 1.89e-04 | grad 2.72 | tok/s 17323
step   1960 | loss 1.4624 | lr 1.89e-04 | grad 12.00 | tok/s 16158
step   1970 | loss 1.6417 | lr 1.89e-04 | grad 4.59 | tok/s 16643
step   1980 | loss 1.9489 | lr 1.89e-04 | grad 8.38 | tok/s 16714
step   1990 | loss 1.8028 | lr 1.89e-04 | grad 3.31 | tok/s 17340
step   2000 | loss 1.3852 | lr 1.89e-04 | grad 3.25 | tok/s 16753
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3852.pt
step   2010 | loss 1.3908 | lr 1.89e-04 | grad 3.33 | tok/s 5507
step   2020 | loss 0.8530 | lr 1.89e-04 | grad 4.50 | tok/s 17834
step   2030 | loss 1.1026 | lr 1.89e-04 | grad 5.84 | tok/s 15921
step   2040 | loss 1.4337 | lr 1.89e-04 | grad 2.38 | tok/s 17027
step   2050 | loss 1.2522 | lr 1.89e-04 | grad 2.59 | tok/s 17543
step   2060 | loss 1.1386 | lr 1.89e-04 | grad 1.87 | tok/s 17584
step   2070 | loss 1.2067 | lr 1.89e-04 | grad 2.09 | tok/s 17478
step   2080 | loss 1.1537 | lr 1.89e-04 | grad 1.93 | tok/s 17519
step   2090 | loss 1.1559 | lr 1.89e-04 | grad 2.05 | tok/s 17493
step   2100 | loss 1.1578 | lr 1.89e-04 | grad 2.08 | tok/s 17488
step   2110 | loss 1.1054 | lr 1.89e-04 | grad 2.08 | tok/s 17501
step   2120 | loss 1.0997 | lr 1.89e-04 | grad 1.97 | tok/s 17470
step   2130 | loss 1.3295 | lr 1.89e-04 | grad 2.16 | tok/s 17108
step   2140 | loss 1.3107 | lr 1.89e-04 | grad 2.89 | tok/s 17372
step   2150 | loss 1.4370 | lr 1.89e-04 | grad 4.72 | tok/s 16933
step   2160 | loss 1.5008 | lr 1.89e-04 | grad 2.23 | tok/s 16879
step   2170 | loss 1.9129 | lr 1.89e-04 | grad 6.00 | tok/s 17483
step   2180 | loss 1.3704 | lr 1.89e-04 | grad 2.64 | tok/s 17231
step   2190 | loss 1.3429 | lr 1.89e-04 | grad 11.38 | tok/s 16874
step   2200 | loss 1.4578 | lr 1.89e-04 | grad 5.50 | tok/s 17230
step   2210 | loss 1.4044 | lr 1.89e-04 | grad 2.62 | tok/s 16353
step   2220 | loss 1.6534 | lr 1.89e-04 | grad 5.28 | tok/s 16421
step   2230 | loss 1.6031 | lr 1.89e-04 | grad 2.08 | tok/s 16695
step   2240 | loss 1.3498 | lr 1.89e-04 | grad 2.91 | tok/s 16175
step   2250 | loss 1.3380 | lr 1.89e-04 | grad 2.98 | tok/s 16985
step   2260 | loss 1.2607 | lr 1.89e-04 | grad 2.53 | tok/s 17371
step   2270 | loss 1.4113 | lr 1.89e-04 | grad 3.81 | tok/s 17039
step   2280 | loss 1.4565 | lr 1.89e-04 | grad 4.28 | tok/s 17454
step   2290 | loss 1.1141 | lr 1.89e-04 | grad 2.23 | tok/s 17412
step   2300 | loss 1.0440 | lr 1.89e-04 | grad 1.84 | tok/s 17394
step   2310 | loss 1.1024 | lr 1.89e-04 | grad 2.64 | tok/s 16742
step   2320 | loss 1.4011 | lr 1.89e-04 | grad 2.45 | tok/s 16481
step   2330 | loss 1.3344 | lr 1.89e-04 | grad 2.50 | tok/s 16599
step   2340 | loss 1.2024 | lr 1.89e-04 | grad 2.80 | tok/s 16985
step   2350 | loss 1.4041 | lr 1.89e-04 | grad 2.27 | tok/s 16656
step   2360 | loss 1.2290 | lr 1.89e-04 | grad 2.50 | tok/s 17263
step   2370 | loss 1.1173 | lr 1.89e-04 | grad 2.38 | tok/s 17190
step   2380 | loss 1.1948 | lr 1.89e-04 | grad 3.41 | tok/s 17403
step   2390 | loss 1.3065 | lr 1.89e-04 | grad 3.25 | tok/s 16482
