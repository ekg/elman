Using device: cuda
Output directory: benchmark_results/cmaes_4d/fla-gdn_480M_converge0.01_20260203_055712/eval_6/levelfla-gdn_100m_20260203_055719
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 438,568,156 parameters
Using schedule-free AdamW (lr=0.00018289424454169832)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 4.4453 | lr 1.83e-04 | grad 21.75 | tok/s 10801
step     20 | loss 3.0233 | lr 1.83e-04 | grad 9.62 | tok/s 24756
step     30 | loss 3.4122 | lr 1.83e-04 | grad 13.12 | tok/s 26054
step     40 | loss 4.3880 | lr 1.83e-04 | grad 33.25 | tok/s 26493
step     50 | loss 4.8453 | lr 1.83e-04 | grad 26.12 | tok/s 26704
step     60 | loss 4.2556 | lr 1.83e-04 | grad 24.75 | tok/s 26603
step     70 | loss 3.4318 | lr 1.83e-04 | grad 15.44 | tok/s 26418
step     80 | loss 3.1889 | lr 1.83e-04 | grad 14.88 | tok/s 26330
step     90 | loss 2.8770 | lr 1.83e-04 | grad 12.62 | tok/s 26204
step    100 | loss 2.6799 | lr 1.83e-04 | grad 7.81 | tok/s 26056
step    110 | loss 2.6966 | lr 1.83e-04 | grad 6.56 | tok/s 25856
step    120 | loss 3.1946 | lr 1.83e-04 | grad 4.44 | tok/s 24438
step    130 | loss 2.4266 | lr 1.83e-04 | grad 6.62 | tok/s 24930
step    140 | loss 2.6374 | lr 1.83e-04 | grad 12.88 | tok/s 24944
step    150 | loss 1.8780 | lr 1.83e-04 | grad 7.59 | tok/s 25423
step    160 | loss 2.5527 | lr 1.83e-04 | grad 3.23 | tok/s 24488
step    170 | loss 2.4520 | lr 1.83e-04 | grad 3.02 | tok/s 24076
step    180 | loss 2.5408 | lr 1.83e-04 | grad 4.53 | tok/s 24537
step    190 | loss 2.1313 | lr 1.83e-04 | grad 3.56 | tok/s 24015
step    200 | loss 1.9032 | lr 1.83e-04 | grad 3.25 | tok/s 25021
step    210 | loss 2.0450 | lr 1.83e-04 | grad 4.88 | tok/s 23719
step    220 | loss 2.4143 | lr 1.83e-04 | grad 12.75 | tok/s 23873
step    230 | loss 2.1346 | lr 1.83e-04 | grad 3.58 | tok/s 23817
step    240 | loss 2.4830 | lr 1.83e-04 | grad 8.94 | tok/s 24031
step    250 | loss 1.8776 | lr 1.83e-04 | grad 2.23 | tok/s 23847
step    260 | loss 2.0247 | lr 1.83e-04 | grad 3.83 | tok/s 24462
step    270 | loss 1.9159 | lr 1.83e-04 | grad 2.62 | tok/s 23868
step    280 | loss 1.8565 | lr 1.83e-04 | grad 2.59 | tok/s 22417
step    290 | loss 1.7362 | lr 1.83e-04 | grad 3.03 | tok/s 23135
step    300 | loss 2.0530 | lr 1.83e-04 | grad 2.83 | tok/s 23279
step    310 | loss 1.7229 | lr 1.83e-04 | grad 2.44 | tok/s 23136
step    320 | loss 1.9516 | lr 1.83e-04 | grad 5.47 | tok/s 23408
step    330 | loss 1.7724 | lr 1.83e-04 | grad 2.56 | tok/s 23633
step    340 | loss 2.1362 | lr 1.83e-04 | grad 3.27 | tok/s 23571
step    350 | loss 1.9003 | lr 1.83e-04 | grad 2.53 | tok/s 24169
step    360 | loss 1.6366 | lr 1.83e-04 | grad 2.80 | tok/s 23174
step    370 | loss 1.5721 | lr 1.83e-04 | grad 2.33 | tok/s 24399
step    380 | loss 1.2970 | lr 1.83e-04 | grad 2.09 | tok/s 24568
step    390 | loss 1.1720 | lr 1.83e-04 | grad 2.05 | tok/s 24552
step    400 | loss 1.8274 | lr 1.83e-04 | grad 2.19 | tok/s 23327
step    410 | loss 1.7872 | lr 1.83e-04 | grad 3.03 | tok/s 23554
step    420 | loss 1.7620 | lr 1.83e-04 | grad 3.55 | tok/s 24526
step    430 | loss 1.6405 | lr 1.83e-04 | grad 2.44 | tok/s 24115
step    440 | loss 1.7402 | lr 1.83e-04 | grad 2.80 | tok/s 23377
step    450 | loss 1.6467 | lr 1.83e-04 | grad 1.96 | tok/s 23637
step    460 | loss 1.6535 | lr 1.83e-04 | grad 2.64 | tok/s 24011
step    470 | loss 1.6133 | lr 1.83e-04 | grad 4.03 | tok/s 23842
step    480 | loss 1.6531 | lr 1.83e-04 | grad 3.61 | tok/s 24385
step    490 | loss 1.7104 | lr 1.83e-04 | grad 3.05 | tok/s 23403
step    500 | loss 1.8484 | lr 1.83e-04 | grad 2.22 | tok/s 23857
step    510 | loss 1.7148 | lr 1.83e-04 | grad 1.88 | tok/s 22803
step    520 | loss 1.5562 | lr 1.83e-04 | grad 5.12 | tok/s 23880
step    530 | loss 1.7420 | lr 1.83e-04 | grad 3.02 | tok/s 23502
step    540 | loss 1.6466 | lr 1.83e-04 | grad 2.14 | tok/s 23019
step    550 | loss 1.3713 | lr 1.83e-04 | grad 3.53 | tok/s 24073
step    560 | loss 1.4720 | lr 1.83e-04 | grad 2.28 | tok/s 24748
step    570 | loss 1.3631 | lr 1.83e-04 | grad 2.23 | tok/s 24770
step    580 | loss 1.3273 | lr 1.83e-04 | grad 1.69 | tok/s 24793
step    590 | loss 1.3587 | lr 1.83e-04 | grad 1.71 | tok/s 24758
step    600 | loss 1.2931 | lr 1.83e-04 | grad 2.11 | tok/s 24776
step    610 | loss 1.3202 | lr 1.83e-04 | grad 1.96 | tok/s 24743
step    620 | loss 1.3055 | lr 1.83e-04 | grad 2.23 | tok/s 24636
step    630 | loss 1.6484 | lr 1.83e-04 | grad 6.31 | tok/s 23366
step    640 | loss 1.7514 | lr 1.83e-04 | grad 2.25 | tok/s 23653
step    650 | loss 1.5640 | lr 1.83e-04 | grad 2.17 | tok/s 23613
step    660 | loss 1.6203 | lr 1.83e-04 | grad 2.41 | tok/s 24532
step    670 | loss 1.6400 | lr 1.83e-04 | grad 6.44 | tok/s 23785
step    680 | loss 1.6563 | lr 1.83e-04 | grad 2.69 | tok/s 23406
step    690 | loss 1.6093 | lr 1.83e-04 | grad 2.80 | tok/s 23206
step    700 | loss 1.4993 | lr 1.83e-04 | grad 2.06 | tok/s 23744
step    710 | loss 1.6467 | lr 1.83e-04 | grad 4.78 | tok/s 23377
step    720 | loss 1.3238 | lr 1.83e-04 | grad 2.03 | tok/s 24276
step    730 | loss 1.4565 | lr 1.83e-04 | grad 1.87 | tok/s 23888
step    740 | loss 1.8729 | lr 1.83e-04 | grad 5.00 | tok/s 24546
step    750 | loss 1.6151 | lr 1.83e-04 | grad 1.96 | tok/s 24815
step    760 | loss 1.5320 | lr 1.83e-04 | grad 4.31 | tok/s 24314
step    770 | loss 1.5855 | lr 1.83e-04 | grad 2.42 | tok/s 23910
step    780 | loss 1.4973 | lr 1.83e-04 | grad 2.27 | tok/s 24055
step    790 | loss 1.7413 | lr 1.83e-04 | grad 7.78 | tok/s 24604
step    800 | loss 1.3650 | lr 1.83e-04 | grad 2.27 | tok/s 24238
step    810 | loss 1.3395 | lr 1.83e-04 | grad 3.95 | tok/s 23407
step    820 | loss 1.4366 | lr 1.83e-04 | grad 2.31 | tok/s 23841
step    830 | loss 1.5173 | lr 1.83e-04 | grad 1.70 | tok/s 23573
step    840 | loss 1.6232 | lr 1.83e-04 | grad 2.02 | tok/s 23449
step    850 | loss 1.5633 | lr 1.83e-04 | grad 1.97 | tok/s 23905
step    860 | loss 1.6125 | lr 1.83e-04 | grad 3.38 | tok/s 24299
step    870 | loss 1.4974 | lr 1.83e-04 | grad 2.50 | tok/s 24496
step    880 | loss 1.5997 | lr 1.83e-04 | grad 2.09 | tok/s 24017
step    890 | loss 1.4946 | lr 1.83e-04 | grad 1.68 | tok/s 23922
step    900 | loss 1.5418 | lr 1.83e-04 | grad 2.23 | tok/s 23828
step    910 | loss 1.5320 | lr 1.83e-04 | grad 8.06 | tok/s 23564
step    920 | loss 1.4723 | lr 1.83e-04 | grad 2.02 | tok/s 23848
step    930 | loss 1.4109 | lr 1.83e-04 | grad 2.38 | tok/s 24162
step    940 | loss 1.3697 | lr 1.83e-04 | grad 2.30 | tok/s 23634
step    950 | loss 1.5139 | lr 1.83e-04 | grad 2.80 | tok/s 23242
step    960 | loss 1.4523 | lr 1.83e-04 | grad 1.72 | tok/s 23844
step    970 | loss 1.4727 | lr 1.83e-04 | grad 1.99 | tok/s 23862
step    980 | loss 2.1069 | lr 1.83e-04 | grad 5.09 | tok/s 24774
step    990 | loss 1.6022 | lr 1.83e-04 | grad 2.19 | tok/s 23809
step   1000 | loss 1.5978 | lr 1.83e-04 | grad 2.42 | tok/s 23848
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5978.pt
step   1010 | loss 1.3617 | lr 1.83e-04 | grad 3.12 | tok/s 11895
step   1020 | loss 1.2750 | lr 1.83e-04 | grad 1.86 | tok/s 25246
step   1030 | loss 1.5626 | lr 1.83e-04 | grad 4.38 | tok/s 23929
step   1040 | loss 2.1131 | lr 1.83e-04 | grad 4.34 | tok/s 24450
step   1050 | loss 1.5141 | lr 1.83e-04 | grad 3.72 | tok/s 24662
step   1060 | loss 1.1893 | lr 1.83e-04 | grad 3.23 | tok/s 24435
step   1070 | loss 1.4356 | lr 1.83e-04 | grad 2.23 | tok/s 24238
step   1080 | loss 1.2743 | lr 1.83e-04 | grad 1.79 | tok/s 25052
step   1090 | loss 1.2362 | lr 1.83e-04 | grad 1.62 | tok/s 25040
step   1100 | loss 1.2228 | lr 1.83e-04 | grad 1.52 | tok/s 25050
step   1110 | loss 1.1626 | lr 1.83e-04 | grad 1.62 | tok/s 25053
step   1120 | loss 1.4594 | lr 1.83e-04 | grad 5.00 | tok/s 24400
step   1130 | loss 1.7011 | lr 1.83e-04 | grad 1.94 | tok/s 24604
step   1140 | loss 1.7822 | lr 1.83e-04 | grad 2.47 | tok/s 24885
step   1150 | loss 1.7282 | lr 1.83e-04 | grad 2.64 | tok/s 24118
step   1160 | loss 1.7589 | lr 1.83e-04 | grad 2.75 | tok/s 23769
