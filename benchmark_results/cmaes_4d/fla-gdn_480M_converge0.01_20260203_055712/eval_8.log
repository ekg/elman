Using device: cuda
Output directory: benchmark_results/cmaes_4d/fla-gdn_480M_converge0.01_20260203_055712/eval_8/levelfla-gdn_100m_20260203_055719
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 651,291,412 parameters
Using schedule-free AdamW (lr=0.00011629574313770706)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 4.9749 | lr 1.16e-04 | grad 71.50 | tok/s 6234
step     20 | loss 3.3351 | lr 1.16e-04 | grad 33.00 | tok/s 17482
step     30 | loss 3.1625 | lr 1.16e-04 | grad 20.75 | tok/s 17705
step     40 | loss 3.0295 | lr 1.16e-04 | grad 17.50 | tok/s 16944
step     50 | loss 3.8757 | lr 1.16e-04 | grad 35.75 | tok/s 17182
step     60 | loss 3.0367 | lr 1.16e-04 | grad 44.00 | tok/s 17710
step     70 | loss 2.8346 | lr 1.16e-04 | grad 15.62 | tok/s 17951
step     80 | loss 5.7328 | lr 1.16e-04 | grad 41.50 | tok/s 17946
step     90 | loss 5.3674 | lr 1.16e-04 | grad 18.25 | tok/s 18218
step    100 | loss 4.3544 | lr 1.16e-04 | grad 21.62 | tok/s 18159
step    110 | loss 4.1445 | lr 1.16e-04 | grad 36.50 | tok/s 18138
step    120 | loss 3.9681 | lr 1.16e-04 | grad 36.25 | tok/s 18084
step    130 | loss 3.9187 | lr 1.16e-04 | grad 30.12 | tok/s 18023
step    140 | loss 3.4195 | lr 1.16e-04 | grad 21.88 | tok/s 17976
step    150 | loss 3.8575 | lr 1.16e-04 | grad 32.25 | tok/s 17925
step    160 | loss 3.2514 | lr 1.16e-04 | grad 24.12 | tok/s 17892
step    170 | loss 3.3228 | lr 1.16e-04 | grad 27.88 | tok/s 17817
step    180 | loss 3.1458 | lr 1.16e-04 | grad 21.25 | tok/s 17754
step    190 | loss 3.4097 | lr 1.16e-04 | grad 20.62 | tok/s 17693
step    200 | loss 2.9848 | lr 1.16e-04 | grad 24.25 | tok/s 17667
step    210 | loss 3.0227 | lr 1.16e-04 | grad 22.75 | tok/s 17612
step    220 | loss 3.0722 | lr 1.16e-04 | grad 11.31 | tok/s 17406
step    230 | loss 3.6718 | lr 1.16e-04 | grad 12.25 | tok/s 17103
step    240 | loss 2.7907 | lr 1.16e-04 | grad 9.06 | tok/s 16207
step    250 | loss 2.6280 | lr 1.16e-04 | grad 6.19 | tok/s 16599
step    260 | loss 2.3986 | lr 1.16e-04 | grad 7.50 | tok/s 17083
step    270 | loss 2.6509 | lr 1.16e-04 | grad 5.72 | tok/s 16846
step    280 | loss 2.8107 | lr 1.16e-04 | grad 7.16 | tok/s 16520
step    290 | loss 2.5071 | lr 1.16e-04 | grad 10.19 | tok/s 17269
step    300 | loss 1.2371 | lr 1.16e-04 | grad 11.06 | tok/s 17295
step    310 | loss 2.9658 | lr 1.16e-04 | grad 6.62 | tok/s 16970
step    320 | loss 2.6180 | lr 1.16e-04 | grad 13.12 | tok/s 16527
step    330 | loss 2.4277 | lr 1.16e-04 | grad 5.47 | tok/s 15959
step    340 | loss 2.7524 | lr 1.16e-04 | grad 4.97 | tok/s 16174
step    350 | loss 2.5546 | lr 1.16e-04 | grad 12.69 | tok/s 16544
step    360 | loss 2.8649 | lr 1.16e-04 | grad 37.00 | tok/s 16859
step    370 | loss 2.4084 | lr 1.16e-04 | grad 7.22 | tok/s 15292
step    380 | loss 2.3062 | lr 1.16e-04 | grad 5.22 | tok/s 16244
step    390 | loss 2.0761 | lr 1.16e-04 | grad 4.19 | tok/s 16894
step    400 | loss 2.0720 | lr 1.16e-04 | grad 9.38 | tok/s 16748
step    410 | loss 1.9735 | lr 1.16e-04 | grad 5.03 | tok/s 16348
step    420 | loss 2.2162 | lr 1.16e-04 | grad 11.06 | tok/s 15598
step    430 | loss 2.5613 | lr 1.16e-04 | grad 5.41 | tok/s 16589
step    440 | loss 2.5556 | lr 1.16e-04 | grad 6.38 | tok/s 15687
step    450 | loss 3.4585 | lr 1.16e-04 | grad 6.72 | tok/s 16206
step    460 | loss 2.2105 | lr 1.16e-04 | grad 7.28 | tok/s 15841
step    470 | loss 2.2656 | lr 1.16e-04 | grad 4.91 | tok/s 16292
step    480 | loss 2.8654 | lr 1.16e-04 | grad 12.75 | tok/s 16279
step    490 | loss 2.2434 | lr 1.16e-04 | grad 5.25 | tok/s 15380
step    500 | loss 2.1179 | lr 1.16e-04 | grad 5.84 | tok/s 16381
step    510 | loss 2.1015 | lr 1.16e-04 | grad 4.81 | tok/s 16582
step    520 | loss 2.0956 | lr 1.16e-04 | grad 4.06 | tok/s 16543
step    530 | loss 2.3308 | lr 1.16e-04 | grad 4.19 | tok/s 15947
step    540 | loss 2.0209 | lr 1.16e-04 | grad 3.50 | tok/s 15916
step    550 | loss 1.8483 | lr 1.16e-04 | grad 4.91 | tok/s 15584
step    560 | loss 2.0560 | lr 1.16e-04 | grad 4.81 | tok/s 15207
step    570 | loss 1.9883 | lr 1.16e-04 | grad 6.12 | tok/s 15605
step    580 | loss 1.8277 | lr 1.16e-04 | grad 3.78 | tok/s 15520
step    590 | loss 2.2878 | lr 1.16e-04 | grad 5.72 | tok/s 15934
step    600 | loss 2.1032 | lr 1.16e-04 | grad 4.34 | tok/s 15388
step    610 | loss 1.9440 | lr 1.16e-04 | grad 4.31 | tok/s 16143
step    620 | loss 1.7641 | lr 1.16e-04 | grad 3.86 | tok/s 15283
step    630 | loss 1.9164 | lr 1.16e-04 | grad 6.31 | tok/s 15405
step    640 | loss 2.1171 | lr 1.16e-04 | grad 5.00 | tok/s 15794
step    650 | loss 1.9403 | lr 1.16e-04 | grad 4.78 | tok/s 15904
step    660 | loss 1.9633 | lr 1.16e-04 | grad 3.83 | tok/s 15984
step    670 | loss 2.2884 | lr 1.16e-04 | grad 108.00 | tok/s 16110
step    680 | loss 1.9762 | lr 1.16e-04 | grad 4.81 | tok/s 15757
step    690 | loss 2.2948 | lr 1.16e-04 | grad 6.38 | tok/s 16287
step    700 | loss 2.0014 | lr 1.16e-04 | grad 6.91 | tok/s 16643
step    710 | loss 1.8504 | lr 1.16e-04 | grad 4.78 | tok/s 15561
step    720 | loss 1.6880 | lr 1.16e-04 | grad 4.56 | tok/s 15346
step    730 | loss 1.7243 | lr 1.16e-04 | grad 5.00 | tok/s 16617
step    740 | loss 1.8023 | lr 1.16e-04 | grad 4.44 | tok/s 16415
step    750 | loss 1.5645 | lr 1.16e-04 | grad 4.62 | tok/s 16673
step    760 | loss 1.4075 | lr 1.16e-04 | grad 4.28 | tok/s 16642
step    770 | loss 1.3444 | lr 1.16e-04 | grad 3.91 | tok/s 16622
step    780 | loss 1.2828 | lr 1.16e-04 | grad 3.42 | tok/s 16624
step    790 | loss 1.3638 | lr 1.16e-04 | grad 5.62 | tok/s 16145
step    800 | loss 2.2050 | lr 1.16e-04 | grad 10.38 | tok/s 16073
step    810 | loss 1.8985 | lr 1.16e-04 | grad 3.94 | tok/s 15986
step    820 | loss 1.9010 | lr 1.16e-04 | grad 6.22 | tok/s 15376
step    830 | loss 1.9383 | lr 1.16e-04 | grad 4.78 | tok/s 16491
step    840 | loss 1.7903 | lr 1.16e-04 | grad 4.25 | tok/s 16643
step    850 | loss 1.8180 | lr 1.16e-04 | grad 4.25 | tok/s 16544
step    860 | loss 1.7751 | lr 1.16e-04 | grad 5.97 | tok/s 16383
step    870 | loss 1.7140 | lr 1.16e-04 | grad 4.91 | tok/s 15802
step    880 | loss 1.9199 | lr 1.16e-04 | grad 5.53 | tok/s 15886
step    890 | loss 1.8676 | lr 1.16e-04 | grad 5.78 | tok/s 16121
step    900 | loss 1.7527 | lr 1.16e-04 | grad 4.84 | tok/s 16156
step    910 | loss 1.6262 | lr 1.16e-04 | grad 5.81 | tok/s 15828
step    920 | loss 1.8001 | lr 1.16e-04 | grad 6.34 | tok/s 16438
step    930 | loss 1.7615 | lr 1.16e-04 | grad 5.75 | tok/s 15675
step    940 | loss 1.6791 | lr 1.16e-04 | grad 4.00 | tok/s 16520
step    950 | loss 1.7491 | lr 1.16e-04 | grad 4.53 | tok/s 16579
step    960 | loss 1.6220 | lr 1.16e-04 | grad 4.47 | tok/s 16648
step    970 | loss 1.8614 | lr 1.16e-04 | grad 5.38 | tok/s 15664
step    980 | loss 1.7897 | lr 1.16e-04 | grad 3.91 | tok/s 16082
step    990 | loss 1.6530 | lr 1.16e-04 | grad 3.98 | tok/s 16325
step   1000 | loss 2.0659 | lr 1.16e-04 | grad 17.38 | tok/s 15656
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0659.pt
step   1010 | loss 1.9410 | lr 1.16e-04 | grad 3.75 | tok/s 5843
step   1020 | loss 1.8229 | lr 1.16e-04 | grad 4.25 | tok/s 15491
step   1030 | loss 1.5682 | lr 1.16e-04 | grad 3.17 | tok/s 16140
step   1040 | loss 1.6870 | lr 1.16e-04 | grad 5.66 | tok/s 16424
step   1050 | loss 1.7295 | lr 1.16e-04 | grad 3.86 | tok/s 15509
step   1060 | loss 1.9171 | lr 1.16e-04 | grad 4.16 | tok/s 16503
step   1070 | loss 1.9868 | lr 1.16e-04 | grad 6.00 | tok/s 16215
step   1080 | loss 1.5172 | lr 1.16e-04 | grad 4.03 | tok/s 15059
step   1090 | loss 1.1814 | lr 1.16e-04 | grad 2.36 | tok/s 16719
step   1100 | loss 1.6897 | lr 1.16e-04 | grad 5.06 | tok/s 15995
step   1110 | loss 1.5831 | lr 1.16e-04 | grad 3.17 | tok/s 16753
step   1120 | loss 1.4991 | lr 1.16e-04 | grad 3.61 | tok/s 16782
step   1130 | loss 1.4233 | lr 1.16e-04 | grad 3.38 | tok/s 16768
step   1140 | loss 1.4150 | lr 1.16e-04 | grad 3.25 | tok/s 16785
step   1150 | loss 1.4299 | lr 1.16e-04 | grad 3.25 | tok/s 16732
step   1160 | loss 1.3334 | lr 1.16e-04 | grad 3.66 | tok/s 16728
step   1170 | loss 1.3666 | lr 1.16e-04 | grad 3.50 | tok/s 16731
step   1180 | loss 1.4594 | lr 1.16e-04 | grad 3.05 | tok/s 16708
step   1190 | loss 1.3638 | lr 1.16e-04 | grad 3.56 | tok/s 16716
step   1200 | loss 1.3544 | lr 1.16e-04 | grad 3.41 | tok/s 16736
step   1210 | loss 1.3686 | lr 1.16e-04 | grad 3.44 | tok/s 16708
step   1220 | loss 1.3837 | lr 1.16e-04 | grad 2.98 | tok/s 16709
step   1230 | loss 1.3647 | lr 1.16e-04 | grad 2.83 | tok/s 16707
step   1240 | loss 1.3615 | lr 1.16e-04 | grad 4.75 | tok/s 16555
step   1250 | loss 1.9747 | lr 1.16e-04 | grad 3.80 | tok/s 15817
step   1260 | loss 1.4229 | lr 1.16e-04 | grad 3.72 | tok/s 15671
step   1270 | loss 1.8659 | lr 1.16e-04 | grad 4.75 | tok/s 15631
step   1280 | loss 1.7437 | lr 1.16e-04 | grad 4.22 | tok/s 16293
step   1290 | loss 1.6026 | lr 1.16e-04 | grad 5.03 | tok/s 15980
step   1300 | loss 1.6594 | lr 1.16e-04 | grad 3.86 | tok/s 15849
step   1310 | loss 1.6015 | lr 1.16e-04 | grad 3.84 | tok/s 16598
step   1320 | loss 1.7209 | lr 1.16e-04 | grad 4.31 | tok/s 16386
step   1330 | loss 1.6615 | lr 1.16e-04 | grad 3.69 | tok/s 16424
step   1340 | loss 1.7253 | lr 1.16e-04 | grad 6.56 | tok/s 15524
step   1350 | loss 1.8150 | lr 1.16e-04 | grad 3.67 | tok/s 15354
step   1360 | loss 1.6639 | lr 1.16e-04 | grad 3.42 | tok/s 16077
step   1370 | loss 1.6337 | lr 1.16e-04 | grad 11.62 | tok/s 15873
step   1380 | loss 1.7643 | lr 1.16e-04 | grad 4.72 | tok/s 15292
step   1390 | loss 1.5928 | lr 1.16e-04 | grad 4.41 | tok/s 16129
step   1400 | loss 1.5290 | lr 1.16e-04 | grad 3.17 | tok/s 15742
step   1410 | loss 1.7070 | lr 1.16e-04 | grad 19.12 | tok/s 15701
step   1420 | loss 1.7798 | lr 1.16e-04 | grad 4.38 | tok/s 15674
step   1430 | loss 1.4823 | lr 1.16e-04 | grad 3.81 | tok/s 15873
step   1440 | loss 1.2617 | lr 1.16e-04 | grad 3.95 | tok/s 16682
step   1450 | loss 1.2489 | lr 1.16e-04 | grad 4.72 | tok/s 16469
step   1460 | loss 1.7930 | lr 1.16e-04 | grad 3.92 | tok/s 14412
step   1470 | loss 1.6746 | lr 1.16e-04 | grad 3.77 | tok/s 16523
step   1480 | loss 2.2514 | lr 1.16e-04 | grad 6.75 | tok/s 16401
step   1490 | loss 1.8744 | lr 1.16e-04 | grad 4.44 | tok/s 16638
step   1500 | loss 1.4999 | lr 1.16e-04 | grad 3.69 | tok/s 16699
step   1510 | loss 1.6657 | lr 1.16e-04 | grad 4.53 | tok/s 16478
step   1520 | loss 1.5302 | lr 1.16e-04 | grad 4.84 | tok/s 16132
step   1530 | loss 1.5188 | lr 1.16e-04 | grad 4.62 | tok/s 16528
step   1540 | loss 1.7550 | lr 1.16e-04 | grad 4.44 | tok/s 15540
step   1550 | loss 1.4087 | lr 1.16e-04 | grad 4.25 | tok/s 16549
step   1560 | loss 1.6731 | lr 1.16e-04 | grad 3.36 | tok/s 15716
step   1570 | loss 1.4449 | lr 1.16e-04 | grad 3.89 | tok/s 16552
step   1580 | loss 2.1009 | lr 1.16e-04 | grad 10.00 | tok/s 16449
step   1590 | loss 1.7753 | lr 1.16e-04 | grad 3.92 | tok/s 15677
step   1600 | loss 1.0396 | lr 1.16e-04 | grad 2.36 | tok/s 16800
step   1610 | loss 1.1624 | lr 1.16e-04 | grad 3.33 | tok/s 15883
step   1620 | loss 1.5977 | lr 1.16e-04 | grad 5.88 | tok/s 15560
step   1630 | loss 1.5376 | lr 1.16e-04 | grad 3.61 | tok/s 16226
step   1640 | loss 1.4483 | lr 1.16e-04 | grad 3.61 | tok/s 15738
step   1650 | loss 1.6356 | lr 1.16e-04 | grad 4.66 | tok/s 14937
step   1660 | loss 1.5065 | lr 1.16e-04 | grad 3.00 | tok/s 16649
step   1670 | loss 1.5526 | lr 1.16e-04 | grad 10.94 | tok/s 16047
step   1680 | loss 1.7936 | lr 1.16e-04 | grad 2.86 | tok/s 15386
step   1690 | loss 1.6351 | lr 1.16e-04 | grad 6.38 | tok/s 16151
step   1700 | loss 1.6476 | lr 1.16e-04 | grad 3.48 | tok/s 15995
step   1710 | loss 1.5518 | lr 1.16e-04 | grad 3.78 | tok/s 15985
step   1720 | loss 1.7248 | lr 1.16e-04 | grad 5.34 | tok/s 16667
step   1730 | loss 1.5577 | lr 1.16e-04 | grad 6.28 | tok/s 16718
step   1740 | loss 1.5692 | lr 1.16e-04 | grad 4.44 | tok/s 16148
step   1750 | loss 1.6630 | lr 1.16e-04 | grad 4.53 | tok/s 16157
step   1760 | loss 1.6414 | lr 1.16e-04 | grad 3.52 | tok/s 16077
step   1770 | loss 1.5306 | lr 1.16e-04 | grad 3.78 | tok/s 15820
step   1780 | loss 1.5575 | lr 1.16e-04 | grad 3.27 | tok/s 16314
step   1790 | loss 1.5209 | lr 1.16e-04 | grad 4.84 | tok/s 16173
step   1800 | loss 1.6619 | lr 1.16e-04 | grad 3.75 | tok/s 15873
step   1810 | loss 1.5942 | lr 1.16e-04 | grad 6.62 | tok/s 15572
step   1820 | loss 1.6030 | lr 1.16e-04 | grad 12.12 | tok/s 16046
step   1830 | loss 1.5221 | lr 1.16e-04 | grad 6.97 | tok/s 16352
step   1840 | loss 1.5533 | lr 1.16e-04 | grad 3.31 | tok/s 15595
step   1850 | loss 1.4801 | lr 1.16e-04 | grad 3.30 | tok/s 16605
step   1860 | loss 1.4248 | lr 1.16e-04 | grad 4.59 | tok/s 15770
step   1870 | loss 1.4842 | lr 1.16e-04 | grad 3.56 | tok/s 16134
step   1880 | loss 1.3598 | lr 1.16e-04 | grad 4.62 | tok/s 15555
step   1890 | loss 1.6721 | lr 1.16e-04 | grad 3.48 | tok/s 15062
step   1900 | loss 1.4579 | lr 1.16e-04 | grad 3.98 | tok/s 16110
step   1910 | loss 1.5414 | lr 1.16e-04 | grad 3.61 | tok/s 15263
step   1920 | loss 1.4801 | lr 1.16e-04 | grad 3.16 | tok/s 16694
step   1930 | loss 1.5204 | lr 1.16e-04 | grad 5.03 | tok/s 15391
step   1940 | loss 1.5156 | lr 1.16e-04 | grad 3.86 | tok/s 16568
step   1950 | loss 2.3424 | lr 1.16e-04 | grad 7.03 | tok/s 16551
step   1960 | loss 1.9460 | lr 1.16e-04 | grad 6.97 | tok/s 16700
step   1970 | loss 1.6879 | lr 1.16e-04 | grad 4.50 | tok/s 16292
step   1980 | loss 1.6554 | lr 1.16e-04 | grad 4.22 | tok/s 15614
step   1990 | loss 1.8564 | lr 1.16e-04 | grad 4.19 | tok/s 15887
step   2000 | loss 1.5640 | lr 1.16e-04 | grad 4.28 | tok/s 16147
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5640.pt
step   2010 | loss 1.2243 | lr 1.16e-04 | grad 3.80 | tok/s 6158
step   2020 | loss 1.3769 | lr 1.16e-04 | grad 4.62 | tok/s 16524
step   2030 | loss 1.1017 | lr 1.16e-04 | grad 8.31 | tok/s 17084
step   2040 | loss 1.5593 | lr 1.16e-04 | grad 5.25 | tok/s 16917
step   2050 | loss 1.3983 | lr 1.16e-04 | grad 3.75 | tok/s 16359
step   2060 | loss 1.7247 | lr 1.16e-04 | grad 3.62 | tok/s 15849
step   2070 | loss 1.8753 | lr 1.16e-04 | grad 7.91 | tok/s 16022
step   2080 | loss 2.2168 | lr 1.16e-04 | grad 5.81 | tok/s 16889
step   2090 | loss 1.7023 | lr 1.16e-04 | grad 3.97 | tok/s 16541
step   2100 | loss 1.5332 | lr 1.16e-04 | grad 4.72 | tok/s 16565
step   2110 | loss 1.6175 | lr 1.16e-04 | grad 5.16 | tok/s 15776
step   2120 | loss 0.9290 | lr 1.16e-04 | grad 3.45 | tok/s 16980
step   2130 | loss 1.3419 | lr 1.16e-04 | grad 6.28 | tok/s 16246
step   2140 | loss 1.5025 | lr 1.16e-04 | grad 3.19 | tok/s 16348
step   2150 | loss 1.3448 | lr 1.16e-04 | grad 4.00 | tok/s 16792
step   2160 | loss 1.2400 | lr 1.16e-04 | grad 3.56 | tok/s 16853
step   2170 | loss 1.3050 | lr 1.16e-04 | grad 3.00 | tok/s 16804
step   2180 | loss 1.2511 | lr 1.16e-04 | grad 3.02 | tok/s 16769
step   2190 | loss 1.2711 | lr 1.16e-04 | grad 3.23 | tok/s 16778
step   2200 | loss 1.2467 | lr 1.16e-04 | grad 2.84 | tok/s 16826
step   2210 | loss 1.2077 | lr 1.16e-04 | grad 3.28 | tok/s 16796
step   2220 | loss 1.1931 | lr 1.16e-04 | grad 3.19 | tok/s 16802
step   2230 | loss 1.4550 | lr 1.16e-04 | grad 4.66 | tok/s 16499
step   2240 | loss 1.4138 | lr 1.16e-04 | grad 3.70 | tok/s 16233
step   2250 | loss 1.7840 | lr 1.16e-04 | grad 9.12 | tok/s 16799
step   2260 | loss 1.6697 | lr 1.16e-04 | grad 3.75 | tok/s 16271
step   2270 | loss 2.1084 | lr 1.16e-04 | grad 4.31 | tok/s 16632
step   2280 | loss 1.5017 | lr 1.16e-04 | grad 4.62 | tok/s 16738
step   2290 | loss 1.6102 | lr 1.16e-04 | grad 13.44 | tok/s 16270
step   2300 | loss 1.7969 | lr 1.16e-04 | grad 5.91 | tok/s 16439
step   2310 | loss 1.6031 | lr 1.16e-04 | grad 4.62 | tok/s 15791
step   2320 | loss 1.9295 | lr 1.16e-04 | grad 6.00 | tok/s 15772
step   2330 | loss 1.6268 | lr 1.16e-04 | grad 5.38 | tok/s 15887
step   2340 | loss 1.6088 | lr 1.16e-04 | grad 4.00 | tok/s 15591
step   2350 | loss 1.4581 | lr 1.16e-04 | grad 5.19 | tok/s 16352
step   2360 | loss 1.3948 | lr 1.16e-04 | grad 3.31 | tok/s 16699
step   2370 | loss 1.6747 | lr 1.16e-04 | grad 6.50 | tok/s 16381
step   2380 | loss 1.7267 | lr 1.16e-04 | grad 5.12 | tok/s 16853
step   2390 | loss 1.2638 | lr 1.16e-04 | grad 4.44 | tok/s 16683
