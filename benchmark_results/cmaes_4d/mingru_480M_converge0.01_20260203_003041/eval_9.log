Using device: cuda
Output directory: benchmark_results/cmaes_4d/mingru_480M_converge0.01_20260203_003041/eval_9/levelmingru_100m_20260203_010105
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level mingru, 1,362,631,680 parameters
Using schedule-free AdamW (lr=0.0003185398859346666)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 30.0 minutes
step     10 | loss 10.9271 | lr 3.19e-04 | grad 26.88 | tok/s 5979
step     20 | loss 3.5934 | lr 3.19e-04 | grad 12.25 | tok/s 6092
step     30 | loss 3.1833 | lr 3.19e-04 | grad 27.62 | tok/s 6090
step     40 | loss 3.1394 | lr 3.19e-04 | grad 7.19 | tok/s 5782
step     50 | loss 2.9774 | lr 3.19e-04 | grad 19.25 | tok/s 5743
step     60 | loss 3.5564 | lr 3.19e-04 | grad 5.56 | tok/s 6107
step     70 | loss 2.8401 | lr 3.19e-04 | grad 7.91 | tok/s 6110
step     80 | loss 5.0630 | lr 3.19e-04 | grad 21.38 | tok/s 6085
step     90 | loss 5.9105 | lr 3.19e-04 | grad 18.12 | tok/s 6256
step    100 | loss 4.4227 | lr 3.19e-04 | grad 8.50 | tok/s 6246
step    110 | loss 4.0903 | lr 3.19e-04 | grad 15.06 | tok/s 6237
step    120 | loss 3.9361 | lr 3.19e-04 | grad 8.50 | tok/s 6227
step    130 | loss 4.0009 | lr 3.19e-04 | grad 5.75 | tok/s 6217
step    140 | loss 3.3426 | lr 3.19e-04 | grad 18.88 | tok/s 6213
step    150 | loss 3.5912 | lr 3.19e-04 | grad 12.69 | tok/s 6209
step    160 | loss 3.3350 | lr 3.19e-04 | grad 6.03 | tok/s 6203
step    170 | loss 3.1589 | lr 3.19e-04 | grad 11.69 | tok/s 6194
step    180 | loss 3.1619 | lr 3.19e-04 | grad 5.69 | tok/s 6201
step    190 | loss 3.0847 | lr 3.19e-04 | grad 7.53 | tok/s 6198
step    200 | loss 3.0854 | lr 3.19e-04 | grad 6.12 | tok/s 6197
step    210 | loss 2.8824 | lr 3.19e-04 | grad 6.72 | tok/s 6193
step    220 | loss 2.7833 | lr 3.19e-04 | grad 4.97 | tok/s 6112
step    230 | loss 3.1315 | lr 3.19e-04 | grad 11.06 | tok/s 6120
step    240 | loss 3.2205 | lr 3.19e-04 | grad 4.91 | tok/s 5798
step    250 | loss 2.7301 | lr 3.19e-04 | grad 4.22 | tok/s 5843
step    260 | loss 2.5450 | lr 3.19e-04 | grad 4.88 | tok/s 6132
step    270 | loss 2.6443 | lr 3.19e-04 | grad 6.72 | tok/s 5917
step    280 | loss 2.6243 | lr 3.19e-04 | grad 8.50 | tok/s 6070
step    290 | loss 2.9415 | lr 3.19e-04 | grad 5.47 | tok/s 6049
step    300 | loss 2.1274 | lr 3.19e-04 | grad 2.89 | tok/s 6226
step    310 | loss 2.6310 | lr 3.19e-04 | grad 5.59 | tok/s 6113
step    320 | loss 2.8266 | lr 3.19e-04 | grad 4.38 | tok/s 6196
step    330 | loss 2.5159 | lr 3.19e-04 | grad 5.00 | tok/s 5610
step    340 | loss 2.7108 | lr 3.19e-04 | grad 3.17 | tok/s 5949
step    350 | loss 2.5456 | lr 3.19e-04 | grad 4.25 | tok/s 5886
step    360 | loss 2.9176 | lr 3.19e-04 | grad 4.62 | tok/s 6202
step    370 | loss 2.5836 | lr 3.19e-04 | grad 4.78 | tok/s 5698
step    380 | loss 2.3945 | lr 3.19e-04 | grad 3.52 | tok/s 5832
step    390 | loss 2.2994 | lr 3.19e-04 | grad 3.33 | tok/s 6137
step    400 | loss 2.2586 | lr 3.19e-04 | grad 4.66 | tok/s 6143
step    410 | loss 2.3498 | lr 3.19e-04 | grad 3.20 | tok/s 6200
step    420 | loss 2.1935 | lr 3.19e-04 | grad 3.20 | tok/s 5675
step    430 | loss 2.6419 | lr 3.19e-04 | grad 6.19 | tok/s 6039
step    440 | loss 2.7185 | lr 3.19e-04 | grad 4.53 | tok/s 5926
step    450 | loss 2.7217 | lr 3.19e-04 | grad 20.75 | tok/s 5937
step    460 | loss 2.4522 | lr 3.19e-04 | grad 2.31 | tok/s 5811
step    470 | loss 2.4752 | lr 3.19e-04 | grad 3.30 | tok/s 5926
step    480 | loss 2.4827 | lr 3.19e-04 | grad 4.12 | tok/s 6036
step    490 | loss 2.8783 | lr 3.19e-04 | grad 3.69 | tok/s 5962
step    500 | loss 2.2172 | lr 3.19e-04 | grad 4.12 | tok/s 5872
step    510 | loss 2.4012 | lr 3.19e-04 | grad 4.00 | tok/s 6120
step    520 | loss 2.3929 | lr 3.19e-04 | grad 3.45 | tok/s 6155
step    530 | loss 2.5138 | lr 3.19e-04 | grad 4.34 | tok/s 6083
step    540 | loss 2.3142 | lr 3.19e-04 | grad 3.05 | tok/s 5929
step    550 | loss 2.1455 | lr 3.19e-04 | grad 3.61 | tok/s 5933
step    560 | loss 2.2325 | lr 3.19e-04 | grad 6.34 | tok/s 5430
step    570 | loss 2.2806 | lr 3.19e-04 | grad 3.42 | tok/s 5934
step    580 | loss 2.2112 | lr 3.19e-04 | grad 3.09 | tok/s 5819
step    590 | loss 2.1373 | lr 3.19e-04 | grad 2.11 | tok/s 5749
step    600 | loss 2.6043 | lr 3.19e-04 | grad 3.25 | tok/s 5886
step    610 | loss 2.2505 | lr 3.19e-04 | grad 4.88 | tok/s 5889
step    620 | loss 2.0929 | lr 3.19e-04 | grad 3.95 | tok/s 5853
step    630 | loss 2.1923 | lr 3.19e-04 | grad 5.19 | tok/s 5735
step    640 | loss 2.4200 | lr 3.19e-04 | grad 4.97 | tok/s 5873
step    650 | loss 2.3169 | lr 3.19e-04 | grad 4.06 | tok/s 5930
step    660 | loss 2.2579 | lr 3.19e-04 | grad 3.97 | tok/s 6022
step    670 | loss 2.1696 | lr 3.19e-04 | grad 2.72 | tok/s 5876
step    680 | loss 2.6870 | lr 3.19e-04 | grad 2.72 | tok/s 6118
step    690 | loss 2.3879 | lr 3.19e-04 | grad 2.50 | tok/s 5812
step    700 | loss 2.5247 | lr 3.19e-04 | grad 3.72 | tok/s 6209
step    710 | loss 2.3383 | lr 3.19e-04 | grad 3.67 | tok/s 6037
step    720 | loss 1.9699 | lr 3.19e-04 | grad 3.48 | tok/s 5490
step    730 | loss 2.3349 | lr 3.19e-04 | grad 4.28 | tok/s 6218
step    740 | loss 2.1268 | lr 3.19e-04 | grad 2.47 | tok/s 6104
step    750 | loss 2.1880 | lr 3.19e-04 | grad 2.27 | tok/s 6208
step    760 | loss 1.9879 | lr 3.19e-04 | grad 2.02 | tok/s 6214
step    770 | loss 2.0452 | lr 3.19e-04 | grad 2.75 | tok/s 6212
step    780 | loss 1.9857 | lr 3.19e-04 | grad 3.38 | tok/s 6212
step    790 | loss 1.9171 | lr 3.19e-04 | grad 2.77 | tok/s 6211
step    800 | loss 2.2204 | lr 3.19e-04 | grad 4.22 | tok/s 5800
step    810 | loss 2.4242 | lr 3.19e-04 | grad 3.11 | tok/s 5965
step    820 | loss 2.1963 | lr 3.19e-04 | grad 3.38 | tok/s 5885
step    830 | loss 2.2337 | lr 3.19e-04 | grad 2.95 | tok/s 5993
step    840 | loss 2.3434 | lr 3.19e-04 | grad 4.12 | tok/s 6201
step    850 | loss 2.3480 | lr 3.19e-04 | grad 8.81 | tok/s 6169
step    860 | loss 2.2211 | lr 3.19e-04 | grad 2.64 | tok/s 6152
step    870 | loss 2.1530 | lr 3.19e-04 | grad 2.44 | tok/s 5928
step    880 | loss 2.2174 | lr 3.19e-04 | grad 3.28 | tok/s 5890
step    890 | loss 2.2619 | lr 3.19e-04 | grad 1.93 | tok/s 6000
step    900 | loss 2.1977 | lr 3.19e-04 | grad 3.02 | tok/s 6023
step    910 | loss 1.9767 | lr 3.19e-04 | grad 2.19 | tok/s 5858
step    920 | loss 2.2647 | lr 3.19e-04 | grad 2.81 | tok/s 6128
step    930 | loss 2.1500 | lr 3.19e-04 | grad 3.55 | tok/s 5902
step    940 | loss 2.2096 | lr 3.19e-04 | grad 3.34 | tok/s 5974
step    950 | loss 2.2925 | lr 3.19e-04 | grad 2.73 | tok/s 6178
step    960 | loss 2.1367 | lr 3.19e-04 | grad 2.91 | tok/s 6212
step    970 | loss 2.1665 | lr 3.19e-04 | grad 2.03 | tok/s 5934
step    980 | loss 2.2959 | lr 3.19e-04 | grad 2.16 | tok/s 5970
step    990 | loss 2.0685 | lr 3.19e-04 | grad 2.39 | tok/s 5957
step   1000 | loss 2.1557 | lr 3.19e-04 | grad 2.52 | tok/s 5884
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1557.pt
step   1010 | loss 2.1522 | lr 3.19e-04 | grad 2.28 | tok/s 1726
step   1020 | loss 2.0479 | lr 3.19e-04 | grad 1.91 | tok/s 5999
step   1030 | loss 2.0625 | lr 3.19e-04 | grad 3.05 | tok/s 6184
step   1040 | loss 2.0175 | lr 3.19e-04 | grad 2.64 | tok/s 5719
step   1050 | loss 2.2731 | lr 3.19e-04 | grad 3.22 | tok/s 6162
step   1060 | loss 2.4109 | lr 3.19e-04 | grad 3.89 | tok/s 6196
step   1070 | loss 2.0743 | lr 3.19e-04 | grad 3.16 | tok/s 5764
step   1080 | loss 1.8815 | lr 3.19e-04 | grad 4.94 | tok/s 5827
step   1090 | loss 1.6772 | lr 3.19e-04 | grad 3.27 | tok/s 5962
step   1100 | loss 2.1381 | lr 3.19e-04 | grad 2.97 | tok/s 6225
step   1110 | loss 2.0308 | lr 3.19e-04 | grad 3.61 | tok/s 6210
step   1120 | loss 2.0082 | lr 3.19e-04 | grad 2.45 | tok/s 6212
step   1130 | loss 1.9416 | lr 3.19e-04 | grad 3.28 | tok/s 6212
step   1140 | loss 1.9613 | lr 3.19e-04 | grad 3.03 | tok/s 6211
step   1150 | loss 1.8665 | lr 3.19e-04 | grad 2.28 | tok/s 6203
step   1160 | loss 1.8977 | lr 3.19e-04 | grad 2.14 | tok/s 6203
step   1170 | loss 2.0163 | lr 3.19e-04 | grad 2.84 | tok/s 6198
step   1180 | loss 1.9491 | lr 3.19e-04 | grad 3.05 | tok/s 6199
step   1190 | loss 1.9023 | lr 3.19e-04 | grad 2.59 | tok/s 6200
step   1200 | loss 1.9205 | lr 3.19e-04 | grad 2.66 | tok/s 6195
step   1210 | loss 1.8937 | lr 3.19e-04 | grad 2.45 | tok/s 6200
step   1220 | loss 1.8741 | lr 3.19e-04 | grad 4.59 | tok/s 6195
step   1230 | loss 1.8611 | lr 3.19e-04 | grad 2.81 | tok/s 6201
step   1240 | loss 2.2977 | lr 3.19e-04 | grad 3.20 | tok/s 5954
step   1250 | loss 2.0511 | lr 3.19e-04 | grad 10.56 | tok/s 5728
step   1260 | loss 1.8831 | lr 3.19e-04 | grad 1.87 | tok/s 5981
step   1270 | loss 2.2204 | lr 3.19e-04 | grad 4.94 | tok/s 5803
step   1280 | loss 2.0171 | lr 3.19e-04 | grad 2.58 | tok/s 6015
step   1290 | loss 2.0881 | lr 3.19e-04 | grad 1.85 | tok/s 6076
step   1300 | loss 1.9996 | lr 3.19e-04 | grad 2.36 | tok/s 5928
step   1310 | loss 2.1102 | lr 3.19e-04 | grad 4.19 | tok/s 6068
step   1320 | loss 2.2789 | lr 3.19e-04 | grad 3.48 | tok/s 6104
step   1330 | loss 1.8691 | lr 3.19e-04 | grad 2.08 | tok/s 5889
step   1340 | loss 2.3567 | lr 3.19e-04 | grad 2.36 | tok/s 5633
step   1350 | loss 2.1062 | lr 3.19e-04 | grad 3.42 | tok/s 5978
step   1360 | loss 1.9650 | lr 3.19e-04 | grad 2.20 | tok/s 5938
step   1370 | loss 2.2147 | lr 3.19e-04 | grad 2.47 | tok/s 5798
step   1380 | loss 2.0302 | lr 3.19e-04 | grad 3.30 | tok/s 5844
step   1390 | loss 1.9699 | lr 3.19e-04 | grad 2.91 | tok/s 5781
step   1400 | loss 2.0163 | lr 3.19e-04 | grad 2.72 | tok/s 5929
step   1410 | loss 2.1807 | lr 3.19e-04 | grad 2.48 | tok/s 5761
step   1420 | loss 2.0771 | lr 3.19e-04 | grad 3.16 | tok/s 6002
step   1430 | loss 1.8895 | lr 3.19e-04 | grad 2.08 | tok/s 6076
step   1440 | loss 1.8244 | lr 3.19e-04 | grad 2.73 | tok/s 6197
step   1450 | loss 2.1508 | lr 3.19e-04 | grad 2.94 | tok/s 5848
step   1460 | loss 2.1142 | lr 3.19e-04 | grad 3.47 | tok/s 6017
step   1470 | loss 2.4535 | lr 3.19e-04 | grad 7.53 | tok/s 6085
step   1480 | loss 2.5918 | lr 3.19e-04 | grad 2.67 | tok/s 6174
step   1490 | loss 2.0698 | lr 3.19e-04 | grad 3.20 | tok/s 6203
step   1500 | loss 2.1072 | lr 3.19e-04 | grad 3.88 | tok/s 6120
step   1510 | loss 2.0043 | lr 3.19e-04 | grad 2.67 | tok/s 6104
step   1520 | loss 1.9899 | lr 3.19e-04 | grad 2.39 | tok/s 6104
step   1530 | loss 2.0522 | lr 3.19e-04 | grad 2.56 | tok/s 5794
step   1540 | loss 1.9863 | lr 3.19e-04 | grad 3.77 | tok/s 6087
step   1550 | loss 1.9927 | lr 3.19e-04 | grad 3.80 | tok/s 5931
step   1560 | loss 1.9843 | lr 3.19e-04 | grad 3.06 | tok/s 6123
step   1570 | loss 2.2617 | lr 3.19e-04 | grad 3.58 | tok/s 6064
step   1580 | loss 2.4928 | lr 3.19e-04 | grad 4.56 | tok/s 5922
step   1590 | loss 1.8562 | lr 3.19e-04 | grad 2.52 | tok/s 6116
step   1600 | loss 1.2861 | lr 3.19e-04 | grad 3.84 | tok/s 6117
step   1610 | loss 1.8912 | lr 3.19e-04 | grad 3.02 | tok/s 5591
step   1620 | loss 2.1679 | lr 3.19e-04 | grad 2.42 | tok/s 6033
step   1630 | loss 1.8977 | lr 3.19e-04 | grad 1.66 | tok/s 6096
step   1640 | loss 2.0174 | lr 3.19e-04 | grad 2.03 | tok/s 5626
step   1650 | loss 2.0474 | lr 3.19e-04 | grad 2.81 | tok/s 5878
step   1660 | loss 1.9556 | lr 3.19e-04 | grad 2.91 | tok/s 6020
step   1670 | loss 2.3198 | lr 3.19e-04 | grad 3.53 | tok/s 5823
step   1680 | loss 1.9577 | lr 3.19e-04 | grad 2.62 | tok/s 5887
step   1690 | loss 2.1207 | lr 3.19e-04 | grad 2.14 | tok/s 5966
step   1700 | loss 2.0680 | lr 3.19e-04 | grad 3.86 | tok/s 5957
step   1710 | loss 2.1171 | lr 3.19e-04 | grad 2.86 | tok/s 6064
step   1720 | loss 2.2189 | lr 3.19e-04 | grad 2.88 | tok/s 6211
step   1730 | loss 2.1321 | lr 3.19e-04 | grad 2.53 | tok/s 6153
step   1740 | loss 2.1114 | lr 3.19e-04 | grad 3.14 | tok/s 5925
step   1750 | loss 1.9793 | lr 3.19e-04 | grad 2.58 | tok/s 5922
step   1760 | loss 1.9795 | lr 3.19e-04 | grad 3.09 | tok/s 6050
step   1770 | loss 1.9682 | lr 3.19e-04 | grad 2.73 | tok/s 5918
step   1780 | loss 1.9604 | lr 3.19e-04 | grad 3.41 | tok/s 5970
step   1790 | loss 2.0278 | lr 3.19e-04 | grad 3.59 | tok/s 5997
step   1800 | loss 2.0191 | lr 3.19e-04 | grad 2.19 | tok/s 5835
step   1810 | loss 2.0417 | lr 3.19e-04 | grad 3.27 | tok/s 5900
step   1820 | loss 1.9857 | lr 3.19e-04 | grad 2.45 | tok/s 6061
step   1830 | loss 2.0248 | lr 3.19e-04 | grad 3.75 | tok/s 5865
step   1840 | loss 2.0124 | lr 3.19e-04 | grad 2.61 | tok/s 6050
step   1850 | loss 1.8749 | lr 3.19e-04 | grad 3.02 | tok/s 6074
step   1860 | loss 1.9262 | lr 3.19e-04 | grad 3.38 | tok/s 5787
step   1870 | loss 1.8383 | lr 3.19e-04 | grad 3.59 | tok/s 5897
step   1880 | loss 2.0070 | lr 3.19e-04 | grad 3.17 | tok/s 5545
step   1890 | loss 1.8925 | lr 3.19e-04 | grad 3.05 | tok/s 6002
step   1900 | loss 1.8741 | lr 3.19e-04 | grad 3.41 | tok/s 5649
step   1910 | loss 1.9903 | lr 3.19e-04 | grad 3.27 | tok/s 6162
step   1920 | loss 1.8771 | lr 3.19e-04 | grad 3.16 | tok/s 5880
step   1930 | loss 1.9339 | lr 3.19e-04 | grad 2.52 | tok/s 6018
step   1940 | loss 2.5407 | lr 3.19e-04 | grad 2.48 | tok/s 6145
step   1950 | loss 2.4017 | lr 3.19e-04 | grad 2.64 | tok/s 6213
step   1960 | loss 2.2387 | lr 3.19e-04 | grad 2.80 | tok/s 6061
step   1970 | loss 2.1122 | lr 3.19e-04 | grad 2.81 | tok/s 5801
step   1980 | loss 1.9829 | lr 3.19e-04 | grad 5.59 | tok/s 5967
step   1990 | loss 2.1333 | lr 3.19e-04 | grad 3.34 | tok/s 6079
step   2000 | loss 1.6295 | lr 3.19e-04 | grad 5.28 | tok/s 6035
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6295.pt
step   2010 | loss 1.4244 | lr 3.19e-04 | grad 1.08 | tok/s 1877
step   2020 | loss 2.0653 | lr 3.19e-04 | grad 2.70 | tok/s 6309
step   2030 | loss 2.0017 | lr 3.19e-04 | grad 2.62 | tok/s 6129
step   2040 | loss 2.0925 | lr 3.19e-04 | grad 4.38 | tok/s 5849
step   2050 | loss 2.1558 | lr 3.19e-04 | grad 6.69 | tok/s 5953
step   2060 | loss 2.6002 | lr 3.19e-04 | grad 3.62 | tok/s 6262
step   2070 | loss 2.1850 | lr 3.19e-04 | grad 1.74 | tok/s 6130
step   2080 | loss 2.0649 | lr 3.19e-04 | grad 3.12 | tok/s 6140
step   2090 | loss 2.1031 | lr 3.19e-04 | grad 2.91 | tok/s 5821
step   2100 | loss 1.3073 | lr 3.19e-04 | grad 4.09 | tok/s 6298
step   2110 | loss 1.7197 | lr 3.19e-04 | grad 4.22 | tok/s 6043
step   2120 | loss 1.9773 | lr 3.19e-04 | grad 3.62 | tok/s 6033
step   2130 | loss 1.8318 | lr 3.19e-04 | grad 2.72 | tok/s 6223
step   2140 | loss 1.7750 | lr 3.19e-04 | grad 2.41 | tok/s 6221
step   2150 | loss 1.8112 | lr 3.19e-04 | grad 4.16 | tok/s 6216
step   2160 | loss 1.7604 | lr 3.19e-04 | grad 2.81 | tok/s 6213
step   2170 | loss 1.7348 | lr 3.19e-04 | grad 2.08 | tok/s 6212
step   2180 | loss 1.7554 | lr 3.19e-04 | grad 1.89 | tok/s 6212
step   2190 | loss 1.6979 | lr 3.19e-04 | grad 2.72 | tok/s 6213
step   2200 | loss 1.6961 | lr 3.19e-04 | grad 2.38 | tok/s 6211
step   2210 | loss 1.8645 | lr 3.19e-04 | grad 2.48 | tok/s 6093
step   2220 | loss 1.9183 | lr 3.19e-04 | grad 3.19 | tok/s 6181
step   2230 | loss 2.2554 | lr 3.19e-04 | grad 3.78 | tok/s 6017
step   2240 | loss 2.2030 | lr 3.19e-04 | grad 2.67 | tok/s 6000
step   2250 | loss 2.4344 | lr 3.19e-04 | grad 4.19 | tok/s 6211
step   2260 | loss 1.9954 | lr 3.19e-04 | grad 2.83 | tok/s 6140
step   2270 | loss 2.0329 | lr 3.19e-04 | grad 7.78 | tok/s 5990
step   2280 | loss 2.4382 | lr 3.19e-04 | grad 2.70 | tok/s 6124
step   2290 | loss 2.0516 | lr 3.19e-04 | grad 3.67 | tok/s 5817
step   2300 | loss 2.2034 | lr 3.19e-04 | grad 2.44 | tok/s 5839
step   2310 | loss 2.0932 | lr 3.19e-04 | grad 3.89 | tok/s 5944
step   2320 | loss 1.9743 | lr 3.19e-04 | grad 2.22 | tok/s 5757
step   2330 | loss 1.9175 | lr 3.19e-04 | grad 1.95 | tok/s 6043
step   2340 | loss 1.9565 | lr 3.19e-04 | grad 2.22 | tok/s 6200
step   2350 | loss 2.0809 | lr 3.19e-04 | grad 2.09 | tok/s 6076
step   2360 | loss 2.1977 | lr 3.19e-04 | grad 4.31 | tok/s 6214
step   2370 | loss 1.8286 | lr 3.19e-04 | grad 2.89 | tok/s 6206
step   2380 | loss 1.7317 | lr 3.19e-04 | grad 2.61 | tok/s 6212
step   2390 | loss 1.6402 | lr 3.19e-04 | grad 2.89 | tok/s 5964
step   2400 | loss 1.9549 | lr 3.19e-04 | grad 3.66 | tok/s 5876
step   2410 | loss 1.9412 | lr 3.19e-04 | grad 3.27 | tok/s 5927
step   2420 | loss 1.8360 | lr 3.19e-04 | grad 3.12 | tok/s 6061
step   2430 | loss 1.9334 | lr 3.19e-04 | grad 3.17 | tok/s 5938
step   2440 | loss 1.8781 | lr 3.19e-04 | grad 2.38 | tok/s 6173
step   2450 | loss 1.7645 | lr 3.19e-04 | grad 3.09 | tok/s 6136
step   2460 | loss 1.8395 | lr 3.19e-04 | grad 4.94 | tok/s 6211
step   2470 | loss 1.8529 | lr 3.19e-04 | grad 3.33 | tok/s 5872
step   2480 | loss 2.1057 | lr 3.19e-04 | grad 2.56 | tok/s 6084
step   2490 | loss 2.0084 | lr 3.19e-04 | grad 3.19 | tok/s 6212
step   2500 | loss 2.1426 | lr 3.19e-04 | grad 1.91 | tok/s 6173
step   2510 | loss 1.8202 | lr 3.19e-04 | grad 2.80 | tok/s 6001
step   2520 | loss 1.9447 | lr 3.19e-04 | grad 3.86 | tok/s 5946
step   2530 | loss 1.7657 | lr 3.19e-04 | grad 2.70 | tok/s 6178
step   2540 | loss 1.9949 | lr 3.19e-04 | grad 3.84 | tok/s 5789
step   2550 | loss 1.9365 | lr 3.19e-04 | grad 2.38 | tok/s 6068
step   2560 | loss 1.8442 | lr 3.19e-04 | grad 2.89 | tok/s 5671
step   2570 | loss 2.0071 | lr 3.19e-04 | grad 4.06 | tok/s 5985
step   2580 | loss 2.0728 | lr 3.19e-04 | grad 5.19 | tok/s 5714
step   2590 | loss 2.1187 | lr 3.19e-04 | grad 2.38 | tok/s 6172
step   2600 | loss 2.0331 | lr 3.19e-04 | grad 3.97 | tok/s 6009
step   2610 | loss 1.9304 | lr 3.19e-04 | grad 3.91 | tok/s 6153
step   2620 | loss 2.0529 | lr 3.19e-04 | grad 3.86 | tok/s 5992
step   2630 | loss 2.2211 | lr 3.19e-04 | grad 2.89 | tok/s 6212
step   2640 | loss 1.7747 | lr 3.19e-04 | grad 2.80 | tok/s 5907

Training complete! Final step: 2646
