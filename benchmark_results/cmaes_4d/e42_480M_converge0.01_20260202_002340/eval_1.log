Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002340/eval_1/level42_100m_20260202_002346
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 468,887,936 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 7.5033 | lr 3.00e-04 | grad 14.88 | tok/s 3914
step     20 | loss 4.5380 | lr 3.00e-04 | grad 15.62 | tok/s 6214
step     30 | loss 4.7623 | lr 3.00e-04 | grad 8.31 | tok/s 5070
step     40 | loss 3.8525 | lr 3.00e-04 | grad 9.06 | tok/s 6128
step     50 | loss 3.6098 | lr 3.00e-04 | grad 9.19 | tok/s 6162
step     60 | loss 3.5559 | lr 3.00e-04 | grad 9.62 | tok/s 6031
step     70 | loss 2.9814 | lr 3.00e-04 | grad 6.53 | tok/s 5798
step     80 | loss 3.2903 | lr 3.00e-04 | grad 6.41 | tok/s 6219
step     90 | loss 2.9173 | lr 3.00e-04 | grad 5.59 | tok/s 5857
step    100 | loss 2.5590 | lr 3.00e-04 | grad 7.44 | tok/s 5182
step    110 | loss 2.7088 | lr 3.00e-04 | grad 4.59 | tok/s 5724
step    120 | loss 2.8732 | lr 3.00e-04 | grad 10.25 | tok/s 5856
step    130 | loss 2.5865 | lr 3.00e-04 | grad 3.22 | tok/s 5149
step    140 | loss 2.3188 | lr 3.00e-04 | grad 5.22 | tok/s 4251
step    150 | loss 2.3899 | lr 3.00e-04 | grad 3.17 | tok/s 4283
step    160 | loss 2.3959 | lr 3.00e-04 | grad 5.81 | tok/s 4342
step    170 | loss 2.4557 | lr 3.00e-04 | grad 6.56 | tok/s 5105
step    180 | loss 2.5083 | lr 3.00e-04 | grad 3.44 | tok/s 5663
step    190 | loss 2.2521 | lr 3.00e-04 | grad 2.55 | tok/s 4759
step    200 | loss 2.2901 | lr 3.00e-04 | grad 4.34 | tok/s 4621
step    210 | loss 2.4032 | lr 3.00e-04 | grad 2.78 | tok/s 4672
step    220 | loss 2.3228 | lr 3.00e-04 | grad 2.12 | tok/s 4603
step    230 | loss 2.2635 | lr 3.00e-04 | grad 2.50 | tok/s 5829
step    240 | loss 2.3720 | lr 3.00e-04 | grad 3.56 | tok/s 5963
step    250 | loss 2.3588 | lr 3.00e-04 | grad 3.23 | tok/s 5829
step    260 | loss 2.2475 | lr 3.00e-04 | grad 3.48 | tok/s 5895
step    270 | loss 2.2834 | lr 3.00e-04 | grad 1.62 | tok/s 5790
step    280 | loss 2.0330 | lr 3.00e-04 | grad 3.34 | tok/s 5953
step    290 | loss 1.9800 | lr 3.00e-04 | grad 2.34 | tok/s 6078
step    300 | loss 2.0054 | lr 3.00e-04 | grad 2.56 | tok/s 6032
step    310 | loss 1.9482 | lr 3.00e-04 | grad 2.05 | tok/s 5125
step    320 | loss 2.2638 | lr 3.00e-04 | grad 2.02 | tok/s 5760
step    330 | loss 2.1654 | lr 3.00e-04 | grad 3.50 | tok/s 4136
step    340 | loss 2.2579 | lr 3.00e-04 | grad 2.86 | tok/s 5078
step    350 | loss 2.1309 | lr 3.00e-04 | grad 3.23 | tok/s 4929
step    360 | loss 2.1349 | lr 3.00e-04 | grad 2.28 | tok/s 5180
step    370 | loss 2.5039 | lr 3.00e-04 | grad 2.80 | tok/s 5128
step    380 | loss 2.1399 | lr 3.00e-04 | grad 1.93 | tok/s 5192
step    390 | loss 2.0282 | lr 3.00e-04 | grad 1.82 | tok/s 5027
step    400 | loss 2.1977 | lr 3.00e-04 | grad 1.80 | tok/s 5484
step    410 | loss 2.0271 | lr 3.00e-04 | grad 1.82 | tok/s 5639
step    420 | loss 2.2599 | lr 3.00e-04 | grad 2.33 | tok/s 5768
step    430 | loss 2.2615 | lr 3.00e-04 | grad 3.41 | tok/s 5940
step    440 | loss 2.1242 | lr 3.00e-04 | grad 2.64 | tok/s 5744
step    450 | loss 1.9355 | lr 3.00e-04 | grad 2.12 | tok/s 5229
step    460 | loss 2.1125 | lr 3.00e-04 | grad 2.30 | tok/s 5112
step    470 | loss 1.9690 | lr 3.00e-04 | grad 2.44 | tok/s 4683
step    480 | loss 1.9379 | lr 3.00e-04 | grad 1.89 | tok/s 5592
step    490 | loss 2.5527 | lr 3.00e-04 | grad 2.47 | tok/s 4137
step    500 | loss 1.8500 | lr 3.00e-04 | grad 1.86 | tok/s 5772
step    510 | loss 1.9824 | lr 3.00e-04 | grad 3.03 | tok/s 5947
step    520 | loss 2.4218 | lr 3.00e-04 | grad 2.20 | tok/s 5887
step    530 | loss 1.8866 | lr 3.00e-04 | grad 2.34 | tok/s 5831
step    540 | loss 1.7980 | lr 3.00e-04 | grad 2.25 | tok/s 6018
step    550 | loss 1.7129 | lr 3.00e-04 | grad 2.61 | tok/s 6028
step    560 | loss 2.1650 | lr 3.00e-04 | grad 2.09 | tok/s 5997
step    570 | loss 2.5895 | lr 3.00e-04 | grad 7.09 | tok/s 6021
step    580 | loss 2.2298 | lr 3.00e-04 | grad 7.75 | tok/s 5730
step    590 | loss 2.1001 | lr 3.00e-04 | grad 2.16 | tok/s 5964
step    600 | loss 1.8634 | lr 3.00e-04 | grad 2.62 | tok/s 5814
step    610 | loss 1.9518 | lr 3.00e-04 | grad 1.92 | tok/s 5911
step    620 | loss 1.9040 | lr 3.00e-04 | grad 4.19 | tok/s 5839
step    630 | loss 2.1055 | lr 3.00e-04 | grad 3.23 | tok/s 6009
step    640 | loss 1.9375 | lr 3.00e-04 | grad 2.38 | tok/s 5734
step    650 | loss 2.3263 | lr 3.00e-04 | grad 2.12 | tok/s 5830
step    660 | loss 1.9782 | lr 3.00e-04 | grad 2.25 | tok/s 5852
step    670 | loss 2.0447 | lr 3.00e-04 | grad 2.89 | tok/s 5749
step    680 | loss 2.1586 | lr 3.00e-04 | grad 6.53 | tok/s 5719
step    690 | loss 1.9641 | lr 3.00e-04 | grad 1.87 | tok/s 5736
step    700 | loss 2.0681 | lr 3.00e-04 | grad 2.42 | tok/s 5687
step    710 | loss 2.1443 | lr 3.00e-04 | grad 1.89 | tok/s 5859
step    720 | loss 1.9909 | lr 3.00e-04 | grad 2.00 | tok/s 5891
step    730 | loss 2.0203 | lr 3.00e-04 | grad 2.73 | tok/s 5788
step    740 | loss 1.9051 | lr 3.00e-04 | grad 1.76 | tok/s 5724
step    750 | loss 2.0804 | lr 3.00e-04 | grad 1.74 | tok/s 5831
step    760 | loss 1.9030 | lr 3.00e-04 | grad 2.27 | tok/s 5782
step    770 | loss 1.9999 | lr 3.00e-04 | grad 2.31 | tok/s 5930
step    780 | loss 1.8943 | lr 3.00e-04 | grad 1.78 | tok/s 5958
step    790 | loss 1.8077 | lr 3.00e-04 | grad 2.64 | tok/s 5735
step    800 | loss 1.9984 | lr 3.00e-04 | grad 3.48 | tok/s 5940
step    810 | loss 2.3994 | lr 3.00e-04 | grad 4.41 | tok/s 6050
step    820 | loss 2.0808 | lr 3.00e-04 | grad 3.91 | tok/s 6009
step    830 | loss 2.0779 | lr 3.00e-04 | grad 4.81 | tok/s 8243
step    840 | loss 1.9203 | lr 3.00e-04 | grad 3.56 | tok/s 5852
step    850 | loss 1.7823 | lr 3.00e-04 | grad 1.96 | tok/s 5852
step    860 | loss 1.8718 | lr 3.00e-04 | grad 2.62 | tok/s 5822
step    870 | loss 1.8844 | lr 3.00e-04 | grad 2.14 | tok/s 5920
step    880 | loss 1.8714 | lr 3.00e-04 | grad 4.53 | tok/s 5717
step    890 | loss 2.3215 | lr 3.00e-04 | grad 3.17 | tok/s 5659
step    900 | loss 1.8009 | lr 3.00e-04 | grad 2.11 | tok/s 5761
step    910 | loss 1.8717 | lr 3.00e-04 | grad 2.97 | tok/s 5866
step    920 | loss 1.8911 | lr 3.00e-04 | grad 1.79 | tok/s 5779
step    930 | loss 1.9214 | lr 3.00e-04 | grad 1.84 | tok/s 5687
step    940 | loss 1.9098 | lr 3.00e-04 | grad 2.31 | tok/s 5899
step    950 | loss 1.6358 | lr 3.00e-04 | grad 2.69 | tok/s 6012
step    960 | loss 1.5579 | lr 3.00e-04 | grad 2.59 | tok/s 6052
step    970 | loss 1.9008 | lr 3.00e-04 | grad 3.41 | tok/s 5797
step    980 | loss 1.9900 | lr 3.00e-04 | grad 2.80 | tok/s 5765
step    990 | loss 2.0642 | lr 3.00e-04 | grad 3.44 | tok/s 5788
step   1000 | loss 1.8178 | lr 3.00e-04 | grad 2.09 | tok/s 6048
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8178.pt
step   1010 | loss 1.7920 | lr 3.00e-04 | grad 3.17 | tok/s 4596
step   1020 | loss 2.0395 | lr 3.00e-04 | grad 2.05 | tok/s 5764
step   1030 | loss 2.0071 | lr 3.00e-04 | grad 2.42 | tok/s 5864
step   1040 | loss 1.6000 | lr 3.00e-04 | grad 2.16 | tok/s 5834
step   1050 | loss 2.2520 | lr 3.00e-04 | grad 2.48 | tok/s 5992
step   1060 | loss 2.4057 | lr 3.00e-04 | grad 2.83 | tok/s 5788
step   1070 | loss 1.9638 | lr 3.00e-04 | grad 2.69 | tok/s 5855
step   1080 | loss 2.0833 | lr 3.00e-04 | grad 2.19 | tok/s 5918
step   1090 | loss 1.9016 | lr 3.00e-04 | grad 3.89 | tok/s 6062
step   1100 | loss 1.8758 | lr 3.00e-04 | grad 1.83 | tok/s 5829
step   1110 | loss 1.8036 | lr 3.00e-04 | grad 1.88 | tok/s 5757
step   1120 | loss 1.8303 | lr 3.00e-04 | grad 1.56 | tok/s 5838
step   1130 | loss 1.8907 | lr 3.00e-04 | grad 3.58 | tok/s 5782
step   1140 | loss 1.8155 | lr 3.00e-04 | grad 7.06 | tok/s 5787
step   1150 | loss 2.0554 | lr 3.00e-04 | grad 1.87 | tok/s 5727
step   1160 | loss 1.7212 | lr 3.00e-04 | grad 2.52 | tok/s 6001
step   1170 | loss 1.6958 | lr 3.00e-04 | grad 2.14 | tok/s 6048
step   1180 | loss 1.6196 | lr 3.00e-04 | grad 1.95 | tok/s 6072
step   1190 | loss 1.5730 | lr 3.00e-04 | grad 2.81 | tok/s 6083
step   1200 | loss 1.5685 | lr 3.00e-04 | grad 1.91 | tok/s 6079
step   1210 | loss 1.6302 | lr 3.00e-04 | grad 1.43 | tok/s 5960
step   1220 | loss 1.7750 | lr 3.00e-04 | grad 1.76 | tok/s 5758
step   1230 | loss 1.8040 | lr 3.00e-04 | grad 2.50 | tok/s 5946
step   1240 | loss 1.9073 | lr 3.00e-04 | grad 2.47 | tok/s 5827
step   1250 | loss 1.9483 | lr 3.00e-04 | grad 2.08 | tok/s 5716
step   1260 | loss 1.7900 | lr 3.00e-04 | grad 1.89 | tok/s 11984
step   1270 | loss 1.9239 | lr 3.00e-04 | grad 2.27 | tok/s 13489
step   1280 | loss 1.7685 | lr 3.00e-04 | grad 2.03 | tok/s 13159

Training complete! Final step: 1280
