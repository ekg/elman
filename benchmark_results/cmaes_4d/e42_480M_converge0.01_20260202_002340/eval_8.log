Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002340/eval_8/level42_100m_20260202_002347
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 461,718,400 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 8.0139 | lr 3.00e-04 | grad 20.25 | tok/s 4677
step     20 | loss 4.9738 | lr 3.00e-04 | grad 16.12 | tok/s 7008
step     30 | loss 4.8071 | lr 3.00e-04 | grad 7.94 | tok/s 7212
step     40 | loss 3.9041 | lr 3.00e-04 | grad 10.50 | tok/s 7153
step     50 | loss 3.7334 | lr 3.00e-04 | grad 9.62 | tok/s 7131
step     60 | loss 3.6174 | lr 3.00e-04 | grad 9.81 | tok/s 6924
step     70 | loss 3.0180 | lr 3.00e-04 | grad 6.25 | tok/s 6719
step     80 | loss 3.2780 | lr 3.00e-04 | grad 6.88 | tok/s 6973
step     90 | loss 2.9118 | lr 3.00e-04 | grad 6.66 | tok/s 6710
step    100 | loss 2.5755 | lr 3.00e-04 | grad 7.72 | tok/s 6762
step    110 | loss 2.7386 | lr 3.00e-04 | grad 3.67 | tok/s 6740
step    120 | loss 2.9077 | lr 3.00e-04 | grad 10.69 | tok/s 6820
step    130 | loss 2.5913 | lr 3.00e-04 | grad 3.42 | tok/s 6934
step    140 | loss 2.4893 | lr 3.00e-04 | grad 5.22 | tok/s 6747
step    150 | loss 2.5339 | lr 3.00e-04 | grad 4.19 | tok/s 6686
step    160 | loss 2.3682 | lr 3.00e-04 | grad 1.57 | tok/s 6570
step    170 | loss 2.5623 | lr 3.00e-04 | grad 5.34 | tok/s 6830
step    180 | loss 2.4306 | lr 3.00e-04 | grad 3.53 | tok/s 6640
step    190 | loss 2.2610 | lr 3.00e-04 | grad 3.69 | tok/s 7014
step    200 | loss 2.2947 | lr 3.00e-04 | grad 3.28 | tok/s 6898
step    210 | loss 2.3839 | lr 3.00e-04 | grad 2.69 | tok/s 6761
step    220 | loss 2.3479 | lr 3.00e-04 | grad 4.56 | tok/s 6737
step    230 | loss 2.2684 | lr 3.00e-04 | grad 2.20 | tok/s 6686
step    240 | loss 2.3391 | lr 3.00e-04 | grad 3.00 | tok/s 6851
step    250 | loss 2.3759 | lr 3.00e-04 | grad 8.56 | tok/s 6718
step    260 | loss 2.2439 | lr 3.00e-04 | grad 3.73 | tok/s 6695
step    270 | loss 2.3227 | lr 3.00e-04 | grad 3.28 | tok/s 6705
step    280 | loss 2.0241 | lr 3.00e-04 | grad 2.86 | tok/s 6756
step    290 | loss 1.9988 | lr 3.00e-04 | grad 2.39 | tok/s 7007
step    300 | loss 1.9976 | lr 3.00e-04 | grad 2.56 | tok/s 7007
step    310 | loss 1.9506 | lr 3.00e-04 | grad 3.23 | tok/s 7011
step    320 | loss 2.1913 | lr 3.00e-04 | grad 5.22 | tok/s 6612
step    330 | loss 2.1256 | lr 3.00e-04 | grad 2.48 | tok/s 6766
step    340 | loss 2.2871 | lr 3.00e-04 | grad 3.02 | tok/s 6657
step    350 | loss 2.1213 | lr 3.00e-04 | grad 3.00 | tok/s 6591
step    360 | loss 2.1929 | lr 3.00e-04 | grad 2.42 | tok/s 6780
step    370 | loss 2.2177 | lr 3.00e-04 | grad 5.34 | tok/s 6783
step    380 | loss 2.3294 | lr 3.00e-04 | grad 2.55 | tok/s 6927
step    390 | loss 2.0477 | lr 3.00e-04 | grad 2.62 | tok/s 6703
step    400 | loss 2.3445 | lr 3.00e-04 | grad 1.95 | tok/s 6930
step    410 | loss 1.9229 | lr 3.00e-04 | grad 2.64 | tok/s 6714
step    420 | loss 2.1888 | lr 3.00e-04 | grad 2.80 | tok/s 6608
step    430 | loss 2.1908 | lr 3.00e-04 | grad 4.88 | tok/s 6797
step    440 | loss 2.2378 | lr 3.00e-04 | grad 1.80 | tok/s 6797
step    450 | loss 1.9548 | lr 3.00e-04 | grad 2.03 | tok/s 6687
step    460 | loss 2.0575 | lr 3.00e-04 | grad 2.94 | tok/s 6625
step    470 | loss 1.9817 | lr 3.00e-04 | grad 1.78 | tok/s 6783
step    480 | loss 1.9669 | lr 3.00e-04 | grad 2.33 | tok/s 6643
step    490 | loss 2.4147 | lr 3.00e-04 | grad 3.39 | tok/s 6897
step    500 | loss 2.2349 | lr 3.00e-04 | grad 1.78 | tok/s 6669
step    510 | loss 1.7856 | lr 3.00e-04 | grad 3.41 | tok/s 6965
step    520 | loss 2.2347 | lr 3.00e-04 | grad 7.66 | tok/s 6695
step    530 | loss 2.2415 | lr 3.00e-04 | grad 2.66 | tok/s 6851
step    540 | loss 1.7424 | lr 3.00e-04 | grad 2.75 | tok/s 6938
step    550 | loss 1.7460 | lr 3.00e-04 | grad 2.62 | tok/s 7010
step    560 | loss 1.7521 | lr 3.00e-04 | grad 3.05 | tok/s 6993
step    570 | loss 2.3396 | lr 3.00e-04 | grad 3.23 | tok/s 6804
step    580 | loss 2.3041 | lr 3.00e-04 | grad 3.55 | tok/s 6667
step    590 | loss 2.0742 | lr 3.00e-04 | grad 2.64 | tok/s 6689
step    600 | loss 2.1223 | lr 3.00e-04 | grad 2.67 | tok/s 6959
step    610 | loss 1.8163 | lr 3.00e-04 | grad 2.20 | tok/s 6813
step    620 | loss 1.8755 | lr 3.00e-04 | grad 2.61 | tok/s 6812
step    630 | loss 2.0975 | lr 3.00e-04 | grad 2.17 | tok/s 6921
step    640 | loss 1.9192 | lr 3.00e-04 | grad 2.83 | tok/s 6718
step    650 | loss 1.9229 | lr 3.00e-04 | grad 2.11 | tok/s 6528
step    660 | loss 2.1697 | lr 3.00e-04 | grad 2.53 | tok/s 6876
step    670 | loss 2.1014 | lr 3.00e-04 | grad 3.03 | tok/s 6833
step    680 | loss 2.1668 | lr 3.00e-04 | grad 3.20 | tok/s 6760
step    690 | loss 2.0606 | lr 3.00e-04 | grad 2.47 | tok/s 6808
step    700 | loss 2.0002 | lr 3.00e-04 | grad 1.75 | tok/s 6635
step    710 | loss 1.8957 | lr 3.00e-04 | grad 4.56 | tok/s 6792
step    720 | loss 2.1368 | lr 3.00e-04 | grad 2.84 | tok/s 6763
step    730 | loss 1.9205 | lr 3.00e-04 | grad 2.91 | tok/s 6735
step    740 | loss 1.9526 | lr 3.00e-04 | grad 2.38 | tok/s 6653
step    750 | loss 2.2263 | lr 3.00e-04 | grad 3.16 | tok/s 6918
step    760 | loss 1.8503 | lr 3.00e-04 | grad 4.59 | tok/s 6799
step    770 | loss 1.9321 | lr 3.00e-04 | grad 4.72 | tok/s 6937
step    780 | loss 1.9012 | lr 3.00e-04 | grad 1.93 | tok/s 6827
step    790 | loss 1.7297 | lr 3.00e-04 | grad 1.80 | tok/s 6900
step    800 | loss 1.8943 | lr 3.00e-04 | grad 3.14 | tok/s 6738
step    810 | loss 2.4921 | lr 3.00e-04 | grad 4.34 | tok/s 6891
step    820 | loss 2.2511 | lr 3.00e-04 | grad 4.19 | tok/s 7020
step    830 | loss 2.0232 | lr 3.00e-04 | grad 4.38 | tok/s 7001
step    840 | loss 2.0451 | lr 3.00e-04 | grad 3.22 | tok/s 6859
step    850 | loss 1.8741 | lr 3.00e-04 | grad 2.23 | tok/s 6521
step    860 | loss 1.7889 | lr 3.00e-04 | grad 2.45 | tok/s 6850
step    870 | loss 1.9439 | lr 3.00e-04 | grad 2.69 | tok/s 6686
step    880 | loss 1.8504 | lr 3.00e-04 | grad 2.38 | tok/s 6801
step    890 | loss 2.0742 | lr 3.00e-04 | grad 2.28 | tok/s 6705
step    900 | loss 1.8390 | lr 3.00e-04 | grad 1.55 | tok/s 6697
step    910 | loss 1.8330 | lr 3.00e-04 | grad 2.17 | tok/s 6696
step    920 | loss 1.9028 | lr 3.00e-04 | grad 2.08 | tok/s 6520
step    930 | loss 1.8749 | lr 3.00e-04 | grad 3.17 | tok/s 6652
step    940 | loss 1.9350 | lr 3.00e-04 | grad 2.36 | tok/s 6769
step    950 | loss 1.6747 | lr 3.00e-04 | grad 2.36 | tok/s 8062
step    960 | loss 1.5992 | lr 3.00e-04 | grad 2.30 | tok/s 9103
step    970 | loss 1.5423 | lr 3.00e-04 | grad 1.87 | tok/s 7022
step    980 | loss 1.9902 | lr 3.00e-04 | grad 3.78 | tok/s 6601
step    990 | loss 1.9500 | lr 3.00e-04 | grad 3.05 | tok/s 6690
step   1000 | loss 1.9918 | lr 3.00e-04 | grad 1.64 | tok/s 6717
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9918.pt
step   1010 | loss 1.7583 | lr 3.00e-04 | grad 2.80 | tok/s 5356
step   1020 | loss 2.1290 | lr 3.00e-04 | grad 20.00 | tok/s 6564
step   1030 | loss 1.7983 | lr 3.00e-04 | grad 2.94 | tok/s 6776
step   1040 | loss 1.7792 | lr 3.00e-04 | grad 2.22 | tok/s 6667
step   1050 | loss 1.8094 | lr 3.00e-04 | grad 2.75 | tok/s 6727
step   1060 | loss 2.1935 | lr 3.00e-04 | grad 3.84 | tok/s 6793
step   1070 | loss 2.2724 | lr 3.00e-04 | grad 2.47 | tok/s 6659
step   1080 | loss 2.1490 | lr 3.00e-04 | grad 2.81 | tok/s 6722
step   1090 | loss 1.8385 | lr 3.00e-04 | grad 4.97 | tok/s 6741
step   1100 | loss 1.7711 | lr 3.00e-04 | grad 2.22 | tok/s 6954
step   1110 | loss 1.9711 | lr 3.00e-04 | grad 2.42 | tok/s 6949
step   1120 | loss 2.0033 | lr 3.00e-04 | grad 4.47 | tok/s 6592
step   1130 | loss 1.7151 | lr 3.00e-04 | grad 1.68 | tok/s 6688
step   1140 | loss 2.0778 | lr 3.00e-04 | grad 3.02 | tok/s 6765
step   1150 | loss 1.7776 | lr 3.00e-04 | grad 2.41 | tok/s 6668
step   1160 | loss 2.0113 | lr 3.00e-04 | grad 1.75 | tok/s 6612
step   1170 | loss 1.7637 | lr 3.00e-04 | grad 2.77 | tok/s 7019
step   1180 | loss 1.6883 | lr 3.00e-04 | grad 1.94 | tok/s 7019
step   1190 | loss 1.6194 | lr 3.00e-04 | grad 2.20 | tok/s 7006
step   1200 | loss 1.5796 | lr 3.00e-04 | grad 2.59 | tok/s 7021
step   1210 | loss 1.6135 | lr 3.00e-04 | grad 2.30 | tok/s 7033
step   1220 | loss 1.7125 | lr 3.00e-04 | grad 2.22 | tok/s 6800
step   1230 | loss 1.7905 | lr 3.00e-04 | grad 2.97 | tok/s 6474
step   1240 | loss 1.7829 | lr 3.00e-04 | grad 3.00 | tok/s 6839
step   1250 | loss 1.8995 | lr 3.00e-04 | grad 5.09 | tok/s 6930
step   1260 | loss 2.0370 | lr 3.00e-04 | grad 2.77 | tok/s 6877
step   1270 | loss 1.8933 | lr 3.00e-04 | grad 1.87 | tok/s 6779
step   1280 | loss 1.7695 | lr 3.00e-04 | grad 2.14 | tok/s 6681
step   1290 | loss 1.9712 | lr 3.00e-04 | grad 2.55 | tok/s 6684
step   1300 | loss 1.9518 | lr 3.00e-04 | grad 2.38 | tok/s 6767
step   1310 | loss 1.8338 | lr 3.00e-04 | grad 2.27 | tok/s 6572
step   1320 | loss 1.8105 | lr 3.00e-04 | grad 2.81 | tok/s 6856
step   1330 | loss 1.7980 | lr 3.00e-04 | grad 2.25 | tok/s 6765
step   1340 | loss 1.7092 | lr 3.00e-04 | grad 2.06 | tok/s 6945
step   1350 | loss 1.7094 | lr 3.00e-04 | grad 2.59 | tok/s 6567
step   1360 | loss 1.8120 | lr 3.00e-04 | grad 2.38 | tok/s 6726
step   1370 | loss 1.9202 | lr 3.00e-04 | grad 1.66 | tok/s 6765
step   1380 | loss 1.8667 | lr 3.00e-04 | grad 2.59 | tok/s 6762
step   1390 | loss 1.7734 | lr 3.00e-04 | grad 4.19 | tok/s 6850
step   1400 | loss 1.7780 | lr 3.00e-04 | grad 1.65 | tok/s 6894
step   1410 | loss 1.7625 | lr 3.00e-04 | grad 1.87 | tok/s 6670
step   1420 | loss 1.7177 | lr 3.00e-04 | grad 2.38 | tok/s 6602
step   1430 | loss 1.5824 | lr 3.00e-04 | grad 1.95 | tok/s 6835
step   1440 | loss 1.6789 | lr 3.00e-04 | grad 1.98 | tok/s 6817
step   1450 | loss 1.7851 | lr 3.00e-04 | grad 2.11 | tok/s 6608
step   1460 | loss 1.8963 | lr 3.00e-04 | grad 1.98 | tok/s 6816
step   1470 | loss 1.6771 | lr 3.00e-04 | grad 1.80 | tok/s 6754
step   1480 | loss 2.0151 | lr 3.00e-04 | grad 5.81 | tok/s 6746
step   1490 | loss 1.8988 | lr 3.00e-04 | grad 2.83 | tok/s 6600
step   1500 | loss 1.8537 | lr 3.00e-04 | grad 2.34 | tok/s 6908
step   1510 | loss 1.7901 | lr 3.00e-04 | grad 2.97 | tok/s 6801
step   1520 | loss 1.7522 | lr 3.00e-04 | grad 2.16 | tok/s 9465
step   1530 | loss 1.6644 | lr 3.00e-04 | grad 1.56 | tok/s 13994
step   1540 | loss 1.6707 | lr 3.00e-04 | grad 2.53 | tok/s 14352

Training complete! Final step: 1545
