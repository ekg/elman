Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002340/eval_6/level42_100m_20260202_002346
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 479,840,896 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 5.5271 | lr 3.00e-04 | grad 15.19 | tok/s 3842
step     20 | loss 4.6644 | lr 3.00e-04 | grad 14.44 | tok/s 5298
step     30 | loss 5.0495 | lr 3.00e-04 | grad 12.81 | tok/s 5425
step     40 | loss 4.2779 | lr 3.00e-04 | grad 10.19 | tok/s 5322
step     50 | loss 4.0319 | lr 3.00e-04 | grad 8.81 | tok/s 5276
step     60 | loss 3.7591 | lr 3.00e-04 | grad 6.66 | tok/s 5138
step     70 | loss 2.9636 | lr 3.00e-04 | grad 4.94 | tok/s 5039
step     80 | loss 3.2272 | lr 3.00e-04 | grad 6.25 | tok/s 5254
step     90 | loss 2.9053 | lr 3.00e-04 | grad 3.58 | tok/s 4987
step    100 | loss 2.5475 | lr 3.00e-04 | grad 6.44 | tok/s 5000
step    110 | loss 2.7092 | lr 3.00e-04 | grad 2.67 | tok/s 4972
step    120 | loss 2.9420 | lr 3.00e-04 | grad 8.12 | tok/s 5045
step    130 | loss 2.5864 | lr 3.00e-04 | grad 1.98 | tok/s 5118
step    140 | loss 2.4764 | lr 3.00e-04 | grad 3.14 | tok/s 4986
step    150 | loss 2.5165 | lr 3.00e-04 | grad 3.16 | tok/s 4932
step    160 | loss 2.3486 | lr 3.00e-04 | grad 1.42 | tok/s 4857
step    170 | loss 2.5483 | lr 3.00e-04 | grad 4.12 | tok/s 5069
step    180 | loss 2.4314 | lr 3.00e-04 | grad 2.92 | tok/s 5004
step    190 | loss 2.2585 | lr 3.00e-04 | grad 2.19 | tok/s 5321
step    200 | loss 2.2892 | lr 3.00e-04 | grad 2.75 | tok/s 5219
step    210 | loss 2.3847 | lr 3.00e-04 | grad 2.02 | tok/s 5185
step    220 | loss 2.3580 | lr 3.00e-04 | grad 2.98 | tok/s 5069
step    230 | loss 2.2682 | lr 3.00e-04 | grad 1.84 | tok/s 5034
step    240 | loss 2.3419 | lr 3.00e-04 | grad 2.00 | tok/s 5106
step    250 | loss 2.3748 | lr 3.00e-04 | grad 7.78 | tok/s 5122
step    260 | loss 2.2391 | lr 3.00e-04 | grad 2.69 | tok/s 5092
step    270 | loss 2.3234 | lr 3.00e-04 | grad 1.88 | tok/s 5041
step    280 | loss 2.0213 | lr 3.00e-04 | grad 2.11 | tok/s 5018
step    290 | loss 1.9987 | lr 3.00e-04 | grad 1.88 | tok/s 5214
step    300 | loss 1.9994 | lr 3.00e-04 | grad 1.89 | tok/s 5194
step    310 | loss 1.9554 | lr 3.00e-04 | grad 2.25 | tok/s 5192
step    320 | loss 2.1960 | lr 3.00e-04 | grad 4.12 | tok/s 4933
step    330 | loss 2.1354 | lr 3.00e-04 | grad 1.85 | tok/s 5052
step    340 | loss 2.2970 | lr 3.00e-04 | grad 2.12 | tok/s 4936
step    350 | loss 2.1363 | lr 3.00e-04 | grad 2.50 | tok/s 4909
step    360 | loss 2.2046 | lr 3.00e-04 | grad 1.68 | tok/s 5052
step    370 | loss 2.2373 | lr 3.00e-04 | grad 5.12 | tok/s 5058
step    380 | loss 2.3710 | lr 3.00e-04 | grad 1.74 | tok/s 5142
step    390 | loss 2.0575 | lr 3.00e-04 | grad 2.27 | tok/s 4982
step    400 | loss 2.3734 | lr 3.00e-04 | grad 1.63 | tok/s 5140
step    410 | loss 1.9143 | lr 3.00e-04 | grad 2.05 | tok/s 4972
step    420 | loss 2.1938 | lr 3.00e-04 | grad 2.12 | tok/s 4922
step    430 | loss 2.1963 | lr 3.00e-04 | grad 4.19 | tok/s 5035
step    440 | loss 2.2657 | lr 3.00e-04 | grad 1.41 | tok/s 5065
step    450 | loss 1.9579 | lr 3.00e-04 | grad 1.58 | tok/s 4985
step    460 | loss 2.0706 | lr 3.00e-04 | grad 2.31 | tok/s 4942
step    470 | loss 1.9880 | lr 3.00e-04 | grad 1.20 | tok/s 5021
step    480 | loss 1.9731 | lr 3.00e-04 | grad 1.90 | tok/s 4938
step    490 | loss 2.4547 | lr 3.00e-04 | grad 3.50 | tok/s 5159
step    500 | loss 2.2372 | lr 3.00e-04 | grad 1.39 | tok/s 4976
step    510 | loss 1.7796 | lr 3.00e-04 | grad 2.92 | tok/s 5187
step    520 | loss 2.2460 | lr 3.00e-04 | grad 6.69 | tok/s 4982
step    530 | loss 2.2524 | lr 3.00e-04 | grad 2.25 | tok/s 5093
step    540 | loss 1.7458 | lr 3.00e-04 | grad 2.09 | tok/s 5154
step    550 | loss 1.7442 | lr 3.00e-04 | grad 1.92 | tok/s 5212
step    560 | loss 1.7483 | lr 3.00e-04 | grad 2.30 | tok/s 5179
step    570 | loss 2.3524 | lr 3.00e-04 | grad 2.66 | tok/s 5044
step    580 | loss 2.3282 | lr 3.00e-04 | grad 2.16 | tok/s 4979
step    590 | loss 2.0691 | lr 3.00e-04 | grad 2.02 | tok/s 4956
step    600 | loss 2.1323 | lr 3.00e-04 | grad 2.34 | tok/s 5196
step    610 | loss 1.8139 | lr 3.00e-04 | grad 2.00 | tok/s 5061
step    620 | loss 1.8716 | lr 3.00e-04 | grad 1.82 | tok/s 5046
step    630 | loss 2.1606 | lr 3.00e-04 | grad 2.02 | tok/s 5150
step    640 | loss 1.9191 | lr 3.00e-04 | grad 2.20 | tok/s 5001
step    650 | loss 1.9225 | lr 3.00e-04 | grad 1.73 | tok/s 4851
step    660 | loss 2.1749 | lr 3.00e-04 | grad 1.99 | tok/s 5116
step    670 | loss 2.1074 | lr 3.00e-04 | grad 2.47 | tok/s 5070
step    680 | loss 2.1769 | lr 3.00e-04 | grad 2.80 | tok/s 5080
step    690 | loss 2.0497 | lr 3.00e-04 | grad 2.17 | tok/s 5045
step    700 | loss 2.0105 | lr 3.00e-04 | grad 1.58 | tok/s 4927
step    710 | loss 1.9054 | lr 3.00e-04 | grad 3.33 | tok/s 5067
step    720 | loss 2.1346 | lr 3.00e-04 | grad 2.23 | tok/s 5033
step    730 | loss 1.9166 | lr 3.00e-04 | grad 2.41 | tok/s 5010
step    740 | loss 1.9488 | lr 3.00e-04 | grad 1.98 | tok/s 4944
step    750 | loss 2.1896 | lr 3.00e-04 | grad 2.58 | tok/s 5168
step    760 | loss 1.8514 | lr 3.00e-04 | grad 3.66 | tok/s 5051
step    770 | loss 1.9247 | lr 3.00e-04 | grad 3.94 | tok/s 5138
step    780 | loss 1.8972 | lr 3.00e-04 | grad 1.57 | tok/s 5070
step    790 | loss 1.7242 | lr 3.00e-04 | grad 1.57 | tok/s 5146
step    800 | loss 1.8939 | lr 3.00e-04 | grad 3.06 | tok/s 5017
step    810 | loss 2.5095 | lr 3.00e-04 | grad 2.48 | tok/s 5154
step    820 | loss 2.2652 | lr 3.00e-04 | grad 3.59 | tok/s 5215
step    830 | loss 2.0207 | lr 3.00e-04 | grad 2.94 | tok/s 5223
step    840 | loss 2.0405 | lr 3.00e-04 | grad 2.19 | tok/s 5084
step    850 | loss 1.8609 | lr 3.00e-04 | grad 1.96 | tok/s 5763
step    860 | loss 1.6707 | lr 3.00e-04 | grad 1.77 | tok/s 5796
step    870 | loss 1.9280 | lr 3.00e-04 | grad 2.23 | tok/s 5068
step    880 | loss 1.7768 | lr 3.00e-04 | grad 1.72 | tok/s 5101
step    890 | loss 2.0705 | lr 3.00e-04 | grad 1.80 | tok/s 5022
step    900 | loss 2.0665 | lr 3.00e-04 | grad 2.08 | tok/s 4809
step    910 | loss 1.8240 | lr 3.00e-04 | grad 1.65 | tok/s 5043
step    920 | loss 1.8810 | lr 3.00e-04 | grad 1.62 | tok/s 5009
step    930 | loss 1.9618 | lr 3.00e-04 | grad 2.53 | tok/s 4976
step    940 | loss 1.8752 | lr 3.00e-04 | grad 1.89 | tok/s 5066
step    950 | loss 1.7965 | lr 3.00e-04 | grad 1.96 | tok/s 5148
step    960 | loss 1.5925 | lr 3.00e-04 | grad 1.74 | tok/s 5212
step    970 | loss 1.5944 | lr 3.00e-04 | grad 2.22 | tok/s 5190
step    980 | loss 1.9959 | lr 3.00e-04 | grad 3.20 | tok/s 4996
step    990 | loss 1.9847 | lr 3.00e-04 | grad 1.15 | tok/s 5007
step   1000 | loss 2.1051 | lr 3.00e-04 | grad 4.47 | tok/s 5058
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1051.pt
step   1010 | loss 1.7522 | lr 3.00e-04 | grad 2.06 | tok/s 4037
step   1020 | loss 2.1436 | lr 3.00e-04 | grad 1.66 | tok/s 4919
step   1030 | loss 1.7708 | lr 3.00e-04 | grad 2.66 | tok/s 5081
step   1040 | loss 1.6917 | lr 3.00e-04 | grad 1.90 | tok/s 4948
step   1050 | loss 1.9504 | lr 3.00e-04 | grad 2.55 | tok/s 5039
step   1060 | loss 2.0503 | lr 3.00e-04 | grad 1.77 | tok/s 5033
step   1070 | loss 2.3184 | lr 3.00e-04 | grad 2.28 | tok/s 4976
step   1080 | loss 2.1037 | lr 3.00e-04 | grad 2.14 | tok/s 5009
step   1090 | loss 1.8302 | lr 3.00e-04 | grad 2.25 | tok/s 5039
step   1100 | loss 1.7550 | lr 3.00e-04 | grad 2.75 | tok/s 5202
step   1110 | loss 1.9602 | lr 3.00e-04 | grad 1.69 | tok/s 5181
step   1120 | loss 1.9857 | lr 3.00e-04 | grad 1.48 | tok/s 4905
step   1130 | loss 1.6861 | lr 3.00e-04 | grad 1.88 | tok/s 5853
step   1140 | loss 2.0611 | lr 3.00e-04 | grad 1.73 | tok/s 11504
step   1150 | loss 1.9545 | lr 3.00e-04 | grad 1.63 | tok/s 11009

Training complete! Final step: 1151
