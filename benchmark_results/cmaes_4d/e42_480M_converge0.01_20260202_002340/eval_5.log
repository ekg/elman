Using device: cuda
Output directory: benchmark_results/cmaes_4d/e42_480M_converge0.01_20260202_002340/eval_5/level42_100m_20260202_002347
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 462,557,952 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 5.5924 | lr 3.00e-04 | grad 10.31 | tok/s 3877
step     20 | loss 4.6039 | lr 3.00e-04 | grad 14.62 | tok/s 5340
step     30 | loss 5.3778 | lr 3.00e-04 | grad 10.50 | tok/s 5487
step     40 | loss 4.1082 | lr 3.00e-04 | grad 10.56 | tok/s 5410
step     50 | loss 3.7962 | lr 3.00e-04 | grad 11.38 | tok/s 5352
step     60 | loss 3.7429 | lr 3.00e-04 | grad 7.00 | tok/s 5196
step     70 | loss 3.0330 | lr 3.00e-04 | grad 5.28 | tok/s 5130
step     80 | loss 3.2749 | lr 3.00e-04 | grad 6.66 | tok/s 5314
step     90 | loss 2.9716 | lr 3.00e-04 | grad 4.53 | tok/s 5021
step    100 | loss 2.5737 | lr 3.00e-04 | grad 5.19 | tok/s 5054
step    110 | loss 2.6971 | lr 3.00e-04 | grad 3.12 | tok/s 5068
step    120 | loss 2.8749 | lr 3.00e-04 | grad 8.12 | tok/s 5135
step    130 | loss 2.5736 | lr 3.00e-04 | grad 2.20 | tok/s 5208
step    140 | loss 2.4553 | lr 3.00e-04 | grad 2.70 | tok/s 5087
step    150 | loss 2.4826 | lr 3.00e-04 | grad 2.98 | tok/s 5011
step    160 | loss 2.3093 | lr 3.00e-04 | grad 1.34 | tok/s 4937
step    170 | loss 2.5049 | lr 3.00e-04 | grad 3.94 | tok/s 5139
step    180 | loss 2.3885 | lr 3.00e-04 | grad 2.69 | tok/s 5045
step    190 | loss 2.2133 | lr 3.00e-04 | grad 2.92 | tok/s 5412
step    200 | loss 2.2260 | lr 3.00e-04 | grad 3.11 | tok/s 5262
step    210 | loss 2.3349 | lr 3.00e-04 | grad 2.03 | tok/s 5258
step    220 | loss 2.2949 | lr 3.00e-04 | grad 3.19 | tok/s 5125
step    230 | loss 2.2224 | lr 3.00e-04 | grad 2.11 | tok/s 5100
step    240 | loss 2.2795 | lr 3.00e-04 | grad 2.41 | tok/s 5181
step    250 | loss 2.3222 | lr 3.00e-04 | grad 7.41 | tok/s 5142
step    260 | loss 2.1842 | lr 3.00e-04 | grad 2.77 | tok/s 5113
step    270 | loss 2.2675 | lr 3.00e-04 | grad 2.22 | tok/s 5089
step    280 | loss 1.9591 | lr 3.00e-04 | grad 2.39 | tok/s 5107
step    290 | loss 1.9217 | lr 3.00e-04 | grad 1.65 | tok/s 5278
step    300 | loss 1.9056 | lr 3.00e-04 | grad 1.75 | tok/s 5286
step    310 | loss 1.8631 | lr 3.00e-04 | grad 2.34 | tok/s 5268
step    320 | loss 2.1336 | lr 3.00e-04 | grad 4.31 | tok/s 4995
step    330 | loss 2.0608 | lr 3.00e-04 | grad 1.94 | tok/s 5111
step    340 | loss 2.2204 | lr 3.00e-04 | grad 2.22 | tok/s 5009
step    350 | loss 2.0741 | lr 3.00e-04 | grad 3.38 | tok/s 4969
step    360 | loss 2.1329 | lr 3.00e-04 | grad 1.87 | tok/s 5117
step    370 | loss 2.1678 | lr 3.00e-04 | grad 5.19 | tok/s 5135
step    380 | loss 2.2948 | lr 3.00e-04 | grad 2.09 | tok/s 5234
step    390 | loss 1.9786 | lr 3.00e-04 | grad 2.48 | tok/s 5051
step    400 | loss 2.2949 | lr 3.00e-04 | grad 1.82 | tok/s 5218
step    410 | loss 1.8541 | lr 3.00e-04 | grad 2.12 | tok/s 5039
step    420 | loss 2.1207 | lr 3.00e-04 | grad 2.16 | tok/s 5000
step    430 | loss 2.1258 | lr 3.00e-04 | grad 4.41 | tok/s 5119
step    440 | loss 2.1724 | lr 3.00e-04 | grad 1.55 | tok/s 5121
step    450 | loss 1.8901 | lr 3.00e-04 | grad 1.72 | tok/s 5048
step    460 | loss 1.9999 | lr 3.00e-04 | grad 2.22 | tok/s 4989
step    470 | loss 1.9039 | lr 3.00e-04 | grad 1.27 | tok/s 5119
step    480 | loss 1.8982 | lr 3.00e-04 | grad 1.99 | tok/s 4991
step    490 | loss 2.3759 | lr 3.00e-04 | grad 3.50 | tok/s 5206
step    500 | loss 2.1670 | lr 3.00e-04 | grad 1.35 | tok/s 5013
step    510 | loss 1.7108 | lr 3.00e-04 | grad 2.97 | tok/s 5249
step    520 | loss 2.1812 | lr 3.00e-04 | grad 7.28 | tok/s 5053
step    530 | loss 2.1936 | lr 3.00e-04 | grad 2.17 | tok/s 5150
step    540 | loss 1.6785 | lr 3.00e-04 | grad 2.31 | tok/s 5237
step    550 | loss 1.6549 | lr 3.00e-04 | grad 1.96 | tok/s 5262
step    560 | loss 1.6608 | lr 3.00e-04 | grad 2.25 | tok/s 5255
step    570 | loss 2.2734 | lr 3.00e-04 | grad 2.59 | tok/s 5126
step    580 | loss 2.2404 | lr 3.00e-04 | grad 2.33 | tok/s 5025
step    590 | loss 2.0087 | lr 3.00e-04 | grad 2.28 | tok/s 5023
step    600 | loss 2.0739 | lr 3.00e-04 | grad 2.28 | tok/s 5259
step    610 | loss 1.7254 | lr 3.00e-04 | grad 1.91 | tok/s 5120
step    620 | loss 1.7991 | lr 3.00e-04 | grad 2.30 | tok/s 5130
step    630 | loss 2.0624 | lr 3.00e-04 | grad 1.91 | tok/s 5206
step    640 | loss 1.8308 | lr 3.00e-04 | grad 2.14 | tok/s 5075
step    650 | loss 1.8472 | lr 3.00e-04 | grad 1.69 | tok/s 4911
step    660 | loss 2.1029 | lr 3.00e-04 | grad 2.08 | tok/s 5167
step    670 | loss 2.0247 | lr 3.00e-04 | grad 2.53 | tok/s 5151
step    680 | loss 2.0947 | lr 3.00e-04 | grad 2.80 | tok/s 5121
step    690 | loss 1.9862 | lr 3.00e-04 | grad 2.06 | tok/s 5125
step    700 | loss 1.9360 | lr 3.00e-04 | grad 1.61 | tok/s 4995
step    710 | loss 1.8302 | lr 3.00e-04 | grad 3.75 | tok/s 5123
step    720 | loss 2.0664 | lr 3.00e-04 | grad 2.22 | tok/s 5126
step    730 | loss 1.8383 | lr 3.00e-04 | grad 2.27 | tok/s 5063
step    740 | loss 1.8813 | lr 3.00e-04 | grad 2.08 | tok/s 5017
step    750 | loss 2.0957 | lr 3.00e-04 | grad 2.70 | tok/s 5211
step    760 | loss 1.7773 | lr 3.00e-04 | grad 3.75 | tok/s 5118
step    770 | loss 1.8495 | lr 3.00e-04 | grad 4.00 | tok/s 5224
step    780 | loss 1.8259 | lr 3.00e-04 | grad 1.79 | tok/s 5115
step    790 | loss 1.6343 | lr 3.00e-04 | grad 1.68 | tok/s 5164
step    800 | loss 1.8161 | lr 3.00e-04 | grad 3.41 | tok/s 5091
step    810 | loss 2.4311 | lr 3.00e-04 | grad 3.36 | tok/s 5204
step    820 | loss 2.2366 | lr 3.00e-04 | grad 3.97 | tok/s 6277
step    830 | loss 1.9997 | lr 3.00e-04 | grad 3.36 | tok/s 5998
step    840 | loss 1.9938 | lr 3.00e-04 | grad 2.75 | tok/s 5197
step    850 | loss 1.7940 | lr 3.00e-04 | grad 1.54 | tok/s 5057
step    860 | loss 1.6719 | lr 3.00e-04 | grad 1.85 | tok/s 5169
step    870 | loss 1.8504 | lr 3.00e-04 | grad 2.17 | tok/s 5157
step    880 | loss 1.7010 | lr 3.00e-04 | grad 1.66 | tok/s 5150
step    890 | loss 1.9996 | lr 3.00e-04 | grad 1.59 | tok/s 5093
step    900 | loss 2.0124 | lr 3.00e-04 | grad 1.92 | tok/s 4880
step    910 | loss 1.7623 | lr 3.00e-04 | grad 1.48 | tok/s 5060
step    920 | loss 1.8135 | lr 3.00e-04 | grad 1.42 | tok/s 5054
step    930 | loss 1.8904 | lr 3.00e-04 | grad 2.19 | tok/s 5095
step    940 | loss 1.8192 | lr 3.00e-04 | grad 1.94 | tok/s 5127
step    950 | loss 1.7215 | lr 3.00e-04 | grad 1.88 | tok/s 5167
step    960 | loss 1.5118 | lr 3.00e-04 | grad 1.54 | tok/s 5328
step    970 | loss 1.5206 | lr 3.00e-04 | grad 2.33 | tok/s 5216
step    980 | loss 1.9241 | lr 3.00e-04 | grad 3.03 | tok/s 5052
step    990 | loss 1.9183 | lr 3.00e-04 | grad 1.23 | tok/s 5075
step   1000 | loss 2.0394 | lr 3.00e-04 | grad 4.69 | tok/s 5112
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0394.pt
step   1010 | loss 1.6929 | lr 3.00e-04 | grad 1.83 | tok/s 4090
step   1020 | loss 2.0830 | lr 3.00e-04 | grad 1.57 | tok/s 4995
step   1030 | loss 1.7027 | lr 3.00e-04 | grad 2.58 | tok/s 5151
step   1040 | loss 1.6423 | lr 3.00e-04 | grad 1.87 | tok/s 5024
step   1050 | loss 1.8824 | lr 3.00e-04 | grad 2.44 | tok/s 5114
step   1060 | loss 2.0106 | lr 3.00e-04 | grad 1.67 | tok/s 5107
step   1070 | loss 2.2883 | lr 3.00e-04 | grad 2.28 | tok/s 5041
step   1080 | loss 2.0518 | lr 3.00e-04 | grad 1.78 | tok/s 5056
step   1090 | loss 1.7638 | lr 3.00e-04 | grad 2.16 | tok/s 5125
step   1100 | loss 1.6995 | lr 3.00e-04 | grad 2.97 | tok/s 5269
step   1110 | loss 1.8640 | lr 3.00e-04 | grad 1.66 | tok/s 5239
step   1120 | loss 1.9235 | lr 3.00e-04 | grad 1.59 | tok/s 4968
step   1130 | loss 1.6289 | lr 3.00e-04 | grad 1.91 | tok/s 5087
step   1140 | loss 2.0400 | lr 3.00e-04 | grad 1.67 | tok/s 5035
step   1150 | loss 1.6079 | lr 3.00e-04 | grad 1.62 | tok/s 9567
step   1160 | loss 2.0506 | lr 3.00e-04 | grad 1.82 | tok/s 10997

Training complete! Final step: 1166
