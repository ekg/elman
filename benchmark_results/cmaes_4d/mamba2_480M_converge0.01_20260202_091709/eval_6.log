Using device: cuda
Output directory: benchmark_results/cmaes_4d/mamba2_480M_converge0.01_20260202_091709/eval_6/levelmamba2_100m_20260202_091714
Model: Level mamba2, 322,232,084 parameters
Using schedule-free AdamW (lr=0.00023161727021388877)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 5.4182 | lr 2.32e-04 | grad 7.91 | tok/s 8627
step     20 | loss 2.7820 | lr 2.32e-04 | grad 2.89 | tok/s 31391
step     30 | loss 2.1838 | lr 2.32e-04 | grad 1.97 | tok/s 31109
step     40 | loss 1.8745 | lr 2.32e-04 | grad 1.54 | tok/s 30883
step     50 | loss 2.6956 | lr 2.32e-04 | grad 3.98 | tok/s 29484
step     60 | loss 2.2450 | lr 2.32e-04 | grad 19.50 | tok/s 29762
step     70 | loss 2.1556 | lr 2.32e-04 | grad 2.11 | tok/s 29350
step     80 | loss 2.1133 | lr 2.32e-04 | grad 2.05 | tok/s 28488
step     90 | loss 1.7016 | lr 2.32e-04 | grad 2.08 | tok/s 29334
step    100 | loss 2.0700 | lr 2.32e-04 | grad 1.56 | tok/s 28162
step    110 | loss 2.0455 | lr 2.32e-04 | grad 1.75 | tok/s 28207
step    120 | loss 1.8514 | lr 2.32e-04 | grad 1.67 | tok/s 28769
step    130 | loss 1.6961 | lr 2.32e-04 | grad 2.33 | tok/s 27365
step    140 | loss 1.7748 | lr 2.32e-04 | grad 1.41 | tok/s 27583
step    150 | loss 1.7734 | lr 2.32e-04 | grad 1.63 | tok/s 27604
step    160 | loss 1.9015 | lr 2.32e-04 | grad 2.62 | tok/s 28192
step    170 | loss 1.5892 | lr 2.32e-04 | grad 1.74 | tok/s 28058
step    180 | loss 1.3569 | lr 2.32e-04 | grad 1.05 | tok/s 29027
step    190 | loss 1.5591 | lr 2.32e-04 | grad 1.57 | tok/s 28179
step    200 | loss 1.7129 | lr 2.32e-04 | grad 1.79 | tok/s 28603
step    210 | loss 1.6554 | lr 2.32e-04 | grad 2.44 | tok/s 28008
step    220 | loss 1.5765 | lr 2.32e-04 | grad 1.98 | tok/s 27960
step    230 | loss 1.6454 | lr 2.32e-04 | grad 1.75 | tok/s 28545
step    240 | loss 1.7390 | lr 2.32e-04 | grad 1.49 | tok/s 27870
step    250 | loss 1.5727 | lr 2.32e-04 | grad 1.60 | tok/s 27335
step    260 | loss 1.5968 | lr 2.32e-04 | grad 1.71 | tok/s 27940
step    270 | loss 1.4331 | lr 2.32e-04 | grad 1.37 | tok/s 28822
step    280 | loss 1.3233 | lr 2.32e-04 | grad 1.44 | tok/s 29168
step    290 | loss 1.3322 | lr 2.32e-04 | grad 1.58 | tok/s 29158
step    300 | loss 1.4255 | lr 2.32e-04 | grad 2.23 | tok/s 28869
step    310 | loss 1.6060 | lr 2.32e-04 | grad 1.33 | tok/s 27612
step    320 | loss 1.6187 | lr 2.32e-04 | grad 2.55 | tok/s 28495
step    330 | loss 1.5856 | lr 2.32e-04 | grad 1.52 | tok/s 27588
step    340 | loss 1.5315 | lr 2.32e-04 | grad 1.23 | tok/s 27419
step    350 | loss 1.4210 | lr 2.32e-04 | grad 1.15 | tok/s 28341
step    360 | loss 1.7902 | lr 2.32e-04 | grad 1.59 | tok/s 28431
step    370 | loss 1.5176 | lr 2.32e-04 | grad 1.45 | tok/s 28872
step    380 | loss 1.4992 | lr 2.32e-04 | grad 1.37 | tok/s 28183
step    390 | loss 1.4933 | lr 2.32e-04 | grad 1.34 | tok/s 28413
step    400 | loss 1.5014 | lr 2.32e-04 | grad 1.51 | tok/s 27567
step    410 | loss 1.5449 | lr 2.32e-04 | grad 1.73 | tok/s 27765
step    420 | loss 1.6014 | lr 2.32e-04 | grad 2.89 | tok/s 28604
step    430 | loss 1.5398 | lr 2.32e-04 | grad 1.28 | tok/s 28130
step    440 | loss 1.5090 | lr 2.32e-04 | grad 1.47 | tok/s 28127
step    450 | loss 1.5164 | lr 2.32e-04 | grad 1.98 | tok/s 28185
step    460 | loss 1.4132 | lr 2.32e-04 | grad 1.17 | tok/s 27375
step    470 | loss 1.4360 | lr 2.32e-04 | grad 1.21 | tok/s 28057
step    480 | loss 1.8314 | lr 2.32e-04 | grad 1.76 | tok/s 28882
step    490 | loss 1.4741 | lr 2.32e-04 | grad 3.08 | tok/s 28204
step    500 | loss 1.3237 | lr 2.32e-04 | grad 1.15 | tok/s 29010
step    510 | loss 1.9195 | lr 2.32e-04 | grad 2.05 | tok/s 28226
step    520 | loss 1.3133 | lr 2.32e-04 | grad 1.14 | tok/s 28623
step    530 | loss 1.3697 | lr 2.32e-04 | grad 1.41 | tok/s 28829
step    540 | loss 1.2313 | lr 2.32e-04 | grad 1.18 | tok/s 29312
step    550 | loss 1.3748 | lr 2.32e-04 | grad 4.31 | tok/s 28932
step    560 | loss 1.7319 | lr 2.32e-04 | grad 1.54 | tok/s 29016
step    570 | loss 1.8173 | lr 2.32e-04 | grad 2.84 | tok/s 27922
step    580 | loss 1.4086 | lr 2.32e-04 | grad 1.86 | tok/s 28204
step    590 | loss 1.4565 | lr 2.32e-04 | grad 1.66 | tok/s 29272
step    600 | loss 1.4021 | lr 2.32e-04 | grad 1.37 | tok/s 27795
step    610 | loss 1.3393 | lr 2.32e-04 | grad 1.20 | tok/s 28804
step    620 | loss 1.4824 | lr 2.32e-04 | grad 1.54 | tok/s 28753
step    630 | loss 1.4262 | lr 2.32e-04 | grad 1.38 | tok/s 28127
step    640 | loss 1.5419 | lr 2.32e-04 | grad 5.06 | tok/s 27792
step    650 | loss 1.5272 | lr 2.32e-04 | grad 2.08 | tok/s 28654
step    660 | loss 1.5104 | lr 2.32e-04 | grad 1.97 | tok/s 28093
step    670 | loss 1.4661 | lr 2.32e-04 | grad 1.70 | tok/s 27850
step    680 | loss 1.5871 | lr 2.32e-04 | grad 2.83 | tok/s 28119
step    690 | loss 1.4541 | lr 2.32e-04 | grad 1.33 | tok/s 28447
step    700 | loss 1.4329 | lr 2.32e-04 | grad 5.22 | tok/s 28136
step    710 | loss 1.5507 | lr 2.32e-04 | grad 1.73 | tok/s 28341
step    720 | loss 1.5470 | lr 2.32e-04 | grad 4.00 | tok/s 28277
step    730 | loss 1.3781 | lr 2.32e-04 | grad 1.66 | tok/s 27955
step    740 | loss 1.6274 | lr 2.32e-04 | grad 1.34 | tok/s 28273
step    750 | loss 1.4108 | lr 2.32e-04 | grad 1.20 | tok/s 28212
step    760 | loss 1.4828 | lr 2.32e-04 | grad 1.44 | tok/s 28673
step    770 | loss 1.3266 | lr 2.32e-04 | grad 1.09 | tok/s 28788
step    780 | loss 1.3526 | lr 2.32e-04 | grad 3.17 | tok/s 28421
step    790 | loss 1.3635 | lr 2.32e-04 | grad 1.74 | tok/s 28209
step    800 | loss 2.0644 | lr 2.32e-04 | grad 2.05 | tok/s 29105
step    810 | loss 1.6554 | lr 2.32e-04 | grad 1.66 | tok/s 29340
step    820 | loss 1.4500 | lr 2.32e-04 | grad 1.48 | tok/s 29331
step    830 | loss 1.5083 | lr 2.32e-04 | grad 1.15 | tok/s 28006
step    840 | loss 1.3970 | lr 2.32e-04 | grad 1.12 | tok/s 28319
step    850 | loss 1.3912 | lr 2.32e-04 | grad 1.65 | tok/s 28229
step    860 | loss 1.4556 | lr 2.32e-04 | grad 1.28 | tok/s 28710
step    870 | loss 1.3848 | lr 2.32e-04 | grad 2.31 | tok/s 27912
step    880 | loss 1.6835 | lr 2.32e-04 | grad 1.48 | tok/s 27758
step    890 | loss 1.3169 | lr 2.32e-04 | grad 1.60 | tok/s 27727
step    900 | loss 1.4566 | lr 2.32e-04 | grad 1.25 | tok/s 28008
step    910 | loss 1.3964 | lr 2.32e-04 | grad 1.75 | tok/s 27868
step    920 | loss 1.4716 | lr 2.32e-04 | grad 1.68 | tok/s 27852
step    930 | loss 1.4216 | lr 2.32e-04 | grad 1.24 | tok/s 28538
step    940 | loss 1.2441 | lr 2.32e-04 | grad 1.27 | tok/s 29314
step    950 | loss 1.1901 | lr 2.32e-04 | grad 1.13 | tok/s 29303
step    960 | loss 1.3782 | lr 2.32e-04 | grad 1.45 | tok/s 28293
step    970 | loss 1.4884 | lr 2.32e-04 | grad 2.23 | tok/s 27772
step    980 | loss 1.4831 | lr 2.32e-04 | grad 1.98 | tok/s 28185
step    990 | loss 1.3640 | lr 2.32e-04 | grad 1.30 | tok/s 28828
step   1000 | loss 1.3388 | lr 2.32e-04 | grad 1.39 | tok/s 27769
  >>> saved checkpoint: checkpoint_step_001000_loss_1.3388.pt
step   1010 | loss 1.6322 | lr 2.32e-04 | grad 1.09 | tok/s 21192
step   1020 | loss 1.5248 | lr 2.32e-04 | grad 1.86 | tok/s 28292
step   1030 | loss 1.1772 | lr 2.32e-04 | grad 0.98 | tok/s 28047
step   1040 | loss 1.6585 | lr 2.32e-04 | grad 1.24 | tok/s 28774
step   1050 | loss 1.8710 | lr 2.32e-04 | grad 1.40 | tok/s 28227
step   1060 | loss 1.4827 | lr 2.32e-04 | grad 1.63 | tok/s 27858
step   1070 | loss 1.5961 | lr 2.32e-04 | grad 1.12 | tok/s 28593
step   1080 | loss 1.4045 | lr 2.32e-04 | grad 2.25 | tok/s 29022
step   1090 | loss 1.3485 | lr 2.32e-04 | grad 1.76 | tok/s 28646
step   1100 | loss 1.4530 | lr 2.32e-04 | grad 1.02 | tok/s 27710
step   1110 | loss 1.4758 | lr 2.32e-04 | grad 1.64 | tok/s 28295
step   1120 | loss 1.5050 | lr 2.32e-04 | grad 3.20 | tok/s 28199
step   1130 | loss 1.4732 | lr 2.32e-04 | grad 2.06 | tok/s 27849
step   1140 | loss 1.5072 | lr 2.32e-04 | grad 1.52 | tok/s 27589
step   1150 | loss 1.3534 | lr 2.32e-04 | grad 1.39 | tok/s 29088
step   1160 | loss 1.3046 | lr 2.32e-04 | grad 1.09 | tok/s 29322
step   1170 | loss 1.2401 | lr 2.32e-04 | grad 1.27 | tok/s 29308
step   1180 | loss 1.1990 | lr 2.32e-04 | grad 1.18 | tok/s 29309
step   1190 | loss 1.1768 | lr 2.32e-04 | grad 1.15 | tok/s 29313
step   1200 | loss 1.2896 | lr 2.32e-04 | grad 0.85 | tok/s 28907
step   1210 | loss 1.3656 | lr 2.32e-04 | grad 1.15 | tok/s 27580
step   1220 | loss 1.4497 | lr 2.32e-04 | grad 1.16 | tok/s 28598
step   1230 | loss 1.3869 | lr 2.32e-04 | grad 2.06 | tok/s 28321
step   1240 | loss 1.6007 | lr 2.32e-04 | grad 1.32 | tok/s 28211
step   1250 | loss 1.4425 | lr 2.32e-04 | grad 1.66 | tok/s 28179
step   1260 | loss 1.4661 | lr 2.32e-04 | grad 1.57 | tok/s 27796
step   1270 | loss 1.4263 | lr 2.32e-04 | grad 1.48 | tok/s 27937
step   1280 | loss 1.4968 | lr 2.32e-04 | grad 1.17 | tok/s 27778
step   1290 | loss 1.4163 | lr 2.32e-04 | grad 1.40 | tok/s 28015
step   1300 | loss 1.4351 | lr 2.32e-04 | grad 1.37 | tok/s 28408
step   1310 | loss 1.2753 | lr 2.32e-04 | grad 1.30 | tok/s 28000
step   1320 | loss 1.2732 | lr 2.32e-04 | grad 1.05 | tok/s 28681
step   1330 | loss 1.3342 | lr 2.32e-04 | grad 1.19 | tok/s 28060
step   1340 | loss 1.3839 | lr 2.32e-04 | grad 1.95 | tok/s 27937
step   1350 | loss 1.5055 | lr 2.32e-04 | grad 1.39 | tok/s 27938
step   1360 | loss 1.3793 | lr 2.32e-04 | grad 1.38 | tok/s 27941
step   1370 | loss 1.3347 | lr 2.32e-04 | grad 1.25 | tok/s 28698
step   1380 | loss 1.4101 | lr 2.32e-04 | grad 1.54 | tok/s 28715
step   1390 | loss 1.4334 | lr 2.32e-04 | grad 2.64 | tok/s 27548
step   1400 | loss 1.3587 | lr 2.32e-04 | grad 1.07 | tok/s 27068
step   1410 | loss 1.2367 | lr 2.32e-04 | grad 1.27 | tok/s 28147
step   1420 | loss 1.2119 | lr 2.32e-04 | grad 1.12 | tok/s 28854
step   1430 | loss 1.4093 | lr 2.32e-04 | grad 1.91 | tok/s 27504
step   1440 | loss 1.4516 | lr 2.32e-04 | grad 1.71 | tok/s 28081
step   1450 | loss 1.3012 | lr 2.32e-04 | grad 3.30 | tok/s 28321
step   1460 | loss 1.3860 | lr 2.32e-04 | grad 1.59 | tok/s 28160
step   1470 | loss 1.6272 | lr 2.32e-04 | grad 2.84 | tok/s 27753
step   1480 | loss 1.3703 | lr 2.32e-04 | grad 1.45 | tok/s 28678
step   1490 | loss 1.4156 | lr 2.32e-04 | grad 1.68 | tok/s 28460
step   1500 | loss 1.3716 | lr 2.32e-04 | grad 1.20 | tok/s 28222
step   1510 | loss 1.3168 | lr 2.32e-04 | grad 1.16 | tok/s 27869
step   1520 | loss 1.2820 | lr 2.32e-04 | grad 4.97 | tok/s 28974
step   1530 | loss 1.7810 | lr 2.32e-04 | grad 2.06 | tok/s 28118
step   1540 | loss 1.3691 | lr 2.32e-04 | grad 1.32 | tok/s 27797
step   1550 | loss 1.4461 | lr 2.32e-04 | grad 1.60 | tok/s 28499
step   1560 | loss 1.2814 | lr 2.32e-04 | grad 1.64 | tok/s 27884
step   1570 | loss 1.4355 | lr 2.32e-04 | grad 1.02 | tok/s 27664
step   1580 | loss 1.2422 | lr 2.32e-04 | grad 1.18 | tok/s 28921
step   1590 | loss 1.4152 | lr 2.32e-04 | grad 1.60 | tok/s 28304
step   1600 | loss 1.3480 | lr 2.32e-04 | grad 1.11 | tok/s 28638
step   1610 | loss 1.3181 | lr 2.32e-04 | grad 1.25 | tok/s 27419
step   1620 | loss 1.3632 | lr 2.32e-04 | grad 3.38 | tok/s 27734
step   1630 | loss 1.3994 | lr 2.32e-04 | grad 1.82 | tok/s 27789
step   1640 | loss 1.3670 | lr 2.32e-04 | grad 2.53 | tok/s 28736
step   1650 | loss 1.6054 | lr 2.32e-04 | grad 1.99 | tok/s 28378
step   1660 | loss 1.2680 | lr 2.32e-04 | grad 1.27 | tok/s 28064
step   1670 | loss 1.4173 | lr 2.32e-04 | grad 1.87 | tok/s 28436
step   1680 | loss 1.4373 | lr 2.32e-04 | grad 1.39 | tok/s 28187
step   1690 | loss 1.3169 | lr 2.32e-04 | grad 2.36 | tok/s 27980
step   1700 | loss 1.4450 | lr 2.32e-04 | grad 1.20 | tok/s 28041
step   1710 | loss 1.3853 | lr 2.32e-04 | grad 1.29 | tok/s 28009
step   1720 | loss 1.3776 | lr 2.32e-04 | grad 1.42 | tok/s 27649
step   1730 | loss 1.4467 | lr 2.32e-04 | grad 1.38 | tok/s 28062
step   1740 | loss 1.5044 | lr 2.32e-04 | grad 2.39 | tok/s 27887
step   1750 | loss 1.3664 | lr 2.32e-04 | grad 2.27 | tok/s 27743
step   1760 | loss 1.4717 | lr 2.32e-04 | grad 1.06 | tok/s 27649
step   1770 | loss 1.3725 | lr 2.32e-04 | grad 2.55 | tok/s 28417
step   1780 | loss 1.2858 | lr 2.32e-04 | grad 1.38 | tok/s 28581
step   1790 | loss 1.2868 | lr 2.32e-04 | grad 0.98 | tok/s 27703
step   1800 | loss 1.3548 | lr 2.32e-04 | grad 0.97 | tok/s 27821
step   1810 | loss 1.4737 | lr 2.32e-04 | grad 1.55 | tok/s 27887
step   1820 | loss 1.4637 | lr 2.32e-04 | grad 1.46 | tok/s 27683
step   1830 | loss 1.2926 | lr 2.32e-04 | grad 1.41 | tok/s 28056
step   1840 | loss 1.2977 | lr 2.32e-04 | grad 1.85 | tok/s 28737
step   1850 | loss 1.3635 | lr 2.32e-04 | grad 1.34 | tok/s 28470
step   1860 | loss 1.4565 | lr 2.32e-04 | grad 2.36 | tok/s 28199
step   1870 | loss 1.3329 | lr 2.32e-04 | grad 1.41 | tok/s 28293
step   1880 | loss 1.4537 | lr 2.32e-04 | grad 1.04 | tok/s 28080
step   1890 | loss 1.2151 | lr 2.32e-04 | grad 1.36 | tok/s 28483
step   1900 | loss 1.2979 | lr 2.32e-04 | grad 1.19 | tok/s 28460
step   1910 | loss 1.3856 | lr 2.32e-04 | grad 1.40 | tok/s 29035
step   1920 | loss 1.3362 | lr 2.32e-04 | grad 2.27 | tok/s 28347
step   1930 | loss 1.3688 | lr 2.32e-04 | grad 1.66 | tok/s 28540
step   1940 | loss 1.2429 | lr 2.32e-04 | grad 1.24 | tok/s 28040
step   1950 | loss 1.4460 | lr 2.32e-04 | grad 4.00 | tok/s 28413
step   1960 | loss 1.3056 | lr 2.32e-04 | grad 1.79 | tok/s 27887
step   1970 | loss 1.2852 | lr 2.32e-04 | grad 1.16 | tok/s 28760
step   1980 | loss 1.0978 | lr 2.32e-04 | grad 1.80 | tok/s 29285
step   1990 | loss 1.1280 | lr 2.32e-04 | grad 4.91 | tok/s 29242
step   2000 | loss 1.3052 | lr 2.32e-04 | grad 1.27 | tok/s 28955
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3052.pt
step   2010 | loss 1.1551 | lr 2.32e-04 | grad 1.29 | tok/s 21985
step   2020 | loss 1.3871 | lr 2.32e-04 | grad 1.74 | tok/s 28129
step   2030 | loss 1.4044 | lr 2.32e-04 | grad 1.25 | tok/s 28568
step   2040 | loss 1.3733 | lr 2.32e-04 | grad 1.99 | tok/s 28300
step   2050 | loss 1.3891 | lr 2.32e-04 | grad 1.41 | tok/s 28281
step   2060 | loss 1.3332 | lr 2.32e-04 | grad 1.34 | tok/s 27962
step   2070 | loss 1.2057 | lr 2.32e-04 | grad 1.30 | tok/s 28538
step   2080 | loss 1.2464 | lr 2.32e-04 | grad 1.64 | tok/s 28099
step   2090 | loss 1.1214 | lr 2.32e-04 | grad 1.84 | tok/s 28528
step   2100 | loss 1.4224 | lr 2.32e-04 | grad 1.39 | tok/s 27981
step   2110 | loss 1.4317 | lr 2.32e-04 | grad 1.99 | tok/s 28222
step   2120 | loss 1.4260 | lr 2.32e-04 | grad 2.81 | tok/s 27931
step   2130 | loss 1.4735 | lr 2.32e-04 | grad 1.27 | tok/s 28409
step   2140 | loss 1.3313 | lr 2.32e-04 | grad 1.83 | tok/s 28201
step   2150 | loss 1.4952 | lr 2.32e-04 | grad 1.95 | tok/s 28867
step   2160 | loss 1.1645 | lr 2.32e-04 | grad 1.12 | tok/s 28995
step   2170 | loss 1.1316 | lr 2.32e-04 | grad 0.97 | tok/s 29369
step   2180 | loss 1.1249 | lr 2.32e-04 | grad 1.09 | tok/s 29356
step   2190 | loss 1.2156 | lr 2.32e-04 | grad 2.92 | tok/s 28761
step   2200 | loss 1.3383 | lr 2.32e-04 | grad 3.41 | tok/s 28804
step   2210 | loss 1.3387 | lr 2.32e-04 | grad 1.34 | tok/s 28993
step   2220 | loss 1.4217 | lr 2.32e-04 | grad 1.26 | tok/s 28574
step   2230 | loss 1.2759 | lr 2.32e-04 | grad 1.21 | tok/s 28224
step   2240 | loss 1.4109 | lr 2.32e-04 | grad 3.06 | tok/s 28255
step   2250 | loss 1.3313 | lr 2.32e-04 | grad 1.48 | tok/s 27968
step   2260 | loss 1.3032 | lr 2.32e-04 | grad 1.38 | tok/s 27820
step   2270 | loss 1.3434 | lr 2.32e-04 | grad 1.42 | tok/s 28578
step   2280 | loss 1.3046 | lr 2.32e-04 | grad 1.09 | tok/s 29256
step   2290 | loss 1.2401 | lr 2.32e-04 | grad 1.03 | tok/s 29258
step   2300 | loss 1.2937 | lr 2.32e-04 | grad 1.39 | tok/s 28874
step   2310 | loss 1.3742 | lr 2.32e-04 | grad 1.32 | tok/s 28300
step   2320 | loss 1.6533 | lr 2.32e-04 | grad 3.06 | tok/s 28389
step   2330 | loss 1.3886 | lr 2.32e-04 | grad 1.41 | tok/s 28429
step   2340 | loss 1.2973 | lr 2.32e-04 | grad 1.27 | tok/s 28140
step   2350 | loss 1.2958 | lr 2.32e-04 | grad 1.68 | tok/s 27916
step   2360 | loss 1.3896 | lr 2.32e-04 | grad 2.91 | tok/s 28012
step   2370 | loss 1.3194 | lr 2.32e-04 | grad 1.59 | tok/s 27837
step   2380 | loss 1.3913 | lr 2.32e-04 | grad 1.12 | tok/s 28276
step   2390 | loss 1.2885 | lr 2.32e-04 | grad 1.86 | tok/s 27636
step   2400 | loss 1.4373 | lr 2.32e-04 | grad 1.42 | tok/s 28459
step   2410 | loss 1.2409 | lr 2.32e-04 | grad 1.92 | tok/s 28197
step   2420 | loss 1.3614 | lr 2.32e-04 | grad 2.19 | tok/s 28538
step   2430 | loss 1.0679 | lr 2.32e-04 | grad 2.00 | tok/s 29298
step   2440 | loss 1.2546 | lr 2.32e-04 | grad 1.27 | tok/s 28143
step   2450 | loss 1.2673 | lr 2.32e-04 | grad 1.30 | tok/s 27833
step   2460 | loss 1.3266 | lr 2.32e-04 | grad 1.80 | tok/s 28492
step   2470 | loss 1.3334 | lr 2.32e-04 | grad 1.49 | tok/s 28499
step   2480 | loss 1.4981 | lr 2.32e-04 | grad 2.75 | tok/s 27981
step   2490 | loss 1.3927 | lr 2.32e-04 | grad 2.36 | tok/s 27651
step   2500 | loss 1.3531 | lr 2.32e-04 | grad 1.55 | tok/s 27916
step   2510 | loss 1.6656 | lr 2.32e-04 | grad 1.67 | tok/s 28168
step   2520 | loss 1.4277 | lr 2.32e-04 | grad 1.76 | tok/s 28609
step   2530 | loss 1.3785 | lr 2.32e-04 | grad 1.82 | tok/s 27847
step   2540 | loss 1.3516 | lr 2.32e-04 | grad 1.12 | tok/s 28411
step   2550 | loss 1.2861 | lr 2.32e-04 | grad 1.58 | tok/s 28027
step   2560 | loss 1.3927 | lr 2.32e-04 | grad 1.09 | tok/s 28816
step   2570 | loss 1.2949 | lr 2.32e-04 | grad 1.15 | tok/s 28698
step   2580 | loss 1.2243 | lr 2.32e-04 | grad 1.31 | tok/s 29266
step   2590 | loss 1.3309 | lr 2.32e-04 | grad 1.79 | tok/s 28476
step   2600 | loss 1.3197 | lr 2.32e-04 | grad 1.65 | tok/s 27908
step   2610 | loss 1.3371 | lr 2.32e-04 | grad 1.96 | tok/s 27173
step   2620 | loss 1.6000 | lr 2.32e-04 | grad 1.04 | tok/s 28962
step   2630 | loss 1.2412 | lr 2.32e-04 | grad 1.07 | tok/s 29297
step   2640 | loss 1.2031 | lr 2.32e-04 | grad 0.95 | tok/s 29306
step   2650 | loss 1.1858 | lr 2.32e-04 | grad 0.97 | tok/s 29276
step   2660 | loss 1.3358 | lr 2.32e-04 | grad 1.37 | tok/s 28021
step   2670 | loss 1.4573 | lr 2.32e-04 | grad 2.36 | tok/s 27763
step   2680 | loss 1.6497 | lr 2.32e-04 | grad 3.16 | tok/s 28769
step   2690 | loss 1.2705 | lr 2.32e-04 | grad 1.10 | tok/s 28050
step   2700 | loss 1.2067 | lr 2.32e-04 | grad 1.22 | tok/s 28438
step   2710 | loss 1.4131 | lr 2.32e-04 | grad 1.20 | tok/s 28012
step   2720 | loss 1.4674 | lr 2.32e-04 | grad 1.38 | tok/s 27595
step   2730 | loss 1.3407 | lr 2.32e-04 | grad 1.30 | tok/s 28619
step   2740 | loss 1.2791 | lr 2.32e-04 | grad 1.05 | tok/s 27729
step   2750 | loss 1.2150 | lr 2.32e-04 | grad 1.25 | tok/s 28008
step   2760 | loss 1.6709 | lr 2.32e-04 | grad 3.03 | tok/s 28203
step   2770 | loss 1.2289 | lr 2.32e-04 | grad 0.95 | tok/s 28753
step   2780 | loss 1.5568 | lr 2.32e-04 | grad 1.94 | tok/s 28604
step   2790 | loss 1.3540 | lr 2.32e-04 | grad 1.54 | tok/s 28646
step   2800 | loss 1.3117 | lr 2.32e-04 | grad 1.30 | tok/s 28273
step   2810 | loss 1.2500 | lr 2.32e-04 | grad 1.90 | tok/s 28522
step   2820 | loss 1.2555 | lr 2.32e-04 | grad 2.31 | tok/s 28148
step   2830 | loss 1.4314 | lr 2.32e-04 | grad 6.31 | tok/s 28584
step   2840 | loss 1.5478 | lr 2.32e-04 | grad 1.61 | tok/s 28410
step   2850 | loss 1.4867 | lr 2.32e-04 | grad 1.43 | tok/s 28753
step   2860 | loss 1.2761 | lr 2.32e-04 | grad 1.14 | tok/s 27882
step   2870 | loss 1.4508 | lr 2.32e-04 | grad 3.22 | tok/s 27956
step   2880 | loss 1.3130 | lr 2.32e-04 | grad 1.09 | tok/s 28522
step   2890 | loss 1.3518 | lr 2.32e-04 | grad 1.12 | tok/s 28119
step   2900 | loss 1.3314 | lr 2.32e-04 | grad 1.98 | tok/s 27969
step   2910 | loss 1.3214 | lr 2.32e-04 | grad 1.91 | tok/s 28045
step   2920 | loss 1.4458 | lr 2.32e-04 | grad 2.30 | tok/s 27812
step   2930 | loss 1.3216 | lr 2.32e-04 | grad 1.23 | tok/s 28074
step   2940 | loss 1.3136 | lr 2.32e-04 | grad 1.16 | tok/s 28643
step   2950 | loss 1.2303 | lr 2.32e-04 | grad 1.27 | tok/s 28286
step   2960 | loss 1.2685 | lr 2.32e-04 | grad 4.28 | tok/s 28363
step   2970 | loss 1.2891 | lr 2.32e-04 | grad 1.63 | tok/s 28287
step   2980 | loss 1.3737 | lr 2.32e-04 | grad 2.14 | tok/s 28497
step   2990 | loss 1.3345 | lr 2.32e-04 | grad 1.27 | tok/s 28131
step   3000 | loss 1.3009 | lr 2.32e-04 | grad 1.48 | tok/s 27605
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3009.pt
step   3010 | loss 1.3610 | lr 2.32e-04 | grad 1.66 | tok/s 20703
step   3020 | loss 1.2429 | lr 2.32e-04 | grad 1.09 | tok/s 28123
step   3030 | loss 1.2888 | lr 2.32e-04 | grad 1.18 | tok/s 28491
step   3040 | loss 1.2252 | lr 2.32e-04 | grad 1.18 | tok/s 28760
step   3050 | loss 1.3091 | lr 2.32e-04 | grad 1.11 | tok/s 28086
step   3060 | loss 1.2693 | lr 2.32e-04 | grad 1.00 | tok/s 28137
step   3070 | loss 1.2945 | lr 2.32e-04 | grad 1.14 | tok/s 28143
step   3080 | loss 1.5477 | lr 2.32e-04 | grad 3.44 | tok/s 28354
step   3090 | loss 1.6963 | lr 2.32e-04 | grad 1.34 | tok/s 29290
step   3100 | loss 1.3869 | lr 2.32e-04 | grad 1.67 | tok/s 29290
step   3110 | loss 1.4214 | lr 2.32e-04 | grad 1.52 | tok/s 28229
step   3120 | loss 1.7899 | lr 2.32e-04 | grad 1.26 | tok/s 28010
step   3130 | loss 1.1887 | lr 2.32e-04 | grad 1.41 | tok/s 28450
step   3140 | loss 1.5646 | lr 2.32e-04 | grad 4.38 | tok/s 28366
step   3150 | loss 1.3057 | lr 2.32e-04 | grad 0.98 | tok/s 27913
step   3160 | loss 1.5677 | lr 2.32e-04 | grad 1.48 | tok/s 28744
step   3170 | loss 1.2895 | lr 2.32e-04 | grad 1.52 | tok/s 28795

Training complete! Final step: 3173
