Using device: cuda
Output directory: benchmark_results/cmaes_4d/mamba2_480M_converge0.01_20260202_091709/eval_21/levelmamba2_100m_20260202_101757
Model: Level mamba2, 608,185,980 parameters
Using schedule-free AdamW (lr=0.00012820091287877455)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.9955 | lr 1.28e-04 | grad 10.06 | tok/s 6871
step     20 | loss 2.7262 | lr 1.28e-04 | grad 5.34 | tok/s 16291
step     30 | loss 2.2464 | lr 1.28e-04 | grad 7.78 | tok/s 16085
step     40 | loss 2.6602 | lr 1.28e-04 | grad 31.00 | tok/s 15670
step     50 | loss 2.6607 | lr 1.28e-04 | grad 3.75 | tok/s 15330
step     60 | loss 2.1596 | lr 1.28e-04 | grad 2.03 | tok/s 15489
step     70 | loss 2.2115 | lr 1.28e-04 | grad 3.80 | tok/s 15511
step     80 | loss 1.9395 | lr 1.28e-04 | grad 2.20 | tok/s 14815
step     90 | loss 1.9096 | lr 1.28e-04 | grad 2.09 | tok/s 15116
step    100 | loss 1.8269 | lr 1.28e-04 | grad 2.39 | tok/s 15416
step    110 | loss 1.7872 | lr 1.28e-04 | grad 3.92 | tok/s 15625
step    120 | loss 1.8410 | lr 1.28e-04 | grad 3.59 | tok/s 15413
step    130 | loss 1.8190 | lr 1.28e-04 | grad 2.19 | tok/s 15538
step    140 | loss 1.9193 | lr 1.28e-04 | grad 2.59 | tok/s 15059
step    150 | loss 1.6192 | lr 1.28e-04 | grad 2.17 | tok/s 15224
step    160 | loss 1.5075 | lr 1.28e-04 | grad 2.09 | tok/s 15937
step    170 | loss 1.6282 | lr 1.28e-04 | grad 2.03 | tok/s 15329
step    180 | loss 1.7125 | lr 1.28e-04 | grad 2.20 | tok/s 15158
step    190 | loss 1.7832 | lr 1.28e-04 | grad 3.16 | tok/s 15268
step    200 | loss 1.8622 | lr 1.28e-04 | grad 2.36 | tok/s 15619
step    210 | loss 1.8685 | lr 1.28e-04 | grad 2.09 | tok/s 15252
step    220 | loss 1.5320 | lr 1.28e-04 | grad 1.69 | tok/s 15116
step    230 | loss 1.8557 | lr 1.28e-04 | grad 2.02 | tok/s 15397
step    240 | loss 1.6553 | lr 1.28e-04 | grad 1.80 | tok/s 15081
step    250 | loss 1.5982 | lr 1.28e-04 | grad 1.97 | tok/s 15096
step    260 | loss 1.8708 | lr 1.28e-04 | grad 2.94 | tok/s 15461
step    270 | loss 1.6002 | lr 1.28e-04 | grad 3.48 | tok/s 15502
step    280 | loss 1.8742 | lr 1.28e-04 | grad 3.42 | tok/s 15298
step    290 | loss 1.4027 | lr 1.28e-04 | grad 1.66 | tok/s 15784
step    300 | loss 1.7500 | lr 1.28e-04 | grad 4.75 | tok/s 15833
step    310 | loss 1.8949 | lr 1.28e-04 | grad 2.33 | tok/s 15324
step    320 | loss 1.6611 | lr 1.28e-04 | grad 1.98 | tok/s 15669
step    330 | loss 1.5030 | lr 1.28e-04 | grad 2.41 | tok/s 15549
step    340 | loss 1.5963 | lr 1.28e-04 | grad 1.68 | tok/s 15129
step    350 | loss 1.7865 | lr 1.28e-04 | grad 3.34 | tok/s 15497
step    360 | loss 1.6747 | lr 1.28e-04 | grad 2.42 | tok/s 15319
step    370 | loss 1.7050 | lr 1.28e-04 | grad 2.02 | tok/s 15454
step    380 | loss 1.6706 | lr 1.28e-04 | grad 3.33 | tok/s 15243
step    390 | loss 1.5994 | lr 1.28e-04 | grad 4.22 | tok/s 15227
step    400 | loss 1.6220 | lr 1.28e-04 | grad 2.22 | tok/s 15474
step    410 | loss 1.5735 | lr 1.28e-04 | grad 2.58 | tok/s 15470
step    420 | loss 1.5211 | lr 1.28e-04 | grad 2.73 | tok/s 15357
step    430 | loss 2.1213 | lr 1.28e-04 | grad 3.05 | tok/s 15623
step    440 | loss 1.8890 | lr 1.28e-04 | grad 2.62 | tok/s 15921
step    450 | loss 1.5289 | lr 1.28e-04 | grad 1.80 | tok/s 15253
step    460 | loss 1.6153 | lr 1.28e-04 | grad 1.90 | tok/s 15572
step    470 | loss 1.7611 | lr 1.28e-04 | grad 2.67 | tok/s 15194
step    480 | loss 1.5155 | lr 1.28e-04 | grad 2.11 | tok/s 15417
step    490 | loss 1.5647 | lr 1.28e-04 | grad 2.17 | tok/s 14841
step    500 | loss 1.4849 | lr 1.28e-04 | grad 2.03 | tok/s 15603
step    510 | loss 1.3549 | lr 1.28e-04 | grad 2.45 | tok/s 15841
step    520 | loss 1.6307 | lr 1.28e-04 | grad 2.38 | tok/s 14997
step    530 | loss 1.6223 | lr 1.28e-04 | grad 2.14 | tok/s 15625
step    540 | loss 1.6226 | lr 1.28e-04 | grad 1.88 | tok/s 15175
step    550 | loss 1.5603 | lr 1.28e-04 | grad 4.19 | tok/s 15165
step    560 | loss 2.1203 | lr 1.28e-04 | grad 5.47 | tok/s 15460
step    570 | loss 1.7571 | lr 1.28e-04 | grad 2.77 | tok/s 15428
step    580 | loss 1.6251 | lr 1.28e-04 | grad 2.78 | tok/s 15655
step    590 | loss 1.6627 | lr 1.28e-04 | grad 2.03 | tok/s 15427
step    600 | loss 1.7170 | lr 1.28e-04 | grad 2.69 | tok/s 15230
step    610 | loss 1.5716 | lr 1.28e-04 | grad 1.84 | tok/s 15393
step    620 | loss 1.5024 | lr 1.28e-04 | grad 1.82 | tok/s 15967
step    630 | loss 1.3743 | lr 1.28e-04 | grad 1.94 | tok/s 15976
step    640 | loss 1.3782 | lr 1.28e-04 | grad 2.89 | tok/s 15771
step    650 | loss 1.6154 | lr 1.28e-04 | grad 2.98 | tok/s 15262
step    660 | loss 1.6263 | lr 1.28e-04 | grad 2.34 | tok/s 15435
step    670 | loss 1.6249 | lr 1.28e-04 | grad 2.28 | tok/s 15280
step    680 | loss 1.6387 | lr 1.28e-04 | grad 6.16 | tok/s 15280
step    690 | loss 1.5127 | lr 1.28e-04 | grad 1.67 | tok/s 15159
step    700 | loss 1.4563 | lr 1.28e-04 | grad 1.55 | tok/s 15343
step    710 | loss 1.5412 | lr 1.28e-04 | grad 2.06 | tok/s 15410
step    720 | loss 1.6028 | lr 1.28e-04 | grad 2.48 | tok/s 15315
step    730 | loss 1.6079 | lr 1.28e-04 | grad 6.97 | tok/s 15688
step    740 | loss 1.5236 | lr 1.28e-04 | grad 3.91 | tok/s 15342
step    750 | loss 1.5005 | lr 1.28e-04 | grad 1.83 | tok/s 14972
step    760 | loss 1.4247 | lr 1.28e-04 | grad 1.77 | tok/s 15351
step    770 | loss 1.6431 | lr 1.28e-04 | grad 1.80 | tok/s 15236
step    780 | loss 1.4969 | lr 1.28e-04 | grad 1.80 | tok/s 15299
step    790 | loss 1.6323 | lr 1.28e-04 | grad 2.67 | tok/s 15500
step    800 | loss 1.4927 | lr 1.28e-04 | grad 1.98 | tok/s 15325
step    810 | loss 1.5740 | lr 1.28e-04 | grad 3.80 | tok/s 15561
step    820 | loss 1.6501 | lr 1.28e-04 | grad 3.25 | tok/s 15245
step    830 | loss 1.5901 | lr 1.28e-04 | grad 2.44 | tok/s 14883
step    840 | loss 1.5110 | lr 1.28e-04 | grad 2.08 | tok/s 15443
step    850 | loss 1.5619 | lr 1.28e-04 | grad 2.25 | tok/s 15694
step    860 | loss 1.5513 | lr 1.28e-04 | grad 4.84 | tok/s 14962
step    870 | loss 1.5350 | lr 1.28e-04 | grad 3.14 | tok/s 15448
step    880 | loss 1.7659 | lr 1.28e-04 | grad 2.62 | tok/s 15369
step    890 | loss 1.5399 | lr 1.28e-04 | grad 3.25 | tok/s 15392
step    900 | loss 1.5694 | lr 1.28e-04 | grad 1.87 | tok/s 15021
step    910 | loss 1.4972 | lr 1.28e-04 | grad 1.94 | tok/s 15215
step    920 | loss 1.6296 | lr 1.28e-04 | grad 2.84 | tok/s 15286
step    930 | loss 1.6439 | lr 1.28e-04 | grad 3.70 | tok/s 15244
step    940 | loss 1.5506 | lr 1.28e-04 | grad 3.42 | tok/s 15015
step    950 | loss 1.4579 | lr 1.28e-04 | grad 2.88 | tok/s 15380
step    960 | loss 1.5920 | lr 1.28e-04 | grad 5.59 | tok/s 15477
step    970 | loss 1.5563 | lr 1.28e-04 | grad 1.63 | tok/s 15072
step    980 | loss 1.4769 | lr 1.28e-04 | grad 2.61 | tok/s 15589
step    990 | loss 1.5886 | lr 1.28e-04 | grad 1.95 | tok/s 15278
step   1000 | loss 1.5020 | lr 1.28e-04 | grad 2.45 | tok/s 15182
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5020.pt
step   1010 | loss 1.4647 | lr 1.28e-04 | grad 2.19 | tok/s 11609
step   1020 | loss 1.6101 | lr 1.28e-04 | grad 14.62 | tok/s 15608
step   1030 | loss 1.4232 | lr 1.28e-04 | grad 1.53 | tok/s 15195
step   1040 | loss 1.6434 | lr 1.28e-04 | grad 1.81 | tok/s 15720
step   1050 | loss 1.3079 | lr 1.28e-04 | grad 2.14 | tok/s 15803
step   1060 | loss 1.3704 | lr 1.28e-04 | grad 1.53 | tok/s 15845
step   1070 | loss 1.3759 | lr 1.28e-04 | grad 3.58 | tok/s 15603
step   1080 | loss 1.5232 | lr 1.28e-04 | grad 1.77 | tok/s 15353
step   1090 | loss 1.5473 | lr 1.28e-04 | grad 2.69 | tok/s 15558
step   1100 | loss 1.3908 | lr 1.28e-04 | grad 2.11 | tok/s 15315
step   1110 | loss 1.3315 | lr 1.28e-04 | grad 2.00 | tok/s 15347
step   1120 | loss 1.5618 | lr 1.28e-04 | grad 1.73 | tok/s 15135
step   1130 | loss 1.5955 | lr 1.28e-04 | grad 2.31 | tok/s 15347
step   1140 | loss 1.6395 | lr 1.28e-04 | grad 1.95 | tok/s 15617
step   1150 | loss 1.2394 | lr 1.28e-04 | grad 1.46 | tok/s 15961
step   1160 | loss 1.2616 | lr 1.28e-04 | grad 2.30 | tok/s 15910
step   1170 | loss 1.6192 | lr 1.28e-04 | grad 1.98 | tok/s 15834
step   1180 | loss 1.5315 | lr 1.28e-04 | grad 2.30 | tok/s 15519
step   1190 | loss 1.4986 | lr 1.28e-04 | grad 1.88 | tok/s 15372
step   1200 | loss 1.5110 | lr 1.28e-04 | grad 1.86 | tok/s 15369
step   1210 | loss 1.4475 | lr 1.28e-04 | grad 1.44 | tok/s 15970
step   1220 | loss 1.3764 | lr 1.28e-04 | grad 2.27 | tok/s 15924
step   1230 | loss 1.8662 | lr 1.28e-04 | grad 2.58 | tok/s 15269
step   1240 | loss 1.4870 | lr 1.28e-04 | grad 2.11 | tok/s 15475
step   1250 | loss 1.5540 | lr 1.28e-04 | grad 5.72 | tok/s 15125
step   1260 | loss 1.5366 | lr 1.28e-04 | grad 2.47 | tok/s 15282
step   1270 | loss 1.5100 | lr 1.28e-04 | grad 2.53 | tok/s 15197
step   1280 | loss 1.4230 | lr 1.28e-04 | grad 2.25 | tok/s 15379
step   1290 | loss 1.5054 | lr 1.28e-04 | grad 2.31 | tok/s 15808
step   1300 | loss 1.3970 | lr 1.28e-04 | grad 2.33 | tok/s 15476
step   1310 | loss 1.5350 | lr 1.28e-04 | grad 2.83 | tok/s 15567
step   1320 | loss 1.5551 | lr 1.28e-04 | grad 1.88 | tok/s 15102
step   1330 | loss 1.6994 | lr 1.28e-04 | grad 2.45 | tok/s 15156
step   1340 | loss 1.5956 | lr 1.28e-04 | grad 2.02 | tok/s 15339
step   1350 | loss 1.4088 | lr 1.28e-04 | grad 2.23 | tok/s 15441
step   1360 | loss 1.4494 | lr 1.28e-04 | grad 1.82 | tok/s 15748
step   1370 | loss 1.4263 | lr 1.28e-04 | grad 2.25 | tok/s 15871
step   1380 | loss 1.5155 | lr 1.28e-04 | grad 1.90 | tok/s 15186
step   1390 | loss 1.6241 | lr 1.28e-04 | grad 1.43 | tok/s 15651
step   1400 | loss 1.3371 | lr 1.28e-04 | grad 1.60 | tok/s 16005
step   1410 | loss 1.5044 | lr 1.28e-04 | grad 2.08 | tok/s 15349
step   1420 | loss 1.7397 | lr 1.28e-04 | grad 1.70 | tok/s 15628
step   1430 | loss 1.4556 | lr 1.28e-04 | grad 2.34 | tok/s 15608
step   1440 | loss 1.5700 | lr 1.28e-04 | grad 2.53 | tok/s 15138
step   1450 | loss 1.3732 | lr 1.28e-04 | grad 1.70 | tok/s 15415
step   1460 | loss 1.6263 | lr 1.28e-04 | grad 1.91 | tok/s 15193
step   1470 | loss 1.6195 | lr 1.28e-04 | grad 4.59 | tok/s 15580
step   1480 | loss 1.5491 | lr 1.28e-04 | grad 1.75 | tok/s 15658
step   1490 | loss 1.3994 | lr 1.28e-04 | grad 7.12 | tok/s 15414
step   1500 | loss 1.6594 | lr 1.28e-04 | grad 1.73 | tok/s 15517
step   1510 | loss 1.5243 | lr 1.28e-04 | grad 4.34 | tok/s 15457
step   1520 | loss 1.4224 | lr 1.28e-04 | grad 2.03 | tok/s 15429
step   1530 | loss 1.5620 | lr 1.28e-04 | grad 2.33 | tok/s 15381
step   1540 | loss 1.4216 | lr 1.28e-04 | grad 1.84 | tok/s 15055
step   1550 | loss 1.5555 | lr 1.28e-04 | grad 1.61 | tok/s 15297
step   1560 | loss 1.4387 | lr 1.28e-04 | grad 2.02 | tok/s 15646
step   1570 | loss 1.5371 | lr 1.28e-04 | grad 2.36 | tok/s 15499
step   1580 | loss 1.4449 | lr 1.28e-04 | grad 2.45 | tok/s 15489
step   1590 | loss 1.4880 | lr 1.28e-04 | grad 2.22 | tok/s 15358
step   1600 | loss 1.3760 | lr 1.28e-04 | grad 2.95 | tok/s 15076
step   1610 | loss 1.4367 | lr 1.28e-04 | grad 1.53 | tok/s 15537
step   1620 | loss 1.4621 | lr 1.28e-04 | grad 1.67 | tok/s 15471
step   1630 | loss 1.6727 | lr 1.28e-04 | grad 4.72 | tok/s 15488
step   1640 | loss 1.8481 | lr 1.28e-04 | grad 2.19 | tok/s 15965
step   1650 | loss 1.9418 | lr 1.28e-04 | grad 24.62 | tok/s 15408
step   1660 | loss 1.4924 | lr 1.28e-04 | grad 1.52 | tok/s 15380
step   1670 | loss 1.6315 | lr 1.28e-04 | grad 5.91 | tok/s 15523
step   1680 | loss 1.5645 | lr 1.28e-04 | grad 2.48 | tok/s 15775
step   1690 | loss 1.5564 | lr 1.28e-04 | grad 4.62 | tok/s 15081
step   1700 | loss 1.4055 | lr 1.28e-04 | grad 1.77 | tok/s 15180
step   1710 | loss 1.6075 | lr 1.28e-04 | grad 2.69 | tok/s 15471
step   1720 | loss 1.6144 | lr 1.28e-04 | grad 2.14 | tok/s 15581
step   1730 | loss 1.5178 | lr 1.28e-04 | grad 3.73 | tok/s 15476

Training complete! Final step: 1730
