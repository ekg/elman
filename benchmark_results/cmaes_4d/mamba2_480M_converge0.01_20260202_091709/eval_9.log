Using device: cuda
Output directory: benchmark_results/cmaes_4d/mamba2_480M_converge0.01_20260202_091709/eval_9/levelmamba2_100m_20260202_094735
Model: Level mamba2, 457,727,988 parameters
Using schedule-free AdamW (lr=0.00032314506449371007)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 30.0 minutes
step     10 | loss 5.7121 | lr 3.23e-04 | grad 11.44 | tok/s 4541
step     20 | loss 3.0280 | lr 3.23e-04 | grad 3.59 | tok/s 20426
step     30 | loss 3.9686 | lr 3.23e-04 | grad 20.50 | tok/s 20932
step     40 | loss 3.6866 | lr 3.23e-04 | grad 9.62 | tok/s 21057
step     50 | loss 2.6602 | lr 3.23e-04 | grad 3.06 | tok/s 20935
step     60 | loss 2.2916 | lr 3.23e-04 | grad 5.72 | tok/s 20802
step     70 | loss 2.0589 | lr 3.23e-04 | grad 2.09 | tok/s 20733
step     80 | loss 1.9814 | lr 3.23e-04 | grad 3.08 | tok/s 20673
step     90 | loss 1.8634 | lr 3.23e-04 | grad 3.20 | tok/s 20643
step    100 | loss 2.0326 | lr 3.23e-04 | grad 5.56 | tok/s 20542
step    110 | loss 2.7932 | lr 3.23e-04 | grad 4.66 | tok/s 19635
step    120 | loss 1.9353 | lr 3.23e-04 | grad 3.56 | tok/s 19949
step    130 | loss 2.3602 | lr 3.23e-04 | grad 7.84 | tok/s 19786
step    140 | loss 1.4533 | lr 3.23e-04 | grad 2.80 | tok/s 20459
step    150 | loss 2.3328 | lr 3.23e-04 | grad 3.06 | tok/s 20008
step    160 | loss 2.1653 | lr 3.23e-04 | grad 2.47 | tok/s 19420
step    170 | loss 1.7334 | lr 3.23e-04 | grad 2.91 | tok/s 19965
step    180 | loss 1.8295 | lr 3.23e-04 | grad 2.62 | tok/s 19179
step    190 | loss 1.5521 | lr 3.23e-04 | grad 3.66 | tok/s 20371
step    200 | loss 1.6827 | lr 3.23e-04 | grad 3.25 | tok/s 19329
step    210 | loss 2.0746 | lr 3.23e-04 | grad 4.03 | tok/s 19667
step    220 | loss 1.7500 | lr 3.23e-04 | grad 2.08 | tok/s 19401
step    230 | loss 2.0748 | lr 3.23e-04 | grad 2.33 | tok/s 19754
step    240 | loss 1.7243 | lr 3.23e-04 | grad 1.89 | tok/s 19677
step    250 | loss 1.7277 | lr 3.23e-04 | grad 2.34 | tok/s 20153
step    260 | loss 1.7465 | lr 3.23e-04 | grad 2.31 | tok/s 19756
step    270 | loss 1.6292 | lr 3.23e-04 | grad 1.87 | tok/s 18652
step    280 | loss 1.5558 | lr 3.23e-04 | grad 1.99 | tok/s 19314
step    290 | loss 1.8515 | lr 3.23e-04 | grad 1.71 | tok/s 19303
step    300 | loss 1.5718 | lr 3.23e-04 | grad 1.48 | tok/s 19155
step    310 | loss 1.7566 | lr 3.23e-04 | grad 4.12 | tok/s 19271
step    320 | loss 1.6409 | lr 3.23e-04 | grad 2.62 | tok/s 19751
step    330 | loss 1.8762 | lr 3.23e-04 | grad 4.62 | tok/s 19518
step    340 | loss 1.6617 | lr 3.23e-04 | grad 4.81 | tok/s 20335
step    350 | loss 1.5053 | lr 3.23e-04 | grad 2.00 | tok/s 18981
step    360 | loss 1.4344 | lr 3.23e-04 | grad 1.52 | tok/s 20284
step    370 | loss 1.2116 | lr 3.23e-04 | grad 1.45 | tok/s 20457
step    380 | loss 1.0743 | lr 3.23e-04 | grad 1.31 | tok/s 20450
step    390 | loss 1.6170 | lr 3.23e-04 | grad 2.31 | tok/s 19529
step    400 | loss 1.6443 | lr 3.23e-04 | grad 6.38 | tok/s 19503
step    410 | loss 1.5681 | lr 3.23e-04 | grad 1.89 | tok/s 20303
step    420 | loss 1.5350 | lr 3.23e-04 | grad 1.58 | tok/s 20173
step    430 | loss 1.5828 | lr 3.23e-04 | grad 2.42 | tok/s 19580
step    440 | loss 1.5721 | lr 3.23e-04 | grad 1.68 | tok/s 19551
step    450 | loss 1.5182 | lr 3.23e-04 | grad 1.96 | tok/s 19902
step    460 | loss 1.4279 | lr 3.23e-04 | grad 1.38 | tok/s 19711
step    470 | loss 1.5652 | lr 3.23e-04 | grad 1.79 | tok/s 20312
step    480 | loss 1.6261 | lr 3.23e-04 | grad 1.75 | tok/s 19313
step    490 | loss 1.7209 | lr 3.23e-04 | grad 2.80 | tok/s 19789
step    500 | loss 1.6019 | lr 3.23e-04 | grad 1.39 | tok/s 18888
step    510 | loss 1.4698 | lr 3.23e-04 | grad 3.84 | tok/s 19813
step    520 | loss 1.6106 | lr 3.23e-04 | grad 2.38 | tok/s 19375
step    530 | loss 1.5619 | lr 3.23e-04 | grad 1.43 | tok/s 19428
step    540 | loss 1.2379 | lr 3.23e-04 | grad 1.56 | tok/s 19519
step    550 | loss 1.4423 | lr 3.23e-04 | grad 1.55 | tok/s 20465
step    560 | loss 1.2975 | lr 3.23e-04 | grad 1.43 | tok/s 20445
step    570 | loss 1.2641 | lr 3.23e-04 | grad 1.78 | tok/s 20453
step    580 | loss 1.2954 | lr 3.23e-04 | grad 1.50 | tok/s 20448
step    590 | loss 1.2125 | lr 3.23e-04 | grad 1.36 | tok/s 20443
step    600 | loss 1.2676 | lr 3.23e-04 | grad 1.82 | tok/s 20429
step    610 | loss 1.2232 | lr 3.23e-04 | grad 1.02 | tok/s 20463
step    620 | loss 1.6001 | lr 3.23e-04 | grad 3.66 | tok/s 19287
step    630 | loss 1.5952 | lr 3.23e-04 | grad 2.30 | tok/s 19438
step    640 | loss 1.4786 | lr 3.23e-04 | grad 1.63 | tok/s 19844
step    650 | loss 1.5271 | lr 3.23e-04 | grad 1.84 | tok/s 19935
step    660 | loss 1.5016 | lr 3.23e-04 | grad 1.86 | tok/s 19698
step    670 | loss 1.6049 | lr 3.23e-04 | grad 1.31 | tok/s 19179
step    680 | loss 1.4989 | lr 3.23e-04 | grad 1.65 | tok/s 19255
step    690 | loss 1.4507 | lr 3.23e-04 | grad 1.79 | tok/s 19432
step    700 | loss 1.5104 | lr 3.23e-04 | grad 2.64 | tok/s 19303
step    710 | loss 1.2956 | lr 3.23e-04 | grad 1.55 | tok/s 19973
step    720 | loss 1.3571 | lr 3.23e-04 | grad 1.80 | tok/s 19901
step    730 | loss 1.6563 | lr 3.23e-04 | grad 4.88 | tok/s 19973
step    740 | loss 1.5544 | lr 3.23e-04 | grad 1.60 | tok/s 20410
step    750 | loss 1.4127 | lr 3.23e-04 | grad 1.41 | tok/s 20039
step    760 | loss 1.4932 | lr 3.23e-04 | grad 1.42 | tok/s 19726
step    770 | loss 1.4413 | lr 3.23e-04 | grad 2.03 | tok/s 19739
step    780 | loss 1.5268 | lr 3.23e-04 | grad 3.53 | tok/s 20239
step    790 | loss 1.4081 | lr 3.23e-04 | grad 1.26 | tok/s 19891
step    800 | loss 1.1749 | lr 3.23e-04 | grad 3.00 | tok/s 19276
step    810 | loss 1.3731 | lr 3.23e-04 | grad 2.09 | tok/s 19908
step    820 | loss 1.4531 | lr 3.23e-04 | grad 1.50 | tok/s 19091
step    830 | loss 1.5230 | lr 3.23e-04 | grad 1.89 | tok/s 19450
step    840 | loss 1.4472 | lr 3.23e-04 | grad 2.03 | tok/s 19646
step    850 | loss 1.5067 | lr 3.23e-04 | grad 2.55 | tok/s 19861
step    860 | loss 1.3313 | lr 3.23e-04 | grad 1.72 | tok/s 20352
step    870 | loss 1.5131 | lr 3.23e-04 | grad 2.00 | tok/s 19554
step    880 | loss 1.4459 | lr 3.23e-04 | grad 1.55 | tok/s 19730
step    890 | loss 1.4679 | lr 3.23e-04 | grad 2.08 | tok/s 19673
step    900 | loss 1.3956 | lr 3.23e-04 | grad 1.68 | tok/s 19258
step    910 | loss 1.4415 | lr 3.23e-04 | grad 1.92 | tok/s 19684
step    920 | loss 1.3268 | lr 3.23e-04 | grad 1.41 | tok/s 19792
step    930 | loss 1.3101 | lr 3.23e-04 | grad 2.12 | tok/s 19419
step    940 | loss 1.4080 | lr 3.23e-04 | grad 1.32 | tok/s 18935
step    950 | loss 1.4066 | lr 3.23e-04 | grad 1.53 | tok/s 19584
step    960 | loss 1.4020 | lr 3.23e-04 | grad 1.45 | tok/s 19622
step    970 | loss 1.8282 | lr 3.23e-04 | grad 2.70 | tok/s 20432
step    980 | loss 1.5363 | lr 3.23e-04 | grad 1.45 | tok/s 19624
step    990 | loss 1.4878 | lr 3.23e-04 | grad 1.44 | tok/s 19818
step   1000 | loss 1.1551 | lr 3.23e-04 | grad 1.54 | tok/s 19992
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1551.pt
step   1010 | loss 1.2178 | lr 3.23e-04 | grad 1.59 | tok/s 12376
step   1020 | loss 1.4170 | lr 3.23e-04 | grad 2.20 | tok/s 19729
step   1030 | loss 1.9873 | lr 3.23e-04 | grad 3.64 | tok/s 20164
step   1040 | loss 1.4869 | lr 3.23e-04 | grad 1.37 | tok/s 20289
step   1050 | loss 1.1592 | lr 3.23e-04 | grad 1.65 | tok/s 20030
step   1060 | loss 1.3072 | lr 3.23e-04 | grad 1.58 | tok/s 19997
step   1070 | loss 1.2222 | lr 3.23e-04 | grad 1.41 | tok/s 20608
step   1080 | loss 1.1991 | lr 3.23e-04 | grad 1.40 | tok/s 20605
step   1090 | loss 1.1753 | lr 3.23e-04 | grad 1.32 | tok/s 20609
step   1100 | loss 1.1204 | lr 3.23e-04 | grad 1.19 | tok/s 20587
step   1110 | loss 1.3383 | lr 3.23e-04 | grad 1.58 | tok/s 20003
step   1120 | loss 1.5704 | lr 3.23e-04 | grad 1.38 | tok/s 20213
step   1130 | loss 1.7023 | lr 3.23e-04 | grad 1.44 | tok/s 20427
step   1140 | loss 1.5808 | lr 3.23e-04 | grad 2.83 | tok/s 19948
step   1150 | loss 1.5809 | lr 3.23e-04 | grad 3.47 | tok/s 19310
step   1160 | loss 1.4821 | lr 3.23e-04 | grad 1.82 | tok/s 19293
step   1170 | loss 1.3144 | lr 3.23e-04 | grad 1.30 | tok/s 20286
step   1180 | loss 1.5368 | lr 3.23e-04 | grad 2.34 | tok/s 20320
step   1190 | loss 1.1233 | lr 3.23e-04 | grad 1.21 | tok/s 20533
step   1200 | loss 1.2794 | lr 3.23e-04 | grad 1.49 | tok/s 19347
step   1210 | loss 1.3153 | lr 3.23e-04 | grad 1.59 | tok/s 19978
step   1220 | loss 1.3532 | lr 3.23e-04 | grad 1.30 | tok/s 20111
step   1230 | loss 1.2162 | lr 3.23e-04 | grad 2.02 | tok/s 20312
step   1240 | loss 1.4303 | lr 3.23e-04 | grad 2.23 | tok/s 19832
step   1250 | loss 1.3058 | lr 3.23e-04 | grad 1.47 | tok/s 20350
step   1260 | loss 1.3258 | lr 3.23e-04 | grad 1.98 | tok/s 19747
step   1270 | loss 1.3336 | lr 3.23e-04 | grad 2.00 | tok/s 19802
step   1280 | loss 1.3107 | lr 3.23e-04 | grad 1.75 | tok/s 19344
step   1290 | loss 1.5149 | lr 3.23e-04 | grad 8.00 | tok/s 19289
step   1300 | loss 1.4666 | lr 3.23e-04 | grad 1.80 | tok/s 20063
step   1310 | loss 1.4328 | lr 3.23e-04 | grad 3.03 | tok/s 19997
step   1320 | loss 1.3695 | lr 3.23e-04 | grad 1.53 | tok/s 19977
step   1330 | loss 1.4710 | lr 3.23e-04 | grad 1.65 | tok/s 19352
step   1340 | loss 1.3756 | lr 3.23e-04 | grad 1.45 | tok/s 20101
step   1350 | loss 1.4253 | lr 3.23e-04 | grad 1.44 | tok/s 18894
step   1360 | loss 1.4758 | lr 3.23e-04 | grad 3.22 | tok/s 20178
step   1370 | loss 1.4182 | lr 3.23e-04 | grad 1.70 | tok/s 19528
step   1380 | loss 1.2906 | lr 3.23e-04 | grad 2.12 | tok/s 19922
step   1390 | loss 1.4831 | lr 3.23e-04 | grad 1.30 | tok/s 19507
step   1400 | loss 1.3100 | lr 3.23e-04 | grad 1.47 | tok/s 19131
step   1410 | loss 1.0276 | lr 3.23e-04 | grad 1.62 | tok/s 20313
step   1420 | loss 1.6552 | lr 3.23e-04 | grad 1.18 | tok/s 19651
step   1430 | loss 1.4000 | lr 3.23e-04 | grad 1.98 | tok/s 19832
step   1440 | loss 1.3568 | lr 3.23e-04 | grad 1.45 | tok/s 20046
step   1450 | loss 1.4621 | lr 3.23e-04 | grad 2.14 | tok/s 19481
step   1460 | loss 1.3358 | lr 3.23e-04 | grad 1.57 | tok/s 19248
step   1470 | loss 1.3418 | lr 3.23e-04 | grad 2.31 | tok/s 19909
step   1480 | loss 1.5817 | lr 3.23e-04 | grad 5.41 | tok/s 19759
step   1490 | loss 1.5179 | lr 3.23e-04 | grad 1.32 | tok/s 20036
step   1500 | loss 1.2214 | lr 3.23e-04 | grad 1.45 | tok/s 19650
step   1510 | loss 1.4163 | lr 3.23e-04 | grad 1.95 | tok/s 19583
step   1520 | loss 1.3383 | lr 3.23e-04 | grad 1.54 | tok/s 20103
step   1530 | loss 1.4475 | lr 3.23e-04 | grad 1.38 | tok/s 20130
step   1540 | loss 1.4147 | lr 3.23e-04 | grad 3.88 | tok/s 19759
step   1550 | loss 1.1109 | lr 3.23e-04 | grad 1.10 | tok/s 20375
step   1560 | loss 1.2635 | lr 3.23e-04 | grad 1.27 | tok/s 19898
step   1570 | loss 1.1828 | lr 3.23e-04 | grad 1.66 | tok/s 20056
step   1580 | loss 1.3858 | lr 3.23e-04 | grad 2.95 | tok/s 19322
step   1590 | loss 1.1857 | lr 3.23e-04 | grad 4.97 | tok/s 20367
step   1600 | loss 1.7806 | lr 3.23e-04 | grad 2.61 | tok/s 19879
step   1610 | loss 1.9682 | lr 3.23e-04 | grad 2.33 | tok/s 20505
step   1620 | loss 1.6528 | lr 3.23e-04 | grad 2.45 | tok/s 20490
step   1630 | loss 1.4841 | lr 3.23e-04 | grad 2.39 | tok/s 20489
step   1640 | loss 1.3790 | lr 3.23e-04 | grad 2.12 | tok/s 20463
step   1650 | loss 1.3310 | lr 3.23e-04 | grad 1.80 | tok/s 20464
step   1660 | loss 1.4573 | lr 3.23e-04 | grad 2.55 | tok/s 19865
step   1670 | loss 1.3614 | lr 3.23e-04 | grad 1.86 | tok/s 19658
step   1680 | loss 1.4235 | lr 3.23e-04 | grad 2.44 | tok/s 19198
step   1690 | loss 1.2846 | lr 3.23e-04 | grad 1.52 | tok/s 19950
step   1700 | loss 1.1932 | lr 3.23e-04 | grad 2.28 | tok/s 20022
step   1710 | loss 1.4029 | lr 3.23e-04 | grad 1.30 | tok/s 19414
step   1720 | loss 1.3741 | lr 3.23e-04 | grad 4.56 | tok/s 20036
step   1730 | loss 1.3558 | lr 3.23e-04 | grad 1.75 | tok/s 20146
step   1740 | loss 1.2154 | lr 3.23e-04 | grad 1.56 | tok/s 19686
step   1750 | loss 1.3551 | lr 3.23e-04 | grad 1.55 | tok/s 19496
step   1760 | loss 1.5348 | lr 3.23e-04 | grad 1.75 | tok/s 19961
step   1770 | loss 1.6983 | lr 3.23e-04 | grad 1.48 | tok/s 18860
step   1780 | loss 1.2606 | lr 3.23e-04 | grad 1.95 | tok/s 19177
step   1790 | loss 1.1743 | lr 3.23e-04 | grad 1.38 | tok/s 19649
step   1800 | loss 1.3725 | lr 3.23e-04 | grad 1.74 | tok/s 19816
step   1810 | loss 1.4615 | lr 3.23e-04 | grad 2.84 | tok/s 19534
step   1820 | loss 1.3330 | lr 3.23e-04 | grad 1.48 | tok/s 18990
step   1830 | loss 1.2886 | lr 3.23e-04 | grad 1.46 | tok/s 19658
step   1840 | loss 1.3587 | lr 3.23e-04 | grad 1.98 | tok/s 19545
step   1850 | loss 1.3883 | lr 3.23e-04 | grad 1.66 | tok/s 19456
step   1860 | loss 1.3655 | lr 3.23e-04 | grad 2.00 | tok/s 19781
step   1870 | loss 1.3531 | lr 3.23e-04 | grad 1.73 | tok/s 19949
step   1880 | loss 1.2231 | lr 3.23e-04 | grad 1.69 | tok/s 20482
step   1890 | loss 1.1661 | lr 3.23e-04 | grad 1.41 | tok/s 20484
step   1900 | loss 1.1481 | lr 3.23e-04 | grad 1.61 | tok/s 20489
step   1910 | loss 1.1253 | lr 3.23e-04 | grad 1.47 | tok/s 20493
step   1920 | loss 1.1669 | lr 3.23e-04 | grad 2.67 | tok/s 20429
step   1930 | loss 1.4467 | lr 3.23e-04 | grad 1.70 | tok/s 19434
step   1940 | loss 1.3138 | lr 3.23e-04 | grad 1.38 | tok/s 19279
step   1950 | loss 1.4082 | lr 3.23e-04 | grad 2.31 | tok/s 19545
step   1960 | loss 1.4723 | lr 3.23e-04 | grad 1.60 | tok/s 19959
step   1970 | loss 1.3520 | lr 3.23e-04 | grad 2.28 | tok/s 19344
step   1980 | loss 1.4356 | lr 3.23e-04 | grad 3.61 | tok/s 19852
step   1990 | loss 1.1054 | lr 3.23e-04 | grad 1.20 | tok/s 20512
step   2000 | loss 1.1647 | lr 3.23e-04 | grad 2.03 | tok/s 20066
  >>> saved checkpoint: checkpoint_step_002000_loss_1.1647.pt
step   2010 | loss 1.3459 | lr 3.23e-04 | grad 2.42 | tok/s 11559
step   2020 | loss 1.5872 | lr 3.23e-04 | grad 1.87 | tok/s 19455
step   2030 | loss 1.3977 | lr 3.23e-04 | grad 1.33 | tok/s 19845
step   2040 | loss 1.2876 | lr 3.23e-04 | grad 2.23 | tok/s 19818
step   2050 | loss 1.4747 | lr 3.23e-04 | grad 1.77 | tok/s 20105
step   2060 | loss 1.1671 | lr 3.23e-04 | grad 1.23 | tok/s 20114
step   2070 | loss 1.2283 | lr 3.23e-04 | grad 1.48 | tok/s 19050
step   2080 | loss 1.3243 | lr 3.23e-04 | grad 3.72 | tok/s 20330
step   2090 | loss 1.4778 | lr 3.23e-04 | grad 2.48 | tok/s 20530
step   2100 | loss 1.3272 | lr 3.23e-04 | grad 1.55 | tok/s 19496
step   2110 | loss 2.0849 | lr 3.23e-04 | grad 2.62 | tok/s 19938
step   2120 | loss 1.3657 | lr 3.23e-04 | grad 1.48 | tok/s 19306
step   2130 | loss 1.4770 | lr 3.23e-04 | grad 1.28 | tok/s 20037
step   2140 | loss 1.6199 | lr 3.23e-04 | grad 1.45 | tok/s 19948
step   2150 | loss 1.3680 | lr 3.23e-04 | grad 1.40 | tok/s 19905
step   2160 | loss 1.3916 | lr 3.23e-04 | grad 2.05 | tok/s 19880
step   2170 | loss 1.0970 | lr 3.23e-04 | grad 2.64 | tok/s 20496
step   2180 | loss 1.3769 | lr 3.23e-04 | grad 1.23 | tok/s 19699
step   2190 | loss 1.1893 | lr 3.23e-04 | grad 1.59 | tok/s 20500
step   2200 | loss 1.3496 | lr 3.23e-04 | grad 1.97 | tok/s 20011
step   2210 | loss 1.3528 | lr 3.23e-04 | grad 2.66 | tok/s 19260
step   2220 | loss 1.5413 | lr 3.23e-04 | grad 2.09 | tok/s 19481
step   2230 | loss 1.2525 | lr 3.23e-04 | grad 1.44 | tok/s 19880
step   2240 | loss 1.3326 | lr 3.23e-04 | grad 1.52 | tok/s 19568
step   2250 | loss 1.4467 | lr 3.23e-04 | grad 1.51 | tok/s 20191
step   2260 | loss 1.5834 | lr 3.23e-04 | grad 2.56 | tok/s 19091
step   2270 | loss 1.3686 | lr 3.23e-04 | grad 1.67 | tok/s 19731
step   2280 | loss 1.1880 | lr 3.23e-04 | grad 1.37 | tok/s 19244
step   2290 | loss 1.5086 | lr 3.23e-04 | grad 1.91 | tok/s 19443
step   2300 | loss 1.3844 | lr 3.23e-04 | grad 2.12 | tok/s 19983
step   2310 | loss 1.3525 | lr 3.23e-04 | grad 1.60 | tok/s 20547
step   2320 | loss 1.2739 | lr 3.23e-04 | grad 1.20 | tok/s 20534
step   2330 | loss 1.2245 | lr 3.23e-04 | grad 1.57 | tok/s 20522
step   2340 | loss 1.1981 | lr 3.23e-04 | grad 1.55 | tok/s 20525
step   2350 | loss 1.1724 | lr 3.23e-04 | grad 1.20 | tok/s 20533
step   2360 | loss 1.1435 | lr 3.23e-04 | grad 1.23 | tok/s 20552
step   2370 | loss 1.1308 | lr 3.23e-04 | grad 1.49 | tok/s 20515
step   2380 | loss 1.1485 | lr 3.23e-04 | grad 1.27 | tok/s 20515
step   2390 | loss 1.1073 | lr 3.23e-04 | grad 1.70 | tok/s 20523
step   2400 | loss 1.2156 | lr 3.23e-04 | grad 1.93 | tok/s 20279
step   2410 | loss 1.3961 | lr 3.23e-04 | grad 1.22 | tok/s 20090
step   2420 | loss 1.0530 | lr 3.23e-04 | grad 1.43 | tok/s 19623
step   2430 | loss 1.3571 | lr 3.23e-04 | grad 1.74 | tok/s 18929
step   2440 | loss 1.2768 | lr 3.23e-04 | grad 1.16 | tok/s 19858
step   2450 | loss 1.4727 | lr 3.23e-04 | grad 1.63 | tok/s 20120
step   2460 | loss 1.2786 | lr 3.23e-04 | grad 2.06 | tok/s 19843
step   2470 | loss 1.3753 | lr 3.23e-04 | grad 2.75 | tok/s 19840
step   2480 | loss 1.4388 | lr 3.23e-04 | grad 1.90 | tok/s 19925
step   2490 | loss 1.4570 | lr 3.23e-04 | grad 2.22 | tok/s 19908
step   2500 | loss 1.3838 | lr 3.23e-04 | grad 1.20 | tok/s 19735
step   2510 | loss 1.3211 | lr 3.23e-04 | grad 3.97 | tok/s 19492
step   2520 | loss 1.4182 | lr 3.23e-04 | grad 1.60 | tok/s 19819
step   2530 | loss 1.3755 | lr 3.23e-04 | grad 1.23 | tok/s 19169
step   2540 | loss 1.2861 | lr 3.23e-04 | grad 2.03 | tok/s 19566
step   2550 | loss 1.4426 | lr 3.23e-04 | grad 1.48 | tok/s 19503
step   2560 | loss 1.3069 | lr 3.23e-04 | grad 1.38 | tok/s 19606
step   2570 | loss 1.5380 | lr 3.23e-04 | grad 3.00 | tok/s 19569
step   2580 | loss 1.2704 | lr 3.23e-04 | grad 1.63 | tok/s 19224
step   2590 | loss 1.3853 | lr 3.23e-04 | grad 1.23 | tok/s 19523
step   2600 | loss 1.2792 | lr 3.23e-04 | grad 1.30 | tok/s 20136
step   2610 | loss 1.4446 | lr 3.23e-04 | grad 1.73 | tok/s 19499
step   2620 | loss 1.1366 | lr 3.23e-04 | grad 1.66 | tok/s 20207
step   2630 | loss 1.2506 | lr 3.23e-04 | grad 1.52 | tok/s 19619
step   2640 | loss 1.1687 | lr 3.23e-04 | grad 0.99 | tok/s 19834
step   2650 | loss 1.2761 | lr 3.23e-04 | grad 1.56 | tok/s 19800
step   2660 | loss 1.0921 | lr 3.23e-04 | grad 1.38 | tok/s 20046
step   2670 | loss 1.3241 | lr 3.23e-04 | grad 1.87 | tok/s 19450
step   2680 | loss 1.2727 | lr 3.23e-04 | grad 1.31 | tok/s 19138
step   2690 | loss 1.3479 | lr 3.23e-04 | grad 1.77 | tok/s 19736
step   2700 | loss 1.5882 | lr 3.23e-04 | grad 3.98 | tok/s 19854
step   2710 | loss 1.2606 | lr 3.23e-04 | grad 1.46 | tok/s 19928
step   2720 | loss 1.2410 | lr 3.23e-04 | grad 1.26 | tok/s 19539
step   2730 | loss 1.3682 | lr 3.23e-04 | grad 1.90 | tok/s 19636
step   2740 | loss 1.3536 | lr 3.23e-04 | grad 1.50 | tok/s 19801
step   2750 | loss 1.1547 | lr 3.23e-04 | grad 1.50 | tok/s 20183
step   2760 | loss 1.2767 | lr 3.23e-04 | grad 2.53 | tok/s 19992
step   2770 | loss 1.2182 | lr 3.23e-04 | grad 1.58 | tok/s 19855
step   2780 | loss 1.3858 | lr 3.23e-04 | grad 1.45 | tok/s 19606
step   2790 | loss 1.4272 | lr 3.23e-04 | grad 1.34 | tok/s 19070
step   2800 | loss 1.2903 | lr 3.23e-04 | grad 2.47 | tok/s 19652
step   2810 | loss 1.3105 | lr 3.23e-04 | grad 1.45 | tok/s 18631
step   2820 | loss 1.2121 | lr 3.23e-04 | grad 1.19 | tok/s 19664
step   2830 | loss 1.1400 | lr 3.23e-04 | grad 1.10 | tok/s 19457
step   2840 | loss 1.1599 | lr 3.23e-04 | grad 1.26 | tok/s 20410
step   2850 | loss 1.1271 | lr 3.23e-04 | grad 1.94 | tok/s 20026
step   2860 | loss 1.3017 | lr 3.23e-04 | grad 1.33 | tok/s 19639
step   2870 | loss 1.3399 | lr 3.23e-04 | grad 1.42 | tok/s 19130
step   2880 | loss 1.4183 | lr 3.23e-04 | grad 3.41 | tok/s 19834
step   2890 | loss 1.4295 | lr 3.23e-04 | grad 2.20 | tok/s 19121
step   2900 | loss 1.2533 | lr 3.23e-04 | grad 2.03 | tok/s 20358
step   2910 | loss 1.2553 | lr 3.23e-04 | grad 3.56 | tok/s 19335
step   2920 | loss 1.2618 | lr 3.23e-04 | grad 1.87 | tok/s 20098
step   2930 | loss 1.1993 | lr 3.23e-04 | grad 1.21 | tok/s 19649
step   2940 | loss 1.5405 | lr 3.23e-04 | grad 1.30 | tok/s 19849
step   2950 | loss 1.4369 | lr 3.23e-04 | grad 4.91 | tok/s 19267
step   2960 | loss 1.3857 | lr 3.23e-04 | grad 1.39 | tok/s 19625
step   2970 | loss 1.2778 | lr 3.23e-04 | grad 1.36 | tok/s 20012
step   2980 | loss 1.3558 | lr 3.23e-04 | grad 1.38 | tok/s 19950
step   2990 | loss 1.2883 | lr 3.23e-04 | grad 1.56 | tok/s 19897
step   3000 | loss 1.3181 | lr 3.23e-04 | grad 1.30 | tok/s 20056
  >>> saved checkpoint: checkpoint_step_003000_loss_1.3181.pt
step   3010 | loss 1.2638 | lr 3.23e-04 | grad 1.36 | tok/s 11817
step   3020 | loss 1.2412 | lr 3.23e-04 | grad 2.34 | tok/s 19334
step   3030 | loss 1.2408 | lr 3.23e-04 | grad 1.31 | tok/s 20038
step   3040 | loss 1.1758 | lr 3.23e-04 | grad 1.27 | tok/s 20515
step   3050 | loss 1.2390 | lr 3.23e-04 | grad 9.50 | tok/s 20296
step   3060 | loss 1.8670 | lr 3.23e-04 | grad 5.56 | tok/s 19664
step   3070 | loss 1.4269 | lr 3.23e-04 | grad 3.27 | tok/s 19959
step   3080 | loss 1.3419 | lr 3.23e-04 | grad 1.36 | tok/s 19373
step   3090 | loss 1.2432 | lr 3.23e-04 | grad 1.20 | tok/s 19724
step   3100 | loss 1.2995 | lr 3.23e-04 | grad 2.27 | tok/s 20474
step   3110 | loss 1.3402 | lr 3.23e-04 | grad 1.59 | tok/s 19608
step   3120 | loss 1.2845 | lr 3.23e-04 | grad 2.41 | tok/s 19235
step   3130 | loss 1.1625 | lr 3.23e-04 | grad 2.58 | tok/s 20006
step   3140 | loss 1.2977 | lr 3.23e-04 | grad 1.35 | tok/s 18549
step   3150 | loss 1.4445 | lr 3.23e-04 | grad 1.45 | tok/s 20355
step   3160 | loss 1.2334 | lr 3.23e-04 | grad 1.76 | tok/s 20231
step   3170 | loss 1.1063 | lr 3.23e-04 | grad 1.21 | tok/s 20372
step   3180 | loss 1.4625 | lr 3.23e-04 | grad 1.38 | tok/s 20158
step   3190 | loss 1.2579 | lr 3.23e-04 | grad 1.38 | tok/s 19605
step   3200 | loss 1.3612 | lr 3.23e-04 | grad 1.94 | tok/s 20004
step   3210 | loss 1.2110 | lr 3.23e-04 | grad 1.47 | tok/s 20230
step   3220 | loss 1.2849 | lr 3.23e-04 | grad 1.61 | tok/s 19337
step   3230 | loss 1.2299 | lr 3.23e-04 | grad 1.20 | tok/s 19129
step   3240 | loss 1.2468 | lr 3.23e-04 | grad 1.14 | tok/s 19341
step   3250 | loss 1.3051 | lr 3.23e-04 | grad 1.77 | tok/s 19515
step   3260 | loss 1.2913 | lr 3.23e-04 | grad 1.28 | tok/s 19409
step   3270 | loss 1.3840 | lr 3.23e-04 | grad 1.52 | tok/s 19572
step   3280 | loss 1.1565 | lr 3.23e-04 | grad 1.41 | tok/s 19798
step   3290 | loss 1.3777 | lr 3.23e-04 | grad 2.36 | tok/s 20450
step   3300 | loss 1.5389 | lr 3.23e-04 | grad 3.55 | tok/s 19909
step   3310 | loss 1.4993 | lr 3.23e-04 | grad 1.71 | tok/s 19857
step   3320 | loss 1.2546 | lr 3.23e-04 | grad 1.84 | tok/s 18970
step   3330 | loss 1.1493 | lr 3.23e-04 | grad 1.52 | tok/s 20403
step   3340 | loss 1.3046 | lr 3.23e-04 | grad 2.50 | tok/s 20272
step   3350 | loss 1.2878 | lr 3.23e-04 | grad 3.02 | tok/s 19564
step   3360 | loss 1.3152 | lr 3.23e-04 | grad 1.44 | tok/s 19448
step   3370 | loss 1.3659 | lr 3.23e-04 | grad 1.52 | tok/s 20032
step   3380 | loss 1.1573 | lr 3.23e-04 | grad 1.52 | tok/s 19470
step   3390 | loss 1.3342 | lr 3.23e-04 | grad 3.55 | tok/s 19648
step   3400 | loss 1.3601 | lr 3.23e-04 | grad 1.28 | tok/s 19280
step   3410 | loss 1.3581 | lr 3.23e-04 | grad 1.48 | tok/s 19952
step   3420 | loss 1.3164 | lr 3.23e-04 | grad 1.38 | tok/s 19542
step   3430 | loss 1.2502 | lr 3.23e-04 | grad 1.28 | tok/s 19615
step   3440 | loss 1.3117 | lr 3.23e-04 | grad 1.20 | tok/s 19429
step   3450 | loss 1.3128 | lr 3.23e-04 | grad 2.27 | tok/s 19290
step   3460 | loss 1.5088 | lr 3.23e-04 | grad 1.65 | tok/s 19937
step   3470 | loss 1.1772 | lr 3.23e-04 | grad 1.34 | tok/s 19273
step   3480 | loss 1.4376 | lr 3.23e-04 | grad 1.25 | tok/s 19947
step   3490 | loss 1.4106 | lr 3.23e-04 | grad 1.52 | tok/s 19047
step   3500 | loss 1.3030 | lr 3.23e-04 | grad 1.20 | tok/s 18864
step   3510 | loss 1.2739 | lr 3.23e-04 | grad 2.50 | tok/s 19986
step   3520 | loss 1.4580 | lr 3.23e-04 | grad 2.42 | tok/s 19589
step   3530 | loss 1.3044 | lr 3.23e-04 | grad 1.51 | tok/s 19120
step   3540 | loss 1.2555 | lr 3.23e-04 | grad 1.81 | tok/s 19984
step   3550 | loss 1.3397 | lr 3.23e-04 | grad 2.48 | tok/s 19770
step   3560 | loss 1.2337 | lr 3.23e-04 | grad 1.59 | tok/s 20491
step   3570 | loss 1.1860 | lr 3.23e-04 | grad 1.42 | tok/s 19523
step   3580 | loss 1.2986 | lr 3.23e-04 | grad 1.41 | tok/s 19054
step   3590 | loss 1.1520 | lr 3.23e-04 | grad 1.29 | tok/s 19691
step   3600 | loss 1.3470 | lr 3.23e-04 | grad 1.25 | tok/s 19916
step   3610 | loss 1.2593 | lr 3.23e-04 | grad 1.20 | tok/s 18988
step   3620 | loss 1.3374 | lr 3.23e-04 | grad 3.12 | tok/s 19944
step   3630 | loss 1.4549 | lr 3.23e-04 | grad 1.78 | tok/s 19094
step   3640 | loss 1.4227 | lr 3.23e-04 | grad 2.12 | tok/s 19945
step   3650 | loss 1.3475 | lr 3.23e-04 | grad 1.15 | tok/s 18829
step   3660 | loss 1.2509 | lr 3.23e-04 | grad 1.91 | tok/s 19884
step   3670 | loss 1.1752 | lr 3.23e-04 | grad 1.18 | tok/s 19397
step   3680 | loss 1.2771 | lr 3.23e-04 | grad 1.26 | tok/s 20045
step   3690 | loss 1.1383 | lr 3.23e-04 | grad 2.09 | tok/s 20149
step   3700 | loss 1.3355 | lr 3.23e-04 | grad 1.29 | tok/s 19826
step   3710 | loss 1.2587 | lr 3.23e-04 | grad 1.28 | tok/s 20024
step   3720 | loss 1.4099 | lr 3.23e-04 | grad 4.19 | tok/s 19665
step   3730 | loss 1.3133 | lr 3.23e-04 | grad 3.33 | tok/s 19774
step   3740 | loss 1.2778 | lr 3.23e-04 | grad 1.41 | tok/s 19733
step   3750 | loss 1.2616 | lr 3.23e-04 | grad 1.33 | tok/s 19842
step   3760 | loss 1.4279 | lr 3.23e-04 | grad 6.19 | tok/s 19113
step   3770 | loss 1.2956 | lr 3.23e-04 | grad 1.43 | tok/s 20160
step   3780 | loss 1.1550 | lr 3.23e-04 | grad 1.01 | tok/s 19783
step   3790 | loss 1.1201 | lr 3.23e-04 | grad 1.38 | tok/s 20020
step   3800 | loss 1.2546 | lr 3.23e-04 | grad 1.62 | tok/s 19664
step   3810 | loss 1.2116 | lr 3.23e-04 | grad 1.38 | tok/s 20144
step   3820 | loss 1.2771 | lr 3.23e-04 | grad 1.44 | tok/s 20471
step   3830 | loss 1.3996 | lr 3.23e-04 | grad 1.16 | tok/s 20120
step   3840 | loss 1.2609 | lr 3.23e-04 | grad 1.82 | tok/s 19821
step   3850 | loss 1.3073 | lr 3.23e-04 | grad 1.98 | tok/s 19871
step   3860 | loss 1.3026 | lr 3.23e-04 | grad 2.70 | tok/s 20109
step   3870 | loss 1.0374 | lr 3.23e-04 | grad 1.57 | tok/s 19866
step   3880 | loss 1.1544 | lr 3.23e-04 | grad 6.16 | tok/s 19652
step   3890 | loss 1.1929 | lr 3.23e-04 | grad 1.33 | tok/s 19642
step   3900 | loss 1.3489 | lr 3.23e-04 | grad 1.73 | tok/s 19311
step   3910 | loss 1.2664 | lr 3.23e-04 | grad 1.69 | tok/s 20513
step   3920 | loss 1.2619 | lr 3.23e-04 | grad 1.44 | tok/s 19284
step   3930 | loss 1.2429 | lr 3.23e-04 | grad 1.98 | tok/s 19900
step   3940 | loss 1.2287 | lr 3.23e-04 | grad 1.26 | tok/s 20255
step   3950 | loss 1.1513 | lr 3.23e-04 | grad 1.46 | tok/s 20058
step   3960 | loss 1.0515 | lr 3.23e-04 | grad 1.22 | tok/s 20487
step   3970 | loss 1.0183 | lr 3.23e-04 | grad 1.44 | tok/s 20504
step   3980 | loss 1.0090 | lr 3.23e-04 | grad 1.09 | tok/s 20502
step   3990 | loss 1.0836 | lr 3.23e-04 | grad 8.50 | tok/s 20489
step   4000 | loss 1.3241 | lr 3.23e-04 | grad 1.39 | tok/s 20039
  >>> saved checkpoint: checkpoint_step_004000_loss_1.3241.pt
step   4010 | loss 1.1659 | lr 3.23e-04 | grad 1.46 | tok/s 12487
step   4020 | loss 1.1203 | lr 3.23e-04 | grad 1.47 | tok/s 20689
step   4030 | loss 1.0992 | lr 3.23e-04 | grad 1.16 | tok/s 20661
step   4040 | loss 1.1946 | lr 3.23e-04 | grad 1.55 | tok/s 20153
step   4050 | loss 1.3261 | lr 3.23e-04 | grad 1.18 | tok/s 19534
step   4060 | loss 1.4259 | lr 3.23e-04 | grad 1.90 | tok/s 19375
step   4070 | loss 1.3305 | lr 3.23e-04 | grad 1.20 | tok/s 20586
step   4080 | loss 1.2405 | lr 3.23e-04 | grad 3.64 | tok/s 20014
step   4090 | loss 1.3152 | lr 3.23e-04 | grad 1.50 | tok/s 19663
step   4100 | loss 1.2768 | lr 3.23e-04 | grad 2.02 | tok/s 19703
step   4110 | loss 1.3703 | lr 3.23e-04 | grad 1.33 | tok/s 20017
step   4120 | loss 1.1895 | lr 3.23e-04 | grad 1.81 | tok/s 19646
step   4130 | loss 1.3152 | lr 3.23e-04 | grad 1.63 | tok/s 19514
step   4140 | loss 1.2382 | lr 3.23e-04 | grad 4.44 | tok/s 19490
step   4150 | loss 1.0308 | lr 3.23e-04 | grad 1.38 | tok/s 20296
step   4160 | loss 1.0668 | lr 3.23e-04 | grad 1.70 | tok/s 19965
step   4170 | loss 1.2788 | lr 3.23e-04 | grad 2.62 | tok/s 19271
step   4180 | loss 0.7737 | lr 3.23e-04 | grad 1.27 | tok/s 20607
step   4190 | loss 1.2581 | lr 3.23e-04 | grad 1.17 | tok/s 19302
step   4200 | loss 1.4536 | lr 3.23e-04 | grad 1.62 | tok/s 19766
step   4210 | loss 1.3427 | lr 3.23e-04 | grad 1.49 | tok/s 19293
step   4220 | loss 1.4147 | lr 3.23e-04 | grad 2.22 | tok/s 19878
step   4230 | loss 1.2583 | lr 3.23e-04 | grad 2.91 | tok/s 19644
step   4240 | loss 1.4119 | lr 3.23e-04 | grad 1.65 | tok/s 19278
step   4250 | loss 1.2452 | lr 3.23e-04 | grad 1.95 | tok/s 19561
step   4260 | loss 1.2847 | lr 3.23e-04 | grad 1.24 | tok/s 19572
step   4270 | loss 1.5284 | lr 3.23e-04 | grad 1.61 | tok/s 20218
step   4280 | loss 1.2511 | lr 3.23e-04 | grad 1.39 | tok/s 19467
step   4290 | loss 1.2725 | lr 3.23e-04 | grad 2.59 | tok/s 19970
step   4300 | loss 1.3448 | lr 3.23e-04 | grad 2.53 | tok/s 20377
step   4310 | loss 1.3086 | lr 3.23e-04 | grad 2.88 | tok/s 20123
step   4320 | loss 1.1836 | lr 3.23e-04 | grad 1.16 | tok/s 19889
step   4330 | loss 1.0743 | lr 3.23e-04 | grad 1.15 | tok/s 20484
step   4340 | loss 1.0718 | lr 3.23e-04 | grad 1.05 | tok/s 20518
step   4350 | loss 1.1318 | lr 3.23e-04 | grad 1.20 | tok/s 20519
step   4360 | loss 1.0988 | lr 3.23e-04 | grad 1.19 | tok/s 20526
step   4370 | loss 1.0704 | lr 3.23e-04 | grad 1.23 | tok/s 20514
step   4380 | loss 1.0866 | lr 3.23e-04 | grad 1.18 | tok/s 20522
step   4390 | loss 1.2152 | lr 3.23e-04 | grad 2.45 | tok/s 19793
step   4400 | loss 1.2341 | lr 3.23e-04 | grad 1.50 | tok/s 19716
step   4410 | loss 1.2595 | lr 3.23e-04 | grad 3.20 | tok/s 20447

Training complete! Final step: 4415
