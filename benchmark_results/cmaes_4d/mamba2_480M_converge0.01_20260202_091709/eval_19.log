Using device: cuda
Output directory: benchmark_results/cmaes_4d/mamba2_480M_converge0.01_20260202_091709/eval_19/levelmamba2_100m_20260202_101756
Model: Level mamba2, 608,185,980 parameters
Using schedule-free AdamW (lr=0.00028749642706881986)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 6.7906 | lr 2.87e-04 | grad 13.50 | tok/s 6844
step     20 | loss 3.5629 | lr 2.87e-04 | grad 4.66 | tok/s 16169
step     30 | loss 2.8104 | lr 2.87e-04 | grad 4.09 | tok/s 15818
step     40 | loss 2.8069 | lr 2.87e-04 | grad 5.06 | tok/s 15710
step     50 | loss 2.5961 | lr 2.87e-04 | grad 4.12 | tok/s 15256
step     60 | loss 2.3107 | lr 2.87e-04 | grad 3.64 | tok/s 15204
step     70 | loss 2.1934 | lr 2.87e-04 | grad 3.81 | tok/s 15396
step     80 | loss 1.9540 | lr 2.87e-04 | grad 4.31 | tok/s 14834
step     90 | loss 2.1041 | lr 2.87e-04 | grad 3.53 | tok/s 15215
step    100 | loss 1.7748 | lr 2.87e-04 | grad 3.16 | tok/s 15568
step    110 | loss 1.9712 | lr 2.87e-04 | grad 9.19 | tok/s 15555
step    120 | loss 1.8284 | lr 2.87e-04 | grad 2.45 | tok/s 15211
step    130 | loss 1.8956 | lr 2.87e-04 | grad 6.09 | tok/s 15302
step    140 | loss 1.8512 | lr 2.87e-04 | grad 1.97 | tok/s 14918
step    150 | loss 1.5679 | lr 2.87e-04 | grad 2.31 | tok/s 15419
step    160 | loss 1.4893 | lr 2.87e-04 | grad 2.34 | tok/s 15805
step    170 | loss 1.6149 | lr 2.87e-04 | grad 2.27 | tok/s 15042
step    180 | loss 1.7437 | lr 2.87e-04 | grad 2.09 | tok/s 15159
step    190 | loss 1.7353 | lr 2.87e-04 | grad 5.75 | tok/s 15232
step    200 | loss 1.8534 | lr 2.87e-04 | grad 2.78 | tok/s 15531
step    210 | loss 1.7404 | lr 2.87e-04 | grad 2.66 | tok/s 15322
step    220 | loss 1.6755 | lr 2.87e-04 | grad 2.08 | tok/s 15122
step    230 | loss 1.7891 | lr 2.87e-04 | grad 1.98 | tok/s 15358
step    240 | loss 1.6217 | lr 2.87e-04 | grad 2.03 | tok/s 15282
step    250 | loss 1.5570 | lr 2.87e-04 | grad 1.97 | tok/s 14822
step    260 | loss 1.8301 | lr 2.87e-04 | grad 1.94 | tok/s 15376
step    270 | loss 1.5550 | lr 2.87e-04 | grad 2.78 | tok/s 15320
step    280 | loss 1.8383 | lr 2.87e-04 | grad 1.59 | tok/s 15467
step    290 | loss 1.3722 | lr 2.87e-04 | grad 2.31 | tok/s 15835
step    300 | loss 1.7630 | lr 2.87e-04 | grad 2.72 | tok/s 15770
step    310 | loss 1.8434 | lr 2.87e-04 | grad 1.64 | tok/s 15320
step    320 | loss 1.5630 | lr 2.87e-04 | grad 1.65 | tok/s 15555
step    330 | loss 1.5382 | lr 2.87e-04 | grad 2.58 | tok/s 15533
step    340 | loss 1.5196 | lr 2.87e-04 | grad 1.73 | tok/s 14888
step    350 | loss 1.7485 | lr 2.87e-04 | grad 2.66 | tok/s 15367
step    360 | loss 1.7710 | lr 2.87e-04 | grad 7.09 | tok/s 15362
step    370 | loss 1.5702 | lr 2.87e-04 | grad 1.53 | tok/s 14973
step    380 | loss 1.6857 | lr 2.87e-04 | grad 1.52 | tok/s 15367
step    390 | loss 1.5449 | lr 2.87e-04 | grad 1.82 | tok/s 15033
step    400 | loss 1.6904 | lr 2.87e-04 | grad 1.64 | tok/s 15518
step    410 | loss 1.5149 | lr 2.87e-04 | grad 1.71 | tok/s 15447
step    420 | loss 1.4807 | lr 2.87e-04 | grad 4.25 | tok/s 15295
step    430 | loss 2.2083 | lr 2.87e-04 | grad 2.88 | tok/s 15722
step    440 | loss 1.7519 | lr 2.87e-04 | grad 2.38 | tok/s 15690
step    450 | loss 1.4761 | lr 2.87e-04 | grad 2.09 | tok/s 15153
step    460 | loss 1.5493 | lr 2.87e-04 | grad 2.11 | tok/s 15504
step    470 | loss 1.7321 | lr 2.87e-04 | grad 1.96 | tok/s 15163
step    480 | loss 1.4920 | lr 2.87e-04 | grad 1.55 | tok/s 15185
step    490 | loss 1.4964 | lr 2.87e-04 | grad 2.62 | tok/s 14856
step    500 | loss 1.4244 | lr 2.87e-04 | grad 2.44 | tok/s 15791
step    510 | loss 1.3408 | lr 2.87e-04 | grad 2.42 | tok/s 15576
step    520 | loss 1.6530 | lr 2.87e-04 | grad 2.17 | tok/s 15143
step    530 | loss 1.5130 | lr 2.87e-04 | grad 1.80 | tok/s 15397
step    540 | loss 1.5749 | lr 2.87e-04 | grad 2.27 | tok/s 15152
step    550 | loss 1.5208 | lr 2.87e-04 | grad 3.31 | tok/s 15246
step    560 | loss 1.9962 | lr 2.87e-04 | grad 1.87 | tok/s 15297
step    570 | loss 1.7209 | lr 2.87e-04 | grad 1.70 | tok/s 15403
step    580 | loss 1.5534 | lr 2.87e-04 | grad 2.30 | tok/s 15679
step    590 | loss 1.5931 | lr 2.87e-04 | grad 2.27 | tok/s 15025
step    600 | loss 1.6453 | lr 2.87e-04 | grad 1.92 | tok/s 15127
step    610 | loss 1.5519 | lr 2.87e-04 | grad 1.58 | tok/s 15201
step    620 | loss 1.4326 | lr 2.87e-04 | grad 1.84 | tok/s 15874
step    630 | loss 1.3296 | lr 2.87e-04 | grad 2.11 | tok/s 15880
step    640 | loss 1.3503 | lr 2.87e-04 | grad 1.66 | tok/s 15350
step    650 | loss 1.5657 | lr 2.87e-04 | grad 3.33 | tok/s 15392
step    660 | loss 1.5358 | lr 2.87e-04 | grad 2.36 | tok/s 15327
step    670 | loss 1.5302 | lr 2.87e-04 | grad 1.40 | tok/s 15104
step    680 | loss 1.5828 | lr 2.87e-04 | grad 1.80 | tok/s 14980
step    690 | loss 1.5697 | lr 2.87e-04 | grad 1.81 | tok/s 15221
step    700 | loss 1.3643 | lr 2.87e-04 | grad 2.48 | tok/s 15388
step    710 | loss 1.4799 | lr 2.87e-04 | grad 1.30 | tok/s 14981
step    720 | loss 1.6308 | lr 2.87e-04 | grad 2.03 | tok/s 15439
step    730 | loss 1.5025 | lr 2.87e-04 | grad 2.02 | tok/s 15538
step    740 | loss 1.5021 | lr 2.87e-04 | grad 2.11 | tok/s 15148
step    750 | loss 1.3720 | lr 2.87e-04 | grad 1.86 | tok/s 14916
step    760 | loss 1.3983 | lr 2.87e-04 | grad 2.03 | tok/s 15133
step    770 | loss 1.5415 | lr 2.87e-04 | grad 2.20 | tok/s 15090
step    780 | loss 1.4961 | lr 2.87e-04 | grad 2.47 | tok/s 14935
step    790 | loss 1.4980 | lr 2.87e-04 | grad 2.14 | tok/s 15474
step    800 | loss 1.4960 | lr 2.87e-04 | grad 1.43 | tok/s 15245
step    810 | loss 1.6160 | lr 2.87e-04 | grad 2.70 | tok/s 15509
step    820 | loss 1.4933 | lr 2.87e-04 | grad 1.35 | tok/s 15203
step    830 | loss 1.4090 | lr 2.87e-04 | grad 1.27 | tok/s 14569
step    840 | loss 1.5275 | lr 2.87e-04 | grad 2.44 | tok/s 15736
step    850 | loss 1.4355 | lr 2.87e-04 | grad 2.22 | tok/s 15337
step    860 | loss 1.4661 | lr 2.87e-04 | grad 1.34 | tok/s 14980
step    870 | loss 1.5460 | lr 2.87e-04 | grad 2.88 | tok/s 15324
step    880 | loss 1.5466 | lr 2.87e-04 | grad 2.08 | tok/s 15239
step    890 | loss 1.5185 | lr 2.87e-04 | grad 2.92 | tok/s 15290
step    900 | loss 1.4801 | lr 2.87e-04 | grad 2.17 | tok/s 14885
step    910 | loss 1.4687 | lr 2.87e-04 | grad 2.17 | tok/s 15069
step    920 | loss 1.5986 | lr 2.87e-04 | grad 2.50 | tok/s 15186
step    930 | loss 1.5056 | lr 2.87e-04 | grad 1.92 | tok/s 14898
step    940 | loss 1.4456 | lr 2.87e-04 | grad 1.78 | tok/s 15312
step    950 | loss 1.3948 | lr 2.87e-04 | grad 2.50 | tok/s 15216
step    960 | loss 1.5428 | lr 2.87e-04 | grad 1.53 | tok/s 14894
step    970 | loss 1.4594 | lr 2.87e-04 | grad 1.41 | tok/s 14924
step    980 | loss 1.4449 | lr 2.87e-04 | grad 1.31 | tok/s 15358
step    990 | loss 1.5047 | lr 2.87e-04 | grad 2.39 | tok/s 15189
step   1000 | loss 1.5106 | lr 2.87e-04 | grad 2.59 | tok/s 15103
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5106.pt
step   1010 | loss 1.4684 | lr 2.87e-04 | grad 1.30 | tok/s 11234
step   1020 | loss 1.5615 | lr 2.87e-04 | grad 2.39 | tok/s 15598
step   1030 | loss 1.4252 | lr 2.87e-04 | grad 1.70 | tok/s 15282
step   1040 | loss 1.4079 | lr 2.87e-04 | grad 1.87 | tok/s 15508
step   1050 | loss 1.2073 | lr 2.87e-04 | grad 1.77 | tok/s 15923
step   1060 | loss 1.3696 | lr 2.87e-04 | grad 2.09 | tok/s 15744
step   1070 | loss 1.3719 | lr 2.87e-04 | grad 2.02 | tok/s 15498
step   1080 | loss 1.4732 | lr 2.87e-04 | grad 1.56 | tok/s 15339
step   1090 | loss 1.4880 | lr 2.87e-04 | grad 2.27 | tok/s 15308
step   1100 | loss 1.3285 | lr 2.87e-04 | grad 1.87 | tok/s 15486
step   1110 | loss 1.3526 | lr 2.87e-04 | grad 1.48 | tok/s 15228
step   1120 | loss 1.5493 | lr 2.87e-04 | grad 1.98 | tok/s 15197
step   1130 | loss 1.5232 | lr 2.87e-04 | grad 1.82 | tok/s 15129
step   1140 | loss 1.5142 | lr 2.87e-04 | grad 1.73 | tok/s 15687
step   1150 | loss 1.2344 | lr 2.87e-04 | grad 1.49 | tok/s 15851
step   1160 | loss 1.2906 | lr 2.87e-04 | grad 1.27 | tok/s 15429
step   1170 | loss 1.5093 | lr 2.87e-04 | grad 2.19 | tok/s 15654
step   1180 | loss 1.3961 | lr 2.87e-04 | grad 1.95 | tok/s 15305
step   1190 | loss 1.4158 | lr 2.87e-04 | grad 1.62 | tok/s 15178
step   1200 | loss 1.3920 | lr 2.87e-04 | grad 2.20 | tok/s 15369
step   1210 | loss 1.3872 | lr 2.87e-04 | grad 1.70 | tok/s 15873
step   1220 | loss 1.4052 | lr 2.87e-04 | grad 2.19 | tok/s 15432
step   1230 | loss 1.6661 | lr 2.87e-04 | grad 2.02 | tok/s 15435
step   1240 | loss 1.4084 | lr 2.87e-04 | grad 1.83 | tok/s 15273
step   1250 | loss 1.4153 | lr 2.87e-04 | grad 1.87 | tok/s 15078
step   1260 | loss 1.4172 | lr 2.87e-04 | grad 1.26 | tok/s 15110
step   1270 | loss 1.4269 | lr 2.87e-04 | grad 1.55 | tok/s 15288
step   1280 | loss 1.4532 | lr 2.87e-04 | grad 3.20 | tok/s 15523
step   1290 | loss 1.3540 | lr 2.87e-04 | grad 1.62 | tok/s 15462
step   1300 | loss 1.4067 | lr 2.87e-04 | grad 1.85 | tok/s 15463
step   1310 | loss 1.6079 | lr 2.87e-04 | grad 1.94 | tok/s 15473
step   1320 | loss 1.4808 | lr 2.87e-04 | grad 1.95 | tok/s 14986
step   1330 | loss 1.7124 | lr 2.87e-04 | grad 1.32 | tok/s 15341
step   1340 | loss 1.4430 | lr 2.87e-04 | grad 3.36 | tok/s 15345
step   1350 | loss 1.4378 | lr 2.87e-04 | grad 3.38 | tok/s 15213
step   1360 | loss 1.3816 | lr 2.87e-04 | grad 1.74 | tok/s 15588
step   1370 | loss 1.3409 | lr 2.87e-04 | grad 1.98 | tok/s 15395
step   1380 | loss 1.4156 | lr 2.87e-04 | grad 1.53 | tok/s 15079
step   1390 | loss 1.5220 | lr 2.87e-04 | grad 1.60 | tok/s 15743
step   1400 | loss 1.2892 | lr 2.87e-04 | grad 1.59 | tok/s 15853
step   1410 | loss 1.5346 | lr 2.87e-04 | grad 3.00 | tok/s 15022
step   1420 | loss 1.7326 | lr 2.87e-04 | grad 1.80 | tok/s 15476
step   1430 | loss 1.3977 | lr 2.87e-04 | grad 2.61 | tok/s 15134
step   1440 | loss 1.5253 | lr 2.87e-04 | grad 1.51 | tok/s 14947
step   1450 | loss 1.3455 | lr 2.87e-04 | grad 1.30 | tok/s 15016
step   1460 | loss 1.7390 | lr 2.87e-04 | grad 3.36 | tok/s 15422
step   1470 | loss 1.5026 | lr 2.87e-04 | grad 4.03 | tok/s 15688
step   1480 | loss 1.3954 | lr 2.87e-04 | grad 1.35 | tok/s 15396
step   1490 | loss 1.3589 | lr 2.87e-04 | grad 3.09 | tok/s 15258
step   1500 | loss 1.6466 | lr 2.87e-04 | grad 1.70 | tok/s 15523
step   1510 | loss 1.4435 | lr 2.87e-04 | grad 1.46 | tok/s 15310
step   1520 | loss 1.4828 | lr 2.87e-04 | grad 1.32 | tok/s 15156
step   1530 | loss 1.4792 | lr 2.87e-04 | grad 1.74 | tok/s 15209
step   1540 | loss 1.5026 | lr 2.87e-04 | grad 1.84 | tok/s 15394
step   1550 | loss 1.4194 | lr 2.87e-04 | grad 2.95 | tok/s 15084
step   1560 | loss 1.3246 | lr 2.87e-04 | grad 1.20 | tok/s 15481
step   1570 | loss 1.4448 | lr 2.87e-04 | grad 1.57 | tok/s 15461
step   1580 | loss 1.4475 | lr 2.87e-04 | grad 1.65 | tok/s 15298
step   1590 | loss 1.4156 | lr 2.87e-04 | grad 2.08 | tok/s 15436
step   1600 | loss 1.3862 | lr 2.87e-04 | grad 2.06 | tok/s 15032
step   1610 | loss 1.3329 | lr 2.87e-04 | grad 2.11 | tok/s 15492
step   1620 | loss 1.3543 | lr 2.87e-04 | grad 1.26 | tok/s 15199
step   1630 | loss 1.7479 | lr 2.87e-04 | grad 3.86 | tok/s 15497
step   1640 | loss 1.8403 | lr 2.87e-04 | grad 2.30 | tok/s 15902
step   1650 | loss 1.7919 | lr 2.87e-04 | grad 1.94 | tok/s 15146
step   1660 | loss 1.4836 | lr 2.87e-04 | grad 1.64 | tok/s 15243
step   1670 | loss 1.5821 | lr 2.87e-04 | grad 1.74 | tok/s 15644
step   1680 | loss 1.4262 | lr 2.87e-04 | grad 1.59 | tok/s 15467
step   1690 | loss 1.4316 | lr 2.87e-04 | grad 1.51 | tok/s 14682
step   1700 | loss 1.3626 | lr 2.87e-04 | grad 3.05 | tok/s 14982
step   1710 | loss 1.4968 | lr 2.87e-04 | grad 2.42 | tok/s 15575

Training complete! Final step: 1718
