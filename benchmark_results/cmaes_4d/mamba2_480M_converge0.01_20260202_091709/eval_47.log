Using device: cuda
Output directory: benchmark_results/cmaes_4d/mamba2_480M_converge0.01_20260202_091709/eval_47/levelmamba2_100m_20260202_114857
Model: Level mamba2, 665,830,528 parameters
Using schedule-free AdamW (lr=0.0003155193554747097)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 7.5513 | lr 3.16e-04 | grad 14.25 | tok/s 7104
step     20 | loss 3.4493 | lr 3.16e-04 | grad 5.59 | tok/s 17562
step     30 | loss 2.3931 | lr 3.16e-04 | grad 4.38 | tok/s 17347
step     40 | loss 2.0632 | lr 3.16e-04 | grad 2.58 | tok/s 17240
step     50 | loss 2.9145 | lr 3.16e-04 | grad 4.72 | tok/s 16549
step     60 | loss 2.4815 | lr 3.16e-04 | grad 5.59 | tok/s 16786
step     70 | loss 2.3675 | lr 3.16e-04 | grad 3.62 | tok/s 16642
step     80 | loss 2.3349 | lr 3.16e-04 | grad 4.06 | tok/s 16243
step     90 | loss 1.8926 | lr 3.16e-04 | grad 3.70 | tok/s 16786
step    100 | loss 2.1982 | lr 3.16e-04 | grad 2.28 | tok/s 16241
step    110 | loss 2.0846 | lr 3.16e-04 | grad 2.47 | tok/s 16416
step    120 | loss 1.9931 | lr 3.16e-04 | grad 2.78 | tok/s 16716
step    130 | loss 1.7238 | lr 3.16e-04 | grad 2.77 | tok/s 15846
step    140 | loss 1.7986 | lr 3.16e-04 | grad 2.39 | tok/s 16079
step    150 | loss 1.8820 | lr 3.16e-04 | grad 3.02 | tok/s 16311
step    160 | loss 1.9476 | lr 3.16e-04 | grad 2.73 | tok/s 16459
step    170 | loss 1.6086 | lr 3.16e-04 | grad 2.72 | tok/s 16642
step    180 | loss 1.2992 | lr 3.16e-04 | grad 3.19 | tok/s 17044
step    190 | loss 1.8247 | lr 3.16e-04 | grad 2.16 | tok/s 16527
step    200 | loss 1.6984 | lr 3.16e-04 | grad 2.84 | tok/s 16776
step    210 | loss 1.6732 | lr 3.16e-04 | grad 2.52 | tok/s 16546
step    220 | loss 1.6656 | lr 3.16e-04 | grad 2.34 | tok/s 16753
step    230 | loss 1.6885 | lr 3.16e-04 | grad 2.36 | tok/s 16392
step    240 | loss 1.7556 | lr 3.16e-04 | grad 2.30 | tok/s 16185
step    250 | loss 1.7451 | lr 3.16e-04 | grad 1.83 | tok/s 16412
step    260 | loss 1.4367 | lr 3.16e-04 | grad 2.56 | tok/s 16611
step    270 | loss 1.4173 | lr 3.16e-04 | grad 2.12 | tok/s 17164
step    280 | loss 1.3629 | lr 3.16e-04 | grad 2.44 | tok/s 17155
step    290 | loss 1.3395 | lr 3.16e-04 | grad 2.16 | tok/s 17153
step    300 | loss 1.5863 | lr 3.16e-04 | grad 3.39 | tok/s 16260
step    310 | loss 1.5910 | lr 3.16e-04 | grad 3.25 | tok/s 16672
step    320 | loss 1.6941 | lr 3.16e-04 | grad 2.36 | tok/s 16359
step    330 | loss 1.5914 | lr 3.16e-04 | grad 2.27 | tok/s 16152
step    340 | loss 1.5411 | lr 3.16e-04 | grad 2.38 | tok/s 16478
step    350 | loss 1.5655 | lr 3.16e-04 | grad 4.19 | tok/s 16719
step    360 | loss 1.6989 | lr 3.16e-04 | grad 1.66 | tok/s 17029
step    370 | loss 1.5664 | lr 3.16e-04 | grad 1.98 | tok/s 16570
step    380 | loss 1.6463 | lr 3.16e-04 | grad 1.59 | tok/s 16696
step    390 | loss 1.3702 | lr 3.16e-04 | grad 1.63 | tok/s 16450
step    400 | loss 1.6045 | lr 3.16e-04 | grad 2.17 | tok/s 16213
step    410 | loss 1.6131 | lr 3.16e-04 | grad 2.88 | tok/s 16609
step    420 | loss 1.5837 | lr 3.16e-04 | grad 2.61 | tok/s 16716
step    430 | loss 1.5277 | lr 3.16e-04 | grad 2.28 | tok/s 16447
step    440 | loss 1.5293 | lr 3.16e-04 | grad 1.52 | tok/s 16307
step    450 | loss 1.4270 | lr 3.16e-04 | grad 1.99 | tok/s 16441
step    460 | loss 1.4826 | lr 3.16e-04 | grad 1.81 | tok/s 16209
step    470 | loss 1.8442 | lr 3.16e-04 | grad 4.72 | tok/s 16747
step    480 | loss 1.5870 | lr 3.16e-04 | grad 2.30 | tok/s 16383
step    490 | loss 1.2795 | lr 3.16e-04 | grad 2.03 | tok/s 17019
step    500 | loss 1.7937 | lr 3.16e-04 | grad 6.41 | tok/s 16518
step    510 | loss 1.5991 | lr 3.16e-04 | grad 2.75 | tok/s 16706
step    520 | loss 1.3239 | lr 3.16e-04 | grad 1.71 | tok/s 16889
step    530 | loss 1.2838 | lr 3.16e-04 | grad 5.25 | tok/s 17096
step    540 | loss 1.3209 | lr 3.16e-04 | grad 1.40 | tok/s 16853
step    550 | loss 1.7808 | lr 3.16e-04 | grad 2.17 | tok/s 16894
step    560 | loss 1.8027 | lr 3.16e-04 | grad 4.78 | tok/s 16359
step    570 | loss 1.4945 | lr 3.16e-04 | grad 1.95 | tok/s 16479
step    580 | loss 1.4281 | lr 3.16e-04 | grad 1.92 | tok/s 17016
step    590 | loss 1.4150 | lr 3.16e-04 | grad 1.59 | tok/s 16268
step    600 | loss 1.3571 | lr 3.16e-04 | grad 1.91 | tok/s 16858
step    610 | loss 1.5010 | lr 3.16e-04 | grad 2.06 | tok/s 16828
step    620 | loss 1.4325 | lr 3.16e-04 | grad 1.86 | tok/s 16464
step    630 | loss 1.5567 | lr 3.16e-04 | grad 5.28 | tok/s 16265
step    640 | loss 1.5386 | lr 3.16e-04 | grad 2.55 | tok/s 16791
step    650 | loss 1.5023 | lr 3.16e-04 | grad 2.25 | tok/s 16507
step    660 | loss 1.4091 | lr 3.16e-04 | grad 2.25 | tok/s 16244
step    670 | loss 1.5473 | lr 3.16e-04 | grad 1.68 | tok/s 16435
step    680 | loss 1.4892 | lr 3.16e-04 | grad 3.53 | tok/s 16314
step    690 | loss 1.5701 | lr 3.16e-04 | grad 1.84 | tok/s 16606
step    700 | loss 1.5067 | lr 3.16e-04 | grad 1.71 | tok/s 16578
step    710 | loss 1.4973 | lr 3.16e-04 | grad 1.73 | tok/s 16315
step    720 | loss 1.6412 | lr 3.16e-04 | grad 2.69 | tok/s 16650
step    730 | loss 1.4118 | lr 3.16e-04 | grad 2.69 | tok/s 16441
step    740 | loss 1.5050 | lr 3.16e-04 | grad 4.19 | tok/s 16756
step    750 | loss 1.3956 | lr 3.16e-04 | grad 1.45 | tok/s 16764
step    760 | loss 1.2558 | lr 3.16e-04 | grad 2.22 | tok/s 16941
step    770 | loss 1.3801 | lr 3.16e-04 | grad 3.88 | tok/s 16588
step    780 | loss 2.0833 | lr 3.16e-04 | grad 4.53 | tok/s 16871
step    790 | loss 1.8058 | lr 3.16e-04 | grad 2.30 | tok/s 17165
step    800 | loss 1.5738 | lr 3.16e-04 | grad 3.14 | tok/s 17152
step    810 | loss 1.5123 | lr 3.16e-04 | grad 1.66 | tok/s 16431
step    820 | loss 1.4067 | lr 3.16e-04 | grad 1.37 | tok/s 16576
step    830 | loss 1.4047 | lr 3.16e-04 | grad 1.74 | tok/s 16520
step    840 | loss 1.3865 | lr 3.16e-04 | grad 2.39 | tok/s 16615
step    850 | loss 1.4323 | lr 3.16e-04 | grad 4.97 | tok/s 16513
step    860 | loss 1.6498 | lr 3.16e-04 | grad 1.82 | tok/s 16067
step    870 | loss 1.3373 | lr 3.16e-04 | grad 1.48 | tok/s 16318
step    880 | loss 1.4625 | lr 3.16e-04 | grad 1.83 | tok/s 16502
step    890 | loss 1.3808 | lr 3.16e-04 | grad 1.70 | tok/s 16229
step    900 | loss 1.4883 | lr 3.16e-04 | grad 1.77 | tok/s 16364
step    910 | loss 1.4344 | lr 3.16e-04 | grad 1.61 | tok/s 16864
step    920 | loss 1.2582 | lr 3.16e-04 | grad 2.03 | tok/s 17138
step    930 | loss 1.1984 | lr 3.16e-04 | grad 2.55 | tok/s 17126
step    940 | loss 1.4787 | lr 3.16e-04 | grad 1.76 | tok/s 16306
step    950 | loss 1.5304 | lr 3.16e-04 | grad 2.12 | tok/s 16498
step    960 | loss 1.5367 | lr 3.16e-04 | grad 1.47 | tok/s 16685
step    970 | loss 1.2793 | lr 3.16e-04 | grad 1.94 | tok/s 16593
step    980 | loss 1.6085 | lr 3.16e-04 | grad 1.92 | tok/s 16241
step    990 | loss 1.3990 | lr 3.16e-04 | grad 2.14 | tok/s 16490
step   1000 | loss 1.4204 | lr 3.16e-04 | grad 1.24 | tok/s 16661
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4204.pt
step   1010 | loss 1.6634 | lr 3.16e-04 | grad 1.25 | tok/s 10639
step   1020 | loss 1.8800 | lr 3.16e-04 | grad 1.96 | tok/s 16729
step   1030 | loss 1.4815 | lr 3.16e-04 | grad 2.00 | tok/s 16457
step   1040 | loss 1.6075 | lr 3.16e-04 | grad 1.48 | tok/s 16846
step   1050 | loss 1.3807 | lr 3.16e-04 | grad 2.47 | tok/s 17117
step   1060 | loss 1.3533 | lr 3.16e-04 | grad 2.20 | tok/s 16875
step   1070 | loss 1.4522 | lr 3.16e-04 | grad 1.26 | tok/s 16290
step   1080 | loss 1.4889 | lr 3.16e-04 | grad 1.91 | tok/s 16599
step   1090 | loss 1.4997 | lr 3.16e-04 | grad 3.47 | tok/s 16559
step   1100 | loss 1.4766 | lr 3.16e-04 | grad 2.44 | tok/s 16334
step   1110 | loss 1.5303 | lr 3.16e-04 | grad 1.73 | tok/s 16155
step   1120 | loss 1.3757 | lr 3.16e-04 | grad 2.11 | tok/s 17015
step   1130 | loss 1.3481 | lr 3.16e-04 | grad 1.90 | tok/s 17158
step   1140 | loss 1.2744 | lr 3.16e-04 | grad 2.23 | tok/s 17142
step   1150 | loss 1.2338 | lr 3.16e-04 | grad 1.98 | tok/s 17144
step   1160 | loss 1.2146 | lr 3.16e-04 | grad 1.71 | tok/s 17130
step   1170 | loss 1.2870 | lr 3.16e-04 | grad 1.17 | tok/s 16899
step   1180 | loss 1.3766 | lr 3.16e-04 | grad 1.27 | tok/s 16129
step   1190 | loss 1.4547 | lr 3.16e-04 | grad 1.48 | tok/s 16712
step   1200 | loss 1.4022 | lr 3.16e-04 | grad 6.81 | tok/s 16555
step   1210 | loss 1.5930 | lr 3.16e-04 | grad 1.36 | tok/s 16459
step   1220 | loss 1.4520 | lr 3.16e-04 | grad 2.33 | tok/s 16472
step   1230 | loss 1.4700 | lr 3.16e-04 | grad 1.56 | tok/s 16246
step   1240 | loss 1.4217 | lr 3.16e-04 | grad 1.79 | tok/s 16348
step   1250 | loss 1.4959 | lr 3.16e-04 | grad 1.26 | tok/s 16241
step   1260 | loss 1.4263 | lr 3.16e-04 | grad 1.77 | tok/s 16360
step   1270 | loss 1.4364 | lr 3.16e-04 | grad 1.60 | tok/s 16618
step   1280 | loss 1.2898 | lr 3.16e-04 | grad 1.73 | tok/s 16364
step   1290 | loss 1.2774 | lr 3.16e-04 | grad 1.63 | tok/s 16744
step   1300 | loss 1.3710 | lr 3.16e-04 | grad 1.67 | tok/s 16486
step   1310 | loss 1.4266 | lr 3.16e-04 | grad 3.62 | tok/s 16413
step   1320 | loss 1.4082 | lr 3.16e-04 | grad 1.77 | tok/s 16472
step   1330 | loss 1.4197 | lr 3.16e-04 | grad 2.53 | tok/s 16423
step   1340 | loss 1.4155 | lr 3.16e-04 | grad 14.69 | tok/s 16763
step   1350 | loss 1.3841 | lr 3.16e-04 | grad 2.02 | tok/s 16518
step   1360 | loss 1.4139 | lr 3.16e-04 | grad 1.49 | tok/s 16113
step   1370 | loss 1.3251 | lr 3.16e-04 | grad 2.05 | tok/s 15988
step   1380 | loss 1.2403 | lr 3.16e-04 | grad 1.55 | tok/s 16650
step   1390 | loss 1.3103 | lr 3.16e-04 | grad 1.41 | tok/s 16343
step   1400 | loss 1.4792 | lr 3.16e-04 | grad 2.98 | tok/s 16315
step   1410 | loss 1.4007 | lr 3.16e-04 | grad 1.55 | tok/s 16471
step   1420 | loss 1.3359 | lr 3.16e-04 | grad 1.98 | tok/s 16512
step   1430 | loss 1.4681 | lr 3.16e-04 | grad 2.03 | tok/s 16422
step   1440 | loss 1.4419 | lr 3.16e-04 | grad 2.25 | tok/s 16191
step   1450 | loss 1.4019 | lr 3.16e-04 | grad 1.65 | tok/s 16717
step   1460 | loss 1.3991 | lr 3.16e-04 | grad 2.67 | tok/s 16747
step   1470 | loss 1.3174 | lr 3.16e-04 | grad 1.54 | tok/s 16213
step   1480 | loss 1.2116 | lr 3.16e-04 | grad 1.68 | tok/s 16933
step   1490 | loss 1.8268 | lr 3.16e-04 | grad 2.50 | tok/s 16515
step   1500 | loss 1.3984 | lr 3.16e-04 | grad 1.20 | tok/s 16127
step   1510 | loss 1.4122 | lr 3.16e-04 | grad 1.50 | tok/s 16579
step   1520 | loss 1.3596 | lr 3.16e-04 | grad 1.51 | tok/s 16153
step   1530 | loss 1.4213 | lr 3.16e-04 | grad 1.38 | tok/s 16245
step   1540 | loss 1.2832 | lr 3.16e-04 | grad 2.58 | tok/s 16829
step   1550 | loss 1.4031 | lr 3.16e-04 | grad 1.41 | tok/s 16420
step   1560 | loss 1.3250 | lr 3.16e-04 | grad 1.91 | tok/s 16714
step   1570 | loss 1.3216 | lr 3.16e-04 | grad 1.44 | tok/s 15910
step   1580 | loss 1.3725 | lr 3.16e-04 | grad 2.25 | tok/s 16291
step   1590 | loss 1.3525 | lr 3.16e-04 | grad 2.12 | tok/s 16522
step   1600 | loss 1.4688 | lr 3.16e-04 | grad 1.98 | tok/s 16649
step   1610 | loss 1.4326 | lr 3.16e-04 | grad 1.67 | tok/s 16131
step   1620 | loss 1.3219 | lr 3.16e-04 | grad 2.41 | tok/s 16937
step   1630 | loss 1.4116 | lr 3.16e-04 | grad 3.78 | tok/s 16244
step   1640 | loss 1.3141 | lr 3.16e-04 | grad 1.46 | tok/s 16482
step   1650 | loss 1.4198 | lr 3.16e-04 | grad 2.33 | tok/s 16277
step   1660 | loss 1.4167 | lr 3.16e-04 | grad 1.64 | tok/s 16484
step   1670 | loss 1.3928 | lr 3.16e-04 | grad 1.22 | tok/s 16275
step   1680 | loss 1.5098 | lr 3.16e-04 | grad 1.55 | tok/s 16481
step   1690 | loss 1.4316 | lr 3.16e-04 | grad 1.88 | tok/s 16379
step   1700 | loss 1.3722 | lr 3.16e-04 | grad 1.39 | tok/s 15918
step   1710 | loss 1.4921 | lr 3.16e-04 | grad 1.67 | tok/s 16501
step   1720 | loss 1.3357 | lr 3.16e-04 | grad 1.60 | tok/s 16568
step   1730 | loss 1.4083 | lr 3.16e-04 | grad 1.55 | tok/s 16659
step   1740 | loss 1.2901 | lr 3.16e-04 | grad 1.38 | tok/s 16294
step   1750 | loss 1.3635 | lr 3.16e-04 | grad 1.85 | tok/s 16316
step   1760 | loss 1.3504 | lr 3.16e-04 | grad 1.32 | tok/s 16370
step   1770 | loss 1.5561 | lr 3.16e-04 | grad 2.19 | tok/s 16260
step   1780 | loss 1.3276 | lr 3.16e-04 | grad 1.48 | tok/s 16123
step   1790 | loss 1.3057 | lr 3.16e-04 | grad 2.45 | tok/s 16893
step   1800 | loss 1.3653 | lr 3.16e-04 | grad 1.83 | tok/s 16703
step   1810 | loss 1.3973 | lr 3.16e-04 | grad 2.84 | tok/s 16186
step   1820 | loss 1.3707 | lr 3.16e-04 | grad 1.91 | tok/s 16683
step   1830 | loss 1.4558 | lr 3.16e-04 | grad 1.26 | tok/s 16441
step   1840 | loss 1.2313 | lr 3.16e-04 | grad 1.30 | tok/s 16657
step   1850 | loss 1.3069 | lr 3.16e-04 | grad 1.64 | tok/s 16633

Training complete! Final step: 1855
