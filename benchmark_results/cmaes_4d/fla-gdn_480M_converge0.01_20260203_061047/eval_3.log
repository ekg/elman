Using device: cuda
Output directory: benchmark_results/cmaes_4d/fla-gdn_480M_converge0.01_20260203_061047/eval_3/levelfla-gdn_100m_20260203_061053
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 322,334,360 parameters
Using schedule-free AdamW (lr=0.0005337713190928737)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 30.0 minutes
step     10 | loss 4.9154 | lr 5.34e-04 | grad 11.69 | tok/s 3935
step     20 | loss 3.4393 | lr 5.34e-04 | grad 9.25 | tok/s 34348
step     30 | loss 2.9991 | lr 5.34e-04 | grad 4.25 | tok/s 32767
step     40 | loss 2.4175 | lr 5.34e-04 | grad 3.36 | tok/s 33179
step     50 | loss 2.6040 | lr 5.34e-04 | grad 3.66 | tok/s 32100
step     60 | loss 2.1469 | lr 5.34e-04 | grad 1.88 | tok/s 31862
step     70 | loss 1.7739 | lr 5.34e-04 | grad 1.76 | tok/s 32223
step     80 | loss 2.2570 | lr 5.34e-04 | grad 2.30 | tok/s 31593
step     90 | loss 2.0518 | lr 5.34e-04 | grad 2.12 | tok/s 31605
step    100 | loss 1.8706 | lr 5.34e-04 | grad 1.17 | tok/s 31893
step    110 | loss 1.6705 | lr 5.34e-04 | grad 1.27 | tok/s 30344
step    120 | loss 1.7552 | lr 5.34e-04 | grad 1.57 | tok/s 30740
step    130 | loss 1.8014 | lr 5.34e-04 | grad 1.43 | tok/s 30858
step    140 | loss 1.9055 | lr 5.34e-04 | grad 1.42 | tok/s 31575
step    150 | loss 1.5643 | lr 5.34e-04 | grad 1.49 | tok/s 31090
step    160 | loss 1.1890 | lr 5.34e-04 | grad 1.10 | tok/s 32332
step    170 | loss 1.6448 | lr 5.34e-04 | grad 1.23 | tok/s 30852
step    180 | loss 1.6619 | lr 5.34e-04 | grad 1.09 | tok/s 32034
step    190 | loss 1.6257 | lr 5.34e-04 | grad 1.17 | tok/s 30891
step    200 | loss 1.5527 | lr 5.34e-04 | grad 1.25 | tok/s 31056
step    210 | loss 1.6362 | lr 5.34e-04 | grad 0.92 | tok/s 31301
step    220 | loss 1.7415 | lr 5.34e-04 | grad 1.33 | tok/s 30430
step    230 | loss 1.5600 | lr 5.34e-04 | grad 1.45 | tok/s 30563
step    240 | loss 1.4718 | lr 5.34e-04 | grad 1.19 | tok/s 30657
step    250 | loss 1.3878 | lr 5.34e-04 | grad 0.98 | tok/s 31911
step    260 | loss 1.2974 | lr 5.34e-04 | grad 1.07 | tok/s 32004
step    270 | loss 1.2541 | lr 5.34e-04 | grad 0.94 | tok/s 31995
step    280 | loss 1.4516 | lr 5.34e-04 | grad 1.39 | tok/s 31099
step    290 | loss 1.5570 | lr 5.34e-04 | grad 1.25 | tok/s 30839
step    300 | loss 1.5237 | lr 5.34e-04 | grad 1.42 | tok/s 30907
step    310 | loss 1.6060 | lr 5.34e-04 | grad 1.07 | tok/s 30156
step    320 | loss 1.4963 | lr 5.34e-04 | grad 1.41 | tok/s 30076
step    330 | loss 1.3775 | lr 5.34e-04 | grad 1.14 | tok/s 31078
step    340 | loss 1.6951 | lr 5.34e-04 | grad 1.12 | tok/s 31443
step    350 | loss 1.4810 | lr 5.34e-04 | grad 3.36 | tok/s 31076
step    360 | loss 1.5011 | lr 5.34e-04 | grad 2.09 | tok/s 31051
step    370 | loss 1.3556 | lr 5.34e-04 | grad 1.06 | tok/s 30573
step    380 | loss 1.4904 | lr 5.34e-04 | grad 0.94 | tok/s 30378
step    390 | loss 1.5087 | lr 5.34e-04 | grad 1.56 | tok/s 30604
step    400 | loss 1.4660 | lr 5.34e-04 | grad 0.79 | tok/s 31192
step    410 | loss 1.5061 | lr 5.34e-04 | grad 0.96 | tok/s 30545
step    420 | loss 1.4662 | lr 5.34e-04 | grad 0.84 | tok/s 30407
step    430 | loss 1.4275 | lr 5.34e-04 | grad 1.11 | tok/s 30864
step    440 | loss 1.3760 | lr 5.34e-04 | grad 0.67 | tok/s 29903
step    450 | loss 1.3908 | lr 5.34e-04 | grad 0.68 | tok/s 30285
step    460 | loss 1.7969 | lr 5.34e-04 | grad 1.02 | tok/s 31180
step    470 | loss 1.3616 | lr 5.34e-04 | grad 1.23 | tok/s 30979
step    480 | loss 1.2939 | lr 5.34e-04 | grad 0.96 | tok/s 31416
step    490 | loss 1.8445 | lr 5.34e-04 | grad 1.04 | tok/s 30790
step    500 | loss 1.2357 | lr 5.34e-04 | grad 1.03 | tok/s 30977
step    510 | loss 1.2582 | lr 5.34e-04 | grad 0.80 | tok/s 31780
step    520 | loss 1.1649 | lr 5.34e-04 | grad 0.80 | tok/s 31831
step    530 | loss 1.4140 | lr 5.34e-04 | grad 3.14 | tok/s 31411
step    540 | loss 1.6264 | lr 5.34e-04 | grad 3.08 | tok/s 31140
step    550 | loss 1.6770 | lr 5.34e-04 | grad 0.84 | tok/s 30216
step    560 | loss 1.4681 | lr 5.34e-04 | grad 1.45 | tok/s 31156
step    570 | loss 1.2445 | lr 5.34e-04 | grad 1.14 | tok/s 31512
step    580 | loss 1.3853 | lr 5.34e-04 | grad 0.90 | tok/s 30521
step    590 | loss 1.2690 | lr 5.34e-04 | grad 1.19 | tok/s 31301
step    600 | loss 1.3884 | lr 5.34e-04 | grad 1.01 | tok/s 31315
step    610 | loss 1.3436 | lr 5.34e-04 | grad 0.76 | tok/s 30418
step    620 | loss 1.4895 | lr 5.34e-04 | grad 0.90 | tok/s 30536
step    630 | loss 1.4736 | lr 5.34e-04 | grad 0.83 | tok/s 31221
step    640 | loss 1.3912 | lr 5.34e-04 | grad 1.19 | tok/s 30720
step    650 | loss 1.3744 | lr 5.34e-04 | grad 1.17 | tok/s 30350
step    660 | loss 1.5504 | lr 5.34e-04 | grad 1.24 | tok/s 30678
step    670 | loss 1.3678 | lr 5.34e-04 | grad 0.68 | tok/s 30401
step    680 | loss 1.3724 | lr 5.34e-04 | grad 1.59 | tok/s 30890
step    690 | loss 1.4506 | lr 5.34e-04 | grad 1.16 | tok/s 30987
step    700 | loss 1.4287 | lr 5.34e-04 | grad 0.86 | tok/s 30535
step    710 | loss 1.3916 | lr 5.34e-04 | grad 2.03 | tok/s 30447
step    720 | loss 1.5061 | lr 5.34e-04 | grad 5.94 | tok/s 31098
step    730 | loss 1.3980 | lr 5.34e-04 | grad 0.86 | tok/s 30542
step    740 | loss 1.3894 | lr 5.34e-04 | grad 0.73 | tok/s 31354
step    750 | loss 1.2693 | lr 5.34e-04 | grad 1.80 | tok/s 31075
step    760 | loss 1.2627 | lr 5.34e-04 | grad 0.76 | tok/s 31015
step    770 | loss 1.3336 | lr 5.34e-04 | grad 1.53 | tok/s 31194
step    780 | loss 2.0693 | lr 5.34e-04 | grad 1.38 | tok/s 31730
step    790 | loss 1.5860 | lr 5.34e-04 | grad 1.33 | tok/s 31924
step    800 | loss 1.4080 | lr 5.34e-04 | grad 1.50 | tok/s 31878
step    810 | loss 1.4014 | lr 5.34e-04 | grad 1.05 | tok/s 30340
step    820 | loss 1.2483 | lr 5.34e-04 | grad 0.70 | tok/s 31196
step    830 | loss 1.3670 | lr 5.34e-04 | grad 0.92 | tok/s 30614
step    840 | loss 1.3561 | lr 5.34e-04 | grad 1.09 | tok/s 31035
step    850 | loss 1.3946 | lr 5.34e-04 | grad 1.70 | tok/s 31143
step    860 | loss 1.5259 | lr 5.34e-04 | grad 0.82 | tok/s 29998
step    870 | loss 1.2636 | lr 5.34e-04 | grad 1.16 | tok/s 30499
step    880 | loss 1.3729 | lr 5.34e-04 | grad 0.91 | tok/s 30626
step    890 | loss 1.3022 | lr 5.34e-04 | grad 0.88 | tok/s 30440
step    900 | loss 1.4471 | lr 5.34e-04 | grad 1.60 | tok/s 30489
step    910 | loss 1.2795 | lr 5.34e-04 | grad 0.95 | tok/s 31737
step    920 | loss 1.1711 | lr 5.34e-04 | grad 0.98 | tok/s 32131
step    930 | loss 1.1013 | lr 5.34e-04 | grad 1.09 | tok/s 32110
step    940 | loss 1.3805 | lr 5.34e-04 | grad 1.55 | tok/s 30402
step    950 | loss 1.3742 | lr 5.34e-04 | grad 0.83 | tok/s 30906
step    960 | loss 1.4214 | lr 5.34e-04 | grad 1.33 | tok/s 31152
step    970 | loss 1.1781 | lr 5.34e-04 | grad 0.71 | tok/s 31710
step    980 | loss 1.3099 | lr 5.34e-04 | grad 0.88 | tok/s 30089
step    990 | loss 1.5428 | lr 5.34e-04 | grad 0.68 | tok/s 30796
step   1000 | loss 1.4381 | lr 5.34e-04 | grad 1.02 | tok/s 30872
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4381.pt
step   1010 | loss 1.1561 | lr 5.34e-04 | grad 1.21 | tok/s 21599
step   1020 | loss 1.5388 | lr 5.34e-04 | grad 0.73 | tok/s 31751
step   1030 | loss 1.7652 | lr 5.34e-04 | grad 1.05 | tok/s 30880
step   1040 | loss 1.5074 | lr 5.34e-04 | grad 1.49 | tok/s 30994
step   1050 | loss 1.4052 | lr 5.34e-04 | grad 1.06 | tok/s 31351
step   1060 | loss 1.2939 | lr 5.34e-04 | grad 1.82 | tok/s 31546
step   1070 | loss 1.2835 | lr 5.34e-04 | grad 0.93 | tok/s 31848
step   1080 | loss 1.3548 | lr 5.34e-04 | grad 1.23 | tok/s 30409
step   1090 | loss 1.3987 | lr 5.34e-04 | grad 0.79 | tok/s 30935
step   1100 | loss 1.4454 | lr 5.34e-04 | grad 0.96 | tok/s 30837
step   1110 | loss 1.3220 | lr 5.34e-04 | grad 0.86 | tok/s 30836
step   1120 | loss 1.4534 | lr 5.34e-04 | grad 0.93 | tok/s 30482
step   1130 | loss 1.3318 | lr 5.34e-04 | grad 1.16 | tok/s 32076
step   1140 | loss 1.2240 | lr 5.34e-04 | grad 0.99 | tok/s 32199
step   1150 | loss 1.1586 | lr 5.34e-04 | grad 0.70 | tok/s 32186
step   1160 | loss 1.1282 | lr 5.34e-04 | grad 0.98 | tok/s 32158
step   1170 | loss 1.1329 | lr 5.34e-04 | grad 1.13 | tok/s 32183
step   1180 | loss 1.2145 | lr 5.34e-04 | grad 0.82 | tok/s 31368
step   1190 | loss 1.3123 | lr 5.34e-04 | grad 0.77 | tok/s 30196
step   1200 | loss 1.3469 | lr 5.34e-04 | grad 0.71 | tok/s 31494
step   1210 | loss 1.3467 | lr 5.34e-04 | grad 0.91 | tok/s 31299
step   1220 | loss 1.4799 | lr 5.34e-04 | grad 1.51 | tok/s 30964
step   1230 | loss 1.3505 | lr 5.34e-04 | grad 0.81 | tok/s 31120
step   1240 | loss 1.3469 | lr 5.34e-04 | grad 0.77 | tok/s 30540
step   1250 | loss 1.3433 | lr 5.34e-04 | grad 0.73 | tok/s 30612
step   1260 | loss 1.3896 | lr 5.34e-04 | grad 0.76 | tok/s 30711
step   1270 | loss 1.3741 | lr 5.34e-04 | grad 1.12 | tok/s 30737
step   1280 | loss 1.2651 | lr 5.34e-04 | grad 0.68 | tok/s 31325
step   1290 | loss 1.2116 | lr 5.34e-04 | grad 0.84 | tok/s 30831
step   1300 | loss 1.2349 | lr 5.34e-04 | grad 1.38 | tok/s 31628
step   1310 | loss 1.2324 | lr 5.34e-04 | grad 0.93 | tok/s 30374
step   1320 | loss 1.3583 | lr 5.34e-04 | grad 0.85 | tok/s 30673
step   1330 | loss 1.3670 | lr 5.34e-04 | grad 0.97 | tok/s 31018
step   1340 | loss 1.3232 | lr 5.34e-04 | grad 1.26 | tok/s 30224
step   1350 | loss 1.2932 | lr 5.34e-04 | grad 3.73 | tok/s 31517
step   1360 | loss 1.2766 | lr 5.34e-04 | grad 1.08 | tok/s 31242
step   1370 | loss 1.3525 | lr 5.34e-04 | grad 0.77 | tok/s 30513
step   1380 | loss 1.2408 | lr 5.34e-04 | grad 0.97 | tok/s 30023
step   1390 | loss 1.1874 | lr 5.34e-04 | grad 0.74 | tok/s 30950
step   1400 | loss 1.1515 | lr 5.34e-04 | grad 0.87 | tok/s 31344
step   1410 | loss 1.3451 | lr 5.34e-04 | grad 0.91 | tok/s 30439
step   1420 | loss 1.3308 | lr 5.34e-04 | grad 0.73 | tok/s 31010
step   1430 | loss 1.2820 | lr 5.34e-04 | grad 1.06 | tok/s 30940
step   1440 | loss 1.3701 | lr 5.34e-04 | grad 1.31 | tok/s 30771
step   1450 | loss 1.4141 | lr 5.34e-04 | grad 0.92 | tok/s 30584
step   1460 | loss 1.2976 | lr 5.34e-04 | grad 1.25 | tok/s 31437
step   1470 | loss 1.2712 | lr 5.34e-04 | grad 0.85 | tok/s 31298
step   1480 | loss 1.3144 | lr 5.34e-04 | grad 1.10 | tok/s 30940
step   1490 | loss 1.2175 | lr 5.34e-04 | grad 0.82 | tok/s 30924
step   1500 | loss 1.3294 | lr 5.34e-04 | grad 0.70 | tok/s 31227
step   1510 | loss 1.6671 | lr 5.34e-04 | grad 1.01 | tok/s 31312
step   1520 | loss 1.2326 | lr 5.34e-04 | grad 1.08 | tok/s 30778
step   1530 | loss 1.3159 | lr 5.34e-04 | grad 0.84 | tok/s 31069
step   1540 | loss 1.2573 | lr 5.34e-04 | grad 0.77 | tok/s 30245
step   1550 | loss 1.3238 | lr 5.34e-04 | grad 0.85 | tok/s 30942
step   1560 | loss 1.2507 | lr 5.34e-04 | grad 1.13 | tok/s 31779
step   1570 | loss 1.2947 | lr 5.34e-04 | grad 0.71 | tok/s 30899
step   1580 | loss 1.2609 | lr 5.34e-04 | grad 0.82 | tok/s 31370
step   1590 | loss 1.2477 | lr 5.34e-04 | grad 0.63 | tok/s 30087
step   1600 | loss 1.2698 | lr 5.34e-04 | grad 0.73 | tok/s 30600
step   1610 | loss 1.2712 | lr 5.34e-04 | grad 0.66 | tok/s 30893
step   1620 | loss 1.3238 | lr 5.34e-04 | grad 0.82 | tok/s 31346
step   1630 | loss 1.5625 | lr 5.34e-04 | grad 0.72 | tok/s 30585
step   1640 | loss 1.1782 | lr 5.34e-04 | grad 0.84 | tok/s 31316
step   1650 | loss 1.3198 | lr 5.34e-04 | grad 1.34 | tok/s 31108
step   1660 | loss 1.3314 | lr 5.34e-04 | grad 0.81 | tok/s 30468
step   1670 | loss 1.2554 | lr 5.34e-04 | grad 1.15 | tok/s 30698
step   1680 | loss 1.3261 | lr 5.34e-04 | grad 1.27 | tok/s 30922
step   1690 | loss 1.2985 | lr 5.34e-04 | grad 2.20 | tok/s 30559
step   1700 | loss 1.3515 | lr 5.34e-04 | grad 1.95 | tok/s 30621
step   1710 | loss 1.3343 | lr 5.34e-04 | grad 1.00 | tok/s 30631
step   1720 | loss 1.3640 | lr 5.34e-04 | grad 0.63 | tok/s 29947
step   1730 | loss 1.3476 | lr 5.34e-04 | grad 1.43 | tok/s 31019
step   1740 | loss 1.2915 | lr 5.34e-04 | grad 0.77 | tok/s 30203
step   1750 | loss 1.3479 | lr 5.34e-04 | grad 1.47 | tok/s 31189
step   1760 | loss 1.1841 | lr 5.34e-04 | grad 0.64 | tok/s 30967
step   1770 | loss 1.2281 | lr 5.34e-04 | grad 1.07 | tok/s 30369
step   1780 | loss 1.2720 | lr 5.34e-04 | grad 0.72 | tok/s 30496
step   1790 | loss 1.3735 | lr 5.34e-04 | grad 0.94 | tok/s 30417
step   1800 | loss 1.3784 | lr 5.34e-04 | grad 0.94 | tok/s 30334
step   1810 | loss 1.2253 | lr 5.34e-04 | grad 0.89 | tok/s 30697
step   1820 | loss 1.2022 | lr 5.34e-04 | grad 0.93 | tok/s 31401
step   1830 | loss 1.3010 | lr 5.34e-04 | grad 0.80 | tok/s 31353
step   1840 | loss 1.3327 | lr 5.34e-04 | grad 0.71 | tok/s 30268
step   1850 | loss 1.2508 | lr 5.34e-04 | grad 0.89 | tok/s 31163
step   1860 | loss 1.3697 | lr 5.34e-04 | grad 0.68 | tok/s 30636
step   1870 | loss 1.1426 | lr 5.34e-04 | grad 0.87 | tok/s 31251
step   1880 | loss 1.2480 | lr 5.34e-04 | grad 0.94 | tok/s 31469
step   1890 | loss 1.3377 | lr 5.34e-04 | grad 0.84 | tok/s 31465
step   1900 | loss 1.3001 | lr 5.34e-04 | grad 1.09 | tok/s 31017
step   1910 | loss 1.1968 | lr 5.34e-04 | grad 0.71 | tok/s 30980
step   1920 | loss 1.2049 | lr 5.34e-04 | grad 0.75 | tok/s 30756
step   1930 | loss 1.3046 | lr 5.34e-04 | grad 0.62 | tok/s 31068
step   1940 | loss 1.2213 | lr 5.34e-04 | grad 0.91 | tok/s 30958
step   1950 | loss 1.1752 | lr 5.34e-04 | grad 0.73 | tok/s 31409
step   1960 | loss 1.0372 | lr 5.34e-04 | grad 0.88 | tok/s 32002
step   1970 | loss 1.1189 | lr 5.34e-04 | grad 1.14 | tok/s 31813
step   1980 | loss 1.1910 | lr 5.34e-04 | grad 0.86 | tok/s 31860
step   1990 | loss 1.0822 | lr 5.34e-04 | grad 0.63 | tok/s 32029
step   2000 | loss 1.2906 | lr 5.34e-04 | grad 0.93 | tok/s 30706
  >>> saved checkpoint: checkpoint_step_002000_loss_1.2906.pt
step   2010 | loss 1.3204 | lr 5.34e-04 | grad 1.02 | tok/s 22100
step   2020 | loss 1.2456 | lr 5.34e-04 | grad 0.66 | tok/s 30539
step   2030 | loss 1.3305 | lr 5.34e-04 | grad 0.88 | tok/s 31307
step   2040 | loss 1.2530 | lr 5.34e-04 | grad 0.71 | tok/s 30729
step   2050 | loss 1.0730 | lr 5.34e-04 | grad 0.62 | tok/s 31612
step   2060 | loss 1.1040 | lr 5.34e-04 | grad 0.63 | tok/s 30827
step   2070 | loss 1.1904 | lr 5.34e-04 | grad 0.78 | tok/s 30901
step   2080 | loss 1.3525 | lr 5.34e-04 | grad 1.24 | tok/s 30395
step   2090 | loss 1.3357 | lr 5.34e-04 | grad 0.54 | tok/s 30761
step   2100 | loss 1.3291 | lr 5.34e-04 | grad 0.80 | tok/s 30499
step   2110 | loss 1.3535 | lr 5.34e-04 | grad 0.96 | tok/s 31016
step   2120 | loss 1.2241 | lr 5.34e-04 | grad 0.64 | tok/s 31104
step   2130 | loss 1.4201 | lr 5.34e-04 | grad 0.68 | tok/s 31178
step   2140 | loss 1.0726 | lr 5.34e-04 | grad 0.60 | tok/s 31971
step   2150 | loss 1.0894 | lr 5.34e-04 | grad 0.58 | tok/s 31940
step   2160 | loss 1.0654 | lr 5.34e-04 | grad 0.73 | tok/s 31973
step   2170 | loss 1.1863 | lr 5.34e-04 | grad 0.84 | tok/s 31158
step   2180 | loss 1.2568 | lr 5.34e-04 | grad 1.28 | tok/s 31607
step   2190 | loss 1.2108 | lr 5.34e-04 | grad 1.03 | tok/s 31518
step   2200 | loss 1.3274 | lr 5.34e-04 | grad 1.05 | tok/s 31254
step   2210 | loss 1.1918 | lr 5.34e-04 | grad 0.78 | tok/s 30902
step   2220 | loss 1.3660 | lr 5.34e-04 | grad 0.81 | tok/s 30381
step   2230 | loss 1.2056 | lr 5.34e-04 | grad 0.91 | tok/s 30731
step   2240 | loss 1.2453 | lr 5.34e-04 | grad 0.82 | tok/s 30263
step   2250 | loss 1.2777 | lr 5.34e-04 | grad 0.68 | tok/s 31679
step   2260 | loss 1.2364 | lr 5.34e-04 | grad 0.78 | tok/s 31957
step   2270 | loss 1.1764 | lr 5.34e-04 | grad 0.58 | tok/s 31921
step   2280 | loss 1.2342 | lr 5.34e-04 | grad 0.84 | tok/s 31299
step   2290 | loss 1.2952 | lr 5.34e-04 | grad 1.12 | tok/s 30776
step   2300 | loss 1.5441 | lr 5.34e-04 | grad 1.18 | tok/s 30903
step   2310 | loss 1.2456 | lr 5.34e-04 | grad 0.81 | tok/s 31239
step   2320 | loss 1.2265 | lr 5.34e-04 | grad 1.05 | tok/s 30535
step   2330 | loss 1.1954 | lr 5.34e-04 | grad 0.66 | tok/s 30256
step   2340 | loss 1.3316 | lr 5.34e-04 | grad 0.65 | tok/s 30601
step   2350 | loss 1.2645 | lr 5.34e-04 | grad 0.97 | tok/s 30597
step   2360 | loss 1.2508 | lr 5.34e-04 | grad 0.65 | tok/s 30587
step   2370 | loss 1.3990 | lr 5.34e-04 | grad 1.96 | tok/s 30591
step   2380 | loss 1.1799 | lr 5.34e-04 | grad 0.64 | tok/s 30675
step   2390 | loss 1.2402 | lr 5.34e-04 | grad 1.41 | tok/s 30539
step   2400 | loss 1.1929 | lr 5.34e-04 | grad 1.62 | tok/s 31671
step   2410 | loss 0.9913 | lr 5.34e-04 | grad 0.69 | tok/s 31339
step   2420 | loss 1.1827 | lr 5.34e-04 | grad 0.88 | tok/s 30821
step   2430 | loss 1.1729 | lr 5.34e-04 | grad 0.89 | tok/s 30577
step   2440 | loss 1.3032 | lr 5.34e-04 | grad 2.19 | tok/s 31055
step   2450 | loss 1.1471 | lr 5.34e-04 | grad 0.87 | tok/s 31208
step   2460 | loss 1.5036 | lr 5.34e-04 | grad 1.08 | tok/s 30241
step   2470 | loss 1.2623 | lr 5.34e-04 | grad 0.89 | tok/s 30374
step   2480 | loss 1.2526 | lr 5.34e-04 | grad 0.80 | tok/s 29822
step   2490 | loss 1.5966 | lr 5.34e-04 | grad 0.89 | tok/s 31197
step   2500 | loss 1.3290 | lr 5.34e-04 | grad 0.75 | tok/s 30870
step   2510 | loss 1.3279 | lr 5.34e-04 | grad 0.83 | tok/s 30811
step   2520 | loss 1.2078 | lr 5.34e-04 | grad 0.80 | tok/s 30632
step   2530 | loss 1.2586 | lr 5.34e-04 | grad 0.96 | tok/s 30722
step   2540 | loss 1.2728 | lr 5.34e-04 | grad 0.60 | tok/s 31279
step   2550 | loss 1.2168 | lr 5.34e-04 | grad 0.64 | tok/s 31681
step   2560 | loss 1.1628 | lr 5.34e-04 | grad 0.83 | tok/s 31880
step   2570 | loss 1.2377 | lr 5.34e-04 | grad 0.74 | tok/s 30752
step   2580 | loss 1.2876 | lr 5.34e-04 | grad 1.00 | tok/s 30477
step   2590 | loss 1.1870 | lr 5.34e-04 | grad 0.61 | tok/s 29838
step   2600 | loss 1.5302 | lr 5.34e-04 | grad 0.61 | tok/s 31539
step   2610 | loss 1.1683 | lr 5.34e-04 | grad 0.56 | tok/s 31905
step   2620 | loss 1.1320 | lr 5.34e-04 | grad 0.59 | tok/s 31892
step   2630 | loss 1.2068 | lr 5.34e-04 | grad 2.94 | tok/s 31698
step   2640 | loss 1.2430 | lr 5.34e-04 | grad 0.73 | tok/s 30474
step   2650 | loss 1.3583 | lr 5.34e-04 | grad 1.87 | tok/s 30504
step   2660 | loss 1.5210 | lr 5.34e-04 | grad 0.64 | tok/s 30797
step   2670 | loss 1.1670 | lr 5.34e-04 | grad 1.17 | tok/s 31092
step   2680 | loss 1.1796 | lr 5.34e-04 | grad 1.12 | tok/s 30698
step   2690 | loss 1.2941 | lr 5.34e-04 | grad 1.02 | tok/s 30717
step   2700 | loss 1.3754 | lr 5.34e-04 | grad 1.02 | tok/s 30092
step   2710 | loss 1.2454 | lr 5.34e-04 | grad 0.69 | tok/s 30620
step   2720 | loss 1.1956 | lr 5.34e-04 | grad 0.76 | tok/s 30887
step   2730 | loss 1.2007 | lr 5.34e-04 | grad 1.17 | tok/s 30347
step   2740 | loss 1.5157 | lr 5.34e-04 | grad 0.82 | tok/s 30803
step   2750 | loss 1.1455 | lr 5.34e-04 | grad 1.45 | tok/s 31129
step   2760 | loss 1.4131 | lr 5.34e-04 | grad 1.10 | tok/s 31370
step   2770 | loss 1.2652 | lr 5.34e-04 | grad 1.00 | tok/s 31206
step   2780 | loss 1.2237 | lr 5.34e-04 | grad 0.64 | tok/s 30770
step   2790 | loss 1.1836 | lr 5.34e-04 | grad 0.72 | tok/s 31066
step   2800 | loss 1.1526 | lr 5.34e-04 | grad 0.96 | tok/s 30663
step   2810 | loss 1.5299 | lr 5.34e-04 | grad 3.34 | tok/s 31200
step   2820 | loss 1.2234 | lr 5.34e-04 | grad 0.68 | tok/s 30873
step   2830 | loss 1.4442 | lr 5.34e-04 | grad 0.74 | tok/s 30846
step   2840 | loss 1.2203 | lr 5.34e-04 | grad 2.56 | tok/s 30486
step   2850 | loss 1.3267 | lr 5.34e-04 | grad 0.68 | tok/s 30521
step   2860 | loss 1.2875 | lr 5.34e-04 | grad 1.48 | tok/s 30910
step   2870 | loss 1.2525 | lr 5.34e-04 | grad 0.88 | tok/s 30444
step   2880 | loss 1.2333 | lr 5.34e-04 | grad 0.71 | tok/s 30364
step   2890 | loss 1.2032 | lr 5.34e-04 | grad 0.90 | tok/s 30374
step   2900 | loss 1.3168 | lr 5.34e-04 | grad 0.57 | tok/s 30434
step   2910 | loss 1.2330 | lr 5.34e-04 | grad 0.76 | tok/s 31152
step   2920 | loss 1.2259 | lr 5.34e-04 | grad 1.12 | tok/s 31075
step   2930 | loss 1.1301 | lr 5.34e-04 | grad 0.95 | tok/s 30581
step   2940 | loss 1.1751 | lr 5.34e-04 | grad 1.19 | tok/s 31077
step   2950 | loss 1.2423 | lr 5.34e-04 | grad 0.94 | tok/s 30954
step   2960 | loss 1.2485 | lr 5.34e-04 | grad 1.05 | tok/s 30601
step   2970 | loss 1.2907 | lr 5.34e-04 | grad 0.96 | tok/s 30753
step   2980 | loss 1.2716 | lr 5.34e-04 | grad 4.53 | tok/s 30204
step   2990 | loss 1.1939 | lr 5.34e-04 | grad 0.90 | tok/s 30699
step   3000 | loss 1.1806 | lr 5.34e-04 | grad 0.69 | tok/s 30477
  >>> saved checkpoint: checkpoint_step_003000_loss_1.1806.pt
step   3010 | loss 1.1919 | lr 5.34e-04 | grad 0.79 | tok/s 22004
step   3020 | loss 1.1916 | lr 5.34e-04 | grad 0.77 | tok/s 31317
step   3030 | loss 1.2293 | lr 5.34e-04 | grad 0.86 | tok/s 30524
step   3040 | loss 1.1809 | lr 5.34e-04 | grad 0.70 | tok/s 31069
step   3050 | loss 1.2255 | lr 5.34e-04 | grad 0.82 | tok/s 30293
step   3060 | loss 1.5191 | lr 5.34e-04 | grad 2.23 | tok/s 31428
step   3070 | loss 1.5441 | lr 5.34e-04 | grad 1.77 | tok/s 31993
step   3080 | loss 1.3506 | lr 5.34e-04 | grad 1.24 | tok/s 31973
step   3090 | loss 1.2716 | lr 5.34e-04 | grad 0.66 | tok/s 30247
step   3100 | loss 1.6227 | lr 5.34e-04 | grad 0.88 | tok/s 30946
step   3110 | loss 1.2035 | lr 5.34e-04 | grad 2.52 | tok/s 30902
step   3120 | loss 1.3758 | lr 5.34e-04 | grad 0.70 | tok/s 30824
step   3130 | loss 1.2600 | lr 5.34e-04 | grad 0.84 | tok/s 30731
step   3140 | loss 1.3431 | lr 5.34e-04 | grad 0.68 | tok/s 31475
step   3150 | loss 1.2023 | lr 5.34e-04 | grad 0.59 | tok/s 31383
step   3160 | loss 1.3117 | lr 5.34e-04 | grad 1.84 | tok/s 30689
step   3170 | loss 1.2399 | lr 5.34e-04 | grad 0.60 | tok/s 29448
step   3180 | loss 1.2599 | lr 5.34e-04 | grad 0.70 | tok/s 29405
step   3190 | loss 1.1217 | lr 5.34e-04 | grad 0.87 | tok/s 30748
step   3200 | loss 1.3734 | lr 5.34e-04 | grad 1.11 | tok/s 31195
step   3210 | loss 1.2146 | lr 5.34e-04 | grad 0.91 | tok/s 30684
step   3220 | loss 1.2760 | lr 5.34e-04 | grad 1.19 | tok/s 31780
step   3230 | loss 1.3191 | lr 5.34e-04 | grad 1.73 | tok/s 30308
step   3240 | loss 1.2274 | lr 5.34e-04 | grad 2.75 | tok/s 30755
step   3250 | loss 1.3159 | lr 5.34e-04 | grad 1.09 | tok/s 30415
step   3260 | loss 1.2300 | lr 5.34e-04 | grad 3.42 | tok/s 31148
step   3270 | loss 1.2352 | lr 5.34e-04 | grad 0.73 | tok/s 30230
step   3280 | loss 1.2518 | lr 5.34e-04 | grad 0.73 | tok/s 29840
step   3290 | loss 1.2787 | lr 5.34e-04 | grad 0.64 | tok/s 31878
step   3300 | loss 1.1941 | lr 5.34e-04 | grad 0.93 | tok/s 30965
step   3310 | loss 1.1511 | lr 5.34e-04 | grad 0.70 | tok/s 30785
step   3320 | loss 1.4400 | lr 5.34e-04 | grad 2.09 | tok/s 31721
step   3330 | loss 1.2611 | lr 5.34e-04 | grad 0.68 | tok/s 29465
step   3340 | loss 1.1365 | lr 5.34e-04 | grad 0.89 | tok/s 30281
step   3350 | loss 1.2332 | lr 5.34e-04 | grad 1.78 | tok/s 31405
step   3360 | loss 1.2458 | lr 5.34e-04 | grad 1.63 | tok/s 30962
step   3370 | loss 1.5577 | lr 5.34e-04 | grad 1.22 | tok/s 30946
step   3380 | loss 1.3067 | lr 5.34e-04 | grad 1.07 | tok/s 31069
step   3390 | loss 1.2287 | lr 5.34e-04 | grad 0.79 | tok/s 31055
step   3400 | loss 1.0636 | lr 5.34e-04 | grad 1.23 | tok/s 31468
step   3410 | loss 1.3309 | lr 5.34e-04 | grad 2.38 | tok/s 29573
step   3420 | loss 1.1953 | lr 5.34e-04 | grad 0.84 | tok/s 30558

Training complete! Final step: 3422
