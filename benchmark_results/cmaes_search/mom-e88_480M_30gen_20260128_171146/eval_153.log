Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_153/levelMoME88_100m_20260128_185241
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 477,509,312 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 16.9638 | lr 3.00e-04 | grad 18.25 | tok/s 9380
step     20 | loss 2.9904 | lr 3.00e-04 | grad 82.50 | tok/s 21304
step     30 | loss 3.0764 | lr 3.00e-04 | grad 4.50 | tok/s 22640
step     40 | loss 5.2551 | lr 3.00e-04 | grad 21.88 | tok/s 22675
step     50 | loss 4.1253 | lr 3.00e-04 | grad 11.25 | tok/s 22997
step     60 | loss 4.0041 | lr 3.00e-04 | grad 11.31 | tok/s 22957
step     70 | loss 3.2991 | lr 3.00e-04 | grad 15.75 | tok/s 22802
step     80 | loss 2.9815 | lr 3.00e-04 | grad 5.62 | tok/s 22749
step     90 | loss 2.7622 | lr 3.00e-04 | grad 6.94 | tok/s 22741
step    100 | loss 2.5459 | lr 3.00e-04 | grad 8.06 | tok/s 22693
step    110 | loss 2.7738 | lr 3.00e-04 | grad 16.12 | tok/s 22409
step    120 | loss 2.8793 | lr 3.00e-04 | grad 6.06 | tok/s 21318
step    130 | loss 2.4071 | lr 3.00e-04 | grad 5.22 | tok/s 21994
step    140 | loss 2.7864 | lr 3.00e-04 | grad 10.69 | tok/s 22141
step    150 | loss 2.0912 | lr 3.00e-04 | grad 6.19 | tok/s 22487
step    160 | loss 2.5359 | lr 3.00e-04 | grad 8.31 | tok/s 21547
step    170 | loss 2.5939 | lr 3.00e-04 | grad 3.73 | tok/s 21554
step    180 | loss 2.4645 | lr 3.00e-04 | grad 3.36 | tok/s 21676
step    190 | loss 2.1708 | lr 3.00e-04 | grad 4.62 | tok/s 21772
step    200 | loss 2.0329 | lr 3.00e-04 | grad 3.31 | tok/s 22401
step    210 | loss 2.2480 | lr 3.00e-04 | grad 3.81 | tok/s 21331
step    220 | loss 2.8082 | lr 3.00e-04 | grad 47.00 | tok/s 21555
step    230 | loss 2.3039 | lr 3.00e-04 | grad 4.72 | tok/s 21369
step    240 | loss 2.5288 | lr 3.00e-04 | grad 3.62 | tok/s 21791
step    250 | loss 2.1197 | lr 3.00e-04 | grad 4.66 | tok/s 21792
step    260 | loss 2.2529 | lr 3.00e-04 | grad 5.91 | tok/s 22188
step    270 | loss 2.0494 | lr 3.00e-04 | grad 3.61 | tok/s 21528
step    280 | loss 2.0784 | lr 3.00e-04 | grad 2.95 | tok/s 20681
step    290 | loss 1.9725 | lr 3.00e-04 | grad 3.42 | tok/s 21021
step    300 | loss 2.3171 | lr 3.00e-04 | grad 4.00 | tok/s 21418
step    310 | loss 1.9481 | lr 3.00e-04 | grad 4.16 | tok/s 21068
step    320 | loss 2.1731 | lr 3.00e-04 | grad 2.81 | tok/s 21423
step    330 | loss 2.0124 | lr 3.00e-04 | grad 3.02 | tok/s 21563
step    340 | loss 2.3760 | lr 3.00e-04 | grad 3.67 | tok/s 21657
step    350 | loss 2.1220 | lr 3.00e-04 | grad 3.14 | tok/s 22148
step    360 | loss 1.8738 | lr 3.00e-04 | grad 2.70 | tok/s 21223
step    370 | loss 1.8240 | lr 3.00e-04 | grad 3.33 | tok/s 22303
step    380 | loss 1.5770 | lr 3.00e-04 | grad 2.88 | tok/s 22473
step    390 | loss 1.4691 | lr 3.00e-04 | grad 2.83 | tok/s 22451
step    400 | loss 2.1702 | lr 3.00e-04 | grad 3.34 | tok/s 21319
step    410 | loss 2.0864 | lr 3.00e-04 | grad 3.22 | tok/s 21545
step    420 | loss 1.9943 | lr 3.00e-04 | grad 12.00 | tok/s 22395
step    430 | loss 1.9389 | lr 3.00e-04 | grad 3.34 | tok/s 21901
step    440 | loss 2.0503 | lr 3.00e-04 | grad 4.62 | tok/s 21585
step    450 | loss 1.8960 | lr 3.00e-04 | grad 3.94 | tok/s 21589
step    460 | loss 1.9119 | lr 3.00e-04 | grad 2.25 | tok/s 21808
step    470 | loss 1.9331 | lr 3.00e-04 | grad 4.12 | tok/s 22012
step    480 | loss 1.8950 | lr 3.00e-04 | grad 4.34 | tok/s 21998
step    490 | loss 1.9718 | lr 3.00e-04 | grad 2.88 | tok/s 21626
step    500 | loss 2.1402 | lr 3.00e-04 | grad 3.88 | tok/s 21579
step    510 | loss 1.9383 | lr 3.00e-04 | grad 3.17 | tok/s 20644
step    520 | loss 1.7847 | lr 3.00e-04 | grad 2.95 | tok/s 21691
step    530 | loss 2.0155 | lr 3.00e-04 | grad 2.81 | tok/s 21655
step    540 | loss 1.8862 | lr 3.00e-04 | grad 3.27 | tok/s 20906
step    550 | loss 1.6466 | lr 3.00e-04 | grad 2.92 | tok/s 22028
step    560 | loss 1.6818 | lr 3.00e-04 | grad 2.78 | tok/s 22382
step    570 | loss 1.5857 | lr 3.00e-04 | grad 2.58 | tok/s 22402
step    580 | loss 1.5178 | lr 3.00e-04 | grad 2.50 | tok/s 22427
step    590 | loss 1.5945 | lr 3.00e-04 | grad 3.12 | tok/s 22378
step    600 | loss 1.5142 | lr 3.00e-04 | grad 2.70 | tok/s 22418
step    610 | loss 1.5242 | lr 3.00e-04 | grad 2.55 | tok/s 22435
step    620 | loss 1.7023 | lr 3.00e-04 | grad 16.88 | tok/s 22139
step    630 | loss 2.0084 | lr 3.00e-04 | grad 5.41 | tok/s 21391
step    640 | loss 1.9765 | lr 3.00e-04 | grad 3.98 | tok/s 21455
step    650 | loss 1.8072 | lr 3.00e-04 | grad 4.59 | tok/s 21538
step    660 | loss 1.9172 | lr 3.00e-04 | grad 5.16 | tok/s 22150
step    670 | loss 1.8646 | lr 3.00e-04 | grad 3.75 | tok/s 21297
step    680 | loss 1.8903 | lr 3.00e-04 | grad 2.52 | tok/s 21203
step    690 | loss 1.9346 | lr 3.00e-04 | grad 4.84 | tok/s 21307
step    700 | loss 1.7089 | lr 3.00e-04 | grad 2.45 | tok/s 21412
step    710 | loss 1.9652 | lr 3.00e-04 | grad 4.34 | tok/s 21236
step    720 | loss 1.5470 | lr 3.00e-04 | grad 3.14 | tok/s 21876
step    730 | loss 1.8422 | lr 3.00e-04 | grad 5.12 | tok/s 21468
step    740 | loss 2.1439 | lr 3.00e-04 | grad 5.75 | tok/s 22220
step    750 | loss 1.7676 | lr 3.00e-04 | grad 2.94 | tok/s 22371
step    760 | loss 1.8587 | lr 3.00e-04 | grad 3.84 | tok/s 21942
step    770 | loss 1.8503 | lr 3.00e-04 | grad 3.22 | tok/s 21589
step    780 | loss 1.7157 | lr 3.00e-04 | grad 2.75 | tok/s 21708
step    790 | loss 2.0744 | lr 3.00e-04 | grad 4.62 | tok/s 22128
step    800 | loss 1.4230 | lr 3.00e-04 | grad 1.74 | tok/s 21972

Training complete! Final step: 806
