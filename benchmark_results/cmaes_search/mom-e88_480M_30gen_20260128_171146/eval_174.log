Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_174/levelMoME88_100m_20260128_190316
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 477,509,312 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 16.9105 | lr 3.00e-04 | grad 16.25 | tok/s 9568
step     20 | loss 2.9233 | lr 3.00e-04 | grad 11.50 | tok/s 21668
step     30 | loss 2.9429 | lr 3.00e-04 | grad 3.47 | tok/s 23022
step     40 | loss 5.1337 | lr 3.00e-04 | grad 27.50 | tok/s 23132
step     50 | loss 4.0385 | lr 3.00e-04 | grad 10.06 | tok/s 23429
step     60 | loss 3.8828 | lr 3.00e-04 | grad 12.50 | tok/s 23349
step     70 | loss 3.1506 | lr 3.00e-04 | grad 19.12 | tok/s 23270
step     80 | loss 2.7942 | lr 3.00e-04 | grad 4.28 | tok/s 23226
step     90 | loss 2.6459 | lr 3.00e-04 | grad 6.06 | tok/s 23141
step    100 | loss 2.4684 | lr 3.00e-04 | grad 8.12 | tok/s 23157
step    110 | loss 2.8433 | lr 3.00e-04 | grad 17.88 | tok/s 22816
step    120 | loss 2.8405 | lr 3.00e-04 | grad 5.22 | tok/s 21778
step    130 | loss 2.3502 | lr 3.00e-04 | grad 3.83 | tok/s 22462
step    140 | loss 2.7539 | lr 3.00e-04 | grad 9.56 | tok/s 22627
step    150 | loss 2.0758 | lr 3.00e-04 | grad 9.56 | tok/s 23026
step    160 | loss 2.5555 | lr 3.00e-04 | grad 6.91 | tok/s 21974
step    170 | loss 2.5365 | lr 3.00e-04 | grad 3.73 | tok/s 21954
step    180 | loss 2.4610 | lr 3.00e-04 | grad 3.59 | tok/s 22128
step    190 | loss 2.1623 | lr 3.00e-04 | grad 4.41 | tok/s 22179
step    200 | loss 2.0113 | lr 3.00e-04 | grad 5.72 | tok/s 22897
step    210 | loss 2.2519 | lr 3.00e-04 | grad 4.66 | tok/s 21672
step    220 | loss 2.5868 | lr 3.00e-04 | grad 27.12 | tok/s 21975
step    230 | loss 2.3069 | lr 3.00e-04 | grad 5.09 | tok/s 21729
step    240 | loss 2.5304 | lr 3.00e-04 | grad 3.72 | tok/s 22237
step    250 | loss 2.1143 | lr 3.00e-04 | grad 4.62 | tok/s 22217
step    260 | loss 2.2544 | lr 3.00e-04 | grad 4.38 | tok/s 22692
step    270 | loss 2.0357 | lr 3.00e-04 | grad 3.23 | tok/s 21947
step    280 | loss 2.0732 | lr 3.00e-04 | grad 2.91 | tok/s 21012
step    290 | loss 1.9616 | lr 3.00e-04 | grad 3.59 | tok/s 21395
step    300 | loss 2.2964 | lr 3.00e-04 | grad 3.47 | tok/s 21814
step    310 | loss 1.9622 | lr 3.00e-04 | grad 3.78 | tok/s 21458
step    320 | loss 2.1762 | lr 3.00e-04 | grad 3.59 | tok/s 21841
step    330 | loss 2.0082 | lr 3.00e-04 | grad 3.03 | tok/s 22030
step    340 | loss 2.3904 | lr 3.00e-04 | grad 3.84 | tok/s 22085
step    350 | loss 2.1003 | lr 3.00e-04 | grad 3.14 | tok/s 22680
step    360 | loss 1.8810 | lr 3.00e-04 | grad 2.38 | tok/s 21625
step    370 | loss 1.8324 | lr 3.00e-04 | grad 3.44 | tok/s 22806
step    380 | loss 1.5762 | lr 3.00e-04 | grad 3.20 | tok/s 23005
step    390 | loss 1.4655 | lr 3.00e-04 | grad 2.56 | tok/s 22997
step    400 | loss 2.1529 | lr 3.00e-04 | grad 3.72 | tok/s 21785
step    410 | loss 2.0784 | lr 3.00e-04 | grad 3.27 | tok/s 22005
step    420 | loss 1.9881 | lr 3.00e-04 | grad 10.44 | tok/s 22914
step    430 | loss 1.9196 | lr 3.00e-04 | grad 3.42 | tok/s 22385
step    440 | loss 2.0438 | lr 3.00e-04 | grad 5.03 | tok/s 21994
step    450 | loss 1.8802 | lr 3.00e-04 | grad 3.81 | tok/s 22011
step    460 | loss 1.9091 | lr 3.00e-04 | grad 2.48 | tok/s 22276
step    470 | loss 1.9262 | lr 3.00e-04 | grad 4.81 | tok/s 22501
step    480 | loss 1.9026 | lr 3.00e-04 | grad 3.56 | tok/s 22499
step    490 | loss 1.9665 | lr 3.00e-04 | grad 3.00 | tok/s 22089
step    500 | loss 2.1411 | lr 3.00e-04 | grad 3.91 | tok/s 22088
step    510 | loss 1.9396 | lr 3.00e-04 | grad 3.36 | tok/s 21014
step    520 | loss 1.7887 | lr 3.00e-04 | grad 3.06 | tok/s 22163
step    530 | loss 2.0050 | lr 3.00e-04 | grad 2.89 | tok/s 22094
step    540 | loss 1.8825 | lr 3.00e-04 | grad 4.34 | tok/s 21316
step    550 | loss 1.6533 | lr 3.00e-04 | grad 3.28 | tok/s 22548
step    560 | loss 1.6731 | lr 3.00e-04 | grad 3.03 | tok/s 22989
step    570 | loss 1.5805 | lr 3.00e-04 | grad 2.25 | tok/s 23009
step    580 | loss 1.5160 | lr 3.00e-04 | grad 3.25 | tok/s 22994
step    590 | loss 1.5967 | lr 3.00e-04 | grad 3.14 | tok/s 23004
step    600 | loss 1.5185 | lr 3.00e-04 | grad 2.81 | tok/s 23003
step    610 | loss 1.5175 | lr 3.00e-04 | grad 2.17 | tok/s 23008
step    620 | loss 1.6890 | lr 3.00e-04 | grad 11.56 | tok/s 22684
step    630 | loss 1.9996 | lr 3.00e-04 | grad 5.56 | tok/s 21872
step    640 | loss 1.9731 | lr 3.00e-04 | grad 3.00 | tok/s 21914
step    650 | loss 1.8253 | lr 3.00e-04 | grad 4.75 | tok/s 21986
step    660 | loss 1.9136 | lr 3.00e-04 | grad 6.28 | tok/s 22712
step    670 | loss 1.8473 | lr 3.00e-04 | grad 5.12 | tok/s 21740
step    680 | loss 1.8886 | lr 3.00e-04 | grad 2.17 | tok/s 21632
step    690 | loss 1.9443 | lr 3.00e-04 | grad 7.38 | tok/s 21748
step    700 | loss 1.7030 | lr 3.00e-04 | grad 2.69 | tok/s 21849
step    710 | loss 1.9676 | lr 3.00e-04 | grad 4.66 | tok/s 21662
step    720 | loss 1.5433 | lr 3.00e-04 | grad 2.86 | tok/s 22456
step    730 | loss 1.8426 | lr 3.00e-04 | grad 6.03 | tok/s 21979
step    740 | loss 2.1491 | lr 3.00e-04 | grad 5.94 | tok/s 22781
step    750 | loss 1.7660 | lr 3.00e-04 | grad 3.16 | tok/s 22934
step    760 | loss 1.8665 | lr 3.00e-04 | grad 3.88 | tok/s 22468
step    770 | loss 1.8308 | lr 3.00e-04 | grad 3.34 | tok/s 22078
step    780 | loss 1.7111 | lr 3.00e-04 | grad 2.77 | tok/s 22224
step    790 | loss 2.0666 | lr 3.00e-04 | grad 4.53 | tok/s 22696
step    800 | loss 1.4156 | lr 3.00e-04 | grad 2.31 | tok/s 22491
step    810 | loss 1.7085 | lr 3.00e-04 | grad 3.67 | tok/s 21581
step    820 | loss 1.7398 | lr 3.00e-04 | grad 6.03 | tok/s 22032

Training complete! Final step: 823
