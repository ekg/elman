Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_130/levelMoME88_100m_20260128_183648
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 478,655,036 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 14.6229 | lr 3.00e-04 | grad 10.38 | tok/s 8913
step     20 | loss 3.5130 | lr 3.00e-04 | grad 10.31 | tok/s 17793
step     30 | loss 3.2794 | lr 3.00e-04 | grad 11.25 | tok/s 18751
step     40 | loss 4.8696 | lr 3.00e-04 | grad 16.62 | tok/s 19192
step     50 | loss 4.3967 | lr 3.00e-04 | grad 12.38 | tok/s 19387
step     60 | loss 3.7612 | lr 3.00e-04 | grad 8.06 | tok/s 19371
step     70 | loss 3.1895 | lr 3.00e-04 | grad 5.69 | tok/s 19352
step     80 | loss 2.8768 | lr 3.00e-04 | grad 4.56 | tok/s 19308
step     90 | loss 2.6228 | lr 3.00e-04 | grad 5.12 | tok/s 19296
step    100 | loss 2.4149 | lr 3.00e-04 | grad 7.22 | tok/s 19280
step    110 | loss 2.5357 | lr 3.00e-04 | grad 5.03 | tok/s 19157
step    120 | loss 3.2335 | lr 3.00e-04 | grad 2.75 | tok/s 18263
step    130 | loss 2.3796 | lr 3.00e-04 | grad 5.91 | tok/s 18686
step    140 | loss 2.7038 | lr 3.00e-04 | grad 11.56 | tok/s 18755
step    150 | loss 2.1610 | lr 3.00e-04 | grad 8.25 | tok/s 19235
step    160 | loss 2.6026 | lr 3.00e-04 | grad 2.91 | tok/s 18570
step    170 | loss 2.5272 | lr 3.00e-04 | grad 2.30 | tok/s 18297
step    180 | loss 2.4602 | lr 3.00e-04 | grad 3.53 | tok/s 18707
step    190 | loss 2.1994 | lr 3.00e-04 | grad 2.75 | tok/s 18380
step    200 | loss 2.0103 | lr 3.00e-04 | grad 2.52 | tok/s 19197
step    210 | loss 2.1750 | lr 3.00e-04 | grad 6.44 | tok/s 18270
step    220 | loss 2.5487 | lr 3.00e-04 | grad 25.38 | tok/s 18449
step    230 | loss 2.3563 | lr 3.00e-04 | grad 3.36 | tok/s 18419
step    240 | loss 2.5861 | lr 3.00e-04 | grad 5.91 | tok/s 18657
step    250 | loss 2.0625 | lr 3.00e-04 | grad 2.25 | tok/s 18551
step    260 | loss 2.1956 | lr 3.00e-04 | grad 4.00 | tok/s 19048
step    270 | loss 2.0834 | lr 3.00e-04 | grad 2.83 | tok/s 18633
step    280 | loss 2.0309 | lr 3.00e-04 | grad 2.22 | tok/s 17526
step    290 | loss 1.9528 | lr 3.00e-04 | grad 2.83 | tok/s 18124
step    300 | loss 2.2424 | lr 3.00e-04 | grad 5.28 | tok/s 18266
step    310 | loss 1.9047 | lr 3.00e-04 | grad 2.23 | tok/s 18164
step    320 | loss 2.1540 | lr 3.00e-04 | grad 5.03 | tok/s 18380
step    330 | loss 1.9509 | lr 3.00e-04 | grad 2.62 | tok/s 18569
step    340 | loss 2.3306 | lr 3.00e-04 | grad 3.52 | tok/s 18505
step    350 | loss 2.1306 | lr 3.00e-04 | grad 2.75 | tok/s 19011
step    360 | loss 1.8367 | lr 3.00e-04 | grad 2.52 | tok/s 18211
step    370 | loss 1.8070 | lr 3.00e-04 | grad 3.36 | tok/s 19168
step    380 | loss 1.5670 | lr 3.00e-04 | grad 2.98 | tok/s 19328
step    390 | loss 1.4399 | lr 3.00e-04 | grad 2.09 | tok/s 19335
step    400 | loss 2.0618 | lr 3.00e-04 | grad 2.42 | tok/s 18312
step    410 | loss 2.0076 | lr 3.00e-04 | grad 2.86 | tok/s 18523
step    420 | loss 1.9642 | lr 3.00e-04 | grad 5.50 | tok/s 19280
step    430 | loss 1.9315 | lr 3.00e-04 | grad 2.23 | tok/s 18981
step    440 | loss 1.9754 | lr 3.00e-04 | grad 3.20 | tok/s 18380
step    450 | loss 1.8603 | lr 3.00e-04 | grad 1.91 | tok/s 18595
step    460 | loss 1.8876 | lr 3.00e-04 | grad 2.66 | tok/s 18857
step    470 | loss 1.8532 | lr 3.00e-04 | grad 5.00 | tok/s 18750
step    480 | loss 1.8860 | lr 3.00e-04 | grad 3.77 | tok/s 19120
step    490 | loss 1.9137 | lr 3.00e-04 | grad 3.25 | tok/s 18388
step    500 | loss 2.0935 | lr 3.00e-04 | grad 2.33 | tok/s 18666
step    510 | loss 1.9145 | lr 3.00e-04 | grad 2.58 | tok/s 17861
step    520 | loss 1.7511 | lr 3.00e-04 | grad 2.75 | tok/s 18673
step    530 | loss 1.9425 | lr 3.00e-04 | grad 2.78 | tok/s 18371
step    540 | loss 1.8537 | lr 3.00e-04 | grad 2.00 | tok/s 18015
step    550 | loss 1.5842 | lr 3.00e-04 | grad 4.25 | tok/s 18829
step    560 | loss 1.6610 | lr 3.00e-04 | grad 2.83 | tok/s 19312
step    570 | loss 1.5371 | lr 3.00e-04 | grad 2.66 | tok/s 19300
step    580 | loss 1.4874 | lr 3.00e-04 | grad 1.95 | tok/s 19296
step    590 | loss 1.5343 | lr 3.00e-04 | grad 1.72 | tok/s 19306
step    600 | loss 1.4830 | lr 3.00e-04 | grad 2.20 | tok/s 19292
step    610 | loss 1.4924 | lr 3.00e-04 | grad 1.99 | tok/s 19294
step    620 | loss 1.4839 | lr 3.00e-04 | grad 2.00 | tok/s 19239
step    630 | loss 2.0443 | lr 3.00e-04 | grad 5.78 | tok/s 18231
step    640 | loss 1.9807 | lr 3.00e-04 | grad 2.83 | tok/s 18448
step    650 | loss 1.7743 | lr 3.00e-04 | grad 2.23 | tok/s 18432
step    660 | loss 1.8176 | lr 3.00e-04 | grad 2.53 | tok/s 19109
step    670 | loss 1.8689 | lr 3.00e-04 | grad 6.94 | tok/s 18499
step    680 | loss 1.8647 | lr 3.00e-04 | grad 3.05 | tok/s 18211
step    690 | loss 1.8561 | lr 3.00e-04 | grad 3.86 | tok/s 18081

Training complete! Final step: 692
