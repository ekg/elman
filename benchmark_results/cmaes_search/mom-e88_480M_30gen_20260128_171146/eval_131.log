Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_131/levelMoME88_100m_20260128_183648
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 475,532,936 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 15.0165 | lr 3.00e-04 | grad 14.44 | tok/s 8919
step     20 | loss 3.4189 | lr 3.00e-04 | grad 4.69 | tok/s 17893
step     30 | loss 3.1428 | lr 3.00e-04 | grad 9.00 | tok/s 18969
step     40 | loss 5.0115 | lr 3.00e-04 | grad 21.00 | tok/s 19245
step     50 | loss 4.3449 | lr 3.00e-04 | grad 13.25 | tok/s 19522
step     60 | loss 3.8929 | lr 3.00e-04 | grad 8.75 | tok/s 19423
step     70 | loss 3.1776 | lr 3.00e-04 | grad 9.56 | tok/s 19420
step     80 | loss 2.8678 | lr 3.00e-04 | grad 6.38 | tok/s 19472
step     90 | loss 2.6993 | lr 3.00e-04 | grad 5.97 | tok/s 19325
step    100 | loss 2.4832 | lr 3.00e-04 | grad 6.22 | tok/s 19422
step    110 | loss 2.5393 | lr 3.00e-04 | grad 6.78 | tok/s 19189
step    120 | loss 3.2588 | lr 3.00e-04 | grad 4.56 | tok/s 18299
step    130 | loss 2.4160 | lr 3.00e-04 | grad 6.56 | tok/s 18715
step    140 | loss 2.7056 | lr 3.00e-04 | grad 13.56 | tok/s 18843
step    150 | loss 2.1810 | lr 3.00e-04 | grad 8.38 | tok/s 19335
step    160 | loss 2.6126 | lr 3.00e-04 | grad 3.17 | tok/s 18633
step    170 | loss 2.5320 | lr 3.00e-04 | grad 3.22 | tok/s 18341
step    180 | loss 2.4558 | lr 3.00e-04 | grad 4.09 | tok/s 18767
step    190 | loss 2.2160 | lr 3.00e-04 | grad 3.38 | tok/s 18394
step    200 | loss 2.0412 | lr 3.00e-04 | grad 2.92 | tok/s 19164
step    210 | loss 2.2107 | lr 3.00e-04 | grad 7.88 | tok/s 18291
step    220 | loss 2.5119 | lr 3.00e-04 | grad 8.94 | tok/s 18520
step    230 | loss 2.2598 | lr 3.00e-04 | grad 4.38 | tok/s 18528
step    240 | loss 2.6056 | lr 3.00e-04 | grad 6.06 | tok/s 18651
step    250 | loss 2.0704 | lr 3.00e-04 | grad 2.39 | tok/s 18568
step    260 | loss 2.2256 | lr 3.00e-04 | grad 3.98 | tok/s 19139
step    270 | loss 2.0886 | lr 3.00e-04 | grad 3.48 | tok/s 18669
step    280 | loss 2.0399 | lr 3.00e-04 | grad 2.75 | tok/s 17466
step    290 | loss 1.9617 | lr 3.00e-04 | grad 3.19 | tok/s 18168
step    300 | loss 2.2413 | lr 3.00e-04 | grad 2.98 | tok/s 18207
step    310 | loss 1.9109 | lr 3.00e-04 | grad 2.66 | tok/s 18235
step    320 | loss 2.1687 | lr 3.00e-04 | grad 5.78 | tok/s 18275
step    330 | loss 1.9706 | lr 3.00e-04 | grad 2.77 | tok/s 18551
step    340 | loss 2.3884 | lr 3.00e-04 | grad 3.50 | tok/s 18553
step    350 | loss 2.1199 | lr 3.00e-04 | grad 3.06 | tok/s 19138
step    360 | loss 1.8523 | lr 3.00e-04 | grad 2.66 | tok/s 18246
step    370 | loss 1.8277 | lr 3.00e-04 | grad 2.48 | tok/s 19244
step    380 | loss 1.5658 | lr 3.00e-04 | grad 2.64 | tok/s 19284
step    390 | loss 1.4435 | lr 3.00e-04 | grad 2.05 | tok/s 19382
step    400 | loss 2.0715 | lr 3.00e-04 | grad 2.53 | tok/s 18280
step    410 | loss 2.0471 | lr 3.00e-04 | grad 3.30 | tok/s 18676
step    420 | loss 2.0070 | lr 3.00e-04 | grad 6.50 | tok/s 19334
step    430 | loss 1.9188 | lr 3.00e-04 | grad 2.28 | tok/s 19074
step    440 | loss 1.9824 | lr 3.00e-04 | grad 3.20 | tok/s 18444
step    450 | loss 1.8781 | lr 3.00e-04 | grad 2.23 | tok/s 18649
step    460 | loss 1.8804 | lr 3.00e-04 | grad 2.70 | tok/s 18974
step    470 | loss 1.8582 | lr 3.00e-04 | grad 5.28 | tok/s 18676
step    480 | loss 1.9370 | lr 3.00e-04 | grad 4.56 | tok/s 19172
step    490 | loss 1.9359 | lr 3.00e-04 | grad 3.56 | tok/s 18364
step    500 | loss 2.0826 | lr 3.00e-04 | grad 2.64 | tok/s 18916
step    510 | loss 1.9243 | lr 3.00e-04 | grad 2.58 | tok/s 17824
step    520 | loss 1.7718 | lr 3.00e-04 | grad 2.61 | tok/s 18706
step    530 | loss 1.9570 | lr 3.00e-04 | grad 2.97 | tok/s 18481
step    540 | loss 1.8694 | lr 3.00e-04 | grad 2.27 | tok/s 17962
step    550 | loss 1.6022 | lr 3.00e-04 | grad 5.06 | tok/s 18935
step    560 | loss 1.6780 | lr 3.00e-04 | grad 2.88 | tok/s 19314
step    570 | loss 1.5647 | lr 3.00e-04 | grad 2.84 | tok/s 19354
step    580 | loss 1.5132 | lr 3.00e-04 | grad 2.20 | tok/s 19337
step    590 | loss 1.5530 | lr 3.00e-04 | grad 2.31 | tok/s 19379
step    600 | loss 1.4961 | lr 3.00e-04 | grad 2.27 | tok/s 19298
step    610 | loss 1.5033 | lr 3.00e-04 | grad 2.19 | tok/s 19391
step    620 | loss 1.4981 | lr 3.00e-04 | grad 2.44 | tok/s 19303
step    630 | loss 1.9622 | lr 3.00e-04 | grad 8.31 | tok/s 18309
step    640 | loss 1.9981 | lr 3.00e-04 | grad 2.84 | tok/s 18426
step    650 | loss 1.7785 | lr 3.00e-04 | grad 2.36 | tok/s 18556
step    660 | loss 1.8258 | lr 3.00e-04 | grad 2.83 | tok/s 19120
step    670 | loss 1.8772 | lr 3.00e-04 | grad 7.12 | tok/s 18477
step    680 | loss 1.8806 | lr 3.00e-04 | grad 3.67 | tok/s 18160
step    690 | loss 1.8444 | lr 3.00e-04 | grad 2.66 | tok/s 18096

Training complete! Final step: 693
