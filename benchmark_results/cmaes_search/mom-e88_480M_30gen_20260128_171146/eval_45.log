Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_45/levelMoME88_100m_20260128_173828
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 468,843,170 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 9.8917 | lr 3.00e-04 | grad 89.00 | tok/s 7726
step     20 | loss 3.7042 | lr 3.00e-04 | grad 7.25 | tok/s 12689
step     30 | loss 3.4439 | lr 3.00e-04 | grad 4.25 | tok/s 13404
step     40 | loss 6.7041 | lr 3.00e-04 | grad 27.25 | tok/s 13622
step     50 | loss 4.9550 | lr 3.00e-04 | grad 6.72 | tok/s 13792
step     60 | loss 4.1850 | lr 3.00e-04 | grad 2.92 | tok/s 13797
step     70 | loss 3.5443 | lr 3.00e-04 | grad 6.28 | tok/s 13750
step     80 | loss 3.2001 | lr 3.00e-04 | grad 6.88 | tok/s 13754
step     90 | loss 2.8897 | lr 3.00e-04 | grad 2.66 | tok/s 13753
step    100 | loss 2.6308 | lr 3.00e-04 | grad 2.50 | tok/s 13753
step    110 | loss 2.5793 | lr 3.00e-04 | grad 4.53 | tok/s 13651
step    120 | loss 3.1824 | lr 3.00e-04 | grad 1.31 | tok/s 12987
step    130 | loss 2.4581 | lr 3.00e-04 | grad 3.48 | tok/s 13308
step    140 | loss 2.7652 | lr 3.00e-04 | grad 6.84 | tok/s 13342
step    150 | loss 2.5329 | lr 3.00e-04 | grad 4.31 | tok/s 13685
step    160 | loss 2.8595 | lr 3.00e-04 | grad 1.85 | tok/s 13214
step    170 | loss 2.5567 | lr 3.00e-04 | grad 1.37 | tok/s 13019
step    180 | loss 2.7491 | lr 3.00e-04 | grad 2.14 | tok/s 13332
step    190 | loss 2.2778 | lr 3.00e-04 | grad 1.97 | tok/s 13076
step    200 | loss 2.1380 | lr 3.00e-04 | grad 1.84 | tok/s 13694
step    210 | loss 2.2497 | lr 3.00e-04 | grad 3.39 | tok/s 12978
step    220 | loss 2.5619 | lr 3.00e-04 | grad 9.44 | tok/s 13117
step    230 | loss 2.3474 | lr 3.00e-04 | grad 2.86 | tok/s 13103
step    240 | loss 2.6798 | lr 3.00e-04 | grad 3.81 | tok/s 13277
step    250 | loss 2.1187 | lr 3.00e-04 | grad 1.45 | tok/s 13192
step    260 | loss 2.2518 | lr 3.00e-04 | grad 2.55 | tok/s 13568
step    270 | loss 2.1409 | lr 3.00e-04 | grad 1.53 | tok/s 13253
step    280 | loss 2.0799 | lr 3.00e-04 | grad 1.36 | tok/s 12439
step    290 | loss 1.9996 | lr 3.00e-04 | grad 1.70 | tok/s 12863
step    300 | loss 2.3051 | lr 3.00e-04 | grad 2.83 | tok/s 12968
step    310 | loss 1.9428 | lr 3.00e-04 | grad 1.38 | tok/s 12905
step    320 | loss 2.2010 | lr 3.00e-04 | grad 3.86 | tok/s 13060
step    330 | loss 2.0037 | lr 3.00e-04 | grad 1.56 | tok/s 13200
step    340 | loss 2.3702 | lr 3.00e-04 | grad 2.72 | tok/s 13140
step    350 | loss 2.3070 | lr 3.00e-04 | grad 1.62 | tok/s 13526
step    360 | loss 1.8950 | lr 3.00e-04 | grad 1.84 | tok/s 12934
step    370 | loss 1.9026 | lr 3.00e-04 | grad 1.98 | tok/s 13632
step    380 | loss 1.6705 | lr 3.00e-04 | grad 1.59 | tok/s 13755
step    390 | loss 1.5719 | lr 3.00e-04 | grad 1.67 | tok/s 13754
step    400 | loss 2.1225 | lr 3.00e-04 | grad 2.03 | tok/s 13022
step    410 | loss 2.0369 | lr 3.00e-04 | grad 1.91 | tok/s 13145
step    420 | loss 2.1312 | lr 3.00e-04 | grad 5.09 | tok/s 13714
step    430 | loss 1.9853 | lr 3.00e-04 | grad 1.69 | tok/s 13478
step    440 | loss 2.0169 | lr 3.00e-04 | grad 1.99 | tok/s 13065
step    450 | loss 1.8960 | lr 3.00e-04 | grad 1.41 | tok/s 13213
step    460 | loss 1.9345 | lr 3.00e-04 | grad 2.02 | tok/s 13406
step    470 | loss 1.9284 | lr 3.00e-04 | grad 2.97 | tok/s 13304
step    480 | loss 1.9725 | lr 3.00e-04 | grad 2.39 | tok/s 13599
step    490 | loss 1.9487 | lr 3.00e-04 | grad 2.27 | tok/s 13057

Training complete! Final step: 494
