Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_106/levelMoME88_100m_20260128_182055
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 476,521,952 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 17.1734 | lr 3.00e-04 | grad 10.94 | tok/s 9244
step     20 | loss 3.9772 | lr 3.00e-04 | grad 4.12 | tok/s 19125
step     30 | loss 3.3813 | lr 3.00e-04 | grad 5.12 | tok/s 20194
step     40 | loss 5.1376 | lr 3.00e-04 | grad 18.12 | tok/s 20435
step     50 | loss 4.4434 | lr 3.00e-04 | grad 15.44 | tok/s 20686
step     60 | loss 3.8116 | lr 3.00e-04 | grad 9.88 | tok/s 20688
step     70 | loss 3.2044 | lr 3.00e-04 | grad 11.06 | tok/s 20653
step     80 | loss 2.8474 | lr 3.00e-04 | grad 4.47 | tok/s 20688
step     90 | loss 2.6518 | lr 3.00e-04 | grad 8.50 | tok/s 20687
step    100 | loss 2.4789 | lr 3.00e-04 | grad 5.44 | tok/s 20691
step    110 | loss 2.5146 | lr 3.00e-04 | grad 5.69 | tok/s 20536
step    120 | loss 3.1040 | lr 3.00e-04 | grad 2.75 | tok/s 19541
step    130 | loss 2.4259 | lr 3.00e-04 | grad 17.25 | tok/s 20018
step    140 | loss 2.6879 | lr 3.00e-04 | grad 16.88 | tok/s 20055
step    150 | loss 2.2567 | lr 3.00e-04 | grad 9.69 | tok/s 20525
step    160 | loss 2.6313 | lr 3.00e-04 | grad 3.53 | tok/s 19889
step    170 | loss 2.5483 | lr 3.00e-04 | grad 2.45 | tok/s 19577
step    180 | loss 2.5571 | lr 3.00e-04 | grad 3.86 | tok/s 20063
step    190 | loss 2.2383 | lr 3.00e-04 | grad 3.66 | tok/s 19656
step    200 | loss 2.0794 | lr 3.00e-04 | grad 4.19 | tok/s 20598
step    210 | loss 2.2396 | lr 3.00e-04 | grad 7.41 | tok/s 19522
step    220 | loss 2.5661 | lr 3.00e-04 | grad 12.88 | tok/s 19733
step    230 | loss 2.3436 | lr 3.00e-04 | grad 3.64 | tok/s 19712
step    240 | loss 2.6230 | lr 3.00e-04 | grad 6.59 | tok/s 19967
step    250 | loss 2.1240 | lr 3.00e-04 | grad 2.12 | tok/s 19847
step    260 | loss 2.2510 | lr 3.00e-04 | grad 4.06 | tok/s 20407
step    270 | loss 2.1445 | lr 3.00e-04 | grad 2.81 | tok/s 19933
step    280 | loss 2.0914 | lr 3.00e-04 | grad 2.58 | tok/s 18768
step    290 | loss 2.0175 | lr 3.00e-04 | grad 3.02 | tok/s 19324
step    300 | loss 2.2912 | lr 3.00e-04 | grad 4.41 | tok/s 19507
step    310 | loss 1.9664 | lr 3.00e-04 | grad 2.33 | tok/s 19414
step    320 | loss 2.2189 | lr 3.00e-04 | grad 5.78 | tok/s 19638
step    330 | loss 2.0280 | lr 3.00e-04 | grad 2.62 | tok/s 19826
step    340 | loss 2.3705 | lr 3.00e-04 | grad 3.09 | tok/s 19764
step    350 | loss 2.1867 | lr 3.00e-04 | grad 2.86 | tok/s 20354
step    360 | loss 1.9138 | lr 3.00e-04 | grad 2.75 | tok/s 19468
step    370 | loss 1.8792 | lr 3.00e-04 | grad 2.47 | tok/s 20481
step    380 | loss 1.6360 | lr 3.00e-04 | grad 2.73 | tok/s 20687
step    390 | loss 1.5194 | lr 3.00e-04 | grad 2.08 | tok/s 20688
step    400 | loss 2.1348 | lr 3.00e-04 | grad 2.70 | tok/s 19604
step    410 | loss 2.0761 | lr 3.00e-04 | grad 3.03 | tok/s 19761
step    420 | loss 2.0503 | lr 3.00e-04 | grad 7.94 | tok/s 20620
step    430 | loss 2.0116 | lr 3.00e-04 | grad 2.44 | tok/s 20289
step    440 | loss 2.0570 | lr 3.00e-04 | grad 2.89 | tok/s 19654
step    450 | loss 1.9366 | lr 3.00e-04 | grad 2.02 | tok/s 19867
step    460 | loss 1.9450 | lr 3.00e-04 | grad 2.75 | tok/s 20166
step    470 | loss 1.9280 | lr 3.00e-04 | grad 4.97 | tok/s 20009
step    480 | loss 1.9508 | lr 3.00e-04 | grad 3.86 | tok/s 20463
step    490 | loss 2.0012 | lr 3.00e-04 | grad 3.45 | tok/s 19637
step    500 | loss 2.1547 | lr 3.00e-04 | grad 2.66 | tok/s 19964
step    510 | loss 1.9819 | lr 3.00e-04 | grad 2.42 | tok/s 19061
step    520 | loss 1.8152 | lr 3.00e-04 | grad 2.66 | tok/s 19975
step    530 | loss 2.0130 | lr 3.00e-04 | grad 3.28 | tok/s 19633
step    540 | loss 1.9282 | lr 3.00e-04 | grad 2.11 | tok/s 19255
step    550 | loss 1.6656 | lr 3.00e-04 | grad 4.00 | tok/s 20146
step    560 | loss 1.7415 | lr 3.00e-04 | grad 3.36 | tok/s 20662
step    570 | loss 1.6264 | lr 3.00e-04 | grad 2.73 | tok/s 20689
step    580 | loss 1.5615 | lr 3.00e-04 | grad 2.17 | tok/s 20686
step    590 | loss 1.6053 | lr 3.00e-04 | grad 1.94 | tok/s 20688
step    600 | loss 1.5549 | lr 3.00e-04 | grad 2.17 | tok/s 20690
step    610 | loss 1.5633 | lr 3.00e-04 | grad 2.39 | tok/s 20690
step    620 | loss 1.5511 | lr 3.00e-04 | grad 2.31 | tok/s 20605
step    630 | loss 2.0498 | lr 3.00e-04 | grad 7.12 | tok/s 19501
step    640 | loss 2.0432 | lr 3.00e-04 | grad 2.86 | tok/s 19733
step    650 | loss 1.8395 | lr 3.00e-04 | grad 2.33 | tok/s 19718
step    660 | loss 1.8854 | lr 3.00e-04 | grad 2.80 | tok/s 20538
step    670 | loss 1.9227 | lr 3.00e-04 | grad 6.56 | tok/s 19781
step    680 | loss 1.9348 | lr 3.00e-04 | grad 3.56 | tok/s 19448
step    690 | loss 1.8963 | lr 3.00e-04 | grad 2.75 | tok/s 19407
step    700 | loss 1.7641 | lr 3.00e-04 | grad 2.19 | tok/s 19821
step    710 | loss 1.9611 | lr 3.00e-04 | grad 5.53 | tok/s 19420
step    720 | loss 1.6145 | lr 3.00e-04 | grad 2.39 | tok/s 20187
step    730 | loss 1.7611 | lr 3.00e-04 | grad 1.89 | tok/s 19864
step    740 | loss 2.1680 | lr 3.00e-04 | grad 5.09 | tok/s 20390

Training complete! Final step: 740
