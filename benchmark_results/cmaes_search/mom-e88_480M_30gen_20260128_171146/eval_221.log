Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_221/levelMoME88_100m_20260128_193510
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 475,036,760 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 11.0097 | lr 3.00e-04 | grad 20.75 | tok/s 8607
step     20 | loss 3.3026 | lr 3.00e-04 | grad 2.28 | tok/s 15794
step     30 | loss 3.1286 | lr 3.00e-04 | grad 4.12 | tok/s 16705
step     40 | loss 5.6800 | lr 3.00e-04 | grad 86.50 | tok/s 17021
step     50 | loss 4.7098 | lr 3.00e-04 | grad 9.44 | tok/s 17205
step     60 | loss 3.7563 | lr 3.00e-04 | grad 9.88 | tok/s 17217
step     70 | loss 3.1956 | lr 3.00e-04 | grad 7.56 | tok/s 17212
step     80 | loss 2.8476 | lr 3.00e-04 | grad 3.30 | tok/s 17201
step     90 | loss 2.5962 | lr 3.00e-04 | grad 3.84 | tok/s 17213
step    100 | loss 2.3891 | lr 3.00e-04 | grad 9.19 | tok/s 17209
step    110 | loss 2.4399 | lr 3.00e-04 | grad 3.02 | tok/s 17081
step    120 | loss 3.0406 | lr 3.00e-04 | grad 1.99 | tok/s 16260
step    130 | loss 2.3307 | lr 3.00e-04 | grad 4.59 | tok/s 16658
step    140 | loss 2.6603 | lr 3.00e-04 | grad 7.91 | tok/s 16690
step    150 | loss 2.1703 | lr 3.00e-04 | grad 6.41 | tok/s 17084
step    160 | loss 2.6167 | lr 3.00e-04 | grad 2.25 | tok/s 16534
step    170 | loss 2.4819 | lr 3.00e-04 | grad 1.88 | tok/s 16284
step    180 | loss 2.4826 | lr 3.00e-04 | grad 3.12 | tok/s 16628
step    190 | loss 2.1593 | lr 3.00e-04 | grad 2.00 | tok/s 16353
step    200 | loss 1.9708 | lr 3.00e-04 | grad 2.48 | tok/s 17081
step    210 | loss 2.1375 | lr 3.00e-04 | grad 5.00 | tok/s 16255
step    220 | loss 2.4560 | lr 3.00e-04 | grad 9.00 | tok/s 16413
step    230 | loss 2.1854 | lr 3.00e-04 | grad 2.73 | tok/s 16391
step    240 | loss 2.5441 | lr 3.00e-04 | grad 4.88 | tok/s 16560
step    250 | loss 2.0283 | lr 3.00e-04 | grad 1.78 | tok/s 16485
step    260 | loss 2.1514 | lr 3.00e-04 | grad 3.42 | tok/s 16919
step    270 | loss 2.0517 | lr 3.00e-04 | grad 2.61 | tok/s 16528
step    280 | loss 1.9868 | lr 3.00e-04 | grad 1.84 | tok/s 15566
step    290 | loss 1.9073 | lr 3.00e-04 | grad 2.11 | tok/s 16088
step    300 | loss 2.1980 | lr 3.00e-04 | grad 2.34 | tok/s 16192
step    310 | loss 1.8660 | lr 3.00e-04 | grad 2.12 | tok/s 16149
step    320 | loss 2.1252 | lr 3.00e-04 | grad 4.41 | tok/s 16341
step    330 | loss 1.9237 | lr 3.00e-04 | grad 1.90 | tok/s 16525
step    340 | loss 2.2563 | lr 3.00e-04 | grad 2.53 | tok/s 16447
step    350 | loss 2.1105 | lr 3.00e-04 | grad 2.36 | tok/s 16925
step    360 | loss 1.8147 | lr 3.00e-04 | grad 2.25 | tok/s 16189
step    370 | loss 1.8050 | lr 3.00e-04 | grad 2.09 | tok/s 17060
step    380 | loss 1.5385 | lr 3.00e-04 | grad 2.17 | tok/s 17227
step    390 | loss 1.4395 | lr 3.00e-04 | grad 2.11 | tok/s 17218
step    400 | loss 2.0246 | lr 3.00e-04 | grad 2.03 | tok/s 16294
step    410 | loss 1.9881 | lr 3.00e-04 | grad 2.72 | tok/s 16452
step    420 | loss 1.9849 | lr 3.00e-04 | grad 3.69 | tok/s 17159
step    430 | loss 1.8825 | lr 3.00e-04 | grad 2.36 | tok/s 16878
step    440 | loss 1.9484 | lr 3.00e-04 | grad 2.28 | tok/s 16352
step    450 | loss 1.8376 | lr 3.00e-04 | grad 2.36 | tok/s 16530
step    460 | loss 1.8379 | lr 3.00e-04 | grad 2.34 | tok/s 16762
step    470 | loss 1.8386 | lr 3.00e-04 | grad 3.58 | tok/s 16613
step    480 | loss 1.9032 | lr 3.00e-04 | grad 2.97 | tok/s 17022
step    490 | loss 1.9076 | lr 3.00e-04 | grad 2.61 | tok/s 16336
step    500 | loss 2.0439 | lr 3.00e-04 | grad 2.09 | tok/s 16566
step    510 | loss 1.8959 | lr 3.00e-04 | grad 1.77 | tok/s 15843
step    520 | loss 1.7295 | lr 3.00e-04 | grad 2.09 | tok/s 16610
step    530 | loss 1.9264 | lr 3.00e-04 | grad 2.61 | tok/s 16349
step    540 | loss 1.8455 | lr 3.00e-04 | grad 1.63 | tok/s 15991
step    550 | loss 1.5571 | lr 3.00e-04 | grad 4.00 | tok/s 16700
step    560 | loss 1.6549 | lr 3.00e-04 | grad 2.52 | tok/s 17200
step    570 | loss 1.5400 | lr 3.00e-04 | grad 2.17 | tok/s 17217
step    580 | loss 1.4915 | lr 3.00e-04 | grad 2.25 | tok/s 17208
step    590 | loss 1.5366 | lr 3.00e-04 | grad 1.93 | tok/s 17224
step    600 | loss 1.4831 | lr 3.00e-04 | grad 2.09 | tok/s 17204
step    610 | loss 1.4793 | lr 3.00e-04 | grad 1.57 | tok/s 17152

Training complete! Final step: 616
