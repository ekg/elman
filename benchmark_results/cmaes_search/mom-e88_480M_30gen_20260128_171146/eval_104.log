Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_104/levelMoME88_100m_20260128_181537
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 478,440,968 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 14.0614 | lr 3.00e-04 | grad 15.19 | tok/s 9989
step     20 | loss 3.6086 | lr 3.00e-04 | grad 2.86 | tok/s 22094
step     30 | loss 3.2993 | lr 3.00e-04 | grad 16.88 | tok/s 23325
step     40 | loss 7.3513 | lr 3.00e-04 | grad 17.25 | tok/s 23730
step     50 | loss 5.3486 | lr 3.00e-04 | grad 11.31 | tok/s 24014
step     60 | loss 4.1249 | lr 3.00e-04 | grad 4.75 | tok/s 23892
step     70 | loss 3.2964 | lr 3.00e-04 | grad 7.97 | tok/s 23838
step     80 | loss 2.9792 | lr 3.00e-04 | grad 5.00 | tok/s 23795
step     90 | loss 2.7639 | lr 3.00e-04 | grad 17.25 | tok/s 23770
step    100 | loss 2.4818 | lr 3.00e-04 | grad 4.31 | tok/s 23727
step    110 | loss 2.5222 | lr 3.00e-04 | grad 3.62 | tok/s 23499
step    120 | loss 3.1482 | lr 3.00e-04 | grad 2.48 | tok/s 22368
step    130 | loss 2.4324 | lr 3.00e-04 | grad 7.41 | tok/s 22894
step    140 | loss 2.7116 | lr 3.00e-04 | grad 8.25 | tok/s 22949
step    150 | loss 2.3632 | lr 3.00e-04 | grad 7.34 | tok/s 23547
step    160 | loss 2.7576 | lr 3.00e-04 | grad 2.66 | tok/s 22711
step    170 | loss 2.5546 | lr 3.00e-04 | grad 1.66 | tok/s 22345
step    180 | loss 2.6785 | lr 3.00e-04 | grad 3.92 | tok/s 22883
step    190 | loss 2.2393 | lr 3.00e-04 | grad 2.06 | tok/s 22396
step    200 | loss 2.0652 | lr 3.00e-04 | grad 2.20 | tok/s 23402
step    210 | loss 2.2242 | lr 3.00e-04 | grad 6.69 | tok/s 22274
step    220 | loss 2.5602 | lr 3.00e-04 | grad 18.75 | tok/s 22505
step    230 | loss 2.8725 | lr 3.00e-04 | grad 2.81 | tok/s 22519
step    240 | loss 2.6195 | lr 3.00e-04 | grad 4.47 | tok/s 22826
step    250 | loss 2.0981 | lr 3.00e-04 | grad 1.88 | tok/s 22588
step    260 | loss 2.2315 | lr 3.00e-04 | grad 3.45 | tok/s 23217
step    270 | loss 2.1184 | lr 3.00e-04 | grad 2.23 | tok/s 22752
step    280 | loss 2.0622 | lr 3.00e-04 | grad 1.82 | tok/s 21375
step    290 | loss 1.9772 | lr 3.00e-04 | grad 2.33 | tok/s 22107
step    300 | loss 2.2679 | lr 3.00e-04 | grad 2.12 | tok/s 22303
step    310 | loss 1.9333 | lr 3.00e-04 | grad 1.98 | tok/s 22144
step    320 | loss 2.1843 | lr 3.00e-04 | grad 4.41 | tok/s 22358
step    330 | loss 1.9894 | lr 3.00e-04 | grad 1.98 | tok/s 22576
step    340 | loss 2.3514 | lr 3.00e-04 | grad 3.02 | tok/s 22497
step    350 | loss 2.2073 | lr 3.00e-04 | grad 2.34 | tok/s 23144
step    360 | loss 1.8859 | lr 3.00e-04 | grad 2.45 | tok/s 22154
step    370 | loss 1.8747 | lr 3.00e-04 | grad 1.96 | tok/s 23303
step    380 | loss 1.6140 | lr 3.00e-04 | grad 2.05 | tok/s 23577
step    390 | loss 1.5097 | lr 3.00e-04 | grad 2.61 | tok/s 23516
step    400 | loss 2.1129 | lr 3.00e-04 | grad 2.05 | tok/s 22343
step    410 | loss 2.0360 | lr 3.00e-04 | grad 2.44 | tok/s 22531
step    420 | loss 2.0484 | lr 3.00e-04 | grad 4.81 | tok/s 23457
step    430 | loss 1.9428 | lr 3.00e-04 | grad 1.98 | tok/s 23094
step    440 | loss 2.0197 | lr 3.00e-04 | grad 2.58 | tok/s 22391
step    450 | loss 1.9037 | lr 3.00e-04 | grad 1.79 | tok/s 22609
step    460 | loss 1.9202 | lr 3.00e-04 | grad 2.34 | tok/s 22974
step    470 | loss 1.8979 | lr 3.00e-04 | grad 3.61 | tok/s 22854
step    480 | loss 1.9481 | lr 3.00e-04 | grad 2.94 | tok/s 23283
step    490 | loss 1.9559 | lr 3.00e-04 | grad 2.86 | tok/s 22357
step    500 | loss 2.1179 | lr 3.00e-04 | grad 2.27 | tok/s 22683
step    510 | loss 1.9525 | lr 3.00e-04 | grad 1.84 | tok/s 21767
step    520 | loss 1.8016 | lr 3.00e-04 | grad 2.50 | tok/s 22680
step    530 | loss 1.9780 | lr 3.00e-04 | grad 2.34 | tok/s 22419
step    540 | loss 1.8944 | lr 3.00e-04 | grad 1.69 | tok/s 21887
step    550 | loss 1.6251 | lr 3.00e-04 | grad 3.73 | tok/s 22919
step    560 | loss 1.7097 | lr 3.00e-04 | grad 2.19 | tok/s 23502
step    570 | loss 1.5866 | lr 3.00e-04 | grad 2.14 | tok/s 23456
step    580 | loss 1.5318 | lr 3.00e-04 | grad 1.57 | tok/s 23514
step    590 | loss 1.5709 | lr 3.00e-04 | grad 1.60 | tok/s 23539
step    600 | loss 1.5245 | lr 3.00e-04 | grad 2.02 | tok/s 23525
step    610 | loss 1.5252 | lr 3.00e-04 | grad 1.57 | tok/s 23507
step    620 | loss 1.5160 | lr 3.00e-04 | grad 1.86 | tok/s 23505
step    630 | loss 1.9710 | lr 3.00e-04 | grad 5.53 | tok/s 22232
step    640 | loss 2.0283 | lr 3.00e-04 | grad 3.84 | tok/s 22474
step    650 | loss 1.8212 | lr 3.00e-04 | grad 2.05 | tok/s 22407
step    660 | loss 1.8645 | lr 3.00e-04 | grad 2.34 | tok/s 23276
step    670 | loss 1.9280 | lr 3.00e-04 | grad 6.31 | tok/s 22544
step    680 | loss 1.9104 | lr 3.00e-04 | grad 2.61 | tok/s 22183
step    690 | loss 1.8713 | lr 3.00e-04 | grad 2.30 | tok/s 22015
step    700 | loss 1.7527 | lr 3.00e-04 | grad 3.81 | tok/s 22466
step    710 | loss 1.9353 | lr 3.00e-04 | grad 4.66 | tok/s 22169
step    720 | loss 1.5939 | lr 3.00e-04 | grad 3.58 | tok/s 22982
step    730 | loss 1.7487 | lr 3.00e-04 | grad 1.69 | tok/s 22638
step    740 | loss 2.2208 | lr 3.00e-04 | grad 5.38 | tok/s 23220
step    750 | loss 1.9134 | lr 3.00e-04 | grad 1.98 | tok/s 23487
step    760 | loss 1.7907 | lr 3.00e-04 | grad 3.75 | tok/s 22992
step    770 | loss 1.8370 | lr 3.00e-04 | grad 2.27 | tok/s 22600
step    780 | loss 1.7308 | lr 3.00e-04 | grad 2.55 | tok/s 22765
step    790 | loss 2.0473 | lr 3.00e-04 | grad 5.50 | tok/s 23228
step    800 | loss 1.5991 | lr 3.00e-04 | grad 1.73 | tok/s 22903
step    810 | loss 1.5581 | lr 3.00e-04 | grad 3.91 | tok/s 22154
step    820 | loss 1.6963 | lr 3.00e-04 | grad 2.50 | tok/s 22541
step    830 | loss 1.7740 | lr 3.00e-04 | grad 1.68 | tok/s 22271
step    840 | loss 1.9147 | lr 3.00e-04 | grad 2.08 | tok/s 22126

Training complete! Final step: 843
