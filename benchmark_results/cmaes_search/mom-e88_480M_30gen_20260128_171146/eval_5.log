Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_5/levelMoME88_100m_20260128_171153
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 480,737,600 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 13.2171 | lr 3.00e-04 | grad 7.44 | tok/s 5885
step     20 | loss 3.1054 | lr 3.00e-04 | grad 2.88 | tok/s 16608
step     30 | loss 2.9114 | lr 3.00e-04 | grad 2.58 | tok/s 16811
step     40 | loss 3.4812 | lr 3.00e-04 | grad 2.81 | tok/s 16103
step     50 | loss 3.7385 | lr 3.00e-04 | grad 10.25 | tok/s 16356
step     60 | loss 2.3798 | lr 3.00e-04 | grad 4.31 | tok/s 16828
step     70 | loss 2.3125 | lr 3.00e-04 | grad 3.17 | tok/s 17026
step     80 | loss 13.2128 | lr 3.00e-04 | grad 13.25 | tok/s 17057
step     90 | loss 7.7867 | lr 3.00e-04 | grad 3.58 | tok/s 17380
step    100 | loss 5.2111 | lr 3.00e-04 | grad 4.00 | tok/s 17362
step    110 | loss 4.6548 | lr 3.00e-04 | grad 10.38 | tok/s 17329
step    120 | loss 4.1654 | lr 3.00e-04 | grad 10.94 | tok/s 17325
step    130 | loss 3.8866 | lr 3.00e-04 | grad 4.72 | tok/s 17335
step    140 | loss 3.2191 | lr 3.00e-04 | grad 27.88 | tok/s 17341
step    150 | loss 3.6461 | lr 3.00e-04 | grad 9.31 | tok/s 17357
step    160 | loss 2.9611 | lr 3.00e-04 | grad 4.78 | tok/s 17295
step    170 | loss 2.9717 | lr 3.00e-04 | grad 10.06 | tok/s 17302
step    180 | loss 2.7055 | lr 3.00e-04 | grad 5.12 | tok/s 17270
step    190 | loss 2.9504 | lr 3.00e-04 | grad 6.66 | tok/s 17279
step    200 | loss 2.4777 | lr 3.00e-04 | grad 6.38 | tok/s 17296
step    210 | loss 2.5341 | lr 3.00e-04 | grad 94.00 | tok/s 17238
step    220 | loss 2.6098 | lr 3.00e-04 | grad 2.94 | tok/s 17062
step    230 | loss 3.3514 | lr 3.00e-04 | grad 5.56 | tok/s 16818
step    240 | loss 2.5640 | lr 3.00e-04 | grad 3.08 | tok/s 16012
step    250 | loss 2.3801 | lr 3.00e-04 | grad 1.95 | tok/s 16409
step    260 | loss 2.0776 | lr 3.00e-04 | grad 1.94 | tok/s 16939
step    270 | loss 2.4777 | lr 3.00e-04 | grad 1.63 | tok/s 16738
step    280 | loss 2.5961 | lr 3.00e-04 | grad 4.72 | tok/s 16385
step    290 | loss 2.8264 | lr 3.00e-04 | grad 9.81 | tok/s 17258
step    300 | loss 1.7723 | lr 3.00e-04 | grad 11.12 | tok/s 17290
step    310 | loss 2.8611 | lr 3.00e-04 | grad 3.55 | tok/s 16925
step    320 | loss 2.5242 | lr 3.00e-04 | grad 4.12 | tok/s 16569
step    330 | loss 2.2499 | lr 3.00e-04 | grad 2.17 | tok/s 16037
step    340 | loss 2.6185 | lr 3.00e-04 | grad 2.06 | tok/s 16293
step    350 | loss 2.3979 | lr 3.00e-04 | grad 20.25 | tok/s 16641
step    360 | loss 2.7813 | lr 3.00e-04 | grad 5.03 | tok/s 17007
step    370 | loss 2.1892 | lr 3.00e-04 | grad 2.34 | tok/s 15476
step    380 | loss 2.1481 | lr 3.00e-04 | grad 2.06 | tok/s 16438
step    390 | loss 1.9290 | lr 3.00e-04 | grad 1.54 | tok/s 17176
step    400 | loss 1.9103 | lr 3.00e-04 | grad 2.42 | tok/s 17014
step    410 | loss 1.8269 | lr 3.00e-04 | grad 1.73 | tok/s 16663
step    420 | loss 2.1508 | lr 3.00e-04 | grad 4.78 | tok/s 15917
step    430 | loss 2.5461 | lr 3.00e-04 | grad 2.17 | tok/s 16960
step    440 | loss 2.4894 | lr 3.00e-04 | grad 2.86 | tok/s 16027
step    450 | loss 2.6248 | lr 3.00e-04 | grad 1.70 | tok/s 16557
step    460 | loss 2.1344 | lr 3.00e-04 | grad 3.20 | tok/s 16196
step    470 | loss 2.2108 | lr 3.00e-04 | grad 2.39 | tok/s 16713
step    480 | loss 2.7507 | lr 3.00e-04 | grad 5.44 | tok/s 16697
step    490 | loss 2.1523 | lr 3.00e-04 | grad 2.33 | tok/s 15795
step    500 | loss 2.0650 | lr 3.00e-04 | grad 2.33 | tok/s 16835
step    510 | loss 2.0657 | lr 3.00e-04 | grad 1.94 | tok/s 17060
step    520 | loss 2.0553 | lr 3.00e-04 | grad 1.72 | tok/s 17031
step    530 | loss 2.2929 | lr 3.00e-04 | grad 2.02 | tok/s 16457
step    540 | loss 2.0060 | lr 3.00e-04 | grad 1.95 | tok/s 16398
step    550 | loss 1.8443 | lr 3.00e-04 | grad 2.22 | tok/s 16073
step    560 | loss 2.0371 | lr 3.00e-04 | grad 1.85 | tok/s 15653
step    570 | loss 1.9938 | lr 3.00e-04 | grad 3.08 | tok/s 16132
step    580 | loss 1.8590 | lr 3.00e-04 | grad 1.86 | tok/s 16025
step    590 | loss 2.2648 | lr 3.00e-04 | grad 2.50 | tok/s 16422
step    600 | loss 2.1137 | lr 3.00e-04 | grad 1.90 | tok/s 15869
step    610 | loss 1.9298 | lr 3.00e-04 | grad 1.71 | tok/s 16681
step    620 | loss 1.7980 | lr 3.00e-04 | grad 1.62 | tok/s 15868
step    630 | loss 1.9669 | lr 3.00e-04 | grad 3.33 | tok/s 15959
step    640 | loss 2.1549 | lr 3.00e-04 | grad 1.91 | tok/s 16386
step    650 | loss 1.9665 | lr 3.00e-04 | grad 2.03 | tok/s 16489
step    660 | loss 1.9892 | lr 3.00e-04 | grad 1.70 | tok/s 16552
step    670 | loss 2.2483 | lr 3.00e-04 | grad 19.38 | tok/s 16642
step    680 | loss 1.9921 | lr 3.00e-04 | grad 1.87 | tok/s 16318
step    690 | loss 2.3205 | lr 3.00e-04 | grad 2.67 | tok/s 16908
step    700 | loss 2.1177 | lr 3.00e-04 | grad 3.16 | tok/s 17155
step    710 | loss 1.9101 | lr 3.00e-04 | grad 1.79 | tok/s 16106
step    720 | loss 1.7458 | lr 3.00e-04 | grad 2.30 | tok/s 15835
step    730 | loss 1.7694 | lr 3.00e-04 | grad 2.84 | tok/s 17169
step    740 | loss 1.8567 | lr 3.00e-04 | grad 2.30 | tok/s 16989
step    750 | loss 1.6711 | lr 3.00e-04 | grad 1.96 | tok/s 17220
step    760 | loss 1.5223 | lr 3.00e-04 | grad 2.16 | tok/s 17193
step    770 | loss 1.4780 | lr 3.00e-04 | grad 1.62 | tok/s 17198
step    780 | loss 1.4263 | lr 3.00e-04 | grad 1.71 | tok/s 17203
step    790 | loss 1.4933 | lr 3.00e-04 | grad 3.12 | tok/s 16685
step    800 | loss 2.3311 | lr 3.00e-04 | grad 7.06 | tok/s 16626
step    810 | loss 1.9824 | lr 3.00e-04 | grad 2.11 | tok/s 16526
step    820 | loss 1.9783 | lr 3.00e-04 | grad 3.08 | tok/s 15873
step    830 | loss 2.0165 | lr 3.00e-04 | grad 2.44 | tok/s 17036
step    840 | loss 1.8824 | lr 3.00e-04 | grad 2.06 | tok/s 17166
step    850 | loss 1.9728 | lr 3.00e-04 | grad 1.91 | tok/s 17129
step    860 | loss 1.9292 | lr 3.00e-04 | grad 3.12 | tok/s 16941
step    870 | loss 1.7998 | lr 3.00e-04 | grad 2.19 | tok/s 16325
step    880 | loss 2.0180 | lr 3.00e-04 | grad 2.38 | tok/s 16371
step    890 | loss 1.9623 | lr 3.00e-04 | grad 2.50 | tok/s 16625
step    900 | loss 1.8314 | lr 3.00e-04 | grad 1.96 | tok/s 16629
step    910 | loss 1.7130 | lr 3.00e-04 | grad 3.33 | tok/s 16300
step    920 | loss 1.8976 | lr 3.00e-04 | grad 2.94 | tok/s 16926
step    930 | loss 1.8734 | lr 3.00e-04 | grad 2.91 | tok/s 16227
step    940 | loss 1.8058 | lr 3.00e-04 | grad 2.08 | tok/s 17058
step    950 | loss 1.9034 | lr 3.00e-04 | grad 2.38 | tok/s 17142
step    960 | loss 1.8252 | lr 3.00e-04 | grad 2.41 | tok/s 17184
step    970 | loss 1.9786 | lr 3.00e-04 | grad 2.53 | tok/s 16120
step    980 | loss 1.8994 | lr 3.00e-04 | grad 2.11 | tok/s 16576
step    990 | loss 1.7805 | lr 3.00e-04 | grad 1.79 | tok/s 16879
step   1000 | loss 2.1771 | lr 3.00e-04 | grad 10.56 | tok/s 16173
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1771.pt
step   1010 | loss 2.0496 | lr 3.00e-04 | grad 1.92 | tok/s 6287
step   1020 | loss 1.9168 | lr 3.00e-04 | grad 1.85 | tok/s 15884
step   1030 | loss 1.7149 | lr 3.00e-04 | grad 1.49 | tok/s 16572
step   1040 | loss 1.7754 | lr 3.00e-04 | grad 3.19 | tok/s 16882
step   1050 | loss 1.8375 | lr 3.00e-04 | grad 2.19 | tok/s 15901
step   1060 | loss 2.0308 | lr 3.00e-04 | grad 2.08 | tok/s 16995
step   1070 | loss 2.0612 | lr 3.00e-04 | grad 2.55 | tok/s 16734
step   1080 | loss 1.6429 | lr 3.00e-04 | grad 1.88 | tok/s 15497
step   1090 | loss 1.2960 | lr 3.00e-04 | grad 1.30 | tok/s 17219
step   1100 | loss 1.8126 | lr 3.00e-04 | grad 2.59 | tok/s 16538
step   1110 | loss 1.6881 | lr 3.00e-04 | grad 1.64 | tok/s 17280
step   1120 | loss 1.6069 | lr 3.00e-04 | grad 2.14 | tok/s 17337
step   1130 | loss 1.5244 | lr 3.00e-04 | grad 1.89 | tok/s 17331
step   1140 | loss 1.5281 | lr 3.00e-04 | grad 1.72 | tok/s 17331
step   1150 | loss 1.5383 | lr 3.00e-04 | grad 1.67 | tok/s 17327
step   1160 | loss 1.4395 | lr 3.00e-04 | grad 1.67 | tok/s 17292
step   1170 | loss 1.4860 | lr 3.00e-04 | grad 1.84 | tok/s 17228
step   1180 | loss 1.5726 | lr 3.00e-04 | grad 1.61 | tok/s 17268
step   1190 | loss 1.4870 | lr 3.00e-04 | grad 1.82 | tok/s 17227
step   1200 | loss 1.4792 | lr 3.00e-04 | grad 1.55 | tok/s 17224
step   1210 | loss 1.4904 | lr 3.00e-04 | grad 1.69 | tok/s 17279

Training complete! Final step: 1212
