Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_59/levelMoME88_100m_20260128_174906
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 500,624,852 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 8.6450 | lr 3.00e-04 | grad 5.94 | tok/s 3919
step     20 | loss 2.9270 | lr 3.00e-04 | grad 2.73 | tok/s 6517
step     30 | loss 2.6931 | lr 3.00e-04 | grad 1.49 | tok/s 6611
step     40 | loss 2.9552 | lr 3.00e-04 | grad 1.81 | tok/s 6327
step     50 | loss 3.4534 | lr 3.00e-04 | grad 35.50 | tok/s 6426
step     60 | loss 2.3576 | lr 3.00e-04 | grad 7.44 | tok/s 6635
step     70 | loss 2.2317 | lr 3.00e-04 | grad 2.12 | tok/s 6719
step     80 | loss 8.4304 | lr 3.00e-04 | grad 18.75 | tok/s 6758
step     90 | loss 5.4416 | lr 3.00e-04 | grad 2.66 | tok/s 6879
step    100 | loss 4.4408 | lr 3.00e-04 | grad 3.16 | tok/s 6879
step    110 | loss 4.3405 | lr 3.00e-04 | grad 16.75 | tok/s 6879
step    120 | loss 3.8786 | lr 3.00e-04 | grad 16.38 | tok/s 6879
step    130 | loss 3.7210 | lr 3.00e-04 | grad 9.62 | tok/s 6876
step    140 | loss 3.2044 | lr 3.00e-04 | grad 3.47 | tok/s 6877
step    150 | loss 3.4788 | lr 3.00e-04 | grad 12.12 | tok/s 6874
step    160 | loss 2.9149 | lr 3.00e-04 | grad 3.72 | tok/s 6876
step    170 | loss 2.8769 | lr 3.00e-04 | grad 4.59 | tok/s 6880
step    180 | loss 2.6931 | lr 3.00e-04 | grad 8.38 | tok/s 6878
step    190 | loss 2.7681 | lr 3.00e-04 | grad 4.88 | tok/s 6879
step    200 | loss 2.4324 | lr 3.00e-04 | grad 2.48 | tok/s 6879
step    210 | loss 2.4198 | lr 3.00e-04 | grad 3.16 | tok/s 6878
step    220 | loss 2.5497 | lr 3.00e-04 | grad 2.30 | tok/s 6794
step    230 | loss 3.3791 | lr 3.00e-04 | grad 4.53 | tok/s 6720
step    240 | loss 2.5167 | lr 3.00e-04 | grad 2.52 | tok/s 6388
step    250 | loss 2.3428 | lr 3.00e-04 | grad 1.50 | tok/s 6558
step    260 | loss 2.0282 | lr 3.00e-04 | grad 1.59 | tok/s 6771
step    270 | loss 2.3967 | lr 3.00e-04 | grad 1.34 | tok/s 6682
step    280 | loss 2.6039 | lr 3.00e-04 | grad 8.88 | tok/s 6558
step    290 | loss 2.7180 | lr 3.00e-04 | grad 8.06 | tok/s 6898
step    300 | loss 1.7268 | lr 3.00e-04 | grad 9.50 | tok/s 6895
step    310 | loss 2.7855 | lr 3.00e-04 | grad 2.22 | tok/s 6771
step    320 | loss 2.4557 | lr 3.00e-04 | grad 3.17 | tok/s 6632
step    330 | loss 2.2562 | lr 3.00e-04 | grad 1.65 | tok/s 6408
step    340 | loss 2.6139 | lr 3.00e-04 | grad 1.34 | tok/s 6510
step    350 | loss 2.3548 | lr 3.00e-04 | grad 2.86 | tok/s 6678
step    360 | loss 2.6277 | lr 3.00e-04 | grad 3.59 | tok/s 6824
step    370 | loss 2.1827 | lr 3.00e-04 | grad 1.54 | tok/s 6185
step    380 | loss 2.1106 | lr 3.00e-04 | grad 1.89 | tok/s 6588
step    390 | loss 1.8928 | lr 3.00e-04 | grad 1.19 | tok/s 6878
step    400 | loss 1.8721 | lr 3.00e-04 | grad 1.55 | tok/s 6816
step    410 | loss 1.7992 | lr 3.00e-04 | grad 1.15 | tok/s 6667
step    420 | loss 2.1081 | lr 3.00e-04 | grad 4.03 | tok/s 6368
step    430 | loss 2.4989 | lr 3.00e-04 | grad 1.70 | tok/s 6777
step    440 | loss 2.4200 | lr 3.00e-04 | grad 3.00 | tok/s 6401
step    450 | loss 2.3242 | lr 3.00e-04 | grad 1.52 | tok/s 6628
step    460 | loss 2.1252 | lr 3.00e-04 | grad 5.31 | tok/s 6493
step    470 | loss 2.1853 | lr 3.00e-04 | grad 1.91 | tok/s 6685
step    480 | loss 2.7240 | lr 3.00e-04 | grad 4.56 | tok/s 6690
step    490 | loss 2.1246 | lr 3.00e-04 | grad 1.93 | tok/s 6319

Training complete! Final step: 494
