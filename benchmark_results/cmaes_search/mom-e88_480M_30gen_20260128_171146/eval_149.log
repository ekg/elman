Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_149/levelMoME88_100m_20260128_184723
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 483,143,638 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 17.0268 | lr 3.00e-04 | grad 10.81 | tok/s 5950
step     20 | loss 2.6810 | lr 3.00e-04 | grad 5.41 | tok/s 18206
step     30 | loss 2.6519 | lr 3.00e-04 | grad 5.84 | tok/s 18437
step     40 | loss 2.6089 | lr 3.00e-04 | grad 6.25 | tok/s 17693
step     50 | loss 3.3674 | lr 3.00e-04 | grad 20.50 | tok/s 17950
step     60 | loss 2.3499 | lr 3.00e-04 | grad 20.75 | tok/s 18544
step     70 | loss 2.2671 | lr 3.00e-04 | grad 5.69 | tok/s 18753
step     80 | loss 6.0958 | lr 3.00e-04 | grad 29.25 | tok/s 18878
step     90 | loss 4.7856 | lr 3.00e-04 | grad 8.06 | tok/s 19179
step    100 | loss 3.9770 | lr 3.00e-04 | grad 9.44 | tok/s 19197
step    110 | loss 3.8276 | lr 3.00e-04 | grad 25.75 | tok/s 19163
step    120 | loss 3.5719 | lr 3.00e-04 | grad 30.25 | tok/s 19128
step    130 | loss 3.3550 | lr 3.00e-04 | grad 15.38 | tok/s 19142
step    140 | loss 2.8643 | lr 3.00e-04 | grad 10.25 | tok/s 19113
step    150 | loss 3.1498 | lr 3.00e-04 | grad 21.25 | tok/s 19080
step    160 | loss 2.5885 | lr 3.00e-04 | grad 13.19 | tok/s 19090
step    170 | loss 2.7108 | lr 3.00e-04 | grad 17.88 | tok/s 19079
step    180 | loss 2.4288 | lr 3.00e-04 | grad 17.12 | tok/s 19072
step    190 | loss 2.5338 | lr 3.00e-04 | grad 6.12 | tok/s 19057
step    200 | loss 2.2833 | lr 3.00e-04 | grad 8.19 | tok/s 18975
step    210 | loss 2.2874 | lr 3.00e-04 | grad 10.19 | tok/s 19022
step    220 | loss 2.5531 | lr 3.00e-04 | grad 4.97 | tok/s 18814
step    230 | loss 2.9839 | lr 3.00e-04 | grad 10.00 | tok/s 18598
step    240 | loss 2.6269 | lr 3.00e-04 | grad 6.34 | tok/s 17602
step    250 | loss 2.3746 | lr 3.00e-04 | grad 3.19 | tok/s 18159
step    260 | loss 1.9839 | lr 3.00e-04 | grad 4.16 | tok/s 18738
step    270 | loss 2.4056 | lr 3.00e-04 | grad 3.70 | tok/s 18488
step    280 | loss 2.5791 | lr 3.00e-04 | grad 8.75 | tok/s 18114
step    290 | loss 2.1973 | lr 3.00e-04 | grad 5.69 | tok/s 19135
step    300 | loss 0.9367 | lr 3.00e-04 | grad 5.19 | tok/s 19155
step    310 | loss 2.7608 | lr 3.00e-04 | grad 6.00 | tok/s 18772
step    320 | loss 2.3535 | lr 3.00e-04 | grad 7.12 | tok/s 18361
step    330 | loss 2.2572 | lr 3.00e-04 | grad 3.77 | tok/s 17767
step    340 | loss 2.5811 | lr 3.00e-04 | grad 3.34 | tok/s 18037
step    350 | loss 2.2889 | lr 3.00e-04 | grad 4.56 | tok/s 18483
step    360 | loss 1.9164 | lr 3.00e-04 | grad 8.19 | tok/s 18857
step    370 | loss 2.1778 | lr 3.00e-04 | grad 3.81 | tok/s 17136
step    380 | loss 2.0753 | lr 3.00e-04 | grad 5.16 | tok/s 18243
step    390 | loss 1.8227 | lr 3.00e-04 | grad 3.14 | tok/s 19043
step    400 | loss 1.8197 | lr 3.00e-04 | grad 3.92 | tok/s 18869
step    410 | loss 1.6785 | lr 3.00e-04 | grad 2.28 | tok/s 18457
step    420 | loss 2.1112 | lr 3.00e-04 | grad 5.97 | tok/s 17628
step    430 | loss 2.4966 | lr 3.00e-04 | grad 3.97 | tok/s 18785
step    440 | loss 2.4604 | lr 3.00e-04 | grad 5.00 | tok/s 17727
step    450 | loss 2.3518 | lr 3.00e-04 | grad 3.20 | tok/s 18337
step    460 | loss 2.0451 | lr 3.00e-04 | grad 5.19 | tok/s 17959
step    470 | loss 2.1598 | lr 3.00e-04 | grad 3.59 | tok/s 18515
step    480 | loss 2.6201 | lr 3.00e-04 | grad 8.44 | tok/s 18507
step    490 | loss 2.0791 | lr 3.00e-04 | grad 3.80 | tok/s 17504
step    500 | loss 1.9997 | lr 3.00e-04 | grad 4.50 | tok/s 18676
step    510 | loss 1.9909 | lr 3.00e-04 | grad 2.88 | tok/s 18892
step    520 | loss 1.9691 | lr 3.00e-04 | grad 4.44 | tok/s 18886
step    530 | loss 2.2243 | lr 3.00e-04 | grad 3.09 | tok/s 18187
step    540 | loss 1.9826 | lr 3.00e-04 | grad 3.75 | tok/s 18158
step    550 | loss 1.7961 | lr 3.00e-04 | grad 3.61 | tok/s 17808
step    560 | loss 1.9851 | lr 3.00e-04 | grad 3.38 | tok/s 17338
step    570 | loss 1.9624 | lr 3.00e-04 | grad 4.88 | tok/s 17821
step    580 | loss 1.8061 | lr 3.00e-04 | grad 3.77 | tok/s 17741
step    590 | loss 2.1542 | lr 3.00e-04 | grad 4.06 | tok/s 18192
step    600 | loss 2.0958 | lr 3.00e-04 | grad 3.02 | tok/s 17609
step    610 | loss 1.8885 | lr 3.00e-04 | grad 3.00 | tok/s 18498
step    620 | loss 1.7617 | lr 3.00e-04 | grad 3.11 | tok/s 17531
step    630 | loss 1.9024 | lr 3.00e-04 | grad 5.53 | tok/s 17676
step    640 | loss 2.1066 | lr 3.00e-04 | grad 3.20 | tok/s 18133
step    650 | loss 1.9154 | lr 3.00e-04 | grad 3.62 | tok/s 18223
step    660 | loss 1.9477 | lr 3.00e-04 | grad 3.34 | tok/s 18282
step    670 | loss 2.2041 | lr 3.00e-04 | grad 6.97 | tok/s 18432
step    680 | loss 1.9486 | lr 3.00e-04 | grad 3.42 | tok/s 18071
step    690 | loss 2.2051 | lr 3.00e-04 | grad 4.12 | tok/s 18688
step    700 | loss 1.7759 | lr 3.00e-04 | grad 3.95 | tok/s 19002
step    710 | loss 1.8324 | lr 3.00e-04 | grad 3.08 | tok/s 17802
step    720 | loss 1.6841 | lr 3.00e-04 | grad 5.69 | tok/s 17545
step    730 | loss 1.5942 | lr 3.00e-04 | grad 3.73 | tok/s 19017
step    740 | loss 1.7971 | lr 3.00e-04 | grad 3.53 | tok/s 18774
step    750 | loss 1.5219 | lr 3.00e-04 | grad 3.30 | tok/s 19053
step    760 | loss 1.3872 | lr 3.00e-04 | grad 2.91 | tok/s 19079
step    770 | loss 1.3358 | lr 3.00e-04 | grad 2.66 | tok/s 19074
step    780 | loss 1.2839 | lr 3.00e-04 | grad 2.22 | tok/s 19059
step    790 | loss 1.3953 | lr 3.00e-04 | grad 4.47 | tok/s 18487
step    800 | loss 2.1926 | lr 3.00e-04 | grad 6.22 | tok/s 18373
step    810 | loss 1.9400 | lr 3.00e-04 | grad 3.41 | tok/s 18302
step    820 | loss 1.9794 | lr 3.00e-04 | grad 5.44 | tok/s 17604
step    830 | loss 1.8567 | lr 3.00e-04 | grad 3.09 | tok/s 18888
step    840 | loss 1.6987 | lr 3.00e-04 | grad 2.94 | tok/s 19058
step    850 | loss 1.9105 | lr 3.00e-04 | grad 2.84 | tok/s 18988
step    860 | loss 1.8116 | lr 3.00e-04 | grad 5.22 | tok/s 18743
step    870 | loss 1.7525 | lr 3.00e-04 | grad 3.95 | tok/s 18083
step    880 | loss 1.9879 | lr 3.00e-04 | grad 3.56 | tok/s 18178
step    890 | loss 1.9307 | lr 3.00e-04 | grad 3.86 | tok/s 18442
step    900 | loss 1.8036 | lr 3.00e-04 | grad 3.19 | tok/s 18481
step    910 | loss 1.6552 | lr 3.00e-04 | grad 4.56 | tok/s 18105
step    920 | loss 1.7973 | lr 3.00e-04 | grad 5.09 | tok/s 18796
step    930 | loss 1.8412 | lr 3.00e-04 | grad 4.47 | tok/s 17946
step    940 | loss 1.6673 | lr 3.00e-04 | grad 2.89 | tok/s 18919
step    950 | loss 1.8293 | lr 3.00e-04 | grad 6.19 | tok/s 19019
step    960 | loss 1.6546 | lr 3.00e-04 | grad 3.80 | tok/s 19041
step    970 | loss 1.9639 | lr 3.00e-04 | grad 3.89 | tok/s 17922
step    980 | loss 1.8653 | lr 3.00e-04 | grad 3.12 | tok/s 18424
step    990 | loss 1.7012 | lr 3.00e-04 | grad 2.75 | tok/s 18721
step   1000 | loss 2.0994 | lr 3.00e-04 | grad 14.69 | tok/s 17956
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0994.pt
step   1010 | loss 1.9298 | lr 3.00e-04 | grad 4.41 | tok/s 7263
step   1020 | loss 1.8729 | lr 3.00e-04 | grad 2.59 | tok/s 17568
step   1030 | loss 1.6809 | lr 3.00e-04 | grad 3.11 | tok/s 18294
step   1040 | loss 1.6782 | lr 3.00e-04 | grad 2.59 | tok/s 18861
step   1050 | loss 1.8342 | lr 3.00e-04 | grad 4.59 | tok/s 17467
step   1060 | loss 1.9710 | lr 3.00e-04 | grad 4.59 | tok/s 18839
step   1070 | loss 1.9403 | lr 3.00e-04 | grad 3.70 | tok/s 18773
step   1080 | loss 1.6160 | lr 3.00e-04 | grad 2.70 | tok/s 17087
step   1090 | loss 1.3305 | lr 3.00e-04 | grad 2.44 | tok/s 18833
step   1100 | loss 1.6581 | lr 3.00e-04 | grad 4.88 | tok/s 18293
step   1110 | loss 1.6710 | lr 3.00e-04 | grad 3.03 | tok/s 19184
step   1120 | loss 1.5334 | lr 3.00e-04 | grad 3.41 | tok/s 19146
step   1130 | loss 1.4781 | lr 3.00e-04 | grad 2.80 | tok/s 19160
step   1140 | loss 1.4636 | lr 3.00e-04 | grad 3.17 | tok/s 19159
step   1150 | loss 1.4765 | lr 3.00e-04 | grad 2.78 | tok/s 19177
step   1160 | loss 1.3862 | lr 3.00e-04 | grad 2.59 | tok/s 19158
step   1170 | loss 1.4131 | lr 3.00e-04 | grad 3.38 | tok/s 19152
step   1180 | loss 1.5346 | lr 3.00e-04 | grad 2.33 | tok/s 19160
step   1190 | loss 1.4109 | lr 3.00e-04 | grad 3.30 | tok/s 19133
step   1200 | loss 1.3955 | lr 3.00e-04 | grad 2.81 | tok/s 19144
step   1210 | loss 1.4471 | lr 3.00e-04 | grad 3.03 | tok/s 19156
step   1220 | loss 1.4583 | lr 3.00e-04 | grad 3.27 | tok/s 19157
step   1230 | loss 1.4351 | lr 3.00e-04 | grad 2.70 | tok/s 19126
step   1240 | loss 1.3833 | lr 3.00e-04 | grad 2.08 | tok/s 19125
step   1250 | loss 2.2370 | lr 3.00e-04 | grad 4.66 | tok/s 18117
step   1260 | loss 1.6351 | lr 3.00e-04 | grad 6.25 | tok/s 17989
step   1270 | loss 1.8825 | lr 3.00e-04 | grad 6.38 | tok/s 17925
step   1280 | loss 1.8717 | lr 3.00e-04 | grad 2.84 | tok/s 18414
step   1290 | loss 1.7132 | lr 3.00e-04 | grad 3.30 | tok/s 18318
step   1300 | loss 1.7448 | lr 3.00e-04 | grad 3.23 | tok/s 18455
step   1310 | loss 1.6694 | lr 3.00e-04 | grad 3.25 | tok/s 18758
step   1320 | loss 1.7942 | lr 3.00e-04 | grad 3.00 | tok/s 18807
step   1330 | loss 1.8452 | lr 3.00e-04 | grad 3.48 | tok/s 18806
step   1340 | loss 1.7147 | lr 3.00e-04 | grad 12.81 | tok/s 17956

Training complete! Final step: 1342
