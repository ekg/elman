Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_96/levelMoME88_100m_20260128_181020
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 478,568,224 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 21.2211 | lr 3.00e-04 | grad 47.50 | tok/s 9338
step     20 | loss 4.1912 | lr 3.00e-04 | grad 20.88 | tok/s 20519
step     30 | loss 3.1980 | lr 3.00e-04 | grad 5.94 | tok/s 21837
step     40 | loss 6.8835 | lr 3.00e-04 | grad 44.75 | tok/s 22034
step     50 | loss 5.3277 | lr 3.00e-04 | grad 21.38 | tok/s 22258
step     60 | loss 4.4834 | lr 3.00e-04 | grad 32.25 | tok/s 22213
step     70 | loss 3.8115 | lr 3.00e-04 | grad 18.75 | tok/s 22178
step     80 | loss 3.4539 | lr 3.00e-04 | grad 25.00 | tok/s 22170
step     90 | loss 3.2649 | lr 3.00e-04 | grad 7.69 | tok/s 22145
step    100 | loss 2.9595 | lr 3.00e-04 | grad 7.19 | tok/s 22095
step    110 | loss 3.0043 | lr 3.00e-04 | grad 86.00 | tok/s 21811
step    120 | loss 3.3100 | lr 3.00e-04 | grad 14.56 | tok/s 20716
step    130 | loss 2.6063 | lr 3.00e-04 | grad 6.44 | tok/s 21414
step    140 | loss 2.9159 | lr 3.00e-04 | grad 11.81 | tok/s 21523
step    150 | loss 2.2929 | lr 3.00e-04 | grad 7.00 | tok/s 21930
step    160 | loss 2.6534 | lr 3.00e-04 | grad 8.06 | tok/s 20963
step    170 | loss 2.6896 | lr 3.00e-04 | grad 7.00 | tok/s 21010
step    180 | loss 2.7560 | lr 3.00e-04 | grad 3.97 | tok/s 21126
step    190 | loss 2.2863 | lr 3.00e-04 | grad 7.06 | tok/s 21248
step    200 | loss 2.1812 | lr 3.00e-04 | grad 6.22 | tok/s 21917
step    210 | loss 2.3601 | lr 3.00e-04 | grad 5.91 | tok/s 20790
step    220 | loss 2.9419 | lr 3.00e-04 | grad 107.00 | tok/s 21068
step    230 | loss 2.4492 | lr 3.00e-04 | grad 6.94 | tok/s 20824
step    240 | loss 2.6674 | lr 3.00e-04 | grad 3.81 | tok/s 21271
step    250 | loss 2.2256 | lr 3.00e-04 | grad 4.97 | tok/s 21267
step    260 | loss 2.3609 | lr 3.00e-04 | grad 5.03 | tok/s 21719
step    270 | loss 2.1260 | lr 3.00e-04 | grad 3.84 | tok/s 21028
step    280 | loss 2.1545 | lr 3.00e-04 | grad 3.48 | tok/s 20142
step    290 | loss 2.0638 | lr 3.00e-04 | grad 4.56 | tok/s 20497
step    300 | loss 2.3977 | lr 3.00e-04 | grad 3.55 | tok/s 20883
step    310 | loss 2.0280 | lr 3.00e-04 | grad 7.00 | tok/s 20545
step    320 | loss 2.2810 | lr 3.00e-04 | grad 4.16 | tok/s 20906
step    330 | loss 2.1033 | lr 3.00e-04 | grad 3.55 | tok/s 21080
step    340 | loss 2.4810 | lr 3.00e-04 | grad 4.66 | tok/s 21139
step    350 | loss 2.2659 | lr 3.00e-04 | grad 4.09 | tok/s 21679
step    360 | loss 1.9822 | lr 3.00e-04 | grad 2.89 | tok/s 20718
step    370 | loss 1.9466 | lr 3.00e-04 | grad 4.06 | tok/s 21826
step    380 | loss 1.6916 | lr 3.00e-04 | grad 3.12 | tok/s 22012
step    390 | loss 1.5844 | lr 3.00e-04 | grad 3.08 | tok/s 22014
step    400 | loss 2.2786 | lr 3.00e-04 | grad 3.98 | tok/s 20859
step    410 | loss 2.1547 | lr 3.00e-04 | grad 3.52 | tok/s 21058
step    420 | loss 2.1112 | lr 3.00e-04 | grad 10.00 | tok/s 21934
step    430 | loss 2.0505 | lr 3.00e-04 | grad 7.56 | tok/s 21415
step    440 | loss 2.1496 | lr 3.00e-04 | grad 6.03 | tok/s 21074
step    450 | loss 1.9879 | lr 3.00e-04 | grad 4.38 | tok/s 21044
step    460 | loss 2.0099 | lr 3.00e-04 | grad 2.67 | tok/s 21292
step    470 | loss 2.0878 | lr 3.00e-04 | grad 9.56 | tok/s 21531
step    480 | loss 2.1313 | lr 3.00e-04 | grad 10.06 | tok/s 21528
step    490 | loss 2.0678 | lr 3.00e-04 | grad 3.41 | tok/s 21146
step    500 | loss 2.2339 | lr 3.00e-04 | grad 4.09 | tok/s 21141
step    510 | loss 2.0195 | lr 3.00e-04 | grad 5.06 | tok/s 20143
step    520 | loss 1.8817 | lr 3.00e-04 | grad 4.06 | tok/s 21200
step    530 | loss 2.1144 | lr 3.00e-04 | grad 3.11 | tok/s 21137
step    540 | loss 1.9938 | lr 3.00e-04 | grad 3.47 | tok/s 20388
step    550 | loss 1.7485 | lr 3.00e-04 | grad 3.64 | tok/s 21520
step    560 | loss 1.7775 | lr 3.00e-04 | grad 3.39 | tok/s 21963
step    570 | loss 1.6679 | lr 3.00e-04 | grad 3.08 | tok/s 21939
step    580 | loss 1.5931 | lr 3.00e-04 | grad 2.95 | tok/s 21970
step    590 | loss 1.6834 | lr 3.00e-04 | grad 3.88 | tok/s 21962
step    600 | loss 1.6023 | lr 3.00e-04 | grad 3.52 | tok/s 21938
step    610 | loss 1.5988 | lr 3.00e-04 | grad 2.25 | tok/s 21941
step    620 | loss 1.7989 | lr 3.00e-04 | grad 14.19 | tok/s 21628
step    630 | loss 2.0781 | lr 3.00e-04 | grad 4.22 | tok/s 20851
step    640 | loss 2.0593 | lr 3.00e-04 | grad 5.03 | tok/s 20955
step    650 | loss 1.9054 | lr 3.00e-04 | grad 5.06 | tok/s 21019
step    660 | loss 2.0162 | lr 3.00e-04 | grad 6.72 | tok/s 21710
step    670 | loss 1.9446 | lr 3.00e-04 | grad 4.88 | tok/s 20744
step    680 | loss 1.9813 | lr 3.00e-04 | grad 2.62 | tok/s 20656
step    690 | loss 2.0202 | lr 3.00e-04 | grad 4.91 | tok/s 20808
step    700 | loss 1.8068 | lr 3.00e-04 | grad 6.06 | tok/s 20910
step    710 | loss 2.0535 | lr 3.00e-04 | grad 4.94 | tok/s 20737
step    720 | loss 1.6503 | lr 3.00e-04 | grad 3.17 | tok/s 21472
step    730 | loss 1.9364 | lr 3.00e-04 | grad 6.41 | tok/s 21022
step    740 | loss 2.3200 | lr 3.00e-04 | grad 8.38 | tok/s 21800
step    750 | loss 1.8734 | lr 3.00e-04 | grad 3.92 | tok/s 21942
step    760 | loss 1.9675 | lr 3.00e-04 | grad 4.38 | tok/s 21491
step    770 | loss 1.9497 | lr 3.00e-04 | grad 3.55 | tok/s 21125
step    780 | loss 1.8058 | lr 3.00e-04 | grad 3.19 | tok/s 21261

Training complete! Final step: 787
