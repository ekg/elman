Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_58/levelMoME88_100m_20260128_174906
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 475,389,944 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 11.4752 | lr 3.00e-04 | grad 7.59 | tok/s 9006
step     20 | loss 3.6922 | lr 3.00e-04 | grad 2.94 | tok/s 17326
step     30 | loss 3.1680 | lr 3.00e-04 | grad 4.66 | tok/s 18282
step     40 | loss 7.4739 | lr 3.00e-04 | grad 31.00 | tok/s 18703
step     50 | loss 5.3016 | lr 3.00e-04 | grad 30.50 | tok/s 18904
step     60 | loss 4.3028 | lr 3.00e-04 | grad 30.62 | tok/s 18882
step     70 | loss 3.3832 | lr 3.00e-04 | grad 5.69 | tok/s 18840
step     80 | loss 3.0745 | lr 3.00e-04 | grad 9.94 | tok/s 18851
step     90 | loss 2.7205 | lr 3.00e-04 | grad 2.98 | tok/s 18813
step    100 | loss 2.4816 | lr 3.00e-04 | grad 2.44 | tok/s 18752
step    110 | loss 2.5122 | lr 3.00e-04 | grad 3.12 | tok/s 18628
step    120 | loss 3.1495 | lr 3.00e-04 | grad 2.33 | tok/s 17747
step    130 | loss 2.4438 | lr 3.00e-04 | grad 4.59 | tok/s 18189
step    140 | loss 2.7530 | lr 3.00e-04 | grad 7.94 | tok/s 18201
step    150 | loss 2.4013 | lr 3.00e-04 | grad 5.19 | tok/s 18665
step    160 | loss 2.6953 | lr 3.00e-04 | grad 2.38 | tok/s 18038
step    170 | loss 2.5531 | lr 3.00e-04 | grad 1.66 | tok/s 17787
step    180 | loss 2.6608 | lr 3.00e-04 | grad 2.50 | tok/s 18210
step    190 | loss 2.2414 | lr 3.00e-04 | grad 1.63 | tok/s 17876
step    200 | loss 2.0963 | lr 3.00e-04 | grad 1.86 | tok/s 18685
step    210 | loss 2.2390 | lr 3.00e-04 | grad 5.81 | tok/s 17744
step    220 | loss 2.5688 | lr 3.00e-04 | grad 5.94 | tok/s 17936
step    230 | loss 2.4002 | lr 3.00e-04 | grad 4.88 | tok/s 17895
step    240 | loss 2.6675 | lr 3.00e-04 | grad 5.59 | tok/s 18116
step    250 | loss 2.1105 | lr 3.00e-04 | grad 1.56 | tok/s 18026
step    260 | loss 2.2394 | lr 3.00e-04 | grad 2.81 | tok/s 18538
step    270 | loss 2.1289 | lr 3.00e-04 | grad 1.73 | tok/s 18098
step    280 | loss 2.0670 | lr 3.00e-04 | grad 1.63 | tok/s 17005
step    290 | loss 1.9851 | lr 3.00e-04 | grad 1.82 | tok/s 17576
step    300 | loss 2.2910 | lr 3.00e-04 | grad 3.45 | tok/s 17706
step    310 | loss 1.9434 | lr 3.00e-04 | grad 1.51 | tok/s 17645
step    320 | loss 2.1979 | lr 3.00e-04 | grad 3.97 | tok/s 17837
step    330 | loss 2.0011 | lr 3.00e-04 | grad 1.91 | tok/s 18038
step    340 | loss 2.3455 | lr 3.00e-04 | grad 2.39 | tok/s 17958
step    350 | loss 2.2187 | lr 3.00e-04 | grad 2.09 | tok/s 18466
step    360 | loss 1.8864 | lr 3.00e-04 | grad 2.09 | tok/s 17680
step    370 | loss 1.9141 | lr 3.00e-04 | grad 2.09 | tok/s 18620
step    380 | loss 1.6761 | lr 3.00e-04 | grad 1.85 | tok/s 18785
step    390 | loss 1.5609 | lr 3.00e-04 | grad 1.63 | tok/s 18791
step    400 | loss 2.1016 | lr 3.00e-04 | grad 1.92 | tok/s 17794
step    410 | loss 2.0379 | lr 3.00e-04 | grad 2.06 | tok/s 17957
step    420 | loss 2.1122 | lr 3.00e-04 | grad 2.84 | tok/s 18730
step    430 | loss 1.9967 | lr 3.00e-04 | grad 1.89 | tok/s 18419
step    440 | loss 2.0187 | lr 3.00e-04 | grad 2.41 | tok/s 17852
step    450 | loss 1.9022 | lr 3.00e-04 | grad 1.66 | tok/s 18047
step    460 | loss 1.9150 | lr 3.00e-04 | grad 2.25 | tok/s 18317
step    470 | loss 1.8986 | lr 3.00e-04 | grad 8.12 | tok/s 18179
step    480 | loss 1.9813 | lr 3.00e-04 | grad 2.58 | tok/s 18582
step    490 | loss 1.9668 | lr 3.00e-04 | grad 2.67 | tok/s 17833
step    500 | loss 2.1483 | lr 3.00e-04 | grad 2.06 | tok/s 18135
step    510 | loss 1.9490 | lr 3.00e-04 | grad 1.84 | tok/s 17312
step    520 | loss 1.8033 | lr 3.00e-04 | grad 2.25 | tok/s 18137
step    530 | loss 1.9801 | lr 3.00e-04 | grad 2.11 | tok/s 17838
step    540 | loss 1.9039 | lr 3.00e-04 | grad 1.45 | tok/s 17457
step    550 | loss 1.6260 | lr 3.00e-04 | grad 2.84 | tok/s 18273
step    560 | loss 1.7286 | lr 3.00e-04 | grad 1.91 | tok/s 18792
step    570 | loss 1.6026 | lr 3.00e-04 | grad 2.08 | tok/s 18792
step    580 | loss 1.5430 | lr 3.00e-04 | grad 1.31 | tok/s 18793
step    590 | loss 1.5850 | lr 3.00e-04 | grad 1.47 | tok/s 18781
step    600 | loss 1.5399 | lr 3.00e-04 | grad 1.94 | tok/s 18793
step    610 | loss 1.5373 | lr 3.00e-04 | grad 1.61 | tok/s 18794
step    620 | loss 1.5233 | lr 3.00e-04 | grad 1.61 | tok/s 18713
step    630 | loss 1.9202 | lr 3.00e-04 | grad 5.31 | tok/s 17691
step    640 | loss 2.0086 | lr 3.00e-04 | grad 3.17 | tok/s 17890
step    650 | loss 1.8078 | lr 3.00e-04 | grad 1.81 | tok/s 17875
step    660 | loss 1.8632 | lr 3.00e-04 | grad 3.38 | tok/s 18554
step    670 | loss 1.9129 | lr 3.00e-04 | grad 5.25 | tok/s 17964

Training complete! Final step: 673
