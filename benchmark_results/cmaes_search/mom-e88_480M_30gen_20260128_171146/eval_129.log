Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_129/levelMoME88_100m_20260128_183647
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 474,151,116 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 14.7683 | lr 3.00e-04 | grad 9.81 | tok/s 9852
step     20 | loss 3.6798 | lr 3.00e-04 | grad 2.55 | tok/s 21417
step     30 | loss 3.2522 | lr 3.00e-04 | grad 4.41 | tok/s 22549
step     40 | loss 6.7722 | lr 3.00e-04 | grad 18.75 | tok/s 22925
step     50 | loss 4.7602 | lr 3.00e-04 | grad 13.12 | tok/s 23209
step     60 | loss 3.8216 | lr 3.00e-04 | grad 8.38 | tok/s 23136
step     70 | loss 3.3160 | lr 3.00e-04 | grad 7.81 | tok/s 23004
step     80 | loss 3.0282 | lr 3.00e-04 | grad 7.69 | tok/s 22984
step     90 | loss 2.7952 | lr 3.00e-04 | grad 11.06 | tok/s 23006
step    100 | loss 2.5467 | lr 3.00e-04 | grad 15.56 | tok/s 22999
step    110 | loss 2.5753 | lr 3.00e-04 | grad 5.09 | tok/s 22830
step    120 | loss 3.1071 | lr 3.00e-04 | grad 2.48 | tok/s 21727
step    130 | loss 2.4186 | lr 3.00e-04 | grad 5.31 | tok/s 22138
step    140 | loss 2.6759 | lr 3.00e-04 | grad 8.00 | tok/s 22199
step    150 | loss 2.2023 | lr 3.00e-04 | grad 8.88 | tok/s 22799
step    160 | loss 2.6672 | lr 3.00e-04 | grad 3.11 | tok/s 21872
step    170 | loss 2.5403 | lr 3.00e-04 | grad 2.17 | tok/s 21629
step    180 | loss 2.5612 | lr 3.00e-04 | grad 3.45 | tok/s 22074
step    190 | loss 2.2303 | lr 3.00e-04 | grad 3.09 | tok/s 21725
step    200 | loss 2.0548 | lr 3.00e-04 | grad 2.47 | tok/s 22585
step    210 | loss 2.2175 | lr 3.00e-04 | grad 7.34 | tok/s 21550
step    220 | loss 2.5119 | lr 3.00e-04 | grad 9.50 | tok/s 21715
step    230 | loss 2.2535 | lr 3.00e-04 | grad 3.14 | tok/s 21707
step    240 | loss 2.5898 | lr 3.00e-04 | grad 5.34 | tok/s 21908
step    250 | loss 2.0934 | lr 3.00e-04 | grad 2.02 | tok/s 21801
step    260 | loss 2.2265 | lr 3.00e-04 | grad 3.78 | tok/s 22390
step    270 | loss 2.1052 | lr 3.00e-04 | grad 2.52 | tok/s 21920
step    280 | loss 2.0566 | lr 3.00e-04 | grad 2.11 | tok/s 20605
step    290 | loss 1.9791 | lr 3.00e-04 | grad 2.83 | tok/s 21241
step    300 | loss 2.2596 | lr 3.00e-04 | grad 3.06 | tok/s 21464
step    310 | loss 1.9279 | lr 3.00e-04 | grad 2.02 | tok/s 21329
step    320 | loss 2.1867 | lr 3.00e-04 | grad 4.94 | tok/s 21599
step    330 | loss 1.9919 | lr 3.00e-04 | grad 2.34 | tok/s 21798
step    340 | loss 2.3398 | lr 3.00e-04 | grad 3.02 | tok/s 21713
step    350 | loss 2.1666 | lr 3.00e-04 | grad 2.42 | tok/s 22367
step    360 | loss 1.8812 | lr 3.00e-04 | grad 2.55 | tok/s 21412
step    370 | loss 1.8516 | lr 3.00e-04 | grad 2.20 | tok/s 22540
step    380 | loss 1.5815 | lr 3.00e-04 | grad 2.42 | tok/s 22737
step    390 | loss 1.4756 | lr 3.00e-04 | grad 1.77 | tok/s 22700
step    400 | loss 2.0946 | lr 3.00e-04 | grad 2.42 | tok/s 21535
step    410 | loss 2.0495 | lr 3.00e-04 | grad 2.73 | tok/s 21730
step    420 | loss 2.0325 | lr 3.00e-04 | grad 5.81 | tok/s 22626
step    430 | loss 1.9557 | lr 3.00e-04 | grad 1.97 | tok/s 22285
step    440 | loss 2.0069 | lr 3.00e-04 | grad 2.69 | tok/s 21640
step    450 | loss 1.9036 | lr 3.00e-04 | grad 1.77 | tok/s 21881
step    460 | loss 1.9038 | lr 3.00e-04 | grad 2.53 | tok/s 22140
step    470 | loss 1.8834 | lr 3.00e-04 | grad 4.06 | tok/s 22019
step    480 | loss 1.9431 | lr 3.00e-04 | grad 3.12 | tok/s 22468
step    490 | loss 1.9518 | lr 3.00e-04 | grad 3.27 | tok/s 21603
step    500 | loss 2.0895 | lr 3.00e-04 | grad 2.61 | tok/s 21970
step    510 | loss 1.9550 | lr 3.00e-04 | grad 2.00 | tok/s 20999
step    520 | loss 1.7838 | lr 3.00e-04 | grad 2.33 | tok/s 21916
step    530 | loss 1.9677 | lr 3.00e-04 | grad 2.66 | tok/s 21588
step    540 | loss 1.8836 | lr 3.00e-04 | grad 1.86 | tok/s 21188
step    550 | loss 1.6192 | lr 3.00e-04 | grad 3.61 | tok/s 22137
step    560 | loss 1.6991 | lr 3.00e-04 | grad 2.48 | tok/s 22705
step    570 | loss 1.5862 | lr 3.00e-04 | grad 2.45 | tok/s 22734
step    580 | loss 1.5343 | lr 3.00e-04 | grad 1.94 | tok/s 22690
step    590 | loss 1.5738 | lr 3.00e-04 | grad 1.77 | tok/s 22690
step    600 | loss 1.5126 | lr 3.00e-04 | grad 2.03 | tok/s 22696
step    610 | loss 1.5255 | lr 3.00e-04 | grad 1.83 | tok/s 22675
step    620 | loss 1.5150 | lr 3.00e-04 | grad 2.02 | tok/s 22603
step    630 | loss 1.9766 | lr 3.00e-04 | grad 6.19 | tok/s 21443
step    640 | loss 2.0089 | lr 3.00e-04 | grad 2.78 | tok/s 21695
step    650 | loss 1.8047 | lr 3.00e-04 | grad 2.12 | tok/s 21642
step    660 | loss 1.8490 | lr 3.00e-04 | grad 2.34 | tok/s 22477
step    670 | loss 1.8880 | lr 3.00e-04 | grad 5.88 | tok/s 21736
step    680 | loss 1.8966 | lr 3.00e-04 | grad 2.77 | tok/s 21386
step    690 | loss 1.8597 | lr 3.00e-04 | grad 2.45 | tok/s 21187
step    700 | loss 1.7444 | lr 3.00e-04 | grad 2.66 | tok/s 21655
step    710 | loss 1.9282 | lr 3.00e-04 | grad 5.09 | tok/s 21393
step    720 | loss 1.5932 | lr 3.00e-04 | grad 2.11 | tok/s 22156
step    730 | loss 1.7305 | lr 3.00e-04 | grad 1.73 | tok/s 21839
step    740 | loss 2.1459 | lr 3.00e-04 | grad 4.88 | tok/s 22416
step    750 | loss 1.8809 | lr 3.00e-04 | grad 1.97 | tok/s 22679
step    760 | loss 1.8033 | lr 3.00e-04 | grad 4.28 | tok/s 22146
step    770 | loss 1.8331 | lr 3.00e-04 | grad 2.34 | tok/s 21812
step    780 | loss 1.7209 | lr 3.00e-04 | grad 2.50 | tok/s 21971
step    790 | loss 2.0030 | lr 3.00e-04 | grad 5.62 | tok/s 22457
step    800 | loss 1.5926 | lr 3.00e-04 | grad 4.28 | tok/s 22101
step    810 | loss 1.5662 | lr 3.00e-04 | grad 4.03 | tok/s 21374

Training complete! Final step: 815
