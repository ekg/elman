Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_239/levelMoME88_100m_20260128_194547
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 473,568,068 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 11.7726 | lr 3.00e-04 | grad 13.75 | tok/s 9405
step     20 | loss 3.2635 | lr 3.00e-04 | grad 2.61 | tok/s 19447
step     30 | loss 3.2022 | lr 3.00e-04 | grad 4.12 | tok/s 20549
step     40 | loss 5.3068 | lr 3.00e-04 | grad 18.25 | tok/s 20922
step     50 | loss 4.6878 | lr 3.00e-04 | grad 11.25 | tok/s 21151
step     60 | loss 3.7052 | lr 3.00e-04 | grad 5.91 | tok/s 21104
step     70 | loss 3.3114 | lr 3.00e-04 | grad 11.31 | tok/s 21051
step     80 | loss 2.9367 | lr 3.00e-04 | grad 6.94 | tok/s 21008
step     90 | loss 2.6480 | lr 3.00e-04 | grad 3.75 | tok/s 20985
step    100 | loss 2.4263 | lr 3.00e-04 | grad 3.59 | tok/s 20979
step    110 | loss 2.4783 | lr 3.00e-04 | grad 3.22 | tok/s 20823
step    120 | loss 3.0817 | lr 3.00e-04 | grad 2.42 | tok/s 19812
step    130 | loss 2.3664 | lr 3.00e-04 | grad 4.75 | tok/s 20234
step    140 | loss 2.6158 | lr 3.00e-04 | grad 8.44 | tok/s 20276
step    150 | loss 2.1077 | lr 3.00e-04 | grad 7.12 | tok/s 20792
step    160 | loss 2.6638 | lr 3.00e-04 | grad 2.58 | tok/s 20046
step    170 | loss 2.5208 | lr 3.00e-04 | grad 2.06 | tok/s 19774
step    180 | loss 2.4775 | lr 3.00e-04 | grad 3.23 | tok/s 20243
step    190 | loss 2.1835 | lr 3.00e-04 | grad 2.28 | tok/s 19864
step    200 | loss 1.9821 | lr 3.00e-04 | grad 2.30 | tok/s 20768
step    210 | loss 2.1804 | lr 3.00e-04 | grad 5.91 | tok/s 19715
step    220 | loss 2.4774 | lr 3.00e-04 | grad 8.69 | tok/s 19893
step    230 | loss 2.2242 | lr 3.00e-04 | grad 2.78 | tok/s 19852
step    240 | loss 2.5648 | lr 3.00e-04 | grad 5.09 | tok/s 20101
step    250 | loss 2.0414 | lr 3.00e-04 | grad 2.38 | tok/s 19984
step    260 | loss 2.1698 | lr 3.00e-04 | grad 3.56 | tok/s 20578
step    270 | loss 2.0634 | lr 3.00e-04 | grad 2.45 | tok/s 20075
step    280 | loss 2.0002 | lr 3.00e-04 | grad 2.16 | tok/s 18858
step    290 | loss 1.9306 | lr 3.00e-04 | grad 2.20 | tok/s 19535
step    300 | loss 2.2118 | lr 3.00e-04 | grad 2.70 | tok/s 19652
step    310 | loss 1.8782 | lr 3.00e-04 | grad 1.88 | tok/s 19568
step    320 | loss 2.1341 | lr 3.00e-04 | grad 4.34 | tok/s 19787
step    330 | loss 1.9338 | lr 3.00e-04 | grad 2.16 | tok/s 20029
step    340 | loss 2.2864 | lr 3.00e-04 | grad 2.81 | tok/s 19952
step    350 | loss 2.1150 | lr 3.00e-04 | grad 2.23 | tok/s 20490
step    360 | loss 1.8264 | lr 3.00e-04 | grad 2.38 | tok/s 19629
step    370 | loss 1.8148 | lr 3.00e-04 | grad 2.23 | tok/s 20680
step    380 | loss 1.5367 | lr 3.00e-04 | grad 2.27 | tok/s 20840
step    390 | loss 1.4249 | lr 3.00e-04 | grad 1.91 | tok/s 20862
step    400 | loss 2.0605 | lr 3.00e-04 | grad 2.23 | tok/s 19779
step    410 | loss 1.9975 | lr 3.00e-04 | grad 2.48 | tok/s 19934
step    420 | loss 1.9979 | lr 3.00e-04 | grad 5.03 | tok/s 20763
step    430 | loss 1.9301 | lr 3.00e-04 | grad 2.11 | tok/s 20428
step    440 | loss 1.9656 | lr 3.00e-04 | grad 2.69 | tok/s 19834
step    450 | loss 1.8420 | lr 3.00e-04 | grad 1.72 | tok/s 20056
step    460 | loss 1.8490 | lr 3.00e-04 | grad 2.34 | tok/s 20348
step    470 | loss 1.8413 | lr 3.00e-04 | grad 4.00 | tok/s 20187
step    480 | loss 1.8750 | lr 3.00e-04 | grad 3.75 | tok/s 20611
step    490 | loss 1.9095 | lr 3.00e-04 | grad 2.94 | tok/s 19806
step    500 | loss 2.0715 | lr 3.00e-04 | grad 1.98 | tok/s 20109
step    510 | loss 1.9023 | lr 3.00e-04 | grad 1.84 | tok/s 19234
step    520 | loss 1.7375 | lr 3.00e-04 | grad 2.08 | tok/s 20125
step    530 | loss 1.9313 | lr 3.00e-04 | grad 2.44 | tok/s 19789
step    540 | loss 1.8309 | lr 3.00e-04 | grad 1.79 | tok/s 19383
step    550 | loss 1.5612 | lr 3.00e-04 | grad 3.67 | tok/s 20318
step    560 | loss 1.6497 | lr 3.00e-04 | grad 2.20 | tok/s 20861
step    570 | loss 1.5394 | lr 3.00e-04 | grad 2.45 | tok/s 20851
step    580 | loss 1.4809 | lr 3.00e-04 | grad 1.76 | tok/s 20855
step    590 | loss 1.5239 | lr 3.00e-04 | grad 1.58 | tok/s 20851
step    600 | loss 1.4725 | lr 3.00e-04 | grad 1.98 | tok/s 20855
step    610 | loss 1.4799 | lr 3.00e-04 | grad 1.73 | tok/s 20834
step    620 | loss 1.4699 | lr 3.00e-04 | grad 1.92 | tok/s 20757
step    630 | loss 1.9157 | lr 3.00e-04 | grad 5.25 | tok/s 19643
step    640 | loss 1.9702 | lr 3.00e-04 | grad 3.11 | tok/s 19883
step    650 | loss 1.7635 | lr 3.00e-04 | grad 1.91 | tok/s 19891
step    660 | loss 1.8049 | lr 3.00e-04 | grad 2.23 | tok/s 20641
step    670 | loss 1.8546 | lr 3.00e-04 | grad 5.88 | tok/s 19974
step    680 | loss 1.8585 | lr 3.00e-04 | grad 2.86 | tok/s 19662
step    690 | loss 1.8247 | lr 3.00e-04 | grad 2.23 | tok/s 19482
step    700 | loss 1.6897 | lr 3.00e-04 | grad 2.78 | tok/s 19901
step    710 | loss 1.8770 | lr 3.00e-04 | grad 4.62 | tok/s 19595
step    720 | loss 1.5258 | lr 3.00e-04 | grad 1.98 | tok/s 20339
step    730 | loss 1.6643 | lr 3.00e-04 | grad 1.60 | tok/s 20043
step    740 | loss 2.1016 | lr 3.00e-04 | grad 4.19 | tok/s 20584

Training complete! Final step: 747
