Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_6/levelMoME88_100m_20260128_171153
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 481,184,030 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 6.9966 | lr 3.00e-04 | grad 2.80 | tok/s 3857
step     20 | loss 2.9967 | lr 3.00e-04 | grad 2.58 | tok/s 6325
step     30 | loss 2.7528 | lr 3.00e-04 | grad 0.80 | tok/s 6405
step     40 | loss 2.6543 | lr 3.00e-04 | grad 1.14 | tok/s 6138
step     50 | loss 3.5150 | lr 3.00e-04 | grad 6.19 | tok/s 6229
step     60 | loss 2.4086 | lr 3.00e-04 | grad 4.88 | tok/s 6431
step     70 | loss 2.3311 | lr 3.00e-04 | grad 1.52 | tok/s 6512
step     80 | loss 6.9898 | lr 3.00e-04 | grad 8.44 | tok/s 6537
step     90 | loss 6.0128 | lr 3.00e-04 | grad 1.82 | tok/s 6655
step    100 | loss 4.4062 | lr 3.00e-04 | grad 2.89 | tok/s 6669
step    110 | loss 4.1973 | lr 3.00e-04 | grad 7.38 | tok/s 6663
step    120 | loss 3.9616 | lr 3.00e-04 | grad 7.66 | tok/s 6673
step    130 | loss 3.8338 | lr 3.00e-04 | grad 4.38 | tok/s 6440
step    140 | loss 3.2611 | lr 3.00e-04 | grad 2.75 | tok/s 6663
step    150 | loss 3.6325 | lr 3.00e-04 | grad 3.58 | tok/s 6673
step    160 | loss 3.0371 | lr 3.00e-04 | grad 3.34 | tok/s 6671
step    170 | loss 2.9856 | lr 3.00e-04 | grad 3.31 | tok/s 6655
step    180 | loss 2.8010 | lr 3.00e-04 | grad 1.98 | tok/s 6668
step    190 | loss 2.9194 | lr 3.00e-04 | grad 2.20 | tok/s 6683
step    200 | loss 2.5751 | lr 3.00e-04 | grad 1.86 | tok/s 6681
step    210 | loss 2.5748 | lr 3.00e-04 | grad 3.27 | tok/s 6683
step    220 | loss 2.6037 | lr 3.00e-04 | grad 1.52 | tok/s 6598
step    230 | loss 3.2925 | lr 3.00e-04 | grad 2.61 | tok/s 6522
step    240 | loss 2.5077 | lr 3.00e-04 | grad 1.83 | tok/s 6196
step    250 | loss 2.3480 | lr 3.00e-04 | grad 0.99 | tok/s 6371
step    260 | loss 2.0751 | lr 3.00e-04 | grad 1.03 | tok/s 6569
step    270 | loss 2.3891 | lr 3.00e-04 | grad 1.06 | tok/s 6307
step    280 | loss 2.5925 | lr 3.00e-04 | grad 3.86 | tok/s 6357
step    290 | loss 2.7479 | lr 3.00e-04 | grad 2.28 | tok/s 6668
step    300 | loss 1.8787 | lr 3.00e-04 | grad 1.80 | tok/s 6664
step    310 | loss 2.8670 | lr 3.00e-04 | grad 1.87 | tok/s 6559
step    320 | loss 2.5796 | lr 3.00e-04 | grad 2.33 | tok/s 6421
step    330 | loss 2.2506 | lr 3.00e-04 | grad 1.28 | tok/s 6210
step    340 | loss 2.5891 | lr 3.00e-04 | grad 1.03 | tok/s 6307
step    350 | loss 2.3896 | lr 3.00e-04 | grad 1.71 | tok/s 6471
step    360 | loss 2.7468 | lr 3.00e-04 | grad 4.53 | tok/s 6604
step    370 | loss 2.1746 | lr 3.00e-04 | grad 1.16 | tok/s 5985
step    380 | loss 2.1443 | lr 3.00e-04 | grad 1.16 | tok/s 6388
step    390 | loss 1.9780 | lr 3.00e-04 | grad 0.90 | tok/s 6665
step    400 | loss 1.9524 | lr 3.00e-04 | grad 1.20 | tok/s 6616
step    410 | loss 1.8845 | lr 3.00e-04 | grad 0.86 | tok/s 6299
step    420 | loss 2.1281 | lr 3.00e-04 | grad 1.96 | tok/s 6177
step    430 | loss 2.4931 | lr 3.00e-04 | grad 1.29 | tok/s 6569
step    440 | loss 2.4283 | lr 3.00e-04 | grad 2.02 | tok/s 6204
step    450 | loss 2.4147 | lr 3.00e-04 | grad 1.17 | tok/s 6429
step    460 | loss 2.1429 | lr 3.00e-04 | grad 1.86 | tok/s 6286
step    470 | loss 2.2041 | lr 3.00e-04 | grad 1.19 | tok/s 6483

Training complete! Final step: 478
