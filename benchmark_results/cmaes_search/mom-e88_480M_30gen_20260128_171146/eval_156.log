Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_156/levelMoME88_100m_20260128_185240
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 482,157,380 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 15.2031 | lr 3.00e-04 | grad 10.94 | tok/s 6022
step     20 | loss 2.6961 | lr 3.00e-04 | grad 20.12 | tok/s 18520
step     30 | loss 2.6499 | lr 3.00e-04 | grad 4.03 | tok/s 18757
step     40 | loss 2.7067 | lr 3.00e-04 | grad 5.12 | tok/s 17976
step     50 | loss 3.2992 | lr 3.00e-04 | grad 8.00 | tok/s 18243
step     60 | loss 2.3458 | lr 3.00e-04 | grad 15.38 | tok/s 18857
step     70 | loss 2.2753 | lr 3.00e-04 | grad 5.88 | tok/s 19071
step     80 | loss 6.8368 | lr 3.00e-04 | grad 39.00 | tok/s 19142
step     90 | loss 5.0070 | lr 3.00e-04 | grad 6.88 | tok/s 19453
step    100 | loss 4.0190 | lr 3.00e-04 | grad 9.00 | tok/s 19425
step    110 | loss 3.9753 | lr 3.00e-04 | grad 20.25 | tok/s 19429
step    120 | loss 3.6773 | lr 3.00e-04 | grad 25.25 | tok/s 19407
step    130 | loss 3.4708 | lr 3.00e-04 | grad 11.06 | tok/s 19375
step    140 | loss 2.9754 | lr 3.00e-04 | grad 7.94 | tok/s 19364
step    150 | loss 3.2043 | lr 3.00e-04 | grad 19.62 | tok/s 19398
step    160 | loss 2.7088 | lr 3.00e-04 | grad 7.88 | tok/s 19363
step    170 | loss 2.7169 | lr 3.00e-04 | grad 19.88 | tok/s 19381
step    180 | loss 2.5112 | lr 3.00e-04 | grad 10.56 | tok/s 19376
step    190 | loss 2.5912 | lr 3.00e-04 | grad 15.12 | tok/s 19327
step    200 | loss 2.3160 | lr 3.00e-04 | grad 7.56 | tok/s 19327
step    210 | loss 2.2914 | lr 3.00e-04 | grad 8.56 | tok/s 19332
step    220 | loss 2.5121 | lr 3.00e-04 | grad 3.12 | tok/s 19087
step    230 | loss 2.9561 | lr 3.00e-04 | grad 4.78 | tok/s 18862
step    240 | loss 2.5866 | lr 3.00e-04 | grad 6.16 | tok/s 17925
step    250 | loss 2.4042 | lr 3.00e-04 | grad 3.20 | tok/s 18419
step    260 | loss 1.9767 | lr 3.00e-04 | grad 7.47 | tok/s 18974
step    270 | loss 2.4499 | lr 3.00e-04 | grad 3.58 | tok/s 18741
step    280 | loss 2.5479 | lr 3.00e-04 | grad 9.50 | tok/s 18400
step    290 | loss 2.2666 | lr 3.00e-04 | grad 4.16 | tok/s 19393
step    300 | loss 1.0120 | lr 3.00e-04 | grad 3.67 | tok/s 19391
step    310 | loss 2.8075 | lr 3.00e-04 | grad 5.69 | tok/s 19014
step    320 | loss 2.3949 | lr 3.00e-04 | grad 6.62 | tok/s 18618
step    330 | loss 2.2524 | lr 3.00e-04 | grad 3.56 | tok/s 17983
step    340 | loss 2.5599 | lr 3.00e-04 | grad 3.19 | tok/s 18260
step    350 | loss 2.2980 | lr 3.00e-04 | grad 5.28 | tok/s 18727
step    360 | loss 2.0031 | lr 3.00e-04 | grad 5.16 | tok/s 19120
step    370 | loss 2.1892 | lr 3.00e-04 | grad 3.47 | tok/s 17359
step    380 | loss 2.0500 | lr 3.00e-04 | grad 3.31 | tok/s 18509
step    390 | loss 1.8050 | lr 3.00e-04 | grad 2.67 | tok/s 19289
step    400 | loss 1.8146 | lr 3.00e-04 | grad 3.22 | tok/s 19116
step    410 | loss 1.6827 | lr 3.00e-04 | grad 2.94 | tok/s 18700
step    420 | loss 2.1197 | lr 3.00e-04 | grad 5.56 | tok/s 17894
step    430 | loss 2.4441 | lr 3.00e-04 | grad 4.12 | tok/s 19020
step    440 | loss 2.4436 | lr 3.00e-04 | grad 4.41 | tok/s 17983
step    450 | loss 2.5839 | lr 3.00e-04 | grad 2.83 | tok/s 18572
step    460 | loss 2.0573 | lr 3.00e-04 | grad 4.81 | tok/s 18191
step    470 | loss 2.1581 | lr 3.00e-04 | grad 3.34 | tok/s 18747
step    480 | loss 2.6270 | lr 3.00e-04 | grad 7.41 | tok/s 18772
step    490 | loss 2.0719 | lr 3.00e-04 | grad 3.02 | tok/s 17738
step    500 | loss 1.9898 | lr 3.00e-04 | grad 4.72 | tok/s 18925
step    510 | loss 2.0054 | lr 3.00e-04 | grad 3.00 | tok/s 19206
step    520 | loss 1.9751 | lr 3.00e-04 | grad 2.55 | tok/s 19147
step    530 | loss 2.2157 | lr 3.00e-04 | grad 3.05 | tok/s 18426
step    540 | loss 1.9723 | lr 3.00e-04 | grad 3.25 | tok/s 18400
step    550 | loss 1.7965 | lr 3.00e-04 | grad 3.86 | tok/s 18011
step    560 | loss 1.9925 | lr 3.00e-04 | grad 3.06 | tok/s 17572
step    570 | loss 1.9520 | lr 3.00e-04 | grad 4.31 | tok/s 18049
step    580 | loss 1.8071 | lr 3.00e-04 | grad 3.33 | tok/s 18007
step    590 | loss 2.1577 | lr 3.00e-04 | grad 4.03 | tok/s 18465
step    600 | loss 2.0930 | lr 3.00e-04 | grad 2.81 | tok/s 17831
step    610 | loss 1.8806 | lr 3.00e-04 | grad 2.77 | tok/s 18742
step    620 | loss 1.7620 | lr 3.00e-04 | grad 3.11 | tok/s 17746
step    630 | loss 1.9084 | lr 3.00e-04 | grad 5.25 | tok/s 17892
step    640 | loss 2.0974 | lr 3.00e-04 | grad 3.17 | tok/s 18368
step    650 | loss 1.9244 | lr 3.00e-04 | grad 3.67 | tok/s 18459
step    660 | loss 1.9613 | lr 3.00e-04 | grad 4.28 | tok/s 18569
step    670 | loss 2.2227 | lr 3.00e-04 | grad 4.12 | tok/s 18683
step    680 | loss 1.9489 | lr 3.00e-04 | grad 3.08 | tok/s 18281
step    690 | loss 2.1718 | lr 3.00e-04 | grad 4.28 | tok/s 18900
step    700 | loss 1.8455 | lr 3.00e-04 | grad 3.69 | tok/s 19291
step    710 | loss 1.8332 | lr 3.00e-04 | grad 3.36 | tok/s 18023
step    720 | loss 1.6837 | lr 3.00e-04 | grad 3.92 | tok/s 17760
step    730 | loss 1.6121 | lr 3.00e-04 | grad 3.73 | tok/s 19242
step    740 | loss 1.7895 | lr 3.00e-04 | grad 3.45 | tok/s 18997
step    750 | loss 1.5009 | lr 3.00e-04 | grad 2.95 | tok/s 19300
step    760 | loss 1.3724 | lr 3.00e-04 | grad 3.19 | tok/s 19300
step    770 | loss 1.3195 | lr 3.00e-04 | grad 2.50 | tok/s 19307
step    780 | loss 1.2755 | lr 3.00e-04 | grad 2.39 | tok/s 19302
step    790 | loss 1.3896 | lr 3.00e-04 | grad 4.00 | tok/s 18707
step    800 | loss 2.1667 | lr 3.00e-04 | grad 6.12 | tok/s 18634
step    810 | loss 1.9412 | lr 3.00e-04 | grad 2.67 | tok/s 18508
step    820 | loss 1.9623 | lr 3.00e-04 | grad 5.38 | tok/s 17798
step    830 | loss 1.8920 | lr 3.00e-04 | grad 3.16 | tok/s 19119
step    840 | loss 1.7233 | lr 3.00e-04 | grad 2.81 | tok/s 19289
step    850 | loss 1.9729 | lr 3.00e-04 | grad 2.80 | tok/s 19208
step    860 | loss 1.7968 | lr 3.00e-04 | grad 5.09 | tok/s 18976
step    870 | loss 1.7694 | lr 3.00e-04 | grad 3.50 | tok/s 18290
step    880 | loss 1.9723 | lr 3.00e-04 | grad 3.58 | tok/s 18374
step    890 | loss 1.9190 | lr 3.00e-04 | grad 3.75 | tok/s 18645
step    900 | loss 1.8098 | lr 3.00e-04 | grad 3.00 | tok/s 18660
step    910 | loss 1.6396 | lr 3.00e-04 | grad 4.22 | tok/s 18269
step    920 | loss 1.7996 | lr 3.00e-04 | grad 4.47 | tok/s 18995
step    930 | loss 1.8505 | lr 3.00e-04 | grad 4.28 | tok/s 18128
step    940 | loss 1.6738 | lr 3.00e-04 | grad 4.72 | tok/s 19113
step    950 | loss 1.8459 | lr 3.00e-04 | grad 3.67 | tok/s 19181
step    960 | loss 1.6943 | lr 3.00e-04 | grad 3.53 | tok/s 19210
step    970 | loss 1.9676 | lr 3.00e-04 | grad 3.92 | tok/s 18087
step    980 | loss 1.8648 | lr 3.00e-04 | grad 2.92 | tok/s 18585
step    990 | loss 1.7009 | lr 3.00e-04 | grad 2.70 | tok/s 18881
step   1000 | loss 2.0913 | lr 3.00e-04 | grad 14.06 | tok/s 18126
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0913.pt
step   1010 | loss 1.9410 | lr 3.00e-04 | grad 4.38 | tok/s 7377
step   1020 | loss 1.8470 | lr 3.00e-04 | grad 3.72 | tok/s 17719
step   1030 | loss 1.6887 | lr 3.00e-04 | grad 2.97 | tok/s 18450
step   1040 | loss 1.6771 | lr 3.00e-04 | grad 2.69 | tok/s 19090
step   1050 | loss 1.8253 | lr 3.00e-04 | grad 4.31 | tok/s 17610
step   1060 | loss 1.9772 | lr 3.00e-04 | grad 4.56 | tok/s 19012
step   1070 | loss 2.0027 | lr 3.00e-04 | grad 3.75 | tok/s 18966
step   1080 | loss 1.6075 | lr 3.00e-04 | grad 2.45 | tok/s 17198
step   1090 | loss 1.3126 | lr 3.00e-04 | grad 2.14 | tok/s 18996
step   1100 | loss 1.6400 | lr 3.00e-04 | grad 4.88 | tok/s 18415
step   1110 | loss 1.6557 | lr 3.00e-04 | grad 4.22 | tok/s 19341
step   1120 | loss 1.5334 | lr 3.00e-04 | grad 3.12 | tok/s 19334
step   1130 | loss 1.4663 | lr 3.00e-04 | grad 2.47 | tok/s 19310
step   1140 | loss 1.4584 | lr 3.00e-04 | grad 3.03 | tok/s 19327
step   1150 | loss 1.4761 | lr 3.00e-04 | grad 2.58 | tok/s 19296
step   1160 | loss 1.3812 | lr 3.00e-04 | grad 2.62 | tok/s 19287
step   1170 | loss 1.4099 | lr 3.00e-04 | grad 2.95 | tok/s 19284
step   1180 | loss 1.5303 | lr 3.00e-04 | grad 2.14 | tok/s 19319
step   1190 | loss 1.4056 | lr 3.00e-04 | grad 3.39 | tok/s 19319
step   1200 | loss 1.3978 | lr 3.00e-04 | grad 3.20 | tok/s 19305
step   1210 | loss 1.4448 | lr 3.00e-04 | grad 2.77 | tok/s 19304
step   1220 | loss 1.4580 | lr 3.00e-04 | grad 2.98 | tok/s 19292
step   1230 | loss 1.4325 | lr 3.00e-04 | grad 2.52 | tok/s 19303
step   1240 | loss 1.3809 | lr 3.00e-04 | grad 2.25 | tok/s 19298
step   1250 | loss 2.2829 | lr 3.00e-04 | grad 5.06 | tok/s 18325
step   1260 | loss 1.6057 | lr 3.00e-04 | grad 4.97 | tok/s 18103
step   1270 | loss 1.8745 | lr 3.00e-04 | grad 6.62 | tok/s 18067
step   1280 | loss 1.8713 | lr 3.00e-04 | grad 2.53 | tok/s 18596
step   1290 | loss 1.7068 | lr 3.00e-04 | grad 2.92 | tok/s 18493
step   1300 | loss 1.7286 | lr 3.00e-04 | grad 2.89 | tok/s 18624
step   1310 | loss 1.6718 | lr 3.00e-04 | grad 3.56 | tok/s 18896
step   1320 | loss 1.7903 | lr 3.00e-04 | grad 2.83 | tok/s 18956
step   1330 | loss 1.8571 | lr 3.00e-04 | grad 3.36 | tok/s 18978
step   1340 | loss 1.7306 | lr 3.00e-04 | grad 13.56 | tok/s 18146
step   1350 | loss 1.9368 | lr 3.00e-04 | grad 3.78 | tok/s 17537

Training complete! Final step: 1359
