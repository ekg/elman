Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_184/levelMoME88_100m_20260128_190835
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 478,884,012 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 14.1451 | lr 3.00e-04 | grad 18.00 | tok/s 9516
step     20 | loss 3.3561 | lr 3.00e-04 | grad 9.44 | tok/s 21322
step     30 | loss 3.0610 | lr 3.00e-04 | grad 4.28 | tok/s 22681
step     40 | loss 5.3141 | lr 3.00e-04 | grad 16.12 | tok/s 22914
step     50 | loss 4.0772 | lr 3.00e-04 | grad 9.75 | tok/s 23155
step     60 | loss 3.8151 | lr 3.00e-04 | grad 8.69 | tok/s 23002
step     70 | loss 3.1228 | lr 3.00e-04 | grad 13.44 | tok/s 23000
step     80 | loss 2.7492 | lr 3.00e-04 | grad 3.34 | tok/s 22998
step     90 | loss 2.5716 | lr 3.00e-04 | grad 5.00 | tok/s 22988
step    100 | loss 2.4076 | lr 3.00e-04 | grad 4.38 | tok/s 22982
step    110 | loss 2.7387 | lr 3.00e-04 | grad 15.19 | tok/s 22638
step    120 | loss 2.8927 | lr 3.00e-04 | grad 4.06 | tok/s 21562
step    130 | loss 2.3771 | lr 3.00e-04 | grad 6.88 | tok/s 22362
step    140 | loss 2.7437 | lr 3.00e-04 | grad 8.50 | tok/s 22355
step    150 | loss 2.0971 | lr 3.00e-04 | grad 5.59 | tok/s 22793
step    160 | loss 2.5274 | lr 3.00e-04 | grad 3.14 | tok/s 21773
step    170 | loss 2.5457 | lr 3.00e-04 | grad 2.94 | tok/s 21922
step    180 | loss 2.4978 | lr 3.00e-04 | grad 2.78 | tok/s 22042
step    190 | loss 2.1598 | lr 3.00e-04 | grad 3.48 | tok/s 22079
step    200 | loss 2.0092 | lr 3.00e-04 | grad 2.72 | tok/s 22773
step    210 | loss 2.2513 | lr 3.00e-04 | grad 2.72 | tok/s 21556
step    220 | loss 2.5021 | lr 3.00e-04 | grad 15.44 | tok/s 21851
step    230 | loss 2.2837 | lr 3.00e-04 | grad 4.38 | tok/s 21605
step    240 | loss 2.5105 | lr 3.00e-04 | grad 3.47 | tok/s 22058
step    250 | loss 2.0997 | lr 3.00e-04 | grad 3.08 | tok/s 22057
step    260 | loss 2.2338 | lr 3.00e-04 | grad 3.55 | tok/s 22539
step    270 | loss 2.0230 | lr 3.00e-04 | grad 2.52 | tok/s 21794
step    280 | loss 2.0501 | lr 3.00e-04 | grad 2.25 | tok/s 20864
step    290 | loss 1.9470 | lr 3.00e-04 | grad 2.97 | tok/s 21270
step    300 | loss 2.2706 | lr 3.00e-04 | grad 2.78 | tok/s 21657
step    310 | loss 1.9211 | lr 3.00e-04 | grad 3.17 | tok/s 21264
step    320 | loss 2.1464 | lr 3.00e-04 | grad 2.64 | tok/s 21743
step    330 | loss 1.9946 | lr 3.00e-04 | grad 2.31 | tok/s 21848
step    340 | loss 2.3459 | lr 3.00e-04 | grad 3.05 | tok/s 21890
step    350 | loss 2.1026 | lr 3.00e-04 | grad 2.47 | tok/s 22492
step    360 | loss 1.8465 | lr 3.00e-04 | grad 2.16 | tok/s 21590
step    370 | loss 1.8212 | lr 3.00e-04 | grad 2.73 | tok/s 22660
step    380 | loss 1.5571 | lr 3.00e-04 | grad 2.23 | tok/s 22861
step    390 | loss 1.4418 | lr 3.00e-04 | grad 2.09 | tok/s 22837
step    400 | loss 2.1452 | lr 3.00e-04 | grad 3.00 | tok/s 21675
step    410 | loss 2.0407 | lr 3.00e-04 | grad 2.70 | tok/s 21890
step    420 | loss 2.0062 | lr 3.00e-04 | grad 10.75 | tok/s 22810
step    430 | loss 1.9018 | lr 3.00e-04 | grad 2.45 | tok/s 22281
step    440 | loss 2.0208 | lr 3.00e-04 | grad 3.53 | tok/s 22000
step    450 | loss 1.8691 | lr 3.00e-04 | grad 3.20 | tok/s 21871
step    460 | loss 1.8745 | lr 3.00e-04 | grad 1.77 | tok/s 22170
step    470 | loss 1.9188 | lr 3.00e-04 | grad 3.83 | tok/s 22395
step    480 | loss 1.8948 | lr 3.00e-04 | grad 2.55 | tok/s 22338
step    490 | loss 1.9407 | lr 3.00e-04 | grad 2.50 | tok/s 21956
step    500 | loss 2.1258 | lr 3.00e-04 | grad 3.08 | tok/s 21874
step    510 | loss 1.9110 | lr 3.00e-04 | grad 2.67 | tok/s 20914
step    520 | loss 1.7628 | lr 3.00e-04 | grad 2.30 | tok/s 22020
step    530 | loss 1.9943 | lr 3.00e-04 | grad 2.41 | tok/s 22004
step    540 | loss 1.8583 | lr 3.00e-04 | grad 2.48 | tok/s 21175
step    550 | loss 1.6195 | lr 3.00e-04 | grad 2.39 | tok/s 22288
step    560 | loss 1.6627 | lr 3.00e-04 | grad 2.31 | tok/s 22722
step    570 | loss 1.5661 | lr 3.00e-04 | grad 1.97 | tok/s 22755
step    580 | loss 1.5027 | lr 3.00e-04 | grad 2.00 | tok/s 22743
step    590 | loss 1.5816 | lr 3.00e-04 | grad 2.66 | tok/s 22740
step    600 | loss 1.5066 | lr 3.00e-04 | grad 2.17 | tok/s 22752
step    610 | loss 1.5046 | lr 3.00e-04 | grad 1.84 | tok/s 22732
step    620 | loss 1.6284 | lr 3.00e-04 | grad 13.31 | tok/s 22402
step    630 | loss 1.9951 | lr 3.00e-04 | grad 3.31 | tok/s 21615
step    640 | loss 1.9364 | lr 3.00e-04 | grad 2.47 | tok/s 21716
step    650 | loss 1.7935 | lr 3.00e-04 | grad 3.86 | tok/s 21803
step    660 | loss 1.8997 | lr 3.00e-04 | grad 4.41 | tok/s 22467
step    670 | loss 1.8116 | lr 3.00e-04 | grad 2.91 | tok/s 21518
step    680 | loss 1.8576 | lr 3.00e-04 | grad 1.73 | tok/s 21407
step    690 | loss 1.9059 | lr 3.00e-04 | grad 4.78 | tok/s 21556
step    700 | loss 1.6891 | lr 3.00e-04 | grad 1.90 | tok/s 21658
step    710 | loss 1.9081 | lr 3.00e-04 | grad 3.73 | tok/s 21462
step    720 | loss 1.5223 | lr 3.00e-04 | grad 2.09 | tok/s 22223
step    730 | loss 1.8078 | lr 3.00e-04 | grad 4.19 | tok/s 21790
step    740 | loss 2.1082 | lr 3.00e-04 | grad 4.94 | tok/s 22534
step    750 | loss 1.7439 | lr 3.00e-04 | grad 2.31 | tok/s 22733
step    760 | loss 1.8323 | lr 3.00e-04 | grad 3.08 | tok/s 22240
step    770 | loss 1.8054 | lr 3.00e-04 | grad 2.55 | tok/s 21877
step    780 | loss 1.6976 | lr 3.00e-04 | grad 2.58 | tok/s 22013
step    790 | loss 2.0131 | lr 3.00e-04 | grad 3.47 | tok/s 22436
step    800 | loss 1.3884 | lr 3.00e-04 | grad 1.76 | tok/s 22185
step    810 | loss 1.6863 | lr 3.00e-04 | grad 2.95 | tok/s 21351

Training complete! Final step: 816
