Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_135/levelMoME88_100m_20260128_183648
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 474,116,992 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 11.7594 | lr 3.00e-04 | grad 9.69 | tok/s 9421
step     20 | loss 2.8316 | lr 3.00e-04 | grad 3.20 | tok/s 20054
step     30 | loss 3.0823 | lr 3.00e-04 | grad 7.91 | tok/s 21200
step     40 | loss 4.7084 | lr 3.00e-04 | grad 15.06 | tok/s 21525
step     50 | loss 4.2924 | lr 3.00e-04 | grad 13.19 | tok/s 21761
step     60 | loss 3.7172 | lr 3.00e-04 | grad 7.91 | tok/s 21773
step     70 | loss 3.1232 | lr 3.00e-04 | grad 8.56 | tok/s 21764
step     80 | loss 2.8897 | lr 3.00e-04 | grad 5.50 | tok/s 21775
step     90 | loss 2.7332 | lr 3.00e-04 | grad 6.25 | tok/s 21637
step    100 | loss 2.4651 | lr 3.00e-04 | grad 5.78 | tok/s 21610
step    110 | loss 2.4773 | lr 3.00e-04 | grad 4.03 | tok/s 21451
step    120 | loss 3.1704 | lr 3.00e-04 | grad 2.59 | tok/s 20395
step    130 | loss 2.3720 | lr 3.00e-04 | grad 5.47 | tok/s 20879
step    140 | loss 2.6399 | lr 3.00e-04 | grad 9.06 | tok/s 20949
step    150 | loss 2.0762 | lr 3.00e-04 | grad 7.53 | tok/s 21482
step    160 | loss 2.6637 | lr 3.00e-04 | grad 2.91 | tok/s 20717
step    170 | loss 2.5083 | lr 3.00e-04 | grad 2.31 | tok/s 20407
step    180 | loss 2.4387 | lr 3.00e-04 | grad 3.67 | tok/s 20846
step    190 | loss 2.1831 | lr 3.00e-04 | grad 2.48 | tok/s 20487
step    200 | loss 1.9978 | lr 3.00e-04 | grad 2.28 | tok/s 21427
step    210 | loss 2.1593 | lr 3.00e-04 | grad 7.53 | tok/s 20328
step    220 | loss 2.5626 | lr 3.00e-04 | grad 27.62 | tok/s 20579
step    230 | loss 2.4041 | lr 3.00e-04 | grad 3.12 | tok/s 20525
step    240 | loss 2.5576 | lr 3.00e-04 | grad 5.88 | tok/s 20797
step    250 | loss 2.0446 | lr 3.00e-04 | grad 2.16 | tok/s 20659
step    260 | loss 2.1685 | lr 3.00e-04 | grad 3.84 | tok/s 21233
step    270 | loss 2.0775 | lr 3.00e-04 | grad 2.50 | tok/s 20781
step    280 | loss 2.0082 | lr 3.00e-04 | grad 2.14 | tok/s 19484
step    290 | loss 1.9272 | lr 3.00e-04 | grad 2.45 | tok/s 20182
step    300 | loss 2.2228 | lr 3.00e-04 | grad 2.83 | tok/s 20317
step    310 | loss 1.8909 | lr 3.00e-04 | grad 2.02 | tok/s 20233
step    320 | loss 2.1519 | lr 3.00e-04 | grad 5.09 | tok/s 20445
step    330 | loss 1.9499 | lr 3.00e-04 | grad 2.16 | tok/s 20670
step    340 | loss 2.3194 | lr 3.00e-04 | grad 3.22 | tok/s 20607
step    350 | loss 2.1298 | lr 3.00e-04 | grad 2.69 | tok/s 21158
step    360 | loss 1.8290 | lr 3.00e-04 | grad 2.41 | tok/s 20257
step    370 | loss 1.7875 | lr 3.00e-04 | grad 2.44 | tok/s 21351
step    380 | loss 1.5316 | lr 3.00e-04 | grad 2.31 | tok/s 21553
step    390 | loss 1.4138 | lr 3.00e-04 | grad 2.30 | tok/s 21544
step    400 | loss 2.0417 | lr 3.00e-04 | grad 2.23 | tok/s 20409
step    410 | loss 2.0029 | lr 3.00e-04 | grad 2.67 | tok/s 20607
step    420 | loss 1.9583 | lr 3.00e-04 | grad 5.97 | tok/s 21473
step    430 | loss 1.9034 | lr 3.00e-04 | grad 2.12 | tok/s 21126
step    440 | loss 1.9580 | lr 3.00e-04 | grad 2.75 | tok/s 20467
step    450 | loss 1.8542 | lr 3.00e-04 | grad 1.92 | tok/s 20703
step    460 | loss 1.8428 | lr 3.00e-04 | grad 2.39 | tok/s 21011
step    470 | loss 1.8545 | lr 3.00e-04 | grad 5.00 | tok/s 20842
step    480 | loss 1.9099 | lr 3.00e-04 | grad 3.66 | tok/s 21313
step    490 | loss 1.9139 | lr 3.00e-04 | grad 3.02 | tok/s 20457
step    500 | loss 2.0684 | lr 3.00e-04 | grad 2.27 | tok/s 20795
step    510 | loss 1.9068 | lr 3.00e-04 | grad 4.91 | tok/s 19866
step    520 | loss 1.7455 | lr 3.00e-04 | grad 2.47 | tok/s 20801
step    530 | loss 1.9261 | lr 3.00e-04 | grad 2.67 | tok/s 20473
step    540 | loss 1.8383 | lr 3.00e-04 | grad 1.80 | tok/s 20016
step    550 | loss 1.5752 | lr 3.00e-04 | grad 3.64 | tok/s 20945
step    560 | loss 1.6484 | lr 3.00e-04 | grad 2.30 | tok/s 21492
step    570 | loss 1.5312 | lr 3.00e-04 | grad 2.44 | tok/s 21498
step    580 | loss 1.4877 | lr 3.00e-04 | grad 1.66 | tok/s 21491
step    590 | loss 1.5205 | lr 3.00e-04 | grad 1.59 | tok/s 21530
step    600 | loss 1.4669 | lr 3.00e-04 | grad 1.98 | tok/s 21489
step    610 | loss 1.4798 | lr 3.00e-04 | grad 1.87 | tok/s 21496
step    620 | loss 1.4705 | lr 3.00e-04 | grad 2.03 | tok/s 21406
step    630 | loss 1.9786 | lr 3.00e-04 | grad 5.50 | tok/s 20315
step    640 | loss 1.9666 | lr 3.00e-04 | grad 2.61 | tok/s 20509
step    650 | loss 1.7613 | lr 3.00e-04 | grad 1.98 | tok/s 20501
step    660 | loss 1.8017 | lr 3.00e-04 | grad 2.55 | tok/s 21243
step    670 | loss 1.8471 | lr 3.00e-04 | grad 6.03 | tok/s 20548
step    680 | loss 1.8598 | lr 3.00e-04 | grad 2.70 | tok/s 20238
step    690 | loss 1.8058 | lr 3.00e-04 | grad 2.69 | tok/s 20091
step    700 | loss 1.6917 | lr 3.00e-04 | grad 2.11 | tok/s 20533
step    710 | loss 1.8800 | lr 3.00e-04 | grad 5.16 | tok/s 20229
step    720 | loss 1.5243 | lr 3.00e-04 | grad 2.19 | tok/s 20997
step    730 | loss 1.6831 | lr 3.00e-04 | grad 1.71 | tok/s 20662
step    740 | loss 2.0910 | lr 3.00e-04 | grad 4.66 | tok/s 21210
step    750 | loss 1.8188 | lr 3.00e-04 | grad 1.95 | tok/s 21451
step    760 | loss 1.7381 | lr 3.00e-04 | grad 4.84 | tok/s 21000
step    770 | loss 1.7674 | lr 3.00e-04 | grad 2.28 | tok/s 20663

Training complete! Final step: 771
