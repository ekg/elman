Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_79/levelMoME88_100m_20260128_175943
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 479,644,160 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 13.4912 | lr 3.00e-04 | grad 31.00 | tok/s 9161
step     20 | loss 3.5611 | lr 3.00e-04 | grad 2.45 | tok/s 18166
step     30 | loss 3.2738 | lr 3.00e-04 | grad 11.69 | tok/s 19174
step     40 | loss 7.4057 | lr 3.00e-04 | grad 19.25 | tok/s 19483
step     50 | loss 5.4602 | lr 3.00e-04 | grad 20.12 | tok/s 19692
step     60 | loss 4.1729 | lr 3.00e-04 | grad 4.75 | tok/s 19687
step     70 | loss 3.4215 | lr 3.00e-04 | grad 6.34 | tok/s 19699
step     80 | loss 3.0180 | lr 3.00e-04 | grad 5.03 | tok/s 19599
step     90 | loss 2.7856 | lr 3.00e-04 | grad 5.00 | tok/s 19518
step    100 | loss 2.5408 | lr 3.00e-04 | grad 7.25 | tok/s 19518
step    110 | loss 2.5452 | lr 3.00e-04 | grad 3.19 | tok/s 19369
step    120 | loss 3.1349 | lr 3.00e-04 | grad 1.80 | tok/s 18408
step    130 | loss 2.3997 | lr 3.00e-04 | grad 7.09 | tok/s 18830
step    140 | loss 2.7655 | lr 3.00e-04 | grad 11.50 | tok/s 18871
step    150 | loss 2.4411 | lr 3.00e-04 | grad 5.94 | tok/s 19395
step    160 | loss 2.6685 | lr 3.00e-04 | grad 2.62 | tok/s 18692
step    170 | loss 2.5169 | lr 3.00e-04 | grad 1.91 | tok/s 18407
step    180 | loss 2.6209 | lr 3.00e-04 | grad 3.02 | tok/s 18856
step    190 | loss 2.2108 | lr 3.00e-04 | grad 2.02 | tok/s 18499
step    200 | loss 2.0333 | lr 3.00e-04 | grad 1.96 | tok/s 19339
step    210 | loss 2.1976 | lr 3.00e-04 | grad 6.09 | tok/s 18353
step    220 | loss 2.5438 | lr 3.00e-04 | grad 8.81 | tok/s 18570
step    230 | loss 2.2719 | lr 3.00e-04 | grad 2.69 | tok/s 18577
step    240 | loss 2.6091 | lr 3.00e-04 | grad 4.47 | tok/s 18760
step    250 | loss 2.0765 | lr 3.00e-04 | grad 1.84 | tok/s 18639
step    260 | loss 2.2126 | lr 3.00e-04 | grad 3.38 | tok/s 19165
step    270 | loss 2.1066 | lr 3.00e-04 | grad 2.22 | tok/s 18757
step    280 | loss 2.0441 | lr 3.00e-04 | grad 2.12 | tok/s 17629
step    290 | loss 1.9715 | lr 3.00e-04 | grad 2.16 | tok/s 18202
step    300 | loss 2.2830 | lr 3.00e-04 | grad 6.16 | tok/s 18344
step    310 | loss 1.9332 | lr 3.00e-04 | grad 1.70 | tok/s 18268
step    320 | loss 2.1956 | lr 3.00e-04 | grad 4.47 | tok/s 18487
step    330 | loss 1.9820 | lr 3.00e-04 | grad 1.91 | tok/s 18679
step    340 | loss 2.3407 | lr 3.00e-04 | grad 4.66 | tok/s 18601
step    350 | loss 2.2141 | lr 3.00e-04 | grad 2.34 | tok/s 19100
step    360 | loss 1.8634 | lr 3.00e-04 | grad 2.22 | tok/s 18280
step    370 | loss 1.8635 | lr 3.00e-04 | grad 2.19 | tok/s 19260
step    380 | loss 1.6158 | lr 3.00e-04 | grad 2.05 | tok/s 19434
step    390 | loss 1.5142 | lr 3.00e-04 | grad 4.62 | tok/s 19431
step    400 | loss 2.1020 | lr 3.00e-04 | grad 2.17 | tok/s 18397
step    410 | loss 2.0377 | lr 3.00e-04 | grad 2.52 | tok/s 18582
step    420 | loss 2.0416 | lr 3.00e-04 | grad 5.28 | tok/s 19347
step    430 | loss 1.9344 | lr 3.00e-04 | grad 2.36 | tok/s 19031
step    440 | loss 2.0111 | lr 3.00e-04 | grad 2.39 | tok/s 18477
step    450 | loss 1.8948 | lr 3.00e-04 | grad 1.77 | tok/s 18666
step    460 | loss 1.9062 | lr 3.00e-04 | grad 2.27 | tok/s 18959
step    470 | loss 1.8908 | lr 3.00e-04 | grad 3.88 | tok/s 18825
step    480 | loss 1.9438 | lr 3.00e-04 | grad 2.86 | tok/s 19225
step    490 | loss 1.9498 | lr 3.00e-04 | grad 3.55 | tok/s 18468
step    500 | loss 2.1002 | lr 3.00e-04 | grad 2.02 | tok/s 18765
step    510 | loss 1.9432 | lr 3.00e-04 | grad 1.88 | tok/s 17947
step    520 | loss 1.7809 | lr 3.00e-04 | grad 2.05 | tok/s 18762
step    530 | loss 1.9666 | lr 3.00e-04 | grad 2.67 | tok/s 18448
step    540 | loss 1.8928 | lr 3.00e-04 | grad 1.72 | tok/s 18083
step    550 | loss 1.6044 | lr 3.00e-04 | grad 3.28 | tok/s 18966
step    560 | loss 1.7103 | lr 3.00e-04 | grad 2.02 | tok/s 19404
step    570 | loss 1.6023 | lr 3.00e-04 | grad 2.08 | tok/s 19420
step    580 | loss 1.5402 | lr 3.00e-04 | grad 1.60 | tok/s 19415
step    590 | loss 1.5846 | lr 3.00e-04 | grad 1.89 | tok/s 19411
step    600 | loss 1.5353 | lr 3.00e-04 | grad 1.86 | tok/s 19412
step    610 | loss 1.5409 | lr 3.00e-04 | grad 1.65 | tok/s 19417
step    620 | loss 1.5265 | lr 3.00e-04 | grad 2.22 | tok/s 19334
step    630 | loss 1.9359 | lr 3.00e-04 | grad 5.34 | tok/s 18354
step    640 | loss 2.0098 | lr 3.00e-04 | grad 2.72 | tok/s 18548
step    650 | loss 1.8055 | lr 3.00e-04 | grad 1.88 | tok/s 18543
step    660 | loss 1.8563 | lr 3.00e-04 | grad 2.33 | tok/s 19225
step    670 | loss 1.8994 | lr 3.00e-04 | grad 5.41 | tok/s 18612
step    680 | loss 1.9113 | lr 3.00e-04 | grad 2.61 | tok/s 18322
step    690 | loss 1.8594 | lr 3.00e-04 | grad 2.23 | tok/s 18170

Training complete! Final step: 697
