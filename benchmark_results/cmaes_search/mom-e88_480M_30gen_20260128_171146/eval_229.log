Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_229/levelMoME88_100m_20260128_194028
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 474,375,072 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 10.6266 | lr 3.00e-04 | grad 52.25 | tok/s 9581
step     20 | loss 3.3224 | lr 3.00e-04 | grad 2.20 | tok/s 19524
step     30 | loss 3.2656 | lr 3.00e-04 | grad 3.41 | tok/s 20640
step     40 | loss 5.7843 | lr 3.00e-04 | grad 17.75 | tok/s 21059
step     50 | loss 5.1266 | lr 3.00e-04 | grad 11.69 | tok/s 21305
step     60 | loss 3.8506 | lr 3.00e-04 | grad 6.06 | tok/s 21237
step     70 | loss 3.2895 | lr 3.00e-04 | grad 7.03 | tok/s 21225
step     80 | loss 3.0109 | lr 3.00e-04 | grad 5.97 | tok/s 21243
step     90 | loss 2.7375 | lr 3.00e-04 | grad 3.67 | tok/s 21218
step    100 | loss 2.4913 | lr 3.00e-04 | grad 3.50 | tok/s 21170
step    110 | loss 2.4955 | lr 3.00e-04 | grad 3.33 | tok/s 20980
step    120 | loss 3.1490 | lr 3.00e-04 | grad 1.71 | tok/s 20011
step    130 | loss 2.3688 | lr 3.00e-04 | grad 5.41 | tok/s 20505
step    140 | loss 2.7141 | lr 3.00e-04 | grad 28.62 | tok/s 20556
step    150 | loss 2.4142 | lr 3.00e-04 | grad 5.72 | tok/s 21068
step    160 | loss 2.6350 | lr 3.00e-04 | grad 3.88 | tok/s 20344
step    170 | loss 2.5080 | lr 3.00e-04 | grad 1.77 | tok/s 20049
step    180 | loss 2.5765 | lr 3.00e-04 | grad 2.61 | tok/s 20530
step    190 | loss 2.1771 | lr 3.00e-04 | grad 2.25 | tok/s 20124
step    200 | loss 2.0132 | lr 3.00e-04 | grad 1.67 | tok/s 21075
step    210 | loss 2.1680 | lr 3.00e-04 | grad 6.28 | tok/s 20005
step    220 | loss 2.5107 | lr 3.00e-04 | grad 18.38 | tok/s 20181
step    230 | loss 2.2705 | lr 3.00e-04 | grad 2.88 | tok/s 20144
step    240 | loss 2.5469 | lr 3.00e-04 | grad 4.78 | tok/s 20414
step    250 | loss 2.0293 | lr 3.00e-04 | grad 1.80 | tok/s 20291
step    260 | loss 2.1679 | lr 3.00e-04 | grad 3.22 | tok/s 20836
step    270 | loss 2.0546 | lr 3.00e-04 | grad 2.02 | tok/s 20383
step    280 | loss 2.0027 | lr 3.00e-04 | grad 1.78 | tok/s 19130
step    290 | loss 1.9153 | lr 3.00e-04 | grad 1.94 | tok/s 19779
step    300 | loss 2.2026 | lr 3.00e-04 | grad 2.17 | tok/s 19984
step    310 | loss 1.8749 | lr 3.00e-04 | grad 1.98 | tok/s 19835
step    320 | loss 2.1191 | lr 3.00e-04 | grad 4.47 | tok/s 20056
step    330 | loss 1.9330 | lr 3.00e-04 | grad 1.84 | tok/s 20322
step    340 | loss 2.2661 | lr 3.00e-04 | grad 2.42 | tok/s 20201
step    350 | loss 2.0887 | lr 3.00e-04 | grad 2.05 | tok/s 20750
step    360 | loss 1.8103 | lr 3.00e-04 | grad 3.09 | tok/s 19899
step    370 | loss 1.8048 | lr 3.00e-04 | grad 1.98 | tok/s 20964
step    380 | loss 1.5540 | lr 3.00e-04 | grad 1.77 | tok/s 21139
step    390 | loss 1.4533 | lr 3.00e-04 | grad 1.73 | tok/s 21108
step    400 | loss 2.0253 | lr 3.00e-04 | grad 1.84 | tok/s 20029
step    410 | loss 1.9867 | lr 3.00e-04 | grad 2.31 | tok/s 20226
step    420 | loss 1.9814 | lr 3.00e-04 | grad 6.44 | tok/s 21050
step    430 | loss 1.8738 | lr 3.00e-04 | grad 1.70 | tok/s 20710
step    440 | loss 1.9470 | lr 3.00e-04 | grad 2.25 | tok/s 20071
step    450 | loss 1.8472 | lr 3.00e-04 | grad 1.70 | tok/s 20309
step    460 | loss 1.8442 | lr 3.00e-04 | grad 2.12 | tok/s 20615
step    470 | loss 1.8304 | lr 3.00e-04 | grad 3.33 | tok/s 20439
step    480 | loss 1.8751 | lr 3.00e-04 | grad 2.72 | tok/s 20875
step    490 | loss 1.8952 | lr 3.00e-04 | grad 2.56 | tok/s 20032
step    500 | loss 2.0505 | lr 3.00e-04 | grad 2.03 | tok/s 20372
step    510 | loss 1.9062 | lr 3.00e-04 | grad 1.80 | tok/s 19495
step    520 | loss 1.7365 | lr 3.00e-04 | grad 2.50 | tok/s 20387
step    530 | loss 1.9286 | lr 3.00e-04 | grad 2.55 | tok/s 20049
step    540 | loss 1.8385 | lr 3.00e-04 | grad 1.55 | tok/s 19633
step    550 | loss 1.5738 | lr 3.00e-04 | grad 3.23 | tok/s 20552
step    560 | loss 1.6426 | lr 3.00e-04 | grad 1.93 | tok/s 21119
step    570 | loss 1.5279 | lr 3.00e-04 | grad 2.47 | tok/s 21084
step    580 | loss 1.4830 | lr 3.00e-04 | grad 1.41 | tok/s 21096
step    590 | loss 1.5246 | lr 3.00e-04 | grad 1.42 | tok/s 21112
step    600 | loss 1.4764 | lr 3.00e-04 | grad 1.82 | tok/s 21106
step    610 | loss 1.4808 | lr 3.00e-04 | grad 1.55 | tok/s 21119
step    620 | loss 1.4724 | lr 3.00e-04 | grad 1.65 | tok/s 21033
step    630 | loss 1.8883 | lr 3.00e-04 | grad 4.59 | tok/s 19933
step    640 | loss 1.9441 | lr 3.00e-04 | grad 2.34 | tok/s 20189
step    650 | loss 1.7614 | lr 3.00e-04 | grad 1.79 | tok/s 20155
step    660 | loss 1.7912 | lr 3.00e-04 | grad 2.03 | tok/s 20908
step    670 | loss 1.8109 | lr 3.00e-04 | grad 5.53 | tok/s 20231
step    680 | loss 1.8475 | lr 3.00e-04 | grad 2.41 | tok/s 19919
step    690 | loss 1.8063 | lr 3.00e-04 | grad 2.67 | tok/s 19749
step    700 | loss 1.6985 | lr 3.00e-04 | grad 2.02 | tok/s 20175
step    710 | loss 1.8689 | lr 3.00e-04 | grad 4.44 | tok/s 19853
step    720 | loss 1.5316 | lr 3.00e-04 | grad 2.38 | tok/s 20637
step    730 | loss 1.6714 | lr 3.00e-04 | grad 1.53 | tok/s 20327
step    740 | loss 2.0778 | lr 3.00e-04 | grad 3.70 | tok/s 20876
step    750 | loss 1.8232 | lr 3.00e-04 | grad 1.72 | tok/s 21089

Training complete! Final step: 757
