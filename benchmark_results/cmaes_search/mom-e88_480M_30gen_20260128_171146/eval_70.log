Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_70/levelMoME88_100m_20260128_175425
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 477,540,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 7.8787 | lr 3.00e-04 | grad 28.75 | tok/s 8341
step     20 | loss 3.3704 | lr 3.00e-04 | grad 2.75 | tok/s 14330
step     30 | loss 3.3526 | lr 3.00e-04 | grad 3.52 | tok/s 15158
step     40 | loss 5.8484 | lr 3.00e-04 | grad 27.12 | tok/s 15469
step     50 | loss 4.8405 | lr 3.00e-04 | grad 6.84 | tok/s 15650
step     60 | loss 4.0084 | lr 3.00e-04 | grad 2.61 | tok/s 15616
step     70 | loss 3.3863 | lr 3.00e-04 | grad 5.44 | tok/s 15601
step     80 | loss 3.1569 | lr 3.00e-04 | grad 2.69 | tok/s 15588
step     90 | loss 2.8922 | lr 3.00e-04 | grad 2.91 | tok/s 15582
step    100 | loss 2.6321 | lr 3.00e-04 | grad 2.42 | tok/s 15581
step    110 | loss 2.5624 | lr 3.00e-04 | grad 1.90 | tok/s 15463
step    120 | loss 3.0552 | lr 3.00e-04 | grad 1.23 | tok/s 14712
step    130 | loss 2.4143 | lr 3.00e-04 | grad 8.56 | tok/s 15068
step    140 | loss 2.7273 | lr 3.00e-04 | grad 5.09 | tok/s 15116
step    150 | loss 2.5251 | lr 3.00e-04 | grad 3.98 | tok/s 15481
step    160 | loss 2.6664 | lr 3.00e-04 | grad 1.89 | tok/s 14969
step    170 | loss 2.5202 | lr 3.00e-04 | grad 1.24 | tok/s 14733
step    180 | loss 2.6876 | lr 3.00e-04 | grad 1.67 | tok/s 15073
step    190 | loss 2.2395 | lr 3.00e-04 | grad 1.26 | tok/s 14799
step    200 | loss 2.0732 | lr 3.00e-04 | grad 1.38 | tok/s 15472
step    210 | loss 2.2158 | lr 3.00e-04 | grad 3.83 | tok/s 14684
step    220 | loss 2.5344 | lr 3.00e-04 | grad 2.94 | tok/s 14840
step    230 | loss 2.3112 | lr 3.00e-04 | grad 2.09 | tok/s 14824
step    240 | loss 2.6410 | lr 3.00e-04 | grad 3.31 | tok/s 15017
step    250 | loss 2.0741 | lr 3.00e-04 | grad 1.36 | tok/s 14927
step    260 | loss 2.2109 | lr 3.00e-04 | grad 2.73 | tok/s 15332
step    270 | loss 2.1040 | lr 3.00e-04 | grad 1.38 | tok/s 14984
step    280 | loss 2.0425 | lr 3.00e-04 | grad 1.20 | tok/s 14078
step    290 | loss 1.9523 | lr 3.00e-04 | grad 1.42 | tok/s 14575
step    300 | loss 2.2590 | lr 3.00e-04 | grad 2.03 | tok/s 14666
step    310 | loss 1.9018 | lr 3.00e-04 | grad 1.19 | tok/s 14595
step    320 | loss 2.1608 | lr 3.00e-04 | grad 3.33 | tok/s 14781
step    330 | loss 1.9661 | lr 3.00e-04 | grad 1.26 | tok/s 14939
step    340 | loss 2.3336 | lr 3.00e-04 | grad 1.73 | tok/s 14880
step    350 | loss 2.2244 | lr 3.00e-04 | grad 1.38 | tok/s 15303
step    360 | loss 1.8550 | lr 3.00e-04 | grad 1.62 | tok/s 14644
step    370 | loss 1.8571 | lr 3.00e-04 | grad 1.91 | tok/s 15440
step    380 | loss 1.6403 | lr 3.00e-04 | grad 1.40 | tok/s 15555
step    390 | loss 1.5278 | lr 3.00e-04 | grad 1.41 | tok/s 15553
step    400 | loss 2.0895 | lr 3.00e-04 | grad 1.60 | tok/s 14734
step    410 | loss 1.9919 | lr 3.00e-04 | grad 1.57 | tok/s 14860
step    420 | loss 2.0644 | lr 3.00e-04 | grad 2.41 | tok/s 15501
step    430 | loss 1.9749 | lr 3.00e-04 | grad 1.35 | tok/s 15232
step    440 | loss 1.9748 | lr 3.00e-04 | grad 1.66 | tok/s 14762
step    450 | loss 1.8577 | lr 3.00e-04 | grad 1.20 | tok/s 14937
step    460 | loss 1.8869 | lr 3.00e-04 | grad 1.48 | tok/s 15161
step    470 | loss 1.8782 | lr 3.00e-04 | grad 2.80 | tok/s 15065
step    480 | loss 1.9546 | lr 3.00e-04 | grad 1.92 | tok/s 15382
step    490 | loss 1.9136 | lr 3.00e-04 | grad 1.94 | tok/s 14777
step    500 | loss 2.0815 | lr 3.00e-04 | grad 1.45 | tok/s 15027
step    510 | loss 1.9280 | lr 3.00e-04 | grad 1.26 | tok/s 14356
step    520 | loss 1.7564 | lr 3.00e-04 | grad 1.52 | tok/s 15022
step    530 | loss 1.9525 | lr 3.00e-04 | grad 1.70 | tok/s 14781
step    540 | loss 1.8688 | lr 3.00e-04 | grad 1.22 | tok/s 14460
step    550 | loss 1.5790 | lr 3.00e-04 | grad 2.27 | tok/s 15116

Training complete! Final step: 559
