Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_21/levelMoME88_100m_20260128_172231
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 482,862,954 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 11.6436 | lr 3.00e-04 | grad 8.81 | tok/s 4990
step     20 | loss 2.7593 | lr 3.00e-04 | grad 2.59 | tok/s 10474
step     30 | loss 2.6860 | lr 3.00e-04 | grad 1.70 | tok/s 10593
step     40 | loss 3.3089 | lr 3.00e-04 | grad 3.47 | tok/s 10185
step     50 | loss 3.3605 | lr 3.00e-04 | grad 5.50 | tok/s 10320
step     60 | loss 2.2683 | lr 3.00e-04 | grad 3.11 | tok/s 10655
step     70 | loss 2.1750 | lr 3.00e-04 | grad 4.03 | tok/s 10789
step     80 | loss 9.9342 | lr 3.00e-04 | grad 15.69 | tok/s 10856
step     90 | loss 6.1211 | lr 3.00e-04 | grad 3.45 | tok/s 11058
step    100 | loss 4.9239 | lr 3.00e-04 | grad 4.22 | tok/s 11010
step    110 | loss 4.2377 | lr 3.00e-04 | grad 36.50 | tok/s 11007
step    120 | loss 3.7364 | lr 3.00e-04 | grad 109.50 | tok/s 11053
step    130 | loss 3.3988 | lr 3.00e-04 | grad 5.69 | tok/s 11016
step    140 | loss 2.8845 | lr 3.00e-04 | grad 3.61 | tok/s 11012
step    150 | loss 3.0328 | lr 3.00e-04 | grad 5.34 | tok/s 11022
step    160 | loss 2.5039 | lr 3.00e-04 | grad 9.38 | tok/s 11043
step    170 | loss 2.5292 | lr 3.00e-04 | grad 5.09 | tok/s 11019
step    180 | loss 2.3041 | lr 3.00e-04 | grad 3.53 | tok/s 11023
step    190 | loss 2.4510 | lr 3.00e-04 | grad 2.78 | tok/s 11036
step    200 | loss 2.1884 | lr 3.00e-04 | grad 3.69 | tok/s 11027
step    210 | loss 2.1892 | lr 3.00e-04 | grad 3.50 | tok/s 11065
step    220 | loss 2.4386 | lr 3.00e-04 | grad 2.38 | tok/s 10889
step    230 | loss 3.1268 | lr 3.00e-04 | grad 6.62 | tok/s 10803
step    240 | loss 2.4830 | lr 3.00e-04 | grad 3.36 | tok/s 10202
step    250 | loss 2.3124 | lr 3.00e-04 | grad 1.82 | tok/s 10522
step    260 | loss 1.9742 | lr 3.00e-04 | grad 6.78 | tok/s 10847
step    270 | loss 2.4069 | lr 3.00e-04 | grad 1.72 | tok/s 10707
step    280 | loss 2.5491 | lr 3.00e-04 | grad 3.80 | tok/s 10515
step    290 | loss 2.5822 | lr 3.00e-04 | grad 3.00 | tok/s 11073
step    300 | loss 1.3933 | lr 3.00e-04 | grad 1.98 | tok/s 11078
step    310 | loss 2.8484 | lr 3.00e-04 | grad 2.62 | tok/s 10857
step    320 | loss 2.4281 | lr 3.00e-04 | grad 5.38 | tok/s 10626
step    330 | loss 2.2031 | lr 3.00e-04 | grad 2.19 | tok/s 10262
step    340 | loss 2.5640 | lr 3.00e-04 | grad 1.84 | tok/s 10420
step    350 | loss 2.3177 | lr 3.00e-04 | grad 8.38 | tok/s 10684
step    360 | loss 2.5156 | lr 3.00e-04 | grad 5.28 | tok/s 10920
step    370 | loss 2.1375 | lr 3.00e-04 | grad 1.91 | tok/s 9907
step    380 | loss 2.0460 | lr 3.00e-04 | grad 3.36 | tok/s 10564
step    390 | loss 1.8420 | lr 3.00e-04 | grad 1.56 | tok/s 11033
step    400 | loss 1.8359 | lr 3.00e-04 | grad 2.09 | tok/s 10932
step    410 | loss 1.7382 | lr 3.00e-04 | grad 1.44 | tok/s 10684
step    420 | loss 2.0813 | lr 3.00e-04 | grad 3.62 | tok/s 10208
step    430 | loss 2.4661 | lr 3.00e-04 | grad 2.33 | tok/s 10868
step    440 | loss 2.3946 | lr 3.00e-04 | grad 2.91 | tok/s 10265
step    450 | loss 2.1656 | lr 3.00e-04 | grad 2.02 | tok/s 10630
step    460 | loss 2.0820 | lr 3.00e-04 | grad 3.36 | tok/s 10398
step    470 | loss 2.1398 | lr 3.00e-04 | grad 1.84 | tok/s 10721
step    480 | loss 2.6570 | lr 3.00e-04 | grad 5.09 | tok/s 10735
step    490 | loss 2.0766 | lr 3.00e-04 | grad 2.06 | tok/s 10137
step    500 | loss 2.0019 | lr 3.00e-04 | grad 2.28 | tok/s 10824
step    510 | loss 1.9905 | lr 3.00e-04 | grad 2.03 | tok/s 10964
step    520 | loss 1.9947 | lr 3.00e-04 | grad 2.38 | tok/s 10950
step    530 | loss 2.2125 | lr 3.00e-04 | grad 1.82 | tok/s 10525
step    540 | loss 1.9479 | lr 3.00e-04 | grad 1.77 | tok/s 10533
step    550 | loss 1.7854 | lr 3.00e-04 | grad 2.22 | tok/s 10302
step    560 | loss 1.9738 | lr 3.00e-04 | grad 1.88 | tok/s 10048
step    570 | loss 1.9404 | lr 3.00e-04 | grad 2.89 | tok/s 10314
step    580 | loss 1.7983 | lr 3.00e-04 | grad 1.90 | tok/s 10275
step    590 | loss 2.2179 | lr 3.00e-04 | grad 2.52 | tok/s 10532
step    600 | loss 2.0532 | lr 3.00e-04 | grad 1.99 | tok/s 10174
step    610 | loss 1.8803 | lr 3.00e-04 | grad 1.60 | tok/s 10707
step    620 | loss 1.7508 | lr 3.00e-04 | grad 1.57 | tok/s 10142
step    630 | loss 1.9272 | lr 3.00e-04 | grad 3.12 | tok/s 10226
step    640 | loss 2.0841 | lr 3.00e-04 | grad 1.84 | tok/s 10508
step    650 | loss 1.9136 | lr 3.00e-04 | grad 1.76 | tok/s 10555
step    660 | loss 1.9370 | lr 3.00e-04 | grad 1.96 | tok/s 10602
step    670 | loss 2.2138 | lr 3.00e-04 | grad 4.41 | tok/s 10670
step    680 | loss 1.9396 | lr 3.00e-04 | grad 1.80 | tok/s 10468
step    690 | loss 2.2199 | lr 3.00e-04 | grad 2.72 | tok/s 10818
step    700 | loss 1.9703 | lr 3.00e-04 | grad 2.92 | tok/s 11025
step    710 | loss 1.8497 | lr 3.00e-04 | grad 1.64 | tok/s 10312
step    720 | loss 1.6951 | lr 3.00e-04 | grad 2.17 | tok/s 10157
step    730 | loss 1.6897 | lr 3.00e-04 | grad 2.12 | tok/s 11006
step    740 | loss 1.8025 | lr 3.00e-04 | grad 2.22 | tok/s 10861
step    750 | loss 1.5802 | lr 3.00e-04 | grad 2.05 | tok/s 11035
step    760 | loss 1.4345 | lr 3.00e-04 | grad 2.09 | tok/s 11032
step    770 | loss 1.3871 | lr 3.00e-04 | grad 1.52 | tok/s 11024
step    780 | loss 1.3507 | lr 3.00e-04 | grad 1.80 | tok/s 11023

Training complete! Final step: 789
