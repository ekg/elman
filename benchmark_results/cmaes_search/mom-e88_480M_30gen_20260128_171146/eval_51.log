Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_51/levelMoME88_100m_20260128_174347
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 478,757,744 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 10.1612 | lr 3.00e-04 | grad 4.28 | tok/s 9449
step     20 | loss 3.3135 | lr 3.00e-04 | grad 1.85 | tok/s 18663
step     30 | loss 3.2051 | lr 3.00e-04 | grad 5.81 | tok/s 19717
step     40 | loss 6.5177 | lr 3.00e-04 | grad 24.75 | tok/s 20075
step     50 | loss 4.9701 | lr 3.00e-04 | grad 7.97 | tok/s 20293
step     60 | loss 3.8960 | lr 3.00e-04 | grad 3.19 | tok/s 20230
step     70 | loss 3.2470 | lr 3.00e-04 | grad 6.12 | tok/s 20180
step     80 | loss 2.9106 | lr 3.00e-04 | grad 3.25 | tok/s 20179
step     90 | loss 2.6599 | lr 3.00e-04 | grad 2.77 | tok/s 20111
step    100 | loss 2.4089 | lr 3.00e-04 | grad 3.14 | tok/s 20088
step    110 | loss 2.4553 | lr 3.00e-04 | grad 2.34 | tok/s 19923
step    120 | loss 3.1968 | lr 3.00e-04 | grad 1.40 | tok/s 18973
step    130 | loss 2.3752 | lr 3.00e-04 | grad 3.91 | tok/s 19432
step    140 | loss 2.6884 | lr 3.00e-04 | grad 9.31 | tok/s 19480
step    150 | loss 2.4173 | lr 3.00e-04 | grad 4.84 | tok/s 19997
step    160 | loss 2.7351 | lr 3.00e-04 | grad 2.03 | tok/s 19255
step    170 | loss 2.4909 | lr 3.00e-04 | grad 1.47 | tok/s 18973
step    180 | loss 2.6543 | lr 3.00e-04 | grad 2.34 | tok/s 19417
step    190 | loss 2.1728 | lr 3.00e-04 | grad 1.75 | tok/s 19057
step    200 | loss 1.9915 | lr 3.00e-04 | grad 1.60 | tok/s 19926
step    210 | loss 2.1491 | lr 3.00e-04 | grad 3.94 | tok/s 18898
step    220 | loss 2.4715 | lr 3.00e-04 | grad 3.22 | tok/s 19096
step    230 | loss 2.1779 | lr 3.00e-04 | grad 2.23 | tok/s 19090
step    240 | loss 2.5649 | lr 3.00e-04 | grad 3.52 | tok/s 19320
step    250 | loss 2.0317 | lr 3.00e-04 | grad 1.70 | tok/s 19221
step    260 | loss 2.1727 | lr 3.00e-04 | grad 2.89 | tok/s 19755
step    270 | loss 2.0631 | lr 3.00e-04 | grad 1.98 | tok/s 19303
step    280 | loss 2.0001 | lr 3.00e-04 | grad 1.46 | tok/s 18139
step    290 | loss 1.9203 | lr 3.00e-04 | grad 1.90 | tok/s 18743
step    300 | loss 2.2100 | lr 3.00e-04 | grad 2.16 | tok/s 18913
step    310 | loss 1.8725 | lr 3.00e-04 | grad 1.92 | tok/s 18841
step    320 | loss 2.1235 | lr 3.00e-04 | grad 3.61 | tok/s 19047
step    330 | loss 1.9376 | lr 3.00e-04 | grad 1.49 | tok/s 19230
step    340 | loss 2.2931 | lr 3.00e-04 | grad 2.31 | tok/s 19156
step    350 | loss 2.1655 | lr 3.00e-04 | grad 1.69 | tok/s 19680
step    360 | loss 1.8173 | lr 3.00e-04 | grad 2.64 | tok/s 18845
step    370 | loss 1.8149 | lr 3.00e-04 | grad 1.80 | tok/s 19846
step    380 | loss 1.5714 | lr 3.00e-04 | grad 1.91 | tok/s 20019
step    390 | loss 1.4656 | lr 3.00e-04 | grad 1.85 | tok/s 20021
step    400 | loss 2.0317 | lr 3.00e-04 | grad 1.83 | tok/s 18991
step    410 | loss 1.9795 | lr 3.00e-04 | grad 1.87 | tok/s 19166
step    420 | loss 1.9984 | lr 3.00e-04 | grad 4.03 | tok/s 19972
step    430 | loss 1.8986 | lr 3.00e-04 | grad 1.77 | tok/s 19629
step    440 | loss 1.9418 | lr 3.00e-04 | grad 2.30 | tok/s 19059
step    450 | loss 1.8394 | lr 3.00e-04 | grad 1.56 | tok/s 19268
step    460 | loss 1.8449 | lr 3.00e-04 | grad 1.95 | tok/s 19570
step    470 | loss 1.8411 | lr 3.00e-04 | grad 3.45 | tok/s 19417
step    480 | loss 1.8926 | lr 3.00e-04 | grad 2.81 | tok/s 19825
step    490 | loss 1.9045 | lr 3.00e-04 | grad 2.42 | tok/s 19035
step    500 | loss 2.0831 | lr 3.00e-04 | grad 1.81 | tok/s 19359
step    510 | loss 1.9157 | lr 3.00e-04 | grad 1.60 | tok/s 18501
step    520 | loss 1.7440 | lr 3.00e-04 | grad 1.84 | tok/s 19373
step    530 | loss 1.9193 | lr 3.00e-04 | grad 1.94 | tok/s 19076
step    540 | loss 1.8373 | lr 3.00e-04 | grad 1.59 | tok/s 18652
step    550 | loss 1.6371 | lr 3.00e-04 | grad 2.81 | tok/s 19491
step    560 | loss 1.6535 | lr 3.00e-04 | grad 2.75 | tok/s 20034
step    570 | loss 1.5394 | lr 3.00e-04 | grad 1.77 | tok/s 20052
step    580 | loss 1.4970 | lr 3.00e-04 | grad 1.26 | tok/s 20022
step    590 | loss 1.5324 | lr 3.00e-04 | grad 1.52 | tok/s 20074
step    600 | loss 1.4881 | lr 3.00e-04 | grad 1.65 | tok/s 20050
step    610 | loss 1.4862 | lr 3.00e-04 | grad 1.31 | tok/s 20050
step    620 | loss 1.4783 | lr 3.00e-04 | grad 1.59 | tok/s 19934
step    630 | loss 1.8924 | lr 3.00e-04 | grad 3.86 | tok/s 18900
step    640 | loss 1.9616 | lr 3.00e-04 | grad 2.55 | tok/s 19127
step    650 | loss 1.7554 | lr 3.00e-04 | grad 1.64 | tok/s 19139
step    660 | loss 1.7988 | lr 3.00e-04 | grad 1.80 | tok/s 19841
step    670 | loss 1.8470 | lr 3.00e-04 | grad 5.38 | tok/s 19189
step    680 | loss 1.8484 | lr 3.00e-04 | grad 2.31 | tok/s 18899
step    690 | loss 1.8278 | lr 3.00e-04 | grad 1.92 | tok/s 18747
step    700 | loss 1.6916 | lr 3.00e-04 | grad 1.91 | tok/s 19139
step    710 | loss 1.8654 | lr 3.00e-04 | grad 3.66 | tok/s 18853

Training complete! Final step: 719
