Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_119/levelMoME88_100m_20260128_182613
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 471,428,166 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 9.3331 | lr 3.00e-04 | grad 13.12 | tok/s 8104
step     20 | loss 3.3153 | lr 3.00e-04 | grad 2.50 | tok/s 13957
step     30 | loss 3.1678 | lr 3.00e-04 | grad 4.09 | tok/s 14743
step     40 | loss 6.0112 | lr 3.00e-04 | grad 12.12 | tok/s 15006
step     50 | loss 5.1975 | lr 3.00e-04 | grad 7.44 | tok/s 15189
step     60 | loss 4.1497 | lr 3.00e-04 | grad 3.94 | tok/s 15160
step     70 | loss 3.3326 | lr 3.00e-04 | grad 5.88 | tok/s 15137
step     80 | loss 3.0420 | lr 3.00e-04 | grad 2.62 | tok/s 15126
step     90 | loss 2.7550 | lr 3.00e-04 | grad 2.97 | tok/s 15106
step    100 | loss 2.4853 | lr 3.00e-04 | grad 2.69 | tok/s 15103
step    110 | loss 2.5220 | lr 3.00e-04 | grad 6.16 | tok/s 15012
step    120 | loss 3.2041 | lr 3.00e-04 | grad 1.84 | tok/s 14275
step    130 | loss 2.3968 | lr 3.00e-04 | grad 4.53 | tok/s 14621
step    140 | loss 2.7852 | lr 3.00e-04 | grad 16.62 | tok/s 14650
step    150 | loss 2.4741 | lr 3.00e-04 | grad 5.09 | tok/s 15005
step    160 | loss 2.6808 | lr 3.00e-04 | grad 2.09 | tok/s 14507
step    170 | loss 2.5093 | lr 3.00e-04 | grad 1.31 | tok/s 14288
step    180 | loss 2.7039 | lr 3.00e-04 | grad 2.66 | tok/s 14660
step    190 | loss 2.2123 | lr 3.00e-04 | grad 6.97 | tok/s 14372
step    200 | loss 2.0515 | lr 3.00e-04 | grad 1.60 | tok/s 15013
step    210 | loss 2.1976 | lr 3.00e-04 | grad 5.34 | tok/s 14276
step    220 | loss 2.5337 | lr 3.00e-04 | grad 3.69 | tok/s 14390
step    230 | loss 2.2277 | lr 3.00e-04 | grad 2.30 | tok/s 14402
step    240 | loss 2.6065 | lr 3.00e-04 | grad 3.81 | tok/s 14562
step    250 | loss 2.0557 | lr 3.00e-04 | grad 1.50 | tok/s 14473
step    260 | loss 2.1982 | lr 3.00e-04 | grad 2.69 | tok/s 14862
step    270 | loss 2.0845 | lr 3.00e-04 | grad 1.57 | tok/s 14520
step    280 | loss 2.0200 | lr 3.00e-04 | grad 1.55 | tok/s 13653
step    290 | loss 1.9389 | lr 3.00e-04 | grad 1.67 | tok/s 14147
step    300 | loss 2.2405 | lr 3.00e-04 | grad 1.73 | tok/s 14233
step    310 | loss 1.8964 | lr 3.00e-04 | grad 1.49 | tok/s 14192
step    320 | loss 2.1459 | lr 3.00e-04 | grad 3.56 | tok/s 14344
step    330 | loss 1.9583 | lr 3.00e-04 | grad 1.61 | tok/s 14470
step    340 | loss 2.3301 | lr 3.00e-04 | grad 2.47 | tok/s 14414
step    350 | loss 2.1914 | lr 3.00e-04 | grad 1.83 | tok/s 14846
step    360 | loss 1.8447 | lr 3.00e-04 | grad 2.11 | tok/s 14189
step    370 | loss 1.8393 | lr 3.00e-04 | grad 1.78 | tok/s 14960
step    380 | loss 1.5962 | lr 3.00e-04 | grad 2.20 | tok/s 15079
step    390 | loss 1.4874 | lr 3.00e-04 | grad 2.41 | tok/s 15063
step    400 | loss 2.0703 | lr 3.00e-04 | grad 1.74 | tok/s 14285
step    410 | loss 2.0119 | lr 3.00e-04 | grad 2.02 | tok/s 14434
step    420 | loss 2.0303 | lr 3.00e-04 | grad 4.66 | tok/s 15044
step    430 | loss 1.9582 | lr 3.00e-04 | grad 1.67 | tok/s 14797
step    440 | loss 1.9773 | lr 3.00e-04 | grad 1.97 | tok/s 14336
step    450 | loss 1.8652 | lr 3.00e-04 | grad 1.59 | tok/s 14497
step    460 | loss 1.8876 | lr 3.00e-04 | grad 1.90 | tok/s 14704
step    470 | loss 1.8686 | lr 3.00e-04 | grad 4.91 | tok/s 14577
step    480 | loss 1.9357 | lr 3.00e-04 | grad 2.39 | tok/s 14911
step    490 | loss 1.9220 | lr 3.00e-04 | grad 2.25 | tok/s 14317
step    500 | loss 2.0872 | lr 3.00e-04 | grad 1.60 | tok/s 14562
step    510 | loss 1.9199 | lr 3.00e-04 | grad 1.64 | tok/s 13899
step    520 | loss 1.7594 | lr 3.00e-04 | grad 1.88 | tok/s 14572
step    530 | loss 1.9452 | lr 3.00e-04 | grad 1.95 | tok/s 14332
step    540 | loss 1.8692 | lr 3.00e-04 | grad 1.49 | tok/s 14030

Training complete! Final step: 542
