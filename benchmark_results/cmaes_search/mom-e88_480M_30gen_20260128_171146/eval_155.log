Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_155/levelMoME88_100m_20260128_185240
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 483,111,170 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 16.1288 | lr 3.00e-04 | grad 24.12 | tok/s 5848
step     20 | loss 2.7135 | lr 3.00e-04 | grad 4.41 | tok/s 17847
step     30 | loss 2.6391 | lr 3.00e-04 | grad 4.97 | tok/s 18022
step     40 | loss 2.5998 | lr 3.00e-04 | grad 3.84 | tok/s 17187
step     50 | loss 2.9261 | lr 3.00e-04 | grad 12.44 | tok/s 17475
step     60 | loss 2.3332 | lr 3.00e-04 | grad 26.25 | tok/s 18041
step     70 | loss 2.2620 | lr 3.00e-04 | grad 5.22 | tok/s 18252
step     80 | loss 6.7933 | lr 3.00e-04 | grad 98.50 | tok/s 18321
step     90 | loss 5.3636 | lr 3.00e-04 | grad 6.31 | tok/s 18649
step    100 | loss 4.1117 | lr 3.00e-04 | grad 10.00 | tok/s 18625
step    110 | loss 3.9393 | lr 3.00e-04 | grad 24.62 | tok/s 18555
step    120 | loss 3.7737 | lr 3.00e-04 | grad 30.75 | tok/s 18584
step    130 | loss 3.6089 | lr 3.00e-04 | grad 11.62 | tok/s 18623
step    140 | loss 3.1853 | lr 3.00e-04 | grad 10.25 | tok/s 18580
step    150 | loss 3.3177 | lr 3.00e-04 | grad 17.50 | tok/s 18548
step    160 | loss 2.7621 | lr 3.00e-04 | grad 13.38 | tok/s 18529
step    170 | loss 2.7824 | lr 3.00e-04 | grad 16.25 | tok/s 18582
step    180 | loss 2.5221 | lr 3.00e-04 | grad 11.44 | tok/s 18578
step    190 | loss 2.7279 | lr 3.00e-04 | grad 27.00 | tok/s 18560
step    200 | loss 2.3695 | lr 3.00e-04 | grad 6.81 | tok/s 18537
step    210 | loss 2.3483 | lr 3.00e-04 | grad 9.25 | tok/s 18517
step    220 | loss 2.6947 | lr 3.00e-04 | grad 4.91 | tok/s 18310
step    230 | loss 2.8981 | lr 3.00e-04 | grad 7.62 | tok/s 18127
step    240 | loss 2.5866 | lr 3.00e-04 | grad 5.62 | tok/s 17193
step    250 | loss 2.3730 | lr 3.00e-04 | grad 2.97 | tok/s 17661
step    260 | loss 1.9917 | lr 3.00e-04 | grad 4.00 | tok/s 18175
step    270 | loss 2.4043 | lr 3.00e-04 | grad 3.59 | tok/s 18012
step    280 | loss 2.5591 | lr 3.00e-04 | grad 8.19 | tok/s 17627
step    290 | loss 2.1384 | lr 3.00e-04 | grad 4.06 | tok/s 18611
step    300 | loss 0.9493 | lr 3.00e-04 | grad 12.69 | tok/s 18608
step    310 | loss 2.7825 | lr 3.00e-04 | grad 4.97 | tok/s 18226
step    320 | loss 2.3424 | lr 3.00e-04 | grad 6.62 | tok/s 17844
step    330 | loss 2.2614 | lr 3.00e-04 | grad 3.61 | tok/s 17239
step    340 | loss 2.5906 | lr 3.00e-04 | grad 3.23 | tok/s 17529
step    350 | loss 2.2602 | lr 3.00e-04 | grad 3.97 | tok/s 17950
step    360 | loss 1.9260 | lr 3.00e-04 | grad 7.41 | tok/s 18321
step    370 | loss 2.1696 | lr 3.00e-04 | grad 3.38 | tok/s 16654
step    380 | loss 2.0739 | lr 3.00e-04 | grad 3.89 | tok/s 17682
step    390 | loss 1.8165 | lr 3.00e-04 | grad 3.12 | tok/s 18480
step    400 | loss 1.8441 | lr 3.00e-04 | grad 3.61 | tok/s 18314
step    410 | loss 1.6843 | lr 3.00e-04 | grad 2.45 | tok/s 17956
step    420 | loss 2.1132 | lr 3.00e-04 | grad 6.06 | tok/s 17143
step    430 | loss 2.4709 | lr 3.00e-04 | grad 4.47 | tok/s 18268
step    440 | loss 2.4441 | lr 3.00e-04 | grad 4.50 | tok/s 17270
step    450 | loss 2.4945 | lr 3.00e-04 | grad 3.16 | tok/s 17850
step    460 | loss 2.0442 | lr 3.00e-04 | grad 4.59 | tok/s 17479
step    470 | loss 2.1551 | lr 3.00e-04 | grad 3.61 | tok/s 17995
step    480 | loss 2.6114 | lr 3.00e-04 | grad 7.66 | tok/s 18030
step    490 | loss 2.0916 | lr 3.00e-04 | grad 3.12 | tok/s 17041
step    500 | loss 2.0116 | lr 3.00e-04 | grad 4.78 | tok/s 18187
step    510 | loss 1.9983 | lr 3.00e-04 | grad 3.70 | tok/s 18405
step    520 | loss 1.9793 | lr 3.00e-04 | grad 2.81 | tok/s 18392
step    530 | loss 2.2203 | lr 3.00e-04 | grad 2.94 | tok/s 17724
step    540 | loss 1.9808 | lr 3.00e-04 | grad 3.12 | tok/s 17707
step    550 | loss 1.8075 | lr 3.00e-04 | grad 4.06 | tok/s 17336
step    560 | loss 2.0165 | lr 3.00e-04 | grad 3.20 | tok/s 16883
step    570 | loss 1.9756 | lr 3.00e-04 | grad 4.69 | tok/s 17352
step    580 | loss 1.8153 | lr 3.00e-04 | grad 3.64 | tok/s 17241
step    590 | loss 2.1690 | lr 3.00e-04 | grad 3.75 | tok/s 17696
step    600 | loss 2.0660 | lr 3.00e-04 | grad 3.05 | tok/s 17091
step    610 | loss 1.9011 | lr 3.00e-04 | grad 3.05 | tok/s 17977
step    620 | loss 1.7681 | lr 3.00e-04 | grad 3.16 | tok/s 17028
step    630 | loss 1.9140 | lr 3.00e-04 | grad 5.25 | tok/s 17205
step    640 | loss 2.1049 | lr 3.00e-04 | grad 5.22 | tok/s 17621
step    650 | loss 1.9366 | lr 3.00e-04 | grad 3.67 | tok/s 17696
step    660 | loss 1.9639 | lr 3.00e-04 | grad 4.22 | tok/s 17775
step    670 | loss 2.1909 | lr 3.00e-04 | grad 4.00 | tok/s 17904
step    680 | loss 1.9736 | lr 3.00e-04 | grad 2.98 | tok/s 17539
step    690 | loss 2.1750 | lr 3.00e-04 | grad 4.53 | tok/s 18120
step    700 | loss 1.8057 | lr 3.00e-04 | grad 3.86 | tok/s 18509
step    710 | loss 1.8682 | lr 3.00e-04 | grad 2.75 | tok/s 17280
step    720 | loss 1.7086 | lr 3.00e-04 | grad 3.92 | tok/s 17060
step    730 | loss 1.6144 | lr 3.00e-04 | grad 3.91 | tok/s 18474
step    740 | loss 1.8015 | lr 3.00e-04 | grad 3.06 | tok/s 18263
step    750 | loss 1.5236 | lr 3.00e-04 | grad 2.86 | tok/s 18557
step    760 | loss 1.3960 | lr 3.00e-04 | grad 2.70 | tok/s 18562
step    770 | loss 1.3408 | lr 3.00e-04 | grad 2.42 | tok/s 18556
step    780 | loss 1.2844 | lr 3.00e-04 | grad 2.14 | tok/s 18557
step    790 | loss 1.3968 | lr 3.00e-04 | grad 4.59 | tok/s 17965
step    800 | loss 2.1749 | lr 3.00e-04 | grad 5.94 | tok/s 17899
step    810 | loss 1.9356 | lr 3.00e-04 | grad 3.33 | tok/s 17779
step    820 | loss 1.9731 | lr 3.00e-04 | grad 5.19 | tok/s 17076
step    830 | loss 1.9086 | lr 3.00e-04 | grad 4.19 | tok/s 18354
step    840 | loss 1.7145 | lr 3.00e-04 | grad 2.97 | tok/s 18527
step    850 | loss 1.9021 | lr 3.00e-04 | grad 3.02 | tok/s 18449
step    860 | loss 1.7704 | lr 3.00e-04 | grad 5.25 | tok/s 18220
step    870 | loss 1.7660 | lr 3.00e-04 | grad 3.72 | tok/s 17568
step    880 | loss 1.9751 | lr 3.00e-04 | grad 3.70 | tok/s 17663
step    890 | loss 1.9345 | lr 3.00e-04 | grad 3.97 | tok/s 17924
step    900 | loss 1.8070 | lr 3.00e-04 | grad 3.27 | tok/s 17919
step    910 | loss 1.6328 | lr 3.00e-04 | grad 4.34 | tok/s 17551
step    920 | loss 1.7946 | lr 3.00e-04 | grad 4.81 | tok/s 18272
step    930 | loss 1.8359 | lr 3.00e-04 | grad 4.31 | tok/s 17415
step    940 | loss 1.6939 | lr 3.00e-04 | grad 2.58 | tok/s 18367
step    950 | loss 1.8755 | lr 3.00e-04 | grad 3.72 | tok/s 18434
step    960 | loss 1.6970 | lr 3.00e-04 | grad 3.64 | tok/s 18471
step    970 | loss 1.9745 | lr 3.00e-04 | grad 3.92 | tok/s 17367
step    980 | loss 1.8704 | lr 3.00e-04 | grad 3.16 | tok/s 17792
step    990 | loss 1.7173 | lr 3.00e-04 | grad 2.58 | tok/s 18113
step   1000 | loss 2.1354 | lr 3.00e-04 | grad 18.00 | tok/s 17406
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1354.pt
step   1010 | loss 1.9248 | lr 3.00e-04 | grad 4.03 | tok/s 6884
step   1020 | loss 1.8655 | lr 3.00e-04 | grad 2.81 | tok/s 17042
step   1030 | loss 1.6873 | lr 3.00e-04 | grad 2.97 | tok/s 17699
step   1040 | loss 1.6852 | lr 3.00e-04 | grad 3.14 | tok/s 18285
step   1050 | loss 1.8324 | lr 3.00e-04 | grad 4.28 | tok/s 16927
step   1060 | loss 1.9858 | lr 3.00e-04 | grad 4.84 | tok/s 18278
step   1070 | loss 1.9772 | lr 3.00e-04 | grad 3.55 | tok/s 18193
step   1080 | loss 1.6164 | lr 3.00e-04 | grad 2.42 | tok/s 16531
step   1090 | loss 1.3445 | lr 3.00e-04 | grad 3.36 | tok/s 18266
step   1100 | loss 1.6663 | lr 3.00e-04 | grad 4.69 | tok/s 17734
step   1110 | loss 1.6628 | lr 3.00e-04 | grad 2.95 | tok/s 18574
step   1120 | loss 1.5361 | lr 3.00e-04 | grad 3.47 | tok/s 18564
step   1130 | loss 1.4832 | lr 3.00e-04 | grad 2.64 | tok/s 18558
step   1140 | loss 1.4672 | lr 3.00e-04 | grad 3.03 | tok/s 18563
step   1150 | loss 1.4813 | lr 3.00e-04 | grad 2.62 | tok/s 18587
step   1160 | loss 1.3858 | lr 3.00e-04 | grad 2.58 | tok/s 18580
step   1170 | loss 1.4120 | lr 3.00e-04 | grad 3.14 | tok/s 18593
step   1180 | loss 1.5330 | lr 3.00e-04 | grad 2.31 | tok/s 18593
step   1190 | loss 1.4109 | lr 3.00e-04 | grad 3.06 | tok/s 18576
step   1200 | loss 1.3976 | lr 3.00e-04 | grad 3.03 | tok/s 18569
step   1210 | loss 1.4471 | lr 3.00e-04 | grad 3.17 | tok/s 18545
step   1220 | loss 1.4616 | lr 3.00e-04 | grad 2.91 | tok/s 18579
step   1230 | loss 1.4376 | lr 3.00e-04 | grad 2.70 | tok/s 18538
step   1240 | loss 1.3891 | lr 3.00e-04 | grad 2.05 | tok/s 18528
step   1250 | loss 2.2726 | lr 3.00e-04 | grad 4.34 | tok/s 17576
step   1260 | loss 1.6776 | lr 3.00e-04 | grad 5.75 | tok/s 17422
step   1270 | loss 1.8942 | lr 3.00e-04 | grad 6.94 | tok/s 17353
step   1280 | loss 1.8778 | lr 3.00e-04 | grad 2.92 | tok/s 17878
step   1290 | loss 1.6878 | lr 3.00e-04 | grad 3.05 | tok/s 17768
step   1300 | loss 1.7502 | lr 3.00e-04 | grad 2.92 | tok/s 17911

Training complete! Final step: 1303
