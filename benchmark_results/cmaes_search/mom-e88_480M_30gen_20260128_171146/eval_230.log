Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_230/levelMoME88_100m_20260128_194028
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 472,947,180 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 11.5258 | lr 3.00e-04 | grad 7.34 | tok/s 9640
step     20 | loss 3.1995 | lr 3.00e-04 | grad 2.08 | tok/s 19790
step     30 | loss 3.2550 | lr 3.00e-04 | grad 3.80 | tok/s 21002
step     40 | loss 5.3537 | lr 3.00e-04 | grad 15.75 | tok/s 21326
step     50 | loss 4.6283 | lr 3.00e-04 | grad 10.38 | tok/s 21608
step     60 | loss 3.7536 | lr 3.00e-04 | grad 6.44 | tok/s 21523
step     70 | loss 3.1468 | lr 3.00e-04 | grad 7.81 | tok/s 21506
step     80 | loss 2.8030 | lr 3.00e-04 | grad 4.56 | tok/s 21505
step     90 | loss 2.5320 | lr 3.00e-04 | grad 3.52 | tok/s 21449
step    100 | loss 2.4023 | lr 3.00e-04 | grad 2.61 | tok/s 21428
step    110 | loss 2.4421 | lr 3.00e-04 | grad 4.09 | tok/s 21236
step    120 | loss 3.0613 | lr 3.00e-04 | grad 2.17 | tok/s 20235
step    130 | loss 2.3547 | lr 3.00e-04 | grad 5.41 | tok/s 20711
step    140 | loss 2.6481 | lr 3.00e-04 | grad 6.34 | tok/s 20779
step    150 | loss 2.1469 | lr 3.00e-04 | grad 6.50 | tok/s 21317
step    160 | loss 2.6744 | lr 3.00e-04 | grad 2.28 | tok/s 20563
step    170 | loss 2.5038 | lr 3.00e-04 | grad 1.53 | tok/s 20265
step    180 | loss 2.5147 | lr 3.00e-04 | grad 4.66 | tok/s 20753
step    190 | loss 2.1896 | lr 3.00e-04 | grad 2.09 | tok/s 20376
step    200 | loss 1.9973 | lr 3.00e-04 | grad 1.90 | tok/s 21307
step    210 | loss 2.1734 | lr 3.00e-04 | grad 6.34 | tok/s 20215
step    220 | loss 2.4849 | lr 3.00e-04 | grad 4.50 | tok/s 20445
step    230 | loss 2.1958 | lr 3.00e-04 | grad 2.80 | tok/s 20405
step    240 | loss 2.5789 | lr 3.00e-04 | grad 5.16 | tok/s 20693
step    250 | loss 2.0413 | lr 3.00e-04 | grad 1.80 | tok/s 20528
step    260 | loss 2.1797 | lr 3.00e-04 | grad 3.31 | tok/s 21121
step    270 | loss 2.0817 | lr 3.00e-04 | grad 1.84 | tok/s 20649
step    280 | loss 2.0165 | lr 3.00e-04 | grad 1.84 | tok/s 19375
step    290 | loss 1.9328 | lr 3.00e-04 | grad 2.09 | tok/s 20034
step    300 | loss 2.2249 | lr 3.00e-04 | grad 2.75 | tok/s 20189
step    310 | loss 1.8833 | lr 3.00e-04 | grad 1.68 | tok/s 20085
step    320 | loss 2.1395 | lr 3.00e-04 | grad 3.92 | tok/s 20325
step    330 | loss 1.9456 | lr 3.00e-04 | grad 1.84 | tok/s 20556
step    340 | loss 2.3058 | lr 3.00e-04 | grad 2.56 | tok/s 20494
step    350 | loss 2.1257 | lr 3.00e-04 | grad 2.23 | tok/s 21060
step    360 | loss 1.8328 | lr 3.00e-04 | grad 2.19 | tok/s 20174
step    370 | loss 1.8211 | lr 3.00e-04 | grad 1.96 | tok/s 21261
step    380 | loss 1.5646 | lr 3.00e-04 | grad 1.90 | tok/s 21435
step    390 | loss 1.4554 | lr 3.00e-04 | grad 1.90 | tok/s 21406
step    400 | loss 2.0605 | lr 3.00e-04 | grad 2.27 | tok/s 20292
step    410 | loss 2.0015 | lr 3.00e-04 | grad 2.36 | tok/s 20541
step    420 | loss 2.0048 | lr 3.00e-04 | grad 3.97 | tok/s 21382
step    430 | loss 1.9172 | lr 3.00e-04 | grad 2.00 | tok/s 21037
step    440 | loss 1.9683 | lr 3.00e-04 | grad 2.48 | tok/s 20382
step    450 | loss 1.8637 | lr 3.00e-04 | grad 1.73 | tok/s 20609
step    460 | loss 1.8627 | lr 3.00e-04 | grad 2.31 | tok/s 20909
step    470 | loss 1.8539 | lr 3.00e-04 | grad 4.03 | tok/s 20748
step    480 | loss 1.9420 | lr 3.00e-04 | grad 2.88 | tok/s 21188
step    490 | loss 1.9122 | lr 3.00e-04 | grad 2.72 | tok/s 20344
step    500 | loss 2.0812 | lr 3.00e-04 | grad 1.89 | tok/s 20692
step    510 | loss 1.9273 | lr 3.00e-04 | grad 1.78 | tok/s 19760
step    520 | loss 1.7515 | lr 3.00e-04 | grad 2.12 | tok/s 20723
step    530 | loss 1.9388 | lr 3.00e-04 | grad 2.47 | tok/s 20390
step    540 | loss 1.8485 | lr 3.00e-04 | grad 1.70 | tok/s 19944
step    550 | loss 1.5711 | lr 3.00e-04 | grad 3.20 | tok/s 20885
step    560 | loss 1.6719 | lr 3.00e-04 | grad 2.22 | tok/s 21427
step    570 | loss 1.5419 | lr 3.00e-04 | grad 2.09 | tok/s 21411
step    580 | loss 1.4894 | lr 3.00e-04 | grad 1.52 | tok/s 21404
step    590 | loss 1.5334 | lr 3.00e-04 | grad 1.45 | tok/s 21415
step    600 | loss 1.4799 | lr 3.00e-04 | grad 1.88 | tok/s 21409
step    610 | loss 1.4866 | lr 3.00e-04 | grad 1.53 | tok/s 21419
step    620 | loss 1.4795 | lr 3.00e-04 | grad 1.72 | tok/s 21322
step    630 | loss 1.9386 | lr 3.00e-04 | grad 4.81 | tok/s 20233
step    640 | loss 1.9798 | lr 3.00e-04 | grad 2.66 | tok/s 20458
step    650 | loss 1.7750 | lr 3.00e-04 | grad 2.02 | tok/s 20441
step    660 | loss 1.8167 | lr 3.00e-04 | grad 2.23 | tok/s 21205
step    670 | loss 1.8518 | lr 3.00e-04 | grad 5.34 | tok/s 20521
step    680 | loss 1.8635 | lr 3.00e-04 | grad 2.53 | tok/s 20202
step    690 | loss 1.8181 | lr 3.00e-04 | grad 2.14 | tok/s 20022
step    700 | loss 1.7035 | lr 3.00e-04 | grad 2.03 | tok/s 20483
step    710 | loss 1.9012 | lr 3.00e-04 | grad 4.66 | tok/s 20145
step    720 | loss 1.5439 | lr 3.00e-04 | grad 1.70 | tok/s 20896
step    730 | loss 1.6924 | lr 3.00e-04 | grad 1.58 | tok/s 20607
step    740 | loss 2.1163 | lr 3.00e-04 | grad 4.31 | tok/s 21160
step    750 | loss 1.8461 | lr 3.00e-04 | grad 1.76 | tok/s 21393
step    760 | loss 1.7472 | lr 3.00e-04 | grad 4.12 | tok/s 20942

Training complete! Final step: 767
