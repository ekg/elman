Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_170/levelMoME88_100m_20260128_190316
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 481,345,696 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 17.2222 | lr 3.00e-04 | grad 10.88 | tok/s 6096
step     20 | loss 2.7619 | lr 3.00e-04 | grad 4.91 | tok/s 19629
step     30 | loss 2.7541 | lr 3.00e-04 | grad 5.56 | tok/s 19847
step     40 | loss 2.7531 | lr 3.00e-04 | grad 5.28 | tok/s 19078
step     50 | loss 3.1570 | lr 3.00e-04 | grad 11.25 | tok/s 19409
step     60 | loss 2.3387 | lr 3.00e-04 | grad 39.25 | tok/s 20077
step     70 | loss 2.2368 | lr 3.00e-04 | grad 5.53 | tok/s 20341
step     80 | loss 5.9321 | lr 3.00e-04 | grad 42.25 | tok/s 20470
step     90 | loss 4.8089 | lr 3.00e-04 | grad 11.94 | tok/s 20796
step    100 | loss 4.1001 | lr 3.00e-04 | grad 9.56 | tok/s 20786
step    110 | loss 3.9038 | lr 3.00e-04 | grad 25.75 | tok/s 20715
step    120 | loss 3.7470 | lr 3.00e-04 | grad 38.25 | tok/s 20688
step    130 | loss 3.4181 | lr 3.00e-04 | grad 12.25 | tok/s 20644
step    140 | loss 2.9345 | lr 3.00e-04 | grad 13.81 | tok/s 20609
step    150 | loss 3.0334 | lr 3.00e-04 | grad 18.75 | tok/s 20644
step    160 | loss 2.5786 | lr 3.00e-04 | grad 8.81 | tok/s 20632
step    170 | loss 2.6016 | lr 3.00e-04 | grad 19.75 | tok/s 20616
step    180 | loss 2.4060 | lr 3.00e-04 | grad 7.72 | tok/s 20610
step    190 | loss 2.5302 | lr 3.00e-04 | grad 6.41 | tok/s 20576
step    200 | loss 2.2194 | lr 3.00e-04 | grad 7.56 | tok/s 20587
step    210 | loss 2.2549 | lr 3.00e-04 | grad 14.62 | tok/s 20556
step    220 | loss 2.5143 | lr 3.00e-04 | grad 4.19 | tok/s 20333
step    230 | loss 3.0278 | lr 3.00e-04 | grad 9.50 | tok/s 20075
step    240 | loss 2.5784 | lr 3.00e-04 | grad 5.66 | tok/s 19098
step    250 | loss 2.3642 | lr 3.00e-04 | grad 3.06 | tok/s 19629
step    260 | loss 1.9773 | lr 3.00e-04 | grad 3.61 | tok/s 20210
step    270 | loss 2.4286 | lr 3.00e-04 | grad 3.44 | tok/s 19983
step    280 | loss 2.5695 | lr 3.00e-04 | grad 10.94 | tok/s 19619
step    290 | loss 2.3487 | lr 3.00e-04 | grad 5.97 | tok/s 20657
step    300 | loss 1.0162 | lr 3.00e-04 | grad 3.75 | tok/s 20663
step    310 | loss 2.7681 | lr 3.00e-04 | grad 4.50 | tok/s 20243
step    320 | loss 2.3176 | lr 3.00e-04 | grad 7.00 | tok/s 19778
step    330 | loss 2.2560 | lr 3.00e-04 | grad 3.86 | tok/s 19143
step    340 | loss 2.6538 | lr 3.00e-04 | grad 3.08 | tok/s 19445
step    350 | loss 2.3270 | lr 3.00e-04 | grad 5.09 | tok/s 19965
step    360 | loss 2.0275 | lr 3.00e-04 | grad 8.31 | tok/s 20413
step    370 | loss 2.1805 | lr 3.00e-04 | grad 3.62 | tok/s 18521
step    380 | loss 2.0790 | lr 3.00e-04 | grad 3.39 | tok/s 19707
step    390 | loss 1.8329 | lr 3.00e-04 | grad 3.25 | tok/s 20534
step    400 | loss 1.8437 | lr 3.00e-04 | grad 3.73 | tok/s 20390
step    410 | loss 1.7002 | lr 3.00e-04 | grad 2.62 | tok/s 19892
step    420 | loss 2.1204 | lr 3.00e-04 | grad 7.06 | tok/s 19012
step    430 | loss 2.5040 | lr 3.00e-04 | grad 3.81 | tok/s 20201
step    440 | loss 2.4487 | lr 3.00e-04 | grad 4.81 | tok/s 19153
step    450 | loss 2.2456 | lr 3.00e-04 | grad 3.03 | tok/s 19817
step    460 | loss 2.0543 | lr 3.00e-04 | grad 5.12 | tok/s 19350
step    470 | loss 2.1636 | lr 3.00e-04 | grad 3.45 | tok/s 19961
step    480 | loss 2.6530 | lr 3.00e-04 | grad 8.06 | tok/s 19974
step    490 | loss 2.1009 | lr 3.00e-04 | grad 3.64 | tok/s 18917
step    500 | loss 2.0250 | lr 3.00e-04 | grad 4.78 | tok/s 20186
step    510 | loss 2.0014 | lr 3.00e-04 | grad 3.20 | tok/s 20436
step    520 | loss 2.0035 | lr 3.00e-04 | grad 3.03 | tok/s 20404
step    530 | loss 2.2413 | lr 3.00e-04 | grad 3.14 | tok/s 19660
step    540 | loss 1.9943 | lr 3.00e-04 | grad 3.06 | tok/s 19589
step    550 | loss 1.8120 | lr 3.00e-04 | grad 3.78 | tok/s 19189
step    560 | loss 2.0145 | lr 3.00e-04 | grad 3.45 | tok/s 18670
step    570 | loss 1.9862 | lr 3.00e-04 | grad 4.44 | tok/s 19232
step    580 | loss 1.8338 | lr 3.00e-04 | grad 3.55 | tok/s 19183
step    590 | loss 2.1949 | lr 3.00e-04 | grad 3.88 | tok/s 19598
step    600 | loss 2.1265 | lr 3.00e-04 | grad 3.09 | tok/s 18998
step    610 | loss 1.9022 | lr 3.00e-04 | grad 3.05 | tok/s 19990
step    620 | loss 1.7830 | lr 3.00e-04 | grad 3.02 | tok/s 18971
step    630 | loss 1.9459 | lr 3.00e-04 | grad 5.72 | tok/s 19109
step    640 | loss 2.1287 | lr 3.00e-04 | grad 2.94 | tok/s 19613
step    650 | loss 1.9180 | lr 3.00e-04 | grad 3.45 | tok/s 19696
step    660 | loss 1.9892 | lr 3.00e-04 | grad 3.25 | tok/s 19786
step    670 | loss 2.2729 | lr 3.00e-04 | grad 13.50 | tok/s 19945
step    680 | loss 1.9608 | lr 3.00e-04 | grad 3.27 | tok/s 19542
step    690 | loss 2.2044 | lr 3.00e-04 | grad 4.91 | tok/s 20220
step    700 | loss 1.8279 | lr 3.00e-04 | grad 4.06 | tok/s 20536
step    710 | loss 1.8575 | lr 3.00e-04 | grad 3.05 | tok/s 19257
step    720 | loss 1.7199 | lr 3.00e-04 | grad 8.06 | tok/s 18965
step    730 | loss 1.6428 | lr 3.00e-04 | grad 3.58 | tok/s 20550
step    740 | loss 1.8182 | lr 3.00e-04 | grad 3.47 | tok/s 20277
step    750 | loss 1.5357 | lr 3.00e-04 | grad 3.34 | tok/s 20560
step    760 | loss 1.4084 | lr 3.00e-04 | grad 3.17 | tok/s 20596
step    770 | loss 1.3560 | lr 3.00e-04 | grad 2.67 | tok/s 20620
step    780 | loss 1.3044 | lr 3.00e-04 | grad 2.30 | tok/s 20623
step    790 | loss 1.4041 | lr 3.00e-04 | grad 3.92 | tok/s 19983
step    800 | loss 2.1901 | lr 3.00e-04 | grad 6.12 | tok/s 19887
step    810 | loss 1.9471 | lr 3.00e-04 | grad 2.95 | tok/s 19791
step    820 | loss 1.9866 | lr 3.00e-04 | grad 4.97 | tok/s 18930
step    830 | loss 1.9299 | lr 3.00e-04 | grad 3.12 | tok/s 20374
step    840 | loss 1.7166 | lr 3.00e-04 | grad 3.33 | tok/s 20617
step    850 | loss 1.9044 | lr 3.00e-04 | grad 2.95 | tok/s 20542
step    860 | loss 1.7947 | lr 3.00e-04 | grad 5.28 | tok/s 20262
step    870 | loss 1.7736 | lr 3.00e-04 | grad 4.03 | tok/s 19512
step    880 | loss 2.0170 | lr 3.00e-04 | grad 4.22 | tok/s 19647
step    890 | loss 1.9309 | lr 3.00e-04 | grad 3.86 | tok/s 19873
step    900 | loss 1.8194 | lr 3.00e-04 | grad 3.27 | tok/s 19931
step    910 | loss 1.6660 | lr 3.00e-04 | grad 4.56 | tok/s 19497
step    920 | loss 1.8061 | lr 3.00e-04 | grad 4.69 | tok/s 20256
step    930 | loss 1.8488 | lr 3.00e-04 | grad 4.34 | tok/s 19323
step    940 | loss 1.6819 | lr 3.00e-04 | grad 2.80 | tok/s 20420
step    950 | loss 1.8217 | lr 3.00e-04 | grad 4.72 | tok/s 20486
step    960 | loss 1.6694 | lr 3.00e-04 | grad 3.59 | tok/s 20523
step    970 | loss 1.9994 | lr 3.00e-04 | grad 4.78 | tok/s 19292
step    980 | loss 1.8886 | lr 3.00e-04 | grad 3.34 | tok/s 19816
step    990 | loss 1.7164 | lr 3.00e-04 | grad 2.78 | tok/s 20155
step   1000 | loss 2.1370 | lr 3.00e-04 | grad 15.94 | tok/s 19333
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1370.pt
step   1010 | loss 1.9339 | lr 3.00e-04 | grad 4.41 | tok/s 6930
step   1020 | loss 1.8778 | lr 3.00e-04 | grad 2.84 | tok/s 18839
step   1030 | loss 1.6944 | lr 3.00e-04 | grad 3.11 | tok/s 19619
step   1040 | loss 1.7072 | lr 3.00e-04 | grad 2.88 | tok/s 20270
step   1050 | loss 1.8566 | lr 3.00e-04 | grad 4.34 | tok/s 18799
step   1060 | loss 1.9928 | lr 3.00e-04 | grad 4.34 | tok/s 20328
step   1070 | loss 1.9902 | lr 3.00e-04 | grad 3.73 | tok/s 20231
step   1080 | loss 1.6243 | lr 3.00e-04 | grad 2.61 | tok/s 18484
step   1090 | loss 1.3455 | lr 3.00e-04 | grad 7.31 | tok/s 20423
step   1100 | loss 1.6774 | lr 3.00e-04 | grad 4.75 | tok/s 19778
step   1110 | loss 1.6826 | lr 3.00e-04 | grad 3.06 | tok/s 20673
step   1120 | loss 1.5442 | lr 3.00e-04 | grad 3.50 | tok/s 20680
step   1130 | loss 1.4867 | lr 3.00e-04 | grad 2.66 | tok/s 20665
step   1140 | loss 1.4684 | lr 3.00e-04 | grad 2.97 | tok/s 20635
step   1150 | loss 1.4925 | lr 3.00e-04 | grad 2.72 | tok/s 20683
step   1160 | loss 1.3915 | lr 3.00e-04 | grad 2.50 | tok/s 20610
step   1170 | loss 1.4228 | lr 3.00e-04 | grad 3.19 | tok/s 20616
step   1180 | loss 1.5394 | lr 3.00e-04 | grad 2.42 | tok/s 20643
step   1190 | loss 1.4083 | lr 3.00e-04 | grad 3.03 | tok/s 20648
step   1200 | loss 1.4070 | lr 3.00e-04 | grad 3.06 | tok/s 20624
step   1210 | loss 1.4529 | lr 3.00e-04 | grad 2.86 | tok/s 20624
step   1220 | loss 1.4706 | lr 3.00e-04 | grad 2.94 | tok/s 20620
step   1230 | loss 1.4472 | lr 3.00e-04 | grad 2.75 | tok/s 20648
step   1240 | loss 1.3925 | lr 3.00e-04 | grad 2.16 | tok/s 20645
step   1250 | loss 2.2273 | lr 3.00e-04 | grad 4.34 | tok/s 19558
step   1260 | loss 1.6657 | lr 3.00e-04 | grad 6.84 | tok/s 19371
step   1270 | loss 1.8963 | lr 3.00e-04 | grad 6.91 | tok/s 19284
step   1280 | loss 1.8676 | lr 3.00e-04 | grad 2.77 | tok/s 19895
step   1290 | loss 1.6862 | lr 3.00e-04 | grad 3.09 | tok/s 19789
step   1300 | loss 1.7583 | lr 3.00e-04 | grad 3.22 | tok/s 19918
step   1310 | loss 1.6972 | lr 3.00e-04 | grad 3.64 | tok/s 20233
step   1320 | loss 1.8004 | lr 3.00e-04 | grad 2.81 | tok/s 20287
step   1330 | loss 1.9034 | lr 3.00e-04 | grad 3.58 | tok/s 20314
step   1340 | loss 1.7481 | lr 3.00e-04 | grad 13.06 | tok/s 19446
step   1350 | loss 1.9661 | lr 3.00e-04 | grad 3.81 | tok/s 18775
step   1360 | loss 1.7538 | lr 3.00e-04 | grad 3.72 | tok/s 19895
step   1370 | loss 1.5930 | lr 3.00e-04 | grad 2.36 | tok/s 19653
step   1380 | loss 1.9591 | lr 3.00e-04 | grad 2.77 | tok/s 18879
step   1390 | loss 1.7411 | lr 3.00e-04 | grad 2.86 | tok/s 20026
step   1400 | loss 1.6414 | lr 3.00e-04 | grad 2.59 | tok/s 19339
step   1410 | loss 1.6806 | lr 3.00e-04 | grad 3.91 | tok/s 19378
step   1420 | loss 1.9534 | lr 3.00e-04 | grad 8.38 | tok/s 19444
step   1430 | loss 1.5927 | lr 3.00e-04 | grad 2.47 | tok/s 19782
step   1440 | loss 1.3667 | lr 3.00e-04 | grad 3.17 | tok/s 20425

Training complete! Final step: 1445
