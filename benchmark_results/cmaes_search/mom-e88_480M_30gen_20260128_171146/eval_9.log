Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_9/levelMoME88_100m_20260128_171711
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 485,815,764 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 13.0204 | lr 3.00e-04 | grad 25.00 | tok/s 6179
step     20 | loss 2.9476 | lr 3.00e-04 | grad 3.64 | tok/s 17796
step     30 | loss 2.7510 | lr 3.00e-04 | grad 2.28 | tok/s 18045
step     40 | loss 3.4386 | lr 3.00e-04 | grad 2.73 | tok/s 17308
step     50 | loss 3.6486 | lr 3.00e-04 | grad 7.81 | tok/s 17572
step     60 | loss 2.3526 | lr 3.00e-04 | grad 6.81 | tok/s 18062
step     70 | loss 2.2658 | lr 3.00e-04 | grad 2.95 | tok/s 18281
step     80 | loss 12.3345 | lr 3.00e-04 | grad 37.75 | tok/s 18325
step     90 | loss 7.6801 | lr 3.00e-04 | grad 4.81 | tok/s 18611
step    100 | loss 5.0666 | lr 3.00e-04 | grad 4.25 | tok/s 18541
step    110 | loss 4.4726 | lr 3.00e-04 | grad 9.00 | tok/s 18551
step    120 | loss 4.1968 | lr 3.00e-04 | grad 15.19 | tok/s 18498
step    130 | loss 3.9151 | lr 3.00e-04 | grad 7.12 | tok/s 18491
step    140 | loss 3.1398 | lr 3.00e-04 | grad 5.06 | tok/s 18494
step    150 | loss 3.4208 | lr 3.00e-04 | grad 39.00 | tok/s 18502
step    160 | loss 2.8113 | lr 3.00e-04 | grad 5.69 | tok/s 18495
step    170 | loss 2.8119 | lr 3.00e-04 | grad 22.12 | tok/s 18483
step    180 | loss 2.6001 | lr 3.00e-04 | grad 2.80 | tok/s 18432
step    190 | loss 2.7431 | lr 3.00e-04 | grad 7.97 | tok/s 18453
step    200 | loss 2.3777 | lr 3.00e-04 | grad 6.38 | tok/s 18428
step    210 | loss 2.3993 | lr 3.00e-04 | grad 17.75 | tok/s 18437
step    220 | loss 2.5543 | lr 3.00e-04 | grad 3.39 | tok/s 18190
step    230 | loss 3.1223 | lr 3.00e-04 | grad 4.16 | tok/s 18060
step    240 | loss 2.5449 | lr 3.00e-04 | grad 3.27 | tok/s 17130
step    250 | loss 2.3678 | lr 3.00e-04 | grad 1.95 | tok/s 17595
step    260 | loss 2.0535 | lr 3.00e-04 | grad 2.20 | tok/s 18119
step    270 | loss 2.4717 | lr 3.00e-04 | grad 1.81 | tok/s 17880
step    280 | loss 2.6572 | lr 3.00e-04 | grad 5.38 | tok/s 17551
step    290 | loss 2.7416 | lr 3.00e-04 | grad 4.78 | tok/s 18498
step    300 | loss 1.6404 | lr 3.00e-04 | grad 3.47 | tok/s 18502
step    310 | loss 2.8587 | lr 3.00e-04 | grad 2.94 | tok/s 18114
step    320 | loss 2.4881 | lr 3.00e-04 | grad 5.75 | tok/s 17763
step    330 | loss 2.2568 | lr 3.00e-04 | grad 2.14 | tok/s 17140
step    340 | loss 2.6035 | lr 3.00e-04 | grad 1.84 | tok/s 17442
step    350 | loss 2.3861 | lr 3.00e-04 | grad 5.78 | tok/s 17850
step    360 | loss 2.6351 | lr 3.00e-04 | grad 4.19 | tok/s 18251
step    370 | loss 2.1715 | lr 3.00e-04 | grad 1.98 | tok/s 16613
step    380 | loss 2.1112 | lr 3.00e-04 | grad 2.88 | tok/s 17654
step    390 | loss 1.9072 | lr 3.00e-04 | grad 1.47 | tok/s 18410
step    400 | loss 1.9060 | lr 3.00e-04 | grad 2.06 | tok/s 18230
step    410 | loss 1.8368 | lr 3.00e-04 | grad 1.37 | tok/s 17816
step    420 | loss 2.1328 | lr 3.00e-04 | grad 3.52 | tok/s 17068
step    430 | loss 2.4873 | lr 3.00e-04 | grad 2.22 | tok/s 18186
step    440 | loss 2.4406 | lr 3.00e-04 | grad 2.75 | tok/s 17176
step    450 | loss 2.2711 | lr 3.00e-04 | grad 1.83 | tok/s 17765
step    460 | loss 2.1355 | lr 3.00e-04 | grad 3.73 | tok/s 17394
step    470 | loss 2.2036 | lr 3.00e-04 | grad 2.39 | tok/s 17934
step    480 | loss 2.6975 | lr 3.00e-04 | grad 5.50 | tok/s 17905
step    490 | loss 2.1306 | lr 3.00e-04 | grad 2.31 | tok/s 16950
step    500 | loss 2.0600 | lr 3.00e-04 | grad 2.31 | tok/s 18083
step    510 | loss 2.0464 | lr 3.00e-04 | grad 1.75 | tok/s 18319
step    520 | loss 2.0486 | lr 3.00e-04 | grad 1.77 | tok/s 18291
step    530 | loss 2.2732 | lr 3.00e-04 | grad 1.97 | tok/s 17611
step    540 | loss 1.9997 | lr 3.00e-04 | grad 1.91 | tok/s 17609
step    550 | loss 1.8364 | lr 3.00e-04 | grad 2.27 | tok/s 17250
step    560 | loss 2.0321 | lr 3.00e-04 | grad 1.84 | tok/s 16822
step    570 | loss 1.9975 | lr 3.00e-04 | grad 3.00 | tok/s 17250
step    580 | loss 1.8543 | lr 3.00e-04 | grad 1.84 | tok/s 17190
step    590 | loss 2.2589 | lr 3.00e-04 | grad 2.36 | tok/s 17614
step    600 | loss 2.1073 | lr 3.00e-04 | grad 2.06 | tok/s 17039
step    610 | loss 1.9421 | lr 3.00e-04 | grad 1.59 | tok/s 17877
step    620 | loss 1.7946 | lr 3.00e-04 | grad 1.74 | tok/s 16961
step    630 | loss 1.9674 | lr 3.00e-04 | grad 3.44 | tok/s 17111
step    640 | loss 2.1277 | lr 3.00e-04 | grad 1.91 | tok/s 17552
step    650 | loss 1.9700 | lr 3.00e-04 | grad 1.89 | tok/s 17645
step    660 | loss 1.9970 | lr 3.00e-04 | grad 1.77 | tok/s 17719
step    670 | loss 2.2671 | lr 3.00e-04 | grad 57.75 | tok/s 17820
step    680 | loss 2.0012 | lr 3.00e-04 | grad 1.96 | tok/s 17449
step    690 | loss 2.3172 | lr 3.00e-04 | grad 3.17 | tok/s 17974
step    700 | loss 2.0734 | lr 3.00e-04 | grad 2.97 | tok/s 18371
step    710 | loss 1.9078 | lr 3.00e-04 | grad 1.70 | tok/s 17202
step    720 | loss 1.7479 | lr 3.00e-04 | grad 2.98 | tok/s 16994
step    730 | loss 1.7764 | lr 3.00e-04 | grad 2.39 | tok/s 18339
step    740 | loss 1.8661 | lr 3.00e-04 | grad 2.19 | tok/s 18132
step    750 | loss 1.6656 | lr 3.00e-04 | grad 2.06 | tok/s 18407
step    760 | loss 1.5219 | lr 3.00e-04 | grad 1.86 | tok/s 18390
step    770 | loss 1.4690 | lr 3.00e-04 | grad 1.56 | tok/s 18404
step    780 | loss 1.4168 | lr 3.00e-04 | grad 1.62 | tok/s 18379
step    790 | loss 1.4913 | lr 3.00e-04 | grad 2.56 | tok/s 17848
step    800 | loss 2.3158 | lr 3.00e-04 | grad 5.59 | tok/s 17742
step    810 | loss 1.9657 | lr 3.00e-04 | grad 1.73 | tok/s 17651
step    820 | loss 1.9843 | lr 3.00e-04 | grad 2.94 | tok/s 16978
step    830 | loss 2.0212 | lr 3.00e-04 | grad 2.23 | tok/s 18195
step    840 | loss 1.8866 | lr 3.00e-04 | grad 1.96 | tok/s 18363
step    850 | loss 2.0150 | lr 3.00e-04 | grad 1.87 | tok/s 18292
step    860 | loss 1.8999 | lr 3.00e-04 | grad 3.17 | tok/s 18083
step    870 | loss 1.8065 | lr 3.00e-04 | grad 2.58 | tok/s 17395
step    880 | loss 2.0268 | lr 3.00e-04 | grad 2.11 | tok/s 17528
step    890 | loss 1.9662 | lr 3.00e-04 | grad 2.64 | tok/s 17725
step    900 | loss 1.8339 | lr 3.00e-04 | grad 2.19 | tok/s 17757
step    910 | loss 1.7098 | lr 3.00e-04 | grad 2.84 | tok/s 17376
step    920 | loss 1.8965 | lr 3.00e-04 | grad 3.19 | tok/s 18053
step    930 | loss 1.8784 | lr 3.00e-04 | grad 2.94 | tok/s 17290
step    940 | loss 1.8125 | lr 3.00e-04 | grad 1.98 | tok/s 18196
step    950 | loss 1.8613 | lr 3.00e-04 | grad 2.19 | tok/s 18307
step    960 | loss 1.7961 | lr 3.00e-04 | grad 2.31 | tok/s 18345
step    970 | loss 1.9851 | lr 3.00e-04 | grad 2.70 | tok/s 17269
step    980 | loss 1.9065 | lr 3.00e-04 | grad 1.99 | tok/s 17709
step    990 | loss 1.7751 | lr 3.00e-04 | grad 1.81 | tok/s 17999
step   1000 | loss 2.2152 | lr 3.00e-04 | grad 13.94 | tok/s 17297
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2152.pt
step   1010 | loss 2.0607 | lr 3.00e-04 | grad 2.47 | tok/s 6620
step   1020 | loss 1.8868 | lr 3.00e-04 | grad 1.91 | tok/s 16953
step   1030 | loss 1.7452 | lr 3.00e-04 | grad 1.73 | tok/s 17627
step   1040 | loss 1.7359 | lr 3.00e-04 | grad 1.78 | tok/s 18203
step   1050 | loss 1.8605 | lr 3.00e-04 | grad 2.73 | tok/s 16856
step   1060 | loss 2.0401 | lr 3.00e-04 | grad 2.91 | tok/s 18172
step   1070 | loss 2.0532 | lr 3.00e-04 | grad 2.45 | tok/s 18076
step   1080 | loss 1.6504 | lr 3.00e-04 | grad 1.70 | tok/s 16441
step   1090 | loss 1.4276 | lr 3.00e-04 | grad 2.47 | tok/s 18156
step   1100 | loss 1.6838 | lr 3.00e-04 | grad 3.12 | tok/s 17601
step   1110 | loss 1.7278 | lr 3.00e-04 | grad 1.69 | tok/s 18451
step   1120 | loss 1.6073 | lr 3.00e-04 | grad 2.02 | tok/s 18426
step   1130 | loss 1.5345 | lr 3.00e-04 | grad 2.09 | tok/s 18443
step   1140 | loss 1.5108 | lr 3.00e-04 | grad 1.99 | tok/s 18425
step   1150 | loss 1.5347 | lr 3.00e-04 | grad 1.59 | tok/s 18450
step   1160 | loss 1.4370 | lr 3.00e-04 | grad 1.49 | tok/s 18438
step   1170 | loss 1.4617 | lr 3.00e-04 | grad 1.90 | tok/s 18410
step   1180 | loss 1.5952 | lr 3.00e-04 | grad 1.70 | tok/s 18423
step   1190 | loss 1.4851 | lr 3.00e-04 | grad 2.61 | tok/s 18408
step   1200 | loss 1.4693 | lr 3.00e-04 | grad 1.97 | tok/s 18426
step   1210 | loss 1.4979 | lr 3.00e-04 | grad 1.92 | tok/s 18400
step   1220 | loss 1.5045 | lr 3.00e-04 | grad 1.87 | tok/s 18421
step   1230 | loss 1.4899 | lr 3.00e-04 | grad 1.68 | tok/s 18420
step   1240 | loss 1.4335 | lr 3.00e-04 | grad 1.49 | tok/s 18396
step   1250 | loss 2.1698 | lr 3.00e-04 | grad 3.11 | tok/s 17435
step   1260 | loss 1.6650 | lr 3.00e-04 | grad 4.44 | tok/s 17320
step   1270 | loss 1.9203 | lr 3.00e-04 | grad 4.50 | tok/s 17216
step   1280 | loss 1.9216 | lr 3.00e-04 | grad 1.84 | tok/s 17704
step   1290 | loss 1.7365 | lr 3.00e-04 | grad 2.12 | tok/s 17604

Training complete! Final step: 1297
