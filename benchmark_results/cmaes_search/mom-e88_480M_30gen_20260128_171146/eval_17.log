Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_17/levelMoME88_100m_20260128_172231
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 468,793,730 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 9.4335 | lr 3.00e-04 | grad 25.75 | tok/s 7931
step     20 | loss 3.4474 | lr 3.00e-04 | grad 2.19 | tok/s 13481
step     30 | loss 3.2691 | lr 3.00e-04 | grad 4.41 | tok/s 14261
step     40 | loss 6.1787 | lr 3.00e-04 | grad 16.50 | tok/s 14525
step     50 | loss 5.0920 | lr 3.00e-04 | grad 7.06 | tok/s 14685
step     60 | loss 3.9517 | lr 3.00e-04 | grad 5.06 | tok/s 14655
step     70 | loss 3.2425 | lr 3.00e-04 | grad 5.97 | tok/s 14632
step     80 | loss 2.9757 | lr 3.00e-04 | grad 2.44 | tok/s 14626
step     90 | loss 2.7181 | lr 3.00e-04 | grad 4.19 | tok/s 14623
step    100 | loss 2.4499 | lr 3.00e-04 | grad 2.11 | tok/s 14609
step    110 | loss 2.4815 | lr 3.00e-04 | grad 10.50 | tok/s 14506
step    120 | loss 3.1931 | lr 3.00e-04 | grad 1.48 | tok/s 13802
step    130 | loss 2.4065 | lr 3.00e-04 | grad 5.34 | tok/s 14136
step    140 | loss 2.7399 | lr 3.00e-04 | grad 25.88 | tok/s 14184
step    150 | loss 2.5570 | lr 3.00e-04 | grad 5.00 | tok/s 14520
step    160 | loss 2.6810 | lr 3.00e-04 | grad 1.98 | tok/s 14027
step    170 | loss 2.5222 | lr 3.00e-04 | grad 1.48 | tok/s 13835
step    180 | loss 2.6510 | lr 3.00e-04 | grad 2.09 | tok/s 14171
step    190 | loss 2.2257 | lr 3.00e-04 | grad 1.52 | tok/s 13894
step    200 | loss 2.0412 | lr 3.00e-04 | grad 1.59 | tok/s 14521
step    210 | loss 2.2188 | lr 3.00e-04 | grad 3.44 | tok/s 13795
step    220 | loss 2.5402 | lr 3.00e-04 | grad 2.98 | tok/s 13923
step    230 | loss 2.2445 | lr 3.00e-04 | grad 2.02 | tok/s 13904
step    240 | loss 2.6559 | lr 3.00e-04 | grad 8.44 | tok/s 14088
step    250 | loss 2.0801 | lr 3.00e-04 | grad 1.62 | tok/s 14000
step    260 | loss 2.2104 | lr 3.00e-04 | grad 2.73 | tok/s 14402
step    270 | loss 2.1065 | lr 3.00e-04 | grad 1.50 | tok/s 14085
step    280 | loss 2.0515 | lr 3.00e-04 | grad 1.41 | tok/s 13234
step    290 | loss 1.9599 | lr 3.00e-04 | grad 1.55 | tok/s 13672
step    300 | loss 2.2583 | lr 3.00e-04 | grad 1.79 | tok/s 13771
step    310 | loss 1.9099 | lr 3.00e-04 | grad 1.40 | tok/s 13713
step    320 | loss 2.1617 | lr 3.00e-04 | grad 3.08 | tok/s 13869
step    330 | loss 1.9719 | lr 3.00e-04 | grad 1.61 | tok/s 14005
step    340 | loss 2.3433 | lr 3.00e-04 | grad 5.06 | tok/s 13956
step    350 | loss 2.2064 | lr 3.00e-04 | grad 1.98 | tok/s 14345
step    360 | loss 1.8705 | lr 3.00e-04 | grad 1.91 | tok/s 13724
step    370 | loss 1.8613 | lr 3.00e-04 | grad 1.60 | tok/s 14474
step    380 | loss 1.6349 | lr 3.00e-04 | grad 1.84 | tok/s 14603
step    390 | loss 1.5176 | lr 3.00e-04 | grad 1.73 | tok/s 14595
step    400 | loss 2.0895 | lr 3.00e-04 | grad 1.68 | tok/s 13868
step    410 | loss 2.0184 | lr 3.00e-04 | grad 1.92 | tok/s 13970
step    420 | loss 2.0422 | lr 3.00e-04 | grad 2.47 | tok/s 14581
step    430 | loss 1.9458 | lr 3.00e-04 | grad 1.52 | tok/s 14348
step    440 | loss 1.9774 | lr 3.00e-04 | grad 1.90 | tok/s 13881
step    450 | loss 1.8745 | lr 3.00e-04 | grad 1.38 | tok/s 14041
step    460 | loss 1.8973 | lr 3.00e-04 | grad 1.86 | tok/s 14238
step    470 | loss 1.8853 | lr 3.00e-04 | grad 2.45 | tok/s 14137
step    480 | loss 1.9240 | lr 3.00e-04 | grad 2.31 | tok/s 14441
step    490 | loss 1.9377 | lr 3.00e-04 | grad 2.30 | tok/s 13911
step    500 | loss 2.1211 | lr 3.00e-04 | grad 1.64 | tok/s 14104
step    510 | loss 1.9438 | lr 3.00e-04 | grad 1.45 | tok/s 13477
step    520 | loss 1.7725 | lr 3.00e-04 | grad 1.68 | tok/s 14133

Training complete! Final step: 524
