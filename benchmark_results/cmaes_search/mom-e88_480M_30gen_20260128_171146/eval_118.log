Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_118/levelMoME88_100m_20260128_182613
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 482,454,184 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 12.6455 | lr 3.00e-04 | grad 8.38 | tok/s 6003
step     20 | loss 2.6764 | lr 3.00e-04 | grad 3.55 | tok/s 18236
step     30 | loss 2.6671 | lr 3.00e-04 | grad 2.56 | tok/s 18500
step     40 | loss 3.4263 | lr 3.00e-04 | grad 2.92 | tok/s 17677
step     50 | loss 3.3062 | lr 3.00e-04 | grad 14.56 | tok/s 17998
step     60 | loss 2.2675 | lr 3.00e-04 | grad 7.47 | tok/s 18625
step     70 | loss 2.1954 | lr 3.00e-04 | grad 4.19 | tok/s 18827
step     80 | loss 7.8249 | lr 3.00e-04 | grad 140.00 | tok/s 18929
step     90 | loss 5.4207 | lr 3.00e-04 | grad 5.84 | tok/s 19229
step    100 | loss 4.1363 | lr 3.00e-04 | grad 6.19 | tok/s 19245
step    110 | loss 3.8861 | lr 3.00e-04 | grad 43.50 | tok/s 19224
step    120 | loss 3.5537 | lr 3.00e-04 | grad 21.00 | tok/s 19187
step    130 | loss 3.3741 | lr 3.00e-04 | grad 9.25 | tok/s 19217
step    140 | loss 2.8891 | lr 3.00e-04 | grad 7.50 | tok/s 19216
step    150 | loss 3.1029 | lr 3.00e-04 | grad 10.00 | tok/s 19197
step    160 | loss 2.6283 | lr 3.00e-04 | grad 8.38 | tok/s 19190
step    170 | loss 2.6524 | lr 3.00e-04 | grad 11.00 | tok/s 19182
step    180 | loss 2.4336 | lr 3.00e-04 | grad 7.91 | tok/s 19202
step    190 | loss 2.5488 | lr 3.00e-04 | grad 11.50 | tok/s 19166
step    200 | loss 2.2352 | lr 3.00e-04 | grad 5.28 | tok/s 19186
step    210 | loss 2.2622 | lr 3.00e-04 | grad 4.44 | tok/s 19173
step    220 | loss 2.4609 | lr 3.00e-04 | grad 2.70 | tok/s 18936
step    230 | loss 3.0719 | lr 3.00e-04 | grad 7.12 | tok/s 18727
step    240 | loss 2.5145 | lr 3.00e-04 | grad 3.91 | tok/s 17764
step    250 | loss 2.3134 | lr 3.00e-04 | grad 2.22 | tok/s 18255
step    260 | loss 1.9262 | lr 3.00e-04 | grad 2.36 | tok/s 18833
step    270 | loss 2.3511 | lr 3.00e-04 | grad 2.25 | tok/s 18580
step    280 | loss 2.4949 | lr 3.00e-04 | grad 4.88 | tok/s 18246
step    290 | loss 2.5980 | lr 3.00e-04 | grad 4.31 | tok/s 19176
step    300 | loss 1.1481 | lr 3.00e-04 | grad 2.84 | tok/s 19170
step    310 | loss 2.7824 | lr 3.00e-04 | grad 2.95 | tok/s 18850
step    320 | loss 2.3606 | lr 3.00e-04 | grad 5.12 | tok/s 18449
step    330 | loss 2.2126 | lr 3.00e-04 | grad 2.55 | tok/s 17832
step    340 | loss 2.5609 | lr 3.00e-04 | grad 2.17 | tok/s 18108
step    350 | loss 2.2640 | lr 3.00e-04 | grad 3.92 | tok/s 18587
step    360 | loss 2.1149 | lr 3.00e-04 | grad 4.72 | tok/s 18979
step    370 | loss 2.1185 | lr 3.00e-04 | grad 2.39 | tok/s 17206
step    380 | loss 2.0353 | lr 3.00e-04 | grad 2.56 | tok/s 18336
step    390 | loss 1.7916 | lr 3.00e-04 | grad 1.77 | tok/s 19139
step    400 | loss 1.8020 | lr 3.00e-04 | grad 2.91 | tok/s 18993
step    410 | loss 1.6778 | lr 3.00e-04 | grad 1.89 | tok/s 18545
step    420 | loss 2.0769 | lr 3.00e-04 | grad 4.88 | tok/s 17733
step    430 | loss 2.4417 | lr 3.00e-04 | grad 2.69 | tok/s 18837
step    440 | loss 2.4017 | lr 3.00e-04 | grad 3.50 | tok/s 17837
step    450 | loss 2.2357 | lr 3.00e-04 | grad 2.06 | tok/s 18446
step    460 | loss 2.0538 | lr 3.00e-04 | grad 4.09 | tok/s 18064
step    470 | loss 2.1108 | lr 3.00e-04 | grad 2.11 | tok/s 18640
step    480 | loss 2.5758 | lr 3.00e-04 | grad 6.09 | tok/s 18621
step    490 | loss 2.0312 | lr 3.00e-04 | grad 2.56 | tok/s 17613
step    500 | loss 1.9833 | lr 3.00e-04 | grad 3.05 | tok/s 18802
step    510 | loss 1.9702 | lr 3.00e-04 | grad 2.23 | tok/s 19067
step    520 | loss 1.9628 | lr 3.00e-04 | grad 1.95 | tok/s 19039
step    530 | loss 2.1632 | lr 3.00e-04 | grad 2.31 | tok/s 18267
step    540 | loss 1.9321 | lr 3.00e-04 | grad 2.36 | tok/s 18289
step    550 | loss 1.7778 | lr 3.00e-04 | grad 2.86 | tok/s 17898
step    560 | loss 1.9644 | lr 3.00e-04 | grad 2.38 | tok/s 17435
step    570 | loss 1.9275 | lr 3.00e-04 | grad 3.39 | tok/s 17903
step    580 | loss 1.7930 | lr 3.00e-04 | grad 2.62 | tok/s 17852
step    590 | loss 2.1412 | lr 3.00e-04 | grad 3.09 | tok/s 18294
step    600 | loss 2.0435 | lr 3.00e-04 | grad 2.28 | tok/s 17675
step    610 | loss 1.8533 | lr 3.00e-04 | grad 2.09 | tok/s 18595
step    620 | loss 1.7301 | lr 3.00e-04 | grad 2.30 | tok/s 17613
step    630 | loss 1.8879 | lr 3.00e-04 | grad 4.06 | tok/s 17782
step    640 | loss 2.0632 | lr 3.00e-04 | grad 2.22 | tok/s 18244
step    650 | loss 1.8991 | lr 3.00e-04 | grad 2.45 | tok/s 18318
step    660 | loss 1.9268 | lr 3.00e-04 | grad 3.12 | tok/s 18438
step    670 | loss 2.2180 | lr 3.00e-04 | grad 25.75 | tok/s 18528
step    680 | loss 1.9412 | lr 3.00e-04 | grad 2.41 | tok/s 18186
step    690 | loss 2.1837 | lr 3.00e-04 | grad 3.56 | tok/s 18781
step    700 | loss 1.8565 | lr 3.00e-04 | grad 3.05 | tok/s 19124
step    710 | loss 1.8158 | lr 3.00e-04 | grad 2.22 | tok/s 17886
step    720 | loss 1.6847 | lr 3.00e-04 | grad 2.77 | tok/s 17635
step    730 | loss 1.6397 | lr 3.00e-04 | grad 2.48 | tok/s 19111
step    740 | loss 1.7657 | lr 3.00e-04 | grad 2.42 | tok/s 18864
step    750 | loss 1.5034 | lr 3.00e-04 | grad 2.30 | tok/s 19166
step    760 | loss 1.3835 | lr 3.00e-04 | grad 2.50 | tok/s 19165
step    770 | loss 1.3240 | lr 3.00e-04 | grad 1.84 | tok/s 19156
step    780 | loss 1.2783 | lr 3.00e-04 | grad 1.77 | tok/s 19171
step    790 | loss 1.3731 | lr 3.00e-04 | grad 3.19 | tok/s 18538
step    800 | loss 2.1802 | lr 3.00e-04 | grad 4.91 | tok/s 18464
step    810 | loss 1.9082 | lr 3.00e-04 | grad 1.99 | tok/s 18421
step    820 | loss 1.9198 | lr 3.00e-04 | grad 4.53 | tok/s 17658
step    830 | loss 1.8869 | lr 3.00e-04 | grad 2.45 | tok/s 18961
step    840 | loss 1.7087 | lr 3.00e-04 | grad 2.31 | tok/s 19153
step    850 | loss 1.8294 | lr 3.00e-04 | grad 2.03 | tok/s 19056
step    860 | loss 1.7768 | lr 3.00e-04 | grad 3.81 | tok/s 18837
step    870 | loss 1.7131 | lr 3.00e-04 | grad 2.53 | tok/s 18175
step    880 | loss 1.9395 | lr 3.00e-04 | grad 2.27 | tok/s 18237
step    890 | loss 1.8990 | lr 3.00e-04 | grad 2.92 | tok/s 18481
step    900 | loss 1.7636 | lr 3.00e-04 | grad 2.38 | tok/s 18525
step    910 | loss 1.6396 | lr 3.00e-04 | grad 3.30 | tok/s 18117
step    920 | loss 1.8099 | lr 3.00e-04 | grad 3.62 | tok/s 18838
step    930 | loss 1.8230 | lr 3.00e-04 | grad 3.42 | tok/s 18007
step    940 | loss 1.6709 | lr 3.00e-04 | grad 2.23 | tok/s 18960
step    950 | loss 1.8252 | lr 3.00e-04 | grad 2.91 | tok/s 19030
step    960 | loss 1.6845 | lr 3.00e-04 | grad 3.11 | tok/s 19087
step    970 | loss 1.9293 | lr 3.00e-04 | grad 3.39 | tok/s 17937
step    980 | loss 1.8313 | lr 3.00e-04 | grad 2.36 | tok/s 18416
step    990 | loss 1.6791 | lr 3.00e-04 | grad 2.12 | tok/s 18729
step   1000 | loss 2.1250 | lr 3.00e-04 | grad 13.50 | tok/s 18002
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1250.pt
step   1010 | loss 1.9438 | lr 3.00e-04 | grad 3.03 | tok/s 6845
step   1020 | loss 1.8331 | lr 3.00e-04 | grad 2.06 | tok/s 17591
step   1030 | loss 1.6840 | lr 3.00e-04 | grad 2.11 | tok/s 18281
step   1040 | loss 1.6615 | lr 3.00e-04 | grad 2.30 | tok/s 18882
step   1050 | loss 1.7942 | lr 3.00e-04 | grad 2.91 | tok/s 17482
step   1060 | loss 1.9675 | lr 3.00e-04 | grad 3.72 | tok/s 18834
step   1070 | loss 1.9591 | lr 3.00e-04 | grad 2.78 | tok/s 18786
step   1080 | loss 1.5729 | lr 3.00e-04 | grad 1.94 | tok/s 17060
step   1090 | loss 1.2925 | lr 3.00e-04 | grad 1.54 | tok/s 18815
step   1100 | loss 1.6202 | lr 3.00e-04 | grad 3.44 | tok/s 18259
step   1110 | loss 1.6416 | lr 3.00e-04 | grad 2.19 | tok/s 19177
step   1120 | loss 1.5134 | lr 3.00e-04 | grad 2.23 | tok/s 19179
step   1130 | loss 1.4534 | lr 3.00e-04 | grad 2.02 | tok/s 19174
step   1140 | loss 1.4348 | lr 3.00e-04 | grad 2.33 | tok/s 19157
step   1150 | loss 1.4590 | lr 3.00e-04 | grad 1.79 | tok/s 19104
step   1160 | loss 1.3663 | lr 3.00e-04 | grad 1.88 | tok/s 19181
step   1170 | loss 1.3884 | lr 3.00e-04 | grad 2.05 | tok/s 19180
step   1180 | loss 1.5108 | lr 3.00e-04 | grad 1.80 | tok/s 19159
step   1190 | loss 1.3901 | lr 3.00e-04 | grad 2.28 | tok/s 19163
step   1200 | loss 1.3808 | lr 3.00e-04 | grad 2.31 | tok/s 19136
step   1210 | loss 1.4190 | lr 3.00e-04 | grad 2.06 | tok/s 19135
step   1220 | loss 1.4331 | lr 3.00e-04 | grad 1.96 | tok/s 19193
step   1230 | loss 1.4079 | lr 3.00e-04 | grad 2.25 | tok/s 19190
step   1240 | loss 1.3565 | lr 3.00e-04 | grad 1.70 | tok/s 19196
step   1250 | loss 2.1746 | lr 3.00e-04 | grad 3.30 | tok/s 18153
step   1260 | loss 1.5736 | lr 3.00e-04 | grad 4.59 | tok/s 17974
step   1270 | loss 1.8379 | lr 3.00e-04 | grad 5.41 | tok/s 17928
step   1280 | loss 1.8598 | lr 3.00e-04 | grad 1.90 | tok/s 18477
step   1290 | loss 1.6533 | lr 3.00e-04 | grad 2.16 | tok/s 18346
step   1300 | loss 1.7139 | lr 3.00e-04 | grad 2.41 | tok/s 18486
step   1310 | loss 1.6463 | lr 3.00e-04 | grad 2.34 | tok/s 18810
step   1320 | loss 1.7760 | lr 3.00e-04 | grad 2.33 | tok/s 18838
step   1330 | loss 1.8241 | lr 3.00e-04 | grad 2.59 | tok/s 18843
step   1340 | loss 1.6857 | lr 3.00e-04 | grad 8.81 | tok/s 17970

Training complete! Final step: 1346
