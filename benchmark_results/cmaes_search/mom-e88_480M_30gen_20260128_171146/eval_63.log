Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_63/levelMoME88_100m_20260128_174906
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 474,670,872 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 13.1702 | lr 3.00e-04 | grad 17.12 | tok/s 9590
step     20 | loss 3.4357 | lr 3.00e-04 | grad 2.95 | tok/s 20646
step     30 | loss 3.4246 | lr 3.00e-04 | grad 4.34 | tok/s 21774
step     40 | loss 5.3662 | lr 3.00e-04 | grad 16.62 | tok/s 22155
step     50 | loss 4.5864 | lr 3.00e-04 | grad 11.69 | tok/s 22373
step     60 | loss 3.7799 | lr 3.00e-04 | grad 7.69 | tok/s 22350
step     70 | loss 3.1436 | lr 3.00e-04 | grad 9.88 | tok/s 22268
step     80 | loss 2.9363 | lr 3.00e-04 | grad 6.84 | tok/s 22250
step     90 | loss 2.6305 | lr 3.00e-04 | grad 4.91 | tok/s 22184
step    100 | loss 2.4103 | lr 3.00e-04 | grad 4.06 | tok/s 22176
step    110 | loss 2.5108 | lr 3.00e-04 | grad 3.81 | tok/s 21998
step    120 | loss 3.1270 | lr 3.00e-04 | grad 2.11 | tok/s 20948
step    130 | loss 2.3832 | lr 3.00e-04 | grad 5.12 | tok/s 21409
step    140 | loss 2.6556 | lr 3.00e-04 | grad 8.56 | tok/s 21495
step    150 | loss 2.1622 | lr 3.00e-04 | grad 7.34 | tok/s 21996
step    160 | loss 2.6476 | lr 3.00e-04 | grad 2.84 | tok/s 21234
step    170 | loss 2.5250 | lr 3.00e-04 | grad 2.11 | tok/s 20917
step    180 | loss 2.5087 | lr 3.00e-04 | grad 4.56 | tok/s 21406
step    190 | loss 2.2207 | lr 3.00e-04 | grad 2.25 | tok/s 21015
step    200 | loss 2.0259 | lr 3.00e-04 | grad 2.02 | tok/s 21903
step    210 | loss 2.1927 | lr 3.00e-04 | grad 8.06 | tok/s 20855
step    220 | loss 2.5303 | lr 3.00e-04 | grad 11.62 | tok/s 21063
step    230 | loss 2.3189 | lr 3.00e-04 | grad 3.20 | tok/s 21020
step    240 | loss 2.5749 | lr 3.00e-04 | grad 5.00 | tok/s 21315
step    250 | loss 2.0629 | lr 3.00e-04 | grad 1.81 | tok/s 21148
step    260 | loss 2.2026 | lr 3.00e-04 | grad 7.50 | tok/s 21728
step    270 | loss 2.1204 | lr 3.00e-04 | grad 1.81 | tok/s 21272
step    280 | loss 2.0429 | lr 3.00e-04 | grad 1.83 | tok/s 19998
step    290 | loss 1.9620 | lr 3.00e-04 | grad 2.44 | tok/s 20631
step    300 | loss 2.2449 | lr 3.00e-04 | grad 3.69 | tok/s 20790
step    310 | loss 1.9196 | lr 3.00e-04 | grad 1.92 | tok/s 20718
step    320 | loss 2.2123 | lr 3.00e-04 | grad 7.97 | tok/s 20952
step    330 | loss 1.9843 | lr 3.00e-04 | grad 2.17 | tok/s 21153
step    340 | loss 2.3149 | lr 3.00e-04 | grad 2.98 | tok/s 21098
step    350 | loss 2.1388 | lr 3.00e-04 | grad 2.39 | tok/s 21650
step    360 | loss 1.8601 | lr 3.00e-04 | grad 3.36 | tok/s 20767
step    370 | loss 1.8350 | lr 3.00e-04 | grad 2.17 | tok/s 21837
step    380 | loss 1.5676 | lr 3.00e-04 | grad 2.25 | tok/s 22045
step    390 | loss 1.4709 | lr 3.00e-04 | grad 1.78 | tok/s 22076
step    400 | loss 2.0669 | lr 3.00e-04 | grad 2.70 | tok/s 20889
step    410 | loss 2.0241 | lr 3.00e-04 | grad 2.70 | tok/s 21071
step    420 | loss 2.0151 | lr 3.00e-04 | grad 8.56 | tok/s 21989
step    430 | loss 1.9629 | lr 3.00e-04 | grad 2.14 | tok/s 21626
step    440 | loss 1.9789 | lr 3.00e-04 | grad 2.55 | tok/s 20946
step    450 | loss 1.8723 | lr 3.00e-04 | grad 1.68 | tok/s 21210
step    460 | loss 1.8932 | lr 3.00e-04 | grad 2.17 | tok/s 21493
step    470 | loss 1.8609 | lr 3.00e-04 | grad 4.41 | tok/s 21350
step    480 | loss 1.9175 | lr 3.00e-04 | grad 3.14 | tok/s 21812
step    490 | loss 1.9342 | lr 3.00e-04 | grad 2.89 | tok/s 20963
step    500 | loss 2.0866 | lr 3.00e-04 | grad 2.08 | tok/s 21296
step    510 | loss 1.9305 | lr 3.00e-04 | grad 1.90 | tok/s 20376
step    520 | loss 1.7642 | lr 3.00e-04 | grad 2.05 | tok/s 21304
step    530 | loss 1.9530 | lr 3.00e-04 | grad 2.56 | tok/s 20972
step    540 | loss 1.8594 | lr 3.00e-04 | grad 1.73 | tok/s 20525
step    550 | loss 1.5895 | lr 3.00e-04 | grad 3.55 | tok/s 21495
step    560 | loss 1.6828 | lr 3.00e-04 | grad 2.28 | tok/s 22076
step    570 | loss 1.5564 | lr 3.00e-04 | grad 2.20 | tok/s 22066
step    580 | loss 1.5050 | lr 3.00e-04 | grad 1.53 | tok/s 22061
step    590 | loss 1.5500 | lr 3.00e-04 | grad 1.70 | tok/s 22049
step    600 | loss 1.4954 | lr 3.00e-04 | grad 2.27 | tok/s 22047
step    610 | loss 1.5038 | lr 3.00e-04 | grad 1.80 | tok/s 22046
step    620 | loss 1.4962 | lr 3.00e-04 | grad 2.06 | tok/s 21992
step    630 | loss 2.0318 | lr 3.00e-04 | grad 8.56 | tok/s 20799
step    640 | loss 2.0160 | lr 3.00e-04 | grad 4.81 | tok/s 21017
step    650 | loss 1.7883 | lr 3.00e-04 | grad 2.06 | tok/s 21033
step    660 | loss 1.8248 | lr 3.00e-04 | grad 2.39 | tok/s 21809
step    670 | loss 1.8697 | lr 3.00e-04 | grad 5.69 | tok/s 21108
step    680 | loss 1.8726 | lr 3.00e-04 | grad 2.92 | tok/s 20781
step    690 | loss 1.8538 | lr 3.00e-04 | grad 2.47 | tok/s 20610
step    700 | loss 1.7089 | lr 3.00e-04 | grad 2.08 | tok/s 21073
step    710 | loss 1.8934 | lr 3.00e-04 | grad 5.19 | tok/s 20762
step    720 | loss 1.5546 | lr 3.00e-04 | grad 1.91 | tok/s 21522
step    730 | loss 1.6977 | lr 3.00e-04 | grad 1.80 | tok/s 21220
step    740 | loss 2.1249 | lr 3.00e-04 | grad 4.56 | tok/s 21770
step    750 | loss 1.8581 | lr 3.00e-04 | grad 2.02 | tok/s 22011
step    760 | loss 1.7667 | lr 3.00e-04 | grad 4.28 | tok/s 21562
step    770 | loss 1.7997 | lr 3.00e-04 | grad 2.23 | tok/s 21181
step    780 | loss 1.7043 | lr 3.00e-04 | grad 2.38 | tok/s 21340
step    790 | loss 1.9814 | lr 3.00e-04 | grad 5.34 | tok/s 21799

Training complete! Final step: 790
