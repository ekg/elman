Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_49/levelMoME88_100m_20260128_174347
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 480,842,384 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 5.0 minutes
step     10 | loss 15.7083 | lr 3.00e-04 | grad 35.25 | tok/s 5938
step     20 | loss 2.8296 | lr 3.00e-04 | grad 6.00 | tok/s 17636
step     30 | loss 2.7816 | lr 3.00e-04 | grad 3.34 | tok/s 17854
step     40 | loss 4.0127 | lr 3.00e-04 | grad 3.53 | tok/s 17084
step     50 | loss 3.5401 | lr 3.00e-04 | grad 91.50 | tok/s 17368
step     60 | loss 2.3078 | lr 3.00e-04 | grad 6.38 | tok/s 17860
step     70 | loss 2.2183 | lr 3.00e-04 | grad 4.75 | tok/s 18048
step     80 | loss 11.2668 | lr 3.00e-04 | grad 147.00 | tok/s 18113
step     90 | loss 5.6404 | lr 3.00e-04 | grad 5.06 | tok/s 18375
step    100 | loss 4.2583 | lr 3.00e-04 | grad 7.00 | tok/s 18342
step    110 | loss 3.9038 | lr 3.00e-04 | grad 18.62 | tok/s 18360
step    120 | loss 3.5836 | lr 3.00e-04 | grad 22.12 | tok/s 18308
step    130 | loss 3.3736 | lr 3.00e-04 | grad 8.62 | tok/s 18304
step    140 | loss 2.9435 | lr 3.00e-04 | grad 12.38 | tok/s 18222
step    150 | loss 3.0999 | lr 3.00e-04 | grad 14.75 | tok/s 18257
step    160 | loss 2.5476 | lr 3.00e-04 | grad 6.56 | tok/s 18231
step    170 | loss 2.6878 | lr 3.00e-04 | grad 14.06 | tok/s 18229
step    180 | loss 2.4203 | lr 3.00e-04 | grad 6.72 | tok/s 18197
step    190 | loss 2.5263 | lr 3.00e-04 | grad 11.94 | tok/s 18152
step    200 | loss 2.2561 | lr 3.00e-04 | grad 5.66 | tok/s 18207
step    210 | loss 2.2534 | lr 3.00e-04 | grad 13.56 | tok/s 18154
step    220 | loss 2.5061 | lr 3.00e-04 | grad 3.72 | tok/s 17917
step    230 | loss 3.2180 | lr 3.00e-04 | grad 5.44 | tok/s 17733
step    240 | loss 2.5426 | lr 3.00e-04 | grad 4.34 | tok/s 16893
step    250 | loss 2.3581 | lr 3.00e-04 | grad 2.50 | tok/s 17311
step    260 | loss 1.9830 | lr 3.00e-04 | grad 2.94 | tok/s 17815
step    270 | loss 2.4254 | lr 3.00e-04 | grad 3.09 | tok/s 17639
step    280 | loss 2.5432 | lr 3.00e-04 | grad 6.03 | tok/s 17324
step    290 | loss 2.6220 | lr 3.00e-04 | grad 3.81 | tok/s 18244
step    300 | loss 1.2213 | lr 3.00e-04 | grad 2.75 | tok/s 18227
step    310 | loss 2.8113 | lr 3.00e-04 | grad 3.19 | tok/s 17802
step    320 | loss 2.3815 | lr 3.00e-04 | grad 5.88 | tok/s 17481
step    330 | loss 2.2862 | lr 3.00e-04 | grad 2.86 | tok/s 16958
step    340 | loss 2.6119 | lr 3.00e-04 | grad 2.52 | tok/s 17223
step    350 | loss 2.3406 | lr 3.00e-04 | grad 4.50 | tok/s 17590
step    360 | loss 2.3154 | lr 3.00e-04 | grad 5.78 | tok/s 17951
step    370 | loss 2.1675 | lr 3.00e-04 | grad 2.81 | tok/s 16407
step    380 | loss 2.0761 | lr 3.00e-04 | grad 2.77 | tok/s 17424
step    390 | loss 1.8338 | lr 3.00e-04 | grad 2.19 | tok/s 18097
step    400 | loss 1.8453 | lr 3.00e-04 | grad 2.66 | tok/s 17956
step    410 | loss 1.7557 | lr 3.00e-04 | grad 1.98 | tok/s 17578
step    420 | loss 2.1311 | lr 3.00e-04 | grad 5.34 | tok/s 16852
step    430 | loss 2.4679 | lr 3.00e-04 | grad 3.14 | tok/s 17897
step    440 | loss 2.4295 | lr 3.00e-04 | grad 3.81 | tok/s 16962
step    450 | loss 2.2598 | lr 3.00e-04 | grad 2.56 | tok/s 17508
step    460 | loss 2.0809 | lr 3.00e-04 | grad 13.38 | tok/s 17162
step    470 | loss 2.1854 | lr 3.00e-04 | grad 2.66 | tok/s 17658
step    480 | loss 2.6636 | lr 3.00e-04 | grad 10.00 | tok/s 17683
step    490 | loss 2.0936 | lr 3.00e-04 | grad 2.81 | tok/s 16745
step    500 | loss 2.0196 | lr 3.00e-04 | grad 3.64 | tok/s 17785
step    510 | loss 2.0145 | lr 3.00e-04 | grad 2.25 | tok/s 18057
step    520 | loss 2.0163 | lr 3.00e-04 | grad 2.53 | tok/s 18029
step    530 | loss 2.2324 | lr 3.00e-04 | grad 2.80 | tok/s 17353
step    540 | loss 1.9838 | lr 3.00e-04 | grad 2.52 | tok/s 17331
step    550 | loss 1.8148 | lr 3.00e-04 | grad 3.05 | tok/s 16982
step    560 | loss 2.0233 | lr 3.00e-04 | grad 2.39 | tok/s 16603
step    570 | loss 1.9819 | lr 3.00e-04 | grad 3.61 | tok/s 17042
step    580 | loss 1.8283 | lr 3.00e-04 | grad 2.86 | tok/s 16998
step    590 | loss 2.2096 | lr 3.00e-04 | grad 3.16 | tok/s 17402
step    600 | loss 2.0944 | lr 3.00e-04 | grad 2.55 | tok/s 16848
step    610 | loss 1.9024 | lr 3.00e-04 | grad 2.31 | tok/s 17656
step    620 | loss 1.7822 | lr 3.00e-04 | grad 2.48 | tok/s 16767
step    630 | loss 1.9358 | lr 3.00e-04 | grad 4.44 | tok/s 16915
step    640 | loss 2.1141 | lr 3.00e-04 | grad 2.53 | tok/s 17312
step    650 | loss 1.9087 | lr 3.00e-04 | grad 2.67 | tok/s 17421
step    660 | loss 1.9775 | lr 3.00e-04 | grad 4.59 | tok/s 17485
step    670 | loss 2.2416 | lr 3.00e-04 | grad 18.00 | tok/s 17608
step    680 | loss 1.9572 | lr 3.00e-04 | grad 2.81 | tok/s 17256
step    690 | loss 2.2423 | lr 3.00e-04 | grad 3.36 | tok/s 17823
step    700 | loss 1.9254 | lr 3.00e-04 | grad 3.67 | tok/s 18173
step    710 | loss 1.8619 | lr 3.00e-04 | grad 2.22 | tok/s 17040
step    720 | loss 1.7245 | lr 3.00e-04 | grad 5.03 | tok/s 16798
step    730 | loss 1.6995 | lr 3.00e-04 | grad 2.73 | tok/s 18127
step    740 | loss 1.8325 | lr 3.00e-04 | grad 2.88 | tok/s 17890
step    750 | loss 1.5868 | lr 3.00e-04 | grad 2.80 | tok/s 18150
step    760 | loss 1.4435 | lr 3.00e-04 | grad 2.83 | tok/s 18174
step    770 | loss 1.3955 | lr 3.00e-04 | grad 2.08 | tok/s 18183
step    780 | loss 1.3558 | lr 3.00e-04 | grad 2.16 | tok/s 18160
step    790 | loss 1.4386 | lr 3.00e-04 | grad 3.44 | tok/s 17683
step    800 | loss 2.2398 | lr 3.00e-04 | grad 5.28 | tok/s 17590
step    810 | loss 1.9551 | lr 3.00e-04 | grad 2.45 | tok/s 17485
step    820 | loss 1.9911 | lr 3.00e-04 | grad 4.47 | tok/s 16844
step    830 | loss 1.9329 | lr 3.00e-04 | grad 2.92 | tok/s 18057
step    840 | loss 1.7640 | lr 3.00e-04 | grad 2.53 | tok/s 18214
step    850 | loss 1.8920 | lr 3.00e-04 | grad 2.48 | tok/s 18142
step    860 | loss 1.8298 | lr 3.00e-04 | grad 3.97 | tok/s 17913
step    870 | loss 1.8044 | lr 3.00e-04 | grad 3.86 | tok/s 17299
step    880 | loss 2.0190 | lr 3.00e-04 | grad 3.69 | tok/s 17396
step    890 | loss 1.9592 | lr 3.00e-04 | grad 3.38 | tok/s 17594
step    900 | loss 1.8318 | lr 3.00e-04 | grad 2.64 | tok/s 17631
step    910 | loss 1.6699 | lr 3.00e-04 | grad 3.73 | tok/s 17275
step    920 | loss 1.8549 | lr 3.00e-04 | grad 3.73 | tok/s 17947
step    930 | loss 1.8620 | lr 3.00e-04 | grad 3.67 | tok/s 17159
step    940 | loss 1.7329 | lr 3.00e-04 | grad 2.62 | tok/s 18051
step    950 | loss 1.8592 | lr 3.00e-04 | grad 19.25 | tok/s 18090
step    960 | loss 1.7569 | lr 3.00e-04 | grad 3.11 | tok/s 18141
step    970 | loss 1.9998 | lr 3.00e-04 | grad 3.59 | tok/s 17122
step    980 | loss 1.8899 | lr 3.00e-04 | grad 2.75 | tok/s 17567
step    990 | loss 1.7363 | lr 3.00e-04 | grad 2.41 | tok/s 17858
step   1000 | loss 2.1639 | lr 3.00e-04 | grad 17.25 | tok/s 17137
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1639.pt
step   1010 | loss 2.0085 | lr 3.00e-04 | grad 2.53 | tok/s 6192
step   1020 | loss 1.9162 | lr 3.00e-04 | grad 3.78 | tok/s 16945
step   1030 | loss 1.6684 | lr 3.00e-04 | grad 1.84 | tok/s 17600
step   1040 | loss 1.7710 | lr 3.00e-04 | grad 4.31 | tok/s 17912
step   1050 | loss 1.8300 | lr 3.00e-04 | grad 2.77 | tok/s 16875
step   1060 | loss 2.0082 | lr 3.00e-04 | grad 3.75 | tok/s 18058
step   1070 | loss 2.0056 | lr 3.00e-04 | grad 3.88 | tok/s 17769
step   1080 | loss 1.6234 | lr 3.00e-04 | grad 2.52 | tok/s 16520
step   1090 | loss 1.2967 | lr 3.00e-04 | grad 2.00 | tok/s 18285
step   1100 | loss 1.7944 | lr 3.00e-04 | grad 3.30 | tok/s 17496
step   1110 | loss 1.6499 | lr 3.00e-04 | grad 2.09 | tok/s 18365
step   1120 | loss 1.5637 | lr 3.00e-04 | grad 2.86 | tok/s 18308
step   1130 | loss 1.4958 | lr 3.00e-04 | grad 2.52 | tok/s 18291
step   1140 | loss 1.4908 | lr 3.00e-04 | grad 2.08 | tok/s 18331
step   1150 | loss 1.5047 | lr 3.00e-04 | grad 2.12 | tok/s 18284
step   1160 | loss 1.4070 | lr 3.00e-04 | grad 2.19 | tok/s 18275
step   1170 | loss 1.4598 | lr 3.00e-04 | grad 2.83 | tok/s 18311
step   1180 | loss 1.5348 | lr 3.00e-04 | grad 1.93 | tok/s 18301
step   1190 | loss 1.4487 | lr 3.00e-04 | grad 2.38 | tok/s 18294
step   1200 | loss 1.4431 | lr 3.00e-04 | grad 2.03 | tok/s 18255
step   1210 | loss 1.4603 | lr 3.00e-04 | grad 1.98 | tok/s 18260
step   1220 | loss 1.4747 | lr 3.00e-04 | grad 2.12 | tok/s 18269
step   1230 | loss 1.4658 | lr 3.00e-04 | grad 1.87 | tok/s 18266
step   1240 | loss 1.4592 | lr 3.00e-04 | grad 2.94 | tok/s 18145
step   1250 | loss 2.2704 | lr 3.00e-04 | grad 2.78 | tok/s 17350
step   1260 | loss 1.5989 | lr 3.00e-04 | grad 5.16 | tok/s 17215
step   1270 | loss 2.0044 | lr 3.00e-04 | grad 2.84 | tok/s 17132
step   1280 | loss 1.8602 | lr 3.00e-04 | grad 3.00 | tok/s 17810

Training complete! Final step: 1280
