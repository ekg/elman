Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_480M_30gen_20260128_171146/eval_67/levelMoME88_100m_20260128_175425
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 474,116,992 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 5.0 minutes
step     10 | loss 13.1199 | lr 3.00e-04 | grad 33.75 | tok/s 7833
step     20 | loss 3.5979 | lr 3.00e-04 | grad 24.50 | tok/s 13898
step     30 | loss 3.1439 | lr 3.00e-04 | grad 3.88 | tok/s 14771
step     40 | loss 5.9020 | lr 3.00e-04 | grad 17.12 | tok/s 14918
step     50 | loss 4.1865 | lr 3.00e-04 | grad 12.75 | tok/s 15118
step     60 | loss 3.8306 | lr 3.00e-04 | grad 10.06 | tok/s 15036
step     70 | loss 3.1430 | lr 3.00e-04 | grad 11.94 | tok/s 15011
step     80 | loss 2.9076 | lr 3.00e-04 | grad 5.41 | tok/s 15040
step     90 | loss 2.7497 | lr 3.00e-04 | grad 7.41 | tok/s 15039
step    100 | loss 2.5444 | lr 3.00e-04 | grad 5.84 | tok/s 15002
step    110 | loss 2.7282 | lr 3.00e-04 | grad 11.62 | tok/s 14862
step    120 | loss 2.8528 | lr 3.00e-04 | grad 3.22 | tok/s 14075
step    130 | loss 2.4163 | lr 3.00e-04 | grad 6.75 | tok/s 14613
step    140 | loss 2.8002 | lr 3.00e-04 | grad 7.88 | tok/s 14570
step    150 | loss 2.2654 | lr 3.00e-04 | grad 5.28 | tok/s 14908
step    160 | loss 2.5208 | lr 3.00e-04 | grad 2.72 | tok/s 14312
step    170 | loss 2.5620 | lr 3.00e-04 | grad 2.64 | tok/s 14312
step    180 | loss 2.4736 | lr 3.00e-04 | grad 2.62 | tok/s 14422
step    190 | loss 2.1775 | lr 3.00e-04 | grad 2.45 | tok/s 14519
step    200 | loss 2.0270 | lr 3.00e-04 | grad 2.30 | tok/s 14922
step    210 | loss 2.2254 | lr 3.00e-04 | grad 3.05 | tok/s 14143
step    220 | loss 2.5049 | lr 3.00e-04 | grad 15.81 | tok/s 14333
step    230 | loss 2.3119 | lr 3.00e-04 | grad 4.09 | tok/s 14190
step    240 | loss 2.5232 | lr 3.00e-04 | grad 2.48 | tok/s 14501
step    250 | loss 2.1034 | lr 3.00e-04 | grad 2.91 | tok/s 14507
step    260 | loss 2.2448 | lr 3.00e-04 | grad 3.27 | tok/s 14809
step    270 | loss 2.0331 | lr 3.00e-04 | grad 2.14 | tok/s 14330
step    280 | loss 2.0522 | lr 3.00e-04 | grad 1.96 | tok/s 13712
step    290 | loss 1.9462 | lr 3.00e-04 | grad 2.91 | tok/s 13967
step    300 | loss 2.2725 | lr 3.00e-04 | grad 2.28 | tok/s 14230
step    310 | loss 1.9186 | lr 3.00e-04 | grad 2.89 | tok/s 14001
step    320 | loss 2.1584 | lr 3.00e-04 | grad 2.88 | tok/s 14261
step    330 | loss 1.9875 | lr 3.00e-04 | grad 2.22 | tok/s 14378
step    340 | loss 2.3678 | lr 3.00e-04 | grad 3.05 | tok/s 14414
step    350 | loss 2.1428 | lr 3.00e-04 | grad 2.22 | tok/s 14800
step    360 | loss 1.8709 | lr 3.00e-04 | grad 2.42 | tok/s 14115
step    370 | loss 1.8070 | lr 3.00e-04 | grad 2.52 | tok/s 14883
step    380 | loss 1.5553 | lr 3.00e-04 | grad 2.11 | tok/s 15008
step    390 | loss 1.4454 | lr 3.00e-04 | grad 2.78 | tok/s 14990
step    400 | loss 2.1581 | lr 3.00e-04 | grad 2.59 | tok/s 14214
step    410 | loss 2.0439 | lr 3.00e-04 | grad 2.70 | tok/s 14344
step    420 | loss 1.9950 | lr 3.00e-04 | grad 15.62 | tok/s 14970
step    430 | loss 1.9346 | lr 3.00e-04 | grad 2.69 | tok/s 14605
step    440 | loss 2.0045 | lr 3.00e-04 | grad 3.22 | tok/s 14368
step    450 | loss 1.8575 | lr 3.00e-04 | grad 2.88 | tok/s 14326
step    460 | loss 1.8844 | lr 3.00e-04 | grad 1.79 | tok/s 14525
step    470 | loss 1.9295 | lr 3.00e-04 | grad 4.50 | tok/s 14665
step    480 | loss 1.9119 | lr 3.00e-04 | grad 2.72 | tok/s 14684
step    490 | loss 1.9321 | lr 3.00e-04 | grad 2.31 | tok/s 14407
step    500 | loss 2.0963 | lr 3.00e-04 | grad 2.89 | tok/s 14392
step    510 | loss 1.8997 | lr 3.00e-04 | grad 2.97 | tok/s 13727
step    520 | loss 1.7551 | lr 3.00e-04 | grad 2.17 | tok/s 14455
step    530 | loss 1.9838 | lr 3.00e-04 | grad 2.33 | tok/s 14419

Training complete! Final step: 538
