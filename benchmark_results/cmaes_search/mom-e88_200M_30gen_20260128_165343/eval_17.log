Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_200M_30gen_20260128_165343/eval_17/levelMoME88_100m_20260128_170418
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 196,755,736 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 5.0 minutes
step     10 | loss 11.6105 | lr 3.00e-04 | grad 7.53 | tok/s 15378
step     20 | loss 8.4332 | lr 3.00e-04 | grad 23.00 | tok/s 30020
step     30 | loss 4.7791 | lr 3.00e-04 | grad 8.12 | tok/s 30284
step     40 | loss 3.8537 | lr 3.00e-04 | grad 4.19 | tok/s 30240
step     50 | loss 3.1768 | lr 3.00e-04 | grad 2.31 | tok/s 30040
step     60 | loss 3.1675 | lr 3.00e-04 | grad 3.34 | tok/s 29012
step     70 | loss 3.1811 | lr 3.00e-04 | grad 8.50 | tok/s 29370
step     80 | loss 2.8829 | lr 3.00e-04 | grad 2.89 | tok/s 28900
step     90 | loss 2.8569 | lr 3.00e-04 | grad 2.61 | tok/s 28540
step    100 | loss 2.3467 | lr 3.00e-04 | grad 1.80 | tok/s 29181
step    110 | loss 2.7885 | lr 3.00e-04 | grad 4.31 | tok/s 28640
step    120 | loss 2.6163 | lr 3.00e-04 | grad 2.03 | tok/s 28742
step    130 | loss 2.4651 | lr 3.00e-04 | grad 1.97 | tok/s 29313
step    140 | loss 2.2263 | lr 3.00e-04 | grad 1.43 | tok/s 27865
step    150 | loss 2.3444 | lr 3.00e-04 | grad 1.74 | tok/s 28173
step    160 | loss 2.3170 | lr 3.00e-04 | grad 2.50 | tok/s 28457
step    170 | loss 2.5269 | lr 3.00e-04 | grad 2.44 | tok/s 28994
step    180 | loss 2.2344 | lr 3.00e-04 | grad 1.67 | tok/s 28772
step    190 | loss 2.0763 | lr 3.00e-04 | grad 2.12 | tok/s 30014
step    200 | loss 2.2278 | lr 3.00e-04 | grad 1.80 | tok/s 28822
step    210 | loss 2.3791 | lr 3.00e-04 | grad 2.62 | tok/s 29629
step    220 | loss 2.2147 | lr 3.00e-04 | grad 1.77 | tok/s 28813
step    230 | loss 2.1447 | lr 3.00e-04 | grad 2.14 | tok/s 28826
step    240 | loss 2.2681 | lr 3.00e-04 | grad 1.97 | tok/s 29339
step    250 | loss 2.2442 | lr 3.00e-04 | grad 1.60 | tok/s 28491
step    260 | loss 2.0562 | lr 3.00e-04 | grad 2.42 | tok/s 28331
step    270 | loss 2.0565 | lr 3.00e-04 | grad 1.47 | tok/s 28540
step    280 | loss 1.9926 | lr 3.00e-04 | grad 1.73 | tok/s 29737
step    290 | loss 1.8390 | lr 3.00e-04 | grad 1.37 | tok/s 29951
step    300 | loss 1.8468 | lr 3.00e-04 | grad 1.46 | tok/s 30000
step    310 | loss 1.9476 | lr 3.00e-04 | grad 2.52 | tok/s 29412
step    320 | loss 2.0979 | lr 3.00e-04 | grad 2.42 | tok/s 28600
step    330 | loss 2.0986 | lr 3.00e-04 | grad 3.20 | tok/s 29254
step    340 | loss 2.1611 | lr 3.00e-04 | grad 3.98 | tok/s 28258
step    350 | loss 2.0519 | lr 3.00e-04 | grad 4.34 | tok/s 28266
step    360 | loss 1.9970 | lr 3.00e-04 | grad 6.03 | tok/s 29150
step    370 | loss 2.4562 | lr 3.00e-04 | grad 5.66 | tok/s 29283
step    380 | loss 2.0022 | lr 3.00e-04 | grad 1.45 | tok/s 29385
step    390 | loss 2.0234 | lr 3.00e-04 | grad 2.27 | tok/s 29130
step    400 | loss 2.0210 | lr 3.00e-04 | grad 1.41 | tok/s 28933
step    410 | loss 2.0444 | lr 3.00e-04 | grad 2.30 | tok/s 28206
step    420 | loss 2.1363 | lr 3.00e-04 | grad 7.62 | tok/s 28878
step    430 | loss 2.2828 | lr 3.00e-04 | grad 3.53 | tok/s 29369
step    440 | loss 1.9951 | lr 3.00e-04 | grad 1.59 | tok/s 28665
step    450 | loss 2.0035 | lr 3.00e-04 | grad 4.06 | tok/s 28480
step    460 | loss 1.9907 | lr 3.00e-04 | grad 1.34 | tok/s 28919
step    470 | loss 1.8981 | lr 3.00e-04 | grad 1.42 | tok/s 27953
step    480 | loss 1.8477 | lr 3.00e-04 | grad 1.38 | tok/s 28492
step    490 | loss 2.6302 | lr 3.00e-04 | grad 2.20 | tok/s 29544
step    500 | loss 1.8810 | lr 3.00e-04 | grad 1.88 | tok/s 28862
step    510 | loss 1.8608 | lr 3.00e-04 | grad 1.57 | tok/s 29426
step    520 | loss 2.3628 | lr 3.00e-04 | grad 2.44 | tok/s 28892
step    530 | loss 1.8145 | lr 3.00e-04 | grad 2.45 | tok/s 29210

Training complete! Final step: 539
