Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_200M_30gen_20260128_165343/eval_11/levelMoME88_100m_20260128_165905
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 206,649,824 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 5.0 minutes
step     10 | loss 8.3940 | lr 3.00e-04 | grad 16.62 | tok/s 13726
step     20 | loss 6.3354 | lr 3.00e-04 | grad 25.62 | tok/s 25071
step     30 | loss 4.6266 | lr 3.00e-04 | grad 18.00 | tok/s 25181
step     40 | loss 3.7375 | lr 3.00e-04 | grad 3.62 | tok/s 25045
step     50 | loss 3.1522 | lr 3.00e-04 | grad 1.94 | tok/s 24867
step     60 | loss 3.1101 | lr 3.00e-04 | grad 2.48 | tok/s 24019
step     70 | loss 3.0440 | lr 3.00e-04 | grad 7.56 | tok/s 24367
step     80 | loss 2.8787 | lr 3.00e-04 | grad 2.25 | tok/s 23983
step     90 | loss 2.8382 | lr 3.00e-04 | grad 2.61 | tok/s 23693
step    100 | loss 2.3895 | lr 3.00e-04 | grad 1.69 | tok/s 24263
step    110 | loss 2.8002 | lr 3.00e-04 | grad 4.12 | tok/s 23795
step    120 | loss 2.6381 | lr 3.00e-04 | grad 1.47 | tok/s 23807
step    130 | loss 2.4699 | lr 3.00e-04 | grad 1.33 | tok/s 24294
step    140 | loss 2.2381 | lr 3.00e-04 | grad 1.15 | tok/s 23075
step    150 | loss 2.3236 | lr 3.00e-04 | grad 1.49 | tok/s 23404
step    160 | loss 2.3179 | lr 3.00e-04 | grad 1.34 | tok/s 23573
step    170 | loss 2.5092 | lr 3.00e-04 | grad 5.78 | tok/s 24076
step    180 | loss 2.2182 | lr 3.00e-04 | grad 1.40 | tok/s 23882
step    190 | loss 2.0574 | lr 3.00e-04 | grad 1.59 | tok/s 24875
step    200 | loss 2.2081 | lr 3.00e-04 | grad 1.57 | tok/s 23917
step    210 | loss 2.3654 | lr 3.00e-04 | grad 2.11 | tok/s 24619
step    220 | loss 2.2063 | lr 3.00e-04 | grad 1.70 | tok/s 23918
step    230 | loss 2.1384 | lr 3.00e-04 | grad 1.48 | tok/s 23910
step    240 | loss 2.2308 | lr 3.00e-04 | grad 1.61 | tok/s 24315
step    250 | loss 2.2345 | lr 3.00e-04 | grad 1.42 | tok/s 23628
step    260 | loss 2.0471 | lr 3.00e-04 | grad 1.91 | tok/s 23539
step    270 | loss 2.0386 | lr 3.00e-04 | grad 2.22 | tok/s 23730
step    280 | loss 1.9829 | lr 3.00e-04 | grad 1.52 | tok/s 24731
step    290 | loss 1.8362 | lr 3.00e-04 | grad 1.32 | tok/s 24880
step    300 | loss 1.8516 | lr 3.00e-04 | grad 1.23 | tok/s 24865
step    310 | loss 1.9236 | lr 3.00e-04 | grad 1.52 | tok/s 24385
step    320 | loss 2.0660 | lr 3.00e-04 | grad 2.06 | tok/s 23744
step    330 | loss 2.0630 | lr 3.00e-04 | grad 2.47 | tok/s 24223
step    340 | loss 2.1307 | lr 3.00e-04 | grad 3.55 | tok/s 23447
step    350 | loss 2.0267 | lr 3.00e-04 | grad 3.73 | tok/s 23451
step    360 | loss 1.9685 | lr 3.00e-04 | grad 6.50 | tok/s 24181
step    370 | loss 2.3943 | lr 3.00e-04 | grad 2.23 | tok/s 24302
step    380 | loss 1.9701 | lr 3.00e-04 | grad 1.18 | tok/s 24375
step    390 | loss 1.9925 | lr 3.00e-04 | grad 1.98 | tok/s 24125
step    400 | loss 1.9943 | lr 3.00e-04 | grad 1.20 | tok/s 23965
step    410 | loss 2.0145 | lr 3.00e-04 | grad 1.88 | tok/s 23375
step    420 | loss 2.0807 | lr 3.00e-04 | grad 7.94 | tok/s 23933
step    430 | loss 2.2384 | lr 3.00e-04 | grad 2.62 | tok/s 24343
step    440 | loss 1.9587 | lr 3.00e-04 | grad 1.35 | tok/s 23869

Training complete! Final step: 448
