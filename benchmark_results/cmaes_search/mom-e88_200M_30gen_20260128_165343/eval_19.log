Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_200M_30gen_20260128_165343/eval_19/levelMoME88_100m_20260128_170418
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 198,849,964 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 5.0 minutes
step     10 | loss 14.3221 | lr 3.00e-04 | grad 12.25 | tok/s 15653
step     20 | loss 7.3255 | lr 3.00e-04 | grad 12.50 | tok/s 31944
step     30 | loss 4.4875 | lr 3.00e-04 | grad 6.06 | tok/s 32229
step     40 | loss 3.3868 | lr 3.00e-04 | grad 6.91 | tok/s 32169
step     50 | loss 2.8770 | lr 3.00e-04 | grad 3.55 | tok/s 31994
step     60 | loss 3.1733 | lr 3.00e-04 | grad 3.53 | tok/s 30892
step     70 | loss 3.0104 | lr 3.00e-04 | grad 10.00 | tok/s 31346
step     80 | loss 2.8633 | lr 3.00e-04 | grad 6.72 | tok/s 30760
step     90 | loss 2.8196 | lr 3.00e-04 | grad 3.41 | tok/s 30401
step    100 | loss 2.3223 | lr 3.00e-04 | grad 2.92 | tok/s 31126
step    110 | loss 2.7557 | lr 3.00e-04 | grad 4.47 | tok/s 30494
step    120 | loss 2.5672 | lr 3.00e-04 | grad 1.78 | tok/s 30598
step    130 | loss 2.4297 | lr 3.00e-04 | grad 1.96 | tok/s 31216
step    140 | loss 2.1903 | lr 3.00e-04 | grad 1.71 | tok/s 29679
step    150 | loss 2.2817 | lr 3.00e-04 | grad 2.05 | tok/s 30031
step    160 | loss 2.2845 | lr 3.00e-04 | grad 1.70 | tok/s 30224
step    170 | loss 2.4726 | lr 3.00e-04 | grad 3.45 | tok/s 30828
step    180 | loss 2.1737 | lr 3.00e-04 | grad 1.93 | tok/s 30559
step    190 | loss 1.9769 | lr 3.00e-04 | grad 2.58 | tok/s 31877
step    200 | loss 2.1765 | lr 3.00e-04 | grad 1.87 | tok/s 30608
step    210 | loss 2.3146 | lr 3.00e-04 | grad 2.73 | tok/s 31470
step    220 | loss 2.1776 | lr 3.00e-04 | grad 2.27 | tok/s 30574
step    230 | loss 2.0994 | lr 3.00e-04 | grad 2.11 | tok/s 30582
step    240 | loss 2.2109 | lr 3.00e-04 | grad 2.42 | tok/s 31126
step    250 | loss 2.2063 | lr 3.00e-04 | grad 1.91 | tok/s 30234
step    260 | loss 2.0273 | lr 3.00e-04 | grad 2.47 | tok/s 30107
step    270 | loss 2.0061 | lr 3.00e-04 | grad 1.69 | tok/s 30302
step    280 | loss 1.9434 | lr 3.00e-04 | grad 1.80 | tok/s 31619
step    290 | loss 1.7762 | lr 3.00e-04 | grad 1.76 | tok/s 31849
step    300 | loss 1.7860 | lr 3.00e-04 | grad 1.78 | tok/s 31804
step    310 | loss 1.8992 | lr 3.00e-04 | grad 2.17 | tok/s 31242
step    320 | loss 2.0454 | lr 3.00e-04 | grad 3.09 | tok/s 30426
step    330 | loss 2.0426 | lr 3.00e-04 | grad 3.45 | tok/s 31053
step    340 | loss 2.1125 | lr 3.00e-04 | grad 5.25 | tok/s 29925
step    350 | loss 2.0054 | lr 3.00e-04 | grad 5.53 | tok/s 29952
step    360 | loss 1.9407 | lr 3.00e-04 | grad 5.22 | tok/s 30887
step    370 | loss 2.3658 | lr 3.00e-04 | grad 3.19 | tok/s 31031
step    380 | loss 1.9358 | lr 3.00e-04 | grad 1.73 | tok/s 31140
step    390 | loss 1.9723 | lr 3.00e-04 | grad 2.66 | tok/s 30923
step    400 | loss 1.9414 | lr 3.00e-04 | grad 1.67 | tok/s 30703
step    410 | loss 1.9799 | lr 3.00e-04 | grad 2.59 | tok/s 29914
step    420 | loss 2.0461 | lr 3.00e-04 | grad 7.09 | tok/s 30663
step    430 | loss 2.1571 | lr 3.00e-04 | grad 3.58 | tok/s 31154
step    440 | loss 1.9433 | lr 3.00e-04 | grad 1.69 | tok/s 30595
step    450 | loss 1.9589 | lr 3.00e-04 | grad 4.34 | tok/s 30443
step    460 | loss 1.9290 | lr 3.00e-04 | grad 1.64 | tok/s 29267
step    470 | loss 1.8487 | lr 3.00e-04 | grad 1.66 | tok/s 29651
step    480 | loss 1.7994 | lr 3.00e-04 | grad 1.76 | tok/s 30075
step    490 | loss 2.4372 | lr 3.00e-04 | grad 2.42 | tok/s 31182
step    500 | loss 1.7976 | lr 3.00e-04 | grad 2.03 | tok/s 30620
step    510 | loss 1.7906 | lr 3.00e-04 | grad 2.00 | tok/s 31215
step    520 | loss 2.3292 | lr 3.00e-04 | grad 3.45 | tok/s 30716
step    530 | loss 1.7670 | lr 3.00e-04 | grad 2.42 | tok/s 30912
step    540 | loss 1.7150 | lr 3.00e-04 | grad 1.80 | tok/s 31398
step    550 | loss 1.5709 | lr 3.00e-04 | grad 1.74 | tok/s 31783
step    560 | loss 1.8560 | lr 3.00e-04 | grad 4.81 | tok/s 31352
step    570 | loss 2.1477 | lr 3.00e-04 | grad 2.11 | tok/s 31146

Training complete! Final step: 572
