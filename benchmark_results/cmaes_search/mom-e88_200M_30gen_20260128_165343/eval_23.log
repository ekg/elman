Using device: cuda
Output directory: benchmark_results/cmaes_search/mom-e88_200M_30gen_20260128_165343/eval_23/levelMoME88_100m_20260128_170417
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level MoME88, 202,342,216 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 5.0 minutes
step     10 | loss 11.2705 | lr 3.00e-04 | grad 5.38 | tok/s 15411
step     20 | loss 7.0990 | lr 3.00e-04 | grad 15.44 | tok/s 30063
step     30 | loss 4.5989 | lr 3.00e-04 | grad 5.72 | tok/s 30231
step     40 | loss 3.6382 | lr 3.00e-04 | grad 6.66 | tok/s 30199
step     50 | loss 3.1141 | lr 3.00e-04 | grad 2.91 | tok/s 29907
step     60 | loss 3.1396 | lr 3.00e-04 | grad 1.93 | tok/s 28836
step     70 | loss 3.0586 | lr 3.00e-04 | grad 13.19 | tok/s 29103
step     80 | loss 2.9026 | lr 3.00e-04 | grad 2.62 | tok/s 28612
step     90 | loss 2.8453 | lr 3.00e-04 | grad 2.66 | tok/s 28167
step    100 | loss 2.3753 | lr 3.00e-04 | grad 1.87 | tok/s 28788
step    110 | loss 2.7948 | lr 3.00e-04 | grad 3.59 | tok/s 28168
step    120 | loss 2.6481 | lr 3.00e-04 | grad 2.81 | tok/s 28129
step    130 | loss 2.4707 | lr 3.00e-04 | grad 1.62 | tok/s 28694
step    140 | loss 2.2289 | lr 3.00e-04 | grad 1.56 | tok/s 27194
step    150 | loss 2.3231 | lr 3.00e-04 | grad 2.23 | tok/s 27471
step    160 | loss 2.3112 | lr 3.00e-04 | grad 1.49 | tok/s 27605
step    170 | loss 2.5292 | lr 3.00e-04 | grad 3.09 | tok/s 28170
step    180 | loss 2.2203 | lr 3.00e-04 | grad 1.82 | tok/s 27824
step    190 | loss 2.0447 | lr 3.00e-04 | grad 2.02 | tok/s 28993
step    200 | loss 2.2114 | lr 3.00e-04 | grad 1.81 | tok/s 27813
step    210 | loss 2.3845 | lr 3.00e-04 | grad 2.25 | tok/s 28548
step    220 | loss 2.2041 | lr 3.00e-04 | grad 1.91 | tok/s 27747
step    230 | loss 2.1360 | lr 3.00e-04 | grad 1.63 | tok/s 27734
step    240 | loss 2.2487 | lr 3.00e-04 | grad 1.96 | tok/s 28219
step    250 | loss 2.2321 | lr 3.00e-04 | grad 1.68 | tok/s 27378
step    260 | loss 2.0464 | lr 3.00e-04 | grad 2.03 | tok/s 27221
step    270 | loss 2.0504 | lr 3.00e-04 | grad 1.48 | tok/s 27428
step    280 | loss 2.0123 | lr 3.00e-04 | grad 1.62 | tok/s 28526
step    290 | loss 1.8477 | lr 3.00e-04 | grad 1.48 | tok/s 28738
step    300 | loss 1.8616 | lr 3.00e-04 | grad 1.48 | tok/s 28738
step    310 | loss 1.9452 | lr 3.00e-04 | grad 1.91 | tok/s 28158
step    320 | loss 2.0934 | lr 3.00e-04 | grad 2.42 | tok/s 27358
step    330 | loss 2.0782 | lr 3.00e-04 | grad 3.17 | tok/s 27970
step    340 | loss 2.1476 | lr 3.00e-04 | grad 3.53 | tok/s 26985
step    350 | loss 2.0383 | lr 3.00e-04 | grad 4.34 | tok/s 26965
step    360 | loss 1.9771 | lr 3.00e-04 | grad 4.16 | tok/s 27774
step    370 | loss 2.4140 | lr 3.00e-04 | grad 2.39 | tok/s 27923
step    380 | loss 1.9788 | lr 3.00e-04 | grad 1.30 | tok/s 28043
step    390 | loss 2.0057 | lr 3.00e-04 | grad 2.02 | tok/s 27791
step    400 | loss 2.0116 | lr 3.00e-04 | grad 1.34 | tok/s 27559
step    410 | loss 2.0282 | lr 3.00e-04 | grad 2.28 | tok/s 26848
step    420 | loss 2.0874 | lr 3.00e-04 | grad 6.09 | tok/s 27499
step    430 | loss 2.2712 | lr 3.00e-04 | grad 3.03 | tok/s 27959
step    440 | loss 1.9759 | lr 3.00e-04 | grad 1.49 | tok/s 27430
step    450 | loss 1.9757 | lr 3.00e-04 | grad 3.45 | tok/s 27313
step    460 | loss 1.9698 | lr 3.00e-04 | grad 1.47 | tok/s 27689
step    470 | loss 1.8835 | lr 3.00e-04 | grad 1.51 | tok/s 26775
step    480 | loss 1.8371 | lr 3.00e-04 | grad 1.39 | tok/s 27132
step    490 | loss 2.5797 | lr 3.00e-04 | grad 2.02 | tok/s 28188
step    500 | loss 1.8515 | lr 3.00e-04 | grad 1.67 | tok/s 27595
step    510 | loss 1.8376 | lr 3.00e-04 | grad 1.77 | tok/s 28118
step    520 | loss 2.3551 | lr 3.00e-04 | grad 2.44 | tok/s 26642

Training complete! Final step: 522
