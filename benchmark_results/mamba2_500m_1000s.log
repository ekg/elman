Using p50k_base tokenizer with vocab size 50,281
Loading data from data/fineweb_500mb.txt...
Pre-tokenizing data/fineweb_500mb.txt...
  Tokenized 0/521,845,311 chars...
  Tokenized 100,000,000/521,845,311 chars...
  Tokenized 200,000,000/521,845,311 chars...
  Tokenized 300,000,000/521,845,311 chars...
  Tokenized 400,000,000/521,845,311 chars...
  Tokenized 500,000,000/521,845,311 chars...
Saved 113,733,867 tokens to data/fineweb_500mb.txt.p50k_base.tokens.npy
Loaded 113,733,867 tokens from cache: data/fineweb_500mb.txt.p50k_base.tokens.npy

Creating mamba2 model with ~500m parameters...
Created Mamba2 model: dim=1536, depth=24, expand=2, params=428,730,240

============================================================
Training: mamba2 (timeout=1000.0s)
Parameters: 428.73M
Vocab size: 50,281
============================================================
[mamba2] step    1 | loss 11.1250 | ppl 67846.3 | grad 2.95 | 722 tok/s | 62.4s | 62417ms/step
[mamba2] step   20 | loss 8.8750 | ppl 7150.9 | grad 2.20 | 9701 tok/s | 92.9s | 1620ms/step
[mamba2] step   40 | loss 7.5000 | ppl 1808.0 | grad 1.19 | 14331 tok/s | 125.8s | 1657ms/step
[mamba2] step   60 | loss 7.1250 | ppl 1242.6 | grad 1.38 | 16986 tok/s | 159.2s | 1681ms/step
[mamba2] step   80 | loss 6.8750 | ppl 967.8 | grad 1.27 | 18693 tok/s | 192.8s | 1673ms/step
[mamba2] step  100 | loss 6.6875 | ppl 802.3 | grad 1.44 | 19881 tok/s | 226.6s | 1699ms/step
[mamba2] step  120 | loss 6.3750 | ppl 587.0 | grad 1.20 | 20752 tok/s | 260.5s | 1697ms/step
[mamba2] step  140 | loss 6.2188 | ppl 502.1 | grad 1.20 | 21418 tok/s | 294.5s | 1694ms/step
[mamba2] step  160 | loss 6.1250 | ppl 457.1 | grad 0.70 | 21942 tok/s | 328.6s | 1700ms/step
[mamba2] step  180 | loss 6.0000 | ppl 403.4 | grad 0.82 | 22364 tok/s | 362.6s | 1703ms/step
[mamba2] step  200 | loss 5.9688 | ppl 391.0 | grad 0.85 | 22716 tok/s | 396.7s | 1700ms/step
[mamba2] step  220 | loss 5.8438 | ppl 345.1 | grad 0.48 | 23015 tok/s | 430.7s | 1700ms/step
[mamba2] step  240 | loss 5.8750 | ppl 356.0 | grad 0.95 | 23273 tok/s | 464.6s | 1696ms/step
[mamba2] step  260 | loss 5.8438 | ppl 345.1 | grad 0.77 | 23497 tok/s | 498.6s | 1698ms/step
[mamba2] step  280 | loss 5.7500 | ppl 314.2 | grad 0.65 | 23691 tok/s | 532.5s | 1697ms/step
[mamba2] step  300 | loss 5.6875 | ppl 295.2 | grad 0.79 | 23863 tok/s | 566.4s | 1694ms/step
[mamba2] step  320 | loss 5.6562 | ppl 286.1 | grad 0.52 | 24017 tok/s | 600.3s | 1699ms/step
[mamba2] step  340 | loss 5.5938 | ppl 268.7 | grad 0.74 | 24152 tok/s | 634.3s | 1694ms/step
[mamba2] step  360 | loss 5.5625 | ppl 260.5 | grad 0.52 | 24275 tok/s | 668.2s | 1704ms/step
[mamba2] step  380 | loss 5.5625 | ppl 260.5 | grad 0.62 | 24386 tok/s | 702.1s | 1697ms/step
[mamba2] step  400 | loss 5.4688 | ppl 237.2 | grad 0.61 | 24487 tok/s | 736.0s | 1698ms/step
[mamba2] step  420 | loss 5.5625 | ppl 260.5 | grad 0.56 | 24578 tok/s | 769.9s | 1700ms/step
[mamba2] step  440 | loss 5.4375 | ppl 229.9 | grad 0.58 | 24662 tok/s | 803.9s | 1699ms/step
[mamba2] step  460 | loss 5.4688 | ppl 237.2 | grad 0.54 | 24740 tok/s | 837.7s | 1697ms/step
[mamba2] step  480 | loss 5.3750 | ppl 215.9 | grad 0.55 | 24811 tok/s | 871.7s | 1701ms/step
[mamba2] step  500 | loss 5.3438 | ppl 209.3 | grad 0.56 | 24878 tok/s | 905.5s | 1698ms/step
[mamba2] step  520 | loss 5.4062 | ppl 222.8 | grad 0.53 | 24940 tok/s | 939.4s | 1692ms/step
[mamba2] step  540 | loss 5.3438 | ppl 209.3 | grad 0.55 | 24994 tok/s | 973.4s | 1698ms/step
[mamba2] Timeout reached at 1000.0s

mamba2 Final: loss=6.2020, grad=0.92, steps=557, tokens=25,051,136, time=1000.0s

==========================================================================================
BENCHMARK SUMMARY
==========================================================================================
Model           Params       Loss       Steps    Tokens       tok/s      Time    
------------------------------------------------------------------------------------------
mamba2          428.73M      6.2020     557      25,051,136   25050      1000.0  s

Results saved to: benchmark_results
