
Benchmarking E5 backends on byte-level Pile
Config: dim=1536, depth=21, rank=256
Training: batch=16, seq=512, steps=500

============================================================
Backend: FUSED
Model: dim=1536, depth=21, rank=256
Parameters: 49,973,760
============================================================
step    50 | loss 10.1644 | tok/s 6,508
step   100 | loss 3.2869 | tok/s 6,568
step   150 | loss 3.2253 | tok/s 6,561
step   200 | loss 3.2228 | tok/s 6,570
step   250 | loss 3.1141 | tok/s 6,569
step   300 | loss 3.1806 | tok/s 6,567
step   350 | loss 3.1644 | tok/s 6,559
step   400 | loss 3.2019 | tok/s 6,588
step   450 | loss 3.2722 | tok/s 6,611
step   500 | loss 3.1781 | tok/s 6,606

============================================================
SUMMARY
============================================================

FUSED:
  Final loss:    3.1781
  Avg tok/s:     6,563
  Total time:    624.1s
  Peak memory:   2.04 GB

Results saved to benchmark_results/e5_backends_20260105_204449.json
