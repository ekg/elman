
Benchmarking E5 backends on byte-level Pile
Config: dim=1536, depth=21, rank=256
Training: batch=16, seq=512, steps=500

============================================================
Backend: ORIGINAL
Model: dim=1536, depth=21, rank=256
Parameters: 49,973,760
============================================================
step    50 | loss 10.1650 | tok/s 6,397
step   100 | loss 3.2897 | tok/s 6,537
step   150 | loss 3.2256 | tok/s 6,410
step   200 | loss 3.2216 | tok/s 6,284
step   250 | loss 3.1150 | tok/s 6,269
step   300 | loss 3.1781 | tok/s 6,402
step   350 | loss 3.1641 | tok/s 6,572
step   400 | loss 3.2000 | tok/s 6,570
step   450 | loss 3.2703 | tok/s 6,573
step   500 | loss 3.1766 | tok/s 6,624

============================================================
SUMMARY
============================================================

ORIGINAL:
  Final loss:    3.1766
  Avg tok/s:     6,454
  Total time:    634.6s
  Peak memory:   2.04 GB

Results saved to benchmark_results/e5_backends_20260105_204459.json
