Using p50k_base tokenizer with vocab size 50,281
Loading data from data/fineweb_500mb.txt...
Loaded 113,733,867 tokens from cache: data/fineweb_500mb.txt.p50k_base.tokens.npy

Creating 0 model with ~500m parameters...
Created Level 0 model: dim=1024, depth=18, params=499,885,056

============================================================
Training: 0 (timeout=1000.0s)
Parameters: 499.89M
Vocab size: 50,281
============================================================
[0] step    1 | loss 11.0000 | ppl 59874.1 | grad 4.44 | 25971 tok/s | 1.8s | 1764ms/step
[0] step   20 | loss 9.1250 | ppl 9182.0 | grad 2.00 | 29594 tok/s | 31.1s | 1576ms/step
[0] step   40 | loss 7.5625 | ppl 1924.7 | grad 1.34 | 29116 tok/s | 63.3s | 1641ms/step
[0] step   60 | loss 6.9688 | ppl 1062.9 | grad 0.82 | 28625 tok/s | 96.6s | 1684ms/step
[0] step   80 | loss 6.5938 | ppl 730.5 | grad 1.01 | 28250 tok/s | 130.5s | 1713ms/step
[0] step  100 | loss 6.5312 | ppl 686.3 | grad 1.16 | 27949 tok/s | 164.9s | 1720ms/step
[0] step  120 | loss 6.3125 | ppl 551.4 | grad 0.73 | 27736 tok/s | 199.4s | 1719ms/step
[0] step  140 | loss 6.2188 | ppl 502.1 | grad 0.94 | 27564 tok/s | 234.0s | 1734ms/step
[0] step  160 | loss 6.0625 | ppl 429.4 | grad 0.68 | 27427 tok/s | 268.8s | 1743ms/step
[0] step  180 | loss 6.0625 | ppl 429.4 | grad 0.71 | 27318 tok/s | 303.6s | 1740ms/step
[0] step  200 | loss 5.9688 | ppl 391.0 | grad 0.70 | 27242 tok/s | 338.3s | 1734ms/step
[0] step  220 | loss 5.9688 | ppl 391.0 | grad 0.88 | 27186 tok/s | 372.9s | 1723ms/step
[0] step  240 | loss 5.9375 | ppl 379.0 | grad 0.82 | 27141 tok/s | 407.5s | 1728ms/step
[0] step  260 | loss 5.9375 | ppl 379.0 | grad 0.88 | 27102 tok/s | 442.1s | 1729ms/step
[0] step  280 | loss 5.8438 | ppl 345.1 | grad 0.71 | 27069 tok/s | 476.6s | 1730ms/step
[0] step  300 | loss 5.7812 | ppl 324.2 | grad 0.77 | 27041 tok/s | 511.2s | 1729ms/step
[0] step  320 | loss 5.8438 | ppl 345.1 | grad 0.73 | 27016 tok/s | 545.8s | 1727ms/step
[0] step  340 | loss 5.7812 | ppl 324.2 | grad 0.82 | 26995 tok/s | 580.4s | 1730ms/step
[0] step  360 | loss 5.7500 | ppl 314.2 | grad 0.75 | 26976 tok/s | 614.9s | 1728ms/step
[0] step  380 | loss 5.7188 | ppl 304.5 | grad 0.80 | 26959 tok/s | 649.5s | 1732ms/step
[0] step  400 | loss 5.6562 | ppl 286.1 | grad 0.76 | 26942 tok/s | 684.1s | 1723ms/step
[0] step  420 | loss 5.6875 | ppl 295.2 | grad 0.77 | 26928 tok/s | 718.7s | 1724ms/step
[0] step  440 | loss 5.6250 | ppl 277.3 | grad 0.86 | 26914 tok/s | 753.3s | 1721ms/step
[0] step  460 | loss 5.6250 | ppl 277.3 | grad 0.78 | 26905 tok/s | 787.9s | 1737ms/step
[0] step  480 | loss 5.5938 | ppl 268.7 | grad 0.76 | 26893 tok/s | 822.5s | 1719ms/step
[0] step  500 | loss 5.6875 | ppl 295.2 | grad 0.77 | 26881 tok/s | 857.1s | 1737ms/step
[0] step  520 | loss 5.6250 | ppl 277.3 | grad 0.79 | 26869 tok/s | 891.8s | 1739ms/step
[0] step  540 | loss 5.6250 | ppl 277.3 | grad 0.78 | 26856 tok/s | 926.5s | 1735ms/step
[0] step  560 | loss 5.5938 | ppl 268.7 | grad 0.78 | 26844 tok/s | 961.3s | 1739ms/step
[0] step  580 | loss 5.5625 | ppl 260.5 | grad 0.77 | 26833 tok/s | 996.0s | 1739ms/step
[0] Timeout reached at 1001.2s

0 Final: loss=6.2417, grad=0.98, steps=584, tokens=26,864,640, time=1001.2s

==========================================================================================
BENCHMARK SUMMARY
==========================================================================================
Model           Params       Loss       Steps    Tokens       tok/s      Time    
------------------------------------------------------------------------------------------
0               499.89M      6.2417     584      26,864,640   26833      1001.2  s

Results saved to: benchmark_results
