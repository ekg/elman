Using device: cuda
Output directory: benchmark_results/100m_10min/mamba2/levelmamba2_100m_20260115_105403
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6858 | lr 2.70e-06 | grad 25.88 | tok/s 9371
step     20 | loss 5.6394 | lr 5.70e-06 | grad 6.38 | tok/s 69951
step     30 | loss 5.6997 | lr 8.70e-06 | grad 5.94 | tok/s 73672
step     40 | loss 5.6100 | lr 1.17e-05 | grad 6.31 | tok/s 73495
step     50 | loss 5.4238 | lr 1.47e-05 | grad 5.41 | tok/s 73161
step     60 | loss 5.1981 | lr 1.77e-05 | grad 10.06 | tok/s 71425
step     70 | loss 4.6322 | lr 2.07e-05 | grad 5.00 | tok/s 68960
step     80 | loss 4.1083 | lr 2.37e-05 | grad 6.47 | tok/s 71232
step     90 | loss 3.7915 | lr 2.67e-05 | grad 7.16 | tok/s 68649
step    100 | loss 3.2999 | lr 2.97e-05 | grad 2.89 | tok/s 69169
step    110 | loss 3.0213 | lr 3.27e-05 | grad 2.75 | tok/s 67935
step    120 | loss 3.1235 | lr 3.57e-05 | grad 2.39 | tok/s 66831
step    130 | loss 2.9063 | lr 3.87e-05 | grad 2.03 | tok/s 68042
step    140 | loss 2.5914 | lr 4.17e-05 | grad 1.51 | tok/s 68063
step    150 | loss 2.4205 | lr 4.47e-05 | grad 3.08 | tok/s 64770
step    160 | loss 2.2838 | lr 4.77e-05 | grad 2.95 | tok/s 65080
step    170 | loss 2.4262 | lr 5.07e-05 | grad 5.66 | tok/s 67110
step    180 | loss 2.4581 | lr 5.37e-05 | grad 2.28 | tok/s 67177
step    190 | loss 2.1357 | lr 5.67e-05 | grad 2.66 | tok/s 68105
step    200 | loss 1.8318 | lr 5.97e-05 | grad 1.88 | tok/s 69419
step    210 | loss 2.3216 | lr 6.27e-05 | grad 3.39 | tok/s 66355
step    220 | loss 2.1978 | lr 6.57e-05 | grad 2.08 | tok/s 68346
step    230 | loss 2.0372 | lr 6.87e-05 | grad 2.86 | tok/s 65827
step    240 | loss 2.0323 | lr 7.17e-05 | grad 3.22 | tok/s 67023
step    250 | loss 2.0139 | lr 7.47e-05 | grad 2.52 | tok/s 65975
step    260 | loss 2.0879 | lr 7.77e-05 | grad 1.72 | tok/s 63217
step    270 | loss 1.9242 | lr 8.07e-05 | grad 2.08 | tok/s 65418
step    280 | loss 1.8092 | lr 8.37e-05 | grad 3.00 | tok/s 65377
step    290 | loss 1.6949 | lr 8.67e-05 | grad 1.77 | tok/s 68703
step    300 | loss 1.5780 | lr 8.97e-05 | grad 1.87 | tok/s 68513
step    310 | loss 1.5324 | lr 9.27e-05 | grad 1.48 | tok/s 68373
step    320 | loss 1.7990 | lr 9.57e-05 | grad 3.19 | tok/s 65826
step    330 | loss 1.9118 | lr 9.87e-05 | grad 1.94 | tok/s 64220
step    340 | loss 1.8856 | lr 1.02e-04 | grad 3.97 | tok/s 65515
step    350 | loss 1.8834 | lr 1.05e-04 | grad 1.76 | tok/s 63542
step    360 | loss 1.8400 | lr 1.08e-04 | grad 4.03 | tok/s 64297
step    370 | loss 1.6364 | lr 1.11e-04 | grad 1.88 | tok/s 65390
step    380 | loss 2.2104 | lr 1.14e-04 | grad 2.06 | tok/s 66859
step    390 | loss 1.8060 | lr 1.17e-04 | grad 1.75 | tok/s 64440
step    400 | loss 1.9173 | lr 1.20e-04 | grad 4.19 | tok/s 66070
step    410 | loss 1.6696 | lr 1.23e-04 | grad 3.59 | tok/s 64203
step    420 | loss 1.8096 | lr 1.26e-04 | grad 2.36 | tok/s 63650
step    430 | loss 1.9334 | lr 1.29e-04 | grad 2.08 | tok/s 63430
step    440 | loss 1.9456 | lr 1.32e-04 | grad 1.58 | tok/s 65868
step    450 | loss 1.7977 | lr 1.35e-04 | grad 1.20 | tok/s 64172
step    460 | loss 1.7401 | lr 1.38e-04 | grad 1.30 | tok/s 64296
step    470 | loss 1.7294 | lr 1.41e-04 | grad 1.95 | tok/s 65100
step    480 | loss 1.6467 | lr 1.44e-04 | grad 1.19 | tok/s 62749
step    490 | loss 1.6004 | lr 1.47e-04 | grad 1.18 | tok/s 63889
step    500 | loss 2.2941 | lr 1.50e-04 | grad 2.11 | tok/s 65793
step    510 | loss 1.6650 | lr 1.53e-04 | grad 1.30 | tok/s 64343
step    520 | loss 1.6050 | lr 1.56e-04 | grad 1.32 | tok/s 66376
step    530 | loss 2.2329 | lr 1.59e-04 | grad 2.16 | tok/s 64954
step    540 | loss 1.6783 | lr 1.62e-04 | grad 1.80 | tok/s 64974
step    550 | loss 1.5195 | lr 1.65e-04 | grad 0.88 | tok/s 66723
step    560 | loss 1.3328 | lr 1.68e-04 | grad 0.82 | tok/s 67567
step    570 | loss 1.6854 | lr 1.71e-04 | grad 2.81 | tok/s 66090
step    580 | loss 1.9734 | lr 1.74e-04 | grad 1.22 | tok/s 65172
step    590 | loss 2.2211 | lr 1.77e-04 | grad 1.44 | tok/s 63919
step    600 | loss 1.7255 | lr 1.80e-04 | grad 1.86 | tok/s 63988
step    610 | loss 1.7424 | lr 1.83e-04 | grad 1.57 | tok/s 67094
step    620 | loss 1.6888 | lr 1.86e-04 | grad 1.00 | tok/s 63580
step    630 | loss 1.5623 | lr 1.89e-04 | grad 1.10 | tok/s 65705
step    640 | loss 1.8533 | lr 1.92e-04 | grad 1.16 | tok/s 65669
step    650 | loss 1.6306 | lr 1.95e-04 | grad 1.40 | tok/s 64508
step    660 | loss 1.8872 | lr 1.98e-04 | grad 4.31 | tok/s 63668
step    670 | loss 1.7997 | lr 2.01e-04 | grad 2.00 | tok/s 65850
step    680 | loss 1.7557 | lr 2.04e-04 | grad 1.38 | tok/s 63506
step    690 | loss 1.7576 | lr 2.07e-04 | grad 1.84 | tok/s 63984
step    700 | loss 1.8427 | lr 2.10e-04 | grad 2.02 | tok/s 64354
step    710 | loss 1.7468 | lr 2.13e-04 | grad 1.05 | tok/s 64628
step    720 | loss 1.6454 | lr 2.16e-04 | grad 4.22 | tok/s 64324
step    730 | loss 1.8903 | lr 2.19e-04 | grad 1.64 | tok/s 64902
step    740 | loss 1.7713 | lr 2.22e-04 | grad 2.03 | tok/s 64378
step    750 | loss 1.5625 | lr 2.25e-04 | grad 1.40 | tok/s 63681
step    760 | loss 1.9129 | lr 2.28e-04 | grad 0.75 | tok/s 64433
step    770 | loss 1.5902 | lr 2.31e-04 | grad 1.28 | tok/s 64012
step    780 | loss 1.6645 | lr 2.34e-04 | grad 1.05 | tok/s 64767
step    790 | loss 1.5452 | lr 2.37e-04 | grad 0.70 | tok/s 65242
step    800 | loss 1.5400 | lr 2.40e-04 | grad 1.10 | tok/s 65334
step    810 | loss 1.6194 | lr 2.43e-04 | grad 2.12 | tok/s 64729
step    820 | loss 2.3258 | lr 2.46e-04 | grad 1.38 | tok/s 66368
step    830 | loss 1.7583 | lr 2.49e-04 | grad 0.77 | tok/s 67411
step    840 | loss 1.4132 | lr 2.52e-04 | grad 0.70 | tok/s 67417
step    850 | loss 1.9110 | lr 2.55e-04 | grad 1.15 | tok/s 64086
step    860 | loss 1.6428 | lr 2.58e-04 | grad 0.84 | tok/s 62856
step    870 | loss 1.5542 | lr 2.61e-04 | grad 0.93 | tok/s 64773
step    880 | loss 1.6245 | lr 2.64e-04 | grad 1.01 | tok/s 64502
step    890 | loss 1.5307 | lr 2.67e-04 | grad 0.88 | tok/s 64394
step    900 | loss 1.9690 | lr 2.70e-04 | grad 1.07 | tok/s 62737
step    910 | loss 1.5567 | lr 2.73e-04 | grad 0.89 | tok/s 63964
step    920 | loss 1.5801 | lr 2.76e-04 | grad 0.80 | tok/s 63757
step    930 | loss 1.6367 | lr 2.79e-04 | grad 1.23 | tok/s 48287
step    940 | loss 1.5394 | lr 2.82e-04 | grad 1.63 | tok/s 28390
step    950 | loss 1.6411 | lr 2.85e-04 | grad 1.07 | tok/s 29154
step    960 | loss 1.3650 | lr 2.88e-04 | grad 0.52 | tok/s 29664
step    970 | loss 1.2266 | lr 2.91e-04 | grad 0.46 | tok/s 29059
step    980 | loss 1.4023 | lr 2.94e-04 | grad 2.42 | tok/s 28722
step    990 | loss 1.7153 | lr 2.97e-04 | grad 0.73 | tok/s 27770
step   1000 | loss 1.6012 | lr 3.00e-04 | grad 0.62 | tok/s 27210
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6012.pt
step   1010 | loss 1.8297 | lr 1.06e-06 | grad 1.64 | tok/s 24569
step   1020 | loss 1.5755 | lr 1.27e-06 | grad 0.73 | tok/s 28285
step   1030 | loss 1.9096 | lr 1.62e-06 | grad 0.82 | tok/s 27252
step   1040 | loss 1.5413 | lr 2.12e-06 | grad 1.16 | tok/s 28213
step   1050 | loss 1.5824 | lr 2.77e-06 | grad 0.66 | tok/s 27802
step   1060 | loss 1.8163 | lr 3.56e-06 | grad 1.79 | tok/s 27924
step   1070 | loss 1.9666 | lr 4.50e-06 | grad 0.86 | tok/s 28085
step   1080 | loss 2.3298 | lr 5.58e-06 | grad 1.08 | tok/s 27953
step   1090 | loss 1.9805 | lr 6.81e-06 | grad 0.89 | tok/s 28061
step   1100 | loss 1.6297 | lr 8.17e-06 | grad 0.72 | tok/s 27977
step   1110 | loss 1.6413 | lr 9.68e-06 | grad 0.97 | tok/s 28249
step   1120 | loss 1.9167 | lr 1.13e-05 | grad 0.81 | tok/s 28656
step   1130 | loss 1.6594 | lr 1.31e-05 | grad 0.79 | tok/s 27780
step   1140 | loss 1.5514 | lr 1.50e-05 | grad 0.63 | tok/s 27933
step   1150 | loss 1.7836 | lr 1.71e-05 | grad 1.27 | tok/s 27822
step   1160 | loss 1.4682 | lr 1.93e-05 | grad 0.57 | tok/s 27911
step   1170 | loss 1.8454 | lr 2.16e-05 | grad 0.80 | tok/s 27683
step   1180 | loss 1.6327 | lr 2.40e-05 | grad 0.80 | tok/s 29334
step   1190 | loss 1.5889 | lr 2.66e-05 | grad 0.63 | tok/s 29253
step   1200 | loss 1.5233 | lr 2.93e-05 | grad 0.53 | tok/s 29575
step   1210 | loss 1.4847 | lr 3.21e-05 | grad 0.54 | tok/s 29482
step   1220 | loss 1.4719 | lr 3.50e-05 | grad 0.73 | tok/s 29657
step   1230 | loss 1.4200 | lr 3.80e-05 | grad 0.63 | tok/s 28449
step   1240 | loss 1.5275 | lr 4.12e-05 | grad 0.64 | tok/s 27981
step   1250 | loss 1.6446 | lr 4.45e-05 | grad 3.28 | tok/s 28490
step   1260 | loss 1.6594 | lr 4.78e-05 | grad 2.81 | tok/s 28730
step   1270 | loss 1.7220 | lr 5.13e-05 | grad 1.17 | tok/s 28191
step   1280 | loss 1.5891 | lr 5.48e-05 | grad 0.77 | tok/s 27930
step   1290 | loss 1.5347 | lr 5.85e-05 | grad 0.77 | tok/s 27876
step   1300 | loss 1.5717 | lr 6.22e-05 | grad 0.76 | tok/s 27573
step   1310 | loss 1.6235 | lr 6.61e-05 | grad 0.65 | tok/s 27542
step   1320 | loss 1.5979 | lr 7.00e-05 | grad 1.14 | tok/s 27854
step   1330 | loss 1.5020 | lr 7.40e-05 | grad 0.59 | tok/s 28287
step   1340 | loss 1.4004 | lr 7.81e-05 | grad 0.71 | tok/s 28457
step   1350 | loss 1.4810 | lr 8.22e-05 | grad 1.55 | tok/s 28861
step   1360 | loss 1.4254 | lr 8.64e-05 | grad 0.66 | tok/s 27432
step   1370 | loss 1.5521 | lr 9.07e-05 | grad 0.60 | tok/s 27677
step   1380 | loss 1.5765 | lr 9.50e-05 | grad 0.82 | tok/s 28736
step   1390 | loss 1.4915 | lr 9.94e-05 | grad 1.38 | tok/s 27584
step   1400 | loss 1.5057 | lr 1.04e-04 | grad 5.09 | tok/s 28143
step   1410 | loss 1.5249 | lr 1.08e-04 | grad 1.01 | tok/s 29273
step   1420 | loss 1.5435 | lr 1.13e-04 | grad 0.76 | tok/s 27297
step   1430 | loss 1.4037 | lr 1.17e-04 | grad 0.70 | tok/s 26795
step   1440 | loss 1.3425 | lr 1.22e-04 | grad 0.59 | tok/s 28479
step   1450 | loss 1.3735 | lr 1.27e-04 | grad 1.19 | tok/s 29013
step   1460 | loss 1.4742 | lr 1.31e-04 | grad 0.58 | tok/s 27156
step   1470 | loss 1.5553 | lr 1.36e-04 | grad 1.94 | tok/s 27478
step   1480 | loss 1.4362 | lr 1.41e-04 | grad 1.68 | tok/s 28272
step   1490 | loss 1.5936 | lr 1.45e-04 | grad 2.20 | tok/s 28166
step   1500 | loss 1.7077 | lr 1.50e-04 | grad 1.88 | tok/s 27290
step   1510 | loss 1.5418 | lr 1.55e-04 | grad 0.93 | tok/s 28387
step   1520 | loss 1.5089 | lr 1.59e-04 | grad 1.06 | tok/s 28528
step   1530 | loss 1.4982 | lr 1.64e-04 | grad 0.67 | tok/s 28503
step   1540 | loss 1.4520 | lr 1.69e-04 | grad 0.55 | tok/s 27795
step   1550 | loss 1.4528 | lr 1.73e-04 | grad 2.81 | tok/s 28843
step   1560 | loss 1.9347 | lr 1.78e-04 | grad 1.30 | tok/s 27893
step   1570 | loss 1.4902 | lr 1.83e-04 | grad 1.01 | tok/s 27502
step   1580 | loss 1.6067 | lr 1.87e-04 | grad 0.98 | tok/s 28390

Training complete! Final step: 1581
