Using device: cuda
Output directory: benchmark_results/100m_10min/e71_v3/level71_100m_20260115_131751
Auto r_h_mode: none (level 71 is matrix state - gated update is bounded)
Model: Level 71, 104,022,656 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.8451 | lr 2.70e-06 | grad 106.50 | tok/s 6752
step     20 | loss 5.8180 | lr 5.70e-06 | grad 99.50 | tok/s 8376
step     30 | loss 5.8261 | lr 8.70e-06 | grad 135.00 | tok/s 8868
step     40 | loss 5.8081 | lr 1.17e-05 | grad 104.00 | tok/s 8854
step     50 | loss 5.7474 | lr 1.47e-05 | grad 106.50 | tok/s 8839
step     60 | loss 5.6910 | lr 1.77e-05 | grad 84.50 | tok/s 8661
step     70 | loss 5.4206 | lr 2.07e-05 | grad 72.50 | tok/s 8361
step     80 | loss 5.0588 | lr 2.37e-05 | grad 65.00 | tok/s 8686
step     90 | loss 4.5607 | lr 2.67e-05 | grad 41.25 | tok/s 8392
step    100 | loss 4.0593 | lr 2.97e-05 | grad 14.94 | tok/s 8461
step    110 | loss 3.7100 | lr 3.27e-05 | grad 22.12 | tok/s 8338
step    120 | loss 3.7280 | lr 3.57e-05 | grad 40.50 | tok/s 8223
step    130 | loss 3.6401 | lr 3.87e-05 | grad 10.06 | tok/s 8415
step    140 | loss 3.3798 | lr 4.17e-05 | grad 8.19 | tok/s 8432
step    150 | loss 3.2142 | lr 4.47e-05 | grad 9.69 | tok/s 8039
step    160 | loss 3.1050 | lr 4.77e-05 | grad 4.91 | tok/s 8110
step    170 | loss 3.2394 | lr 5.07e-05 | grad 27.50 | tok/s 8394
step    180 | loss 3.3257 | lr 5.37e-05 | grad 23.00 | tok/s 8422
step    190 | loss 3.1328 | lr 5.67e-05 | grad 6.72 | tok/s 8558
step    200 | loss 3.0507 | lr 5.97e-05 | grad 3.45 | tok/s 8758
step    210 | loss 3.0999 | lr 6.27e-05 | grad 6.66 | tok/s 8383
step    220 | loss 3.1194 | lr 6.57e-05 | grad 4.81 | tok/s 8664
step    230 | loss 2.8639 | lr 6.87e-05 | grad 10.44 | tok/s 8366
step    240 | loss 2.9561 | lr 7.17e-05 | grad 7.16 | tok/s 8539
step    250 | loss 2.8137 | lr 7.47e-05 | grad 4.91 | tok/s 8415
step    260 | loss 2.7803 | lr 7.77e-05 | grad 4.81 | tok/s 8076
step    270 | loss 2.6893 | lr 8.07e-05 | grad 5.69 | tok/s 8382
step    280 | loss 2.5891 | lr 8.37e-05 | grad 6.50 | tok/s 8366
step    290 | loss 2.5607 | lr 8.67e-05 | grad 3.27 | tok/s 8825
step    300 | loss 2.4556 | lr 8.97e-05 | grad 4.94 | tok/s 8823
step    310 | loss 2.3766 | lr 9.27e-05 | grad 3.58 | tok/s 8824
step    320 | loss 2.4708 | lr 9.57e-05 | grad 13.81 | tok/s 8492

Training complete! Final step: 321
