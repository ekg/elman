Using device: cuda
Output directory: benchmark_results/100m_10min/cudagru/levelcudagru_100m_20260115_105402
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level cudagru, 82,735,872 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 6.4112 | lr 2.70e-06 | grad 73.50 | tok/s 9932
step     20 | loss 4.3160 | lr 5.70e-06 | grad 22.75 | tok/s 10649
step     30 | loss 5.0507 | lr 8.70e-06 | grad 10.94 | tok/s 11309
step     40 | loss 4.4911 | lr 1.17e-05 | grad 5.44 | tok/s 11306
step     50 | loss 4.1323 | lr 1.47e-05 | grad 4.03 | tok/s 11320
step     60 | loss 4.0836 | lr 1.77e-05 | grad 6.44 | tok/s 11079
step     70 | loss 3.8250 | lr 2.07e-05 | grad 4.25 | tok/s 10617
step     80 | loss 4.0631 | lr 2.37e-05 | grad 11.12 | tok/s 10955
step     90 | loss 3.9566 | lr 2.67e-05 | grad 4.62 | tok/s 10579
step    100 | loss 3.8051 | lr 2.97e-05 | grad 3.17 | tok/s 10690
step    110 | loss 3.6723 | lr 3.27e-05 | grad 3.58 | tok/s 10522
step    120 | loss 3.6043 | lr 3.57e-05 | grad 2.86 | tok/s 10377
step    130 | loss 3.5662 | lr 3.87e-05 | grad 3.56 | tok/s 10627
step    140 | loss 3.3556 | lr 4.17e-05 | grad 3.28 | tok/s 10651
step    150 | loss 3.1397 | lr 4.47e-05 | grad 3.94 | tok/s 10321
step    160 | loss 3.0413 | lr 4.77e-05 | grad 3.55 | tok/s 10403
step    170 | loss 3.1717 | lr 5.07e-05 | grad 6.41 | tok/s 10723
step    180 | loss 3.1851 | lr 5.37e-05 | grad 5.97 | tok/s 10809
step    190 | loss 2.9934 | lr 5.67e-05 | grad 3.17 | tok/s 10907
step    200 | loss 2.8906 | lr 5.97e-05 | grad 2.61 | tok/s 11160
step    210 | loss 2.9736 | lr 6.27e-05 | grad 4.03 | tok/s 10717
step    220 | loss 2.9899 | lr 6.57e-05 | grad 2.66 | tok/s 11060
step    230 | loss 2.7900 | lr 6.87e-05 | grad 3.19 | tok/s 10674
step    240 | loss 2.8808 | lr 7.17e-05 | grad 4.12 | tok/s 10916
step    250 | loss 2.7634 | lr 7.47e-05 | grad 4.16 | tok/s 10775
step    260 | loss 2.7402 | lr 7.77e-05 | grad 2.66 | tok/s 10292
step    270 | loss 2.6438 | lr 8.07e-05 | grad 3.61 | tok/s 10738
step    280 | loss 2.5481 | lr 8.37e-05 | grad 4.38 | tok/s 10787
step    290 | loss 2.6024 | lr 8.67e-05 | grad 2.69 | tok/s 11364
step    300 | loss 2.5010 | lr 8.97e-05 | grad 2.11 | tok/s 11318
step    310 | loss 2.4429 | lr 9.27e-05 | grad 4.28 | tok/s 11368
step    320 | loss 2.4612 | lr 9.57e-05 | grad 3.09 | tok/s 10899
step    330 | loss 2.5744 | lr 9.87e-05 | grad 2.39 | tok/s 10629
step    340 | loss 2.5928 | lr 1.02e-04 | grad 2.84 | tok/s 10905
step    350 | loss 2.5476 | lr 1.05e-04 | grad 3.25 | tok/s 10595
step    360 | loss 2.5464 | lr 1.08e-04 | grad 3.31 | tok/s 10679
step    370 | loss 2.4566 | lr 1.11e-04 | grad 2.06 | tok/s 10813
step    380 | loss 2.9074 | lr 1.14e-04 | grad 3.06 | tok/s 11218
step    390 | loss 2.4389 | lr 1.17e-04 | grad 2.88 | tok/s 10650
step    400 | loss 2.5857 | lr 1.20e-04 | grad 3.80 | tok/s 11035
step    410 | loss 2.3634 | lr 1.23e-04 | grad 2.44 | tok/s 10732

Training complete! Final step: 412
