Using device: cuda
Output directory: benchmark_results/100m_10min/e72_v4/level72_100m_20260115_140312
Auto r_h_mode: none (level 72 is matrix state - gated update is bounded)
Model: Level 72, 104,024,576 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.9014 | lr 2.70e-06 | grad 44.50 | tok/s 6924
step     20 | loss 5.8501 | lr 5.70e-06 | grad 49.75 | tok/s 8626
step     30 | loss 5.8138 | lr 8.70e-06 | grad 57.50 | tok/s 9129
step     40 | loss 5.7393 | lr 1.17e-05 | grad 136.00 | tok/s 9106
step     50 | loss 5.5940 | lr 1.47e-05 | grad 43.50 | tok/s 9103
step     60 | loss 5.4082 | lr 1.77e-05 | grad 30.12 | tok/s 8921
step     70 | loss 4.9496 | lr 2.07e-05 | grad 17.00 | tok/s 8608
step     80 | loss 4.6036 | lr 2.37e-05 | grad 17.12 | tok/s 8943
step     90 | loss 4.4418 | lr 2.67e-05 | grad 16.62 | tok/s 8637
step    100 | loss 4.0606 | lr 2.97e-05 | grad 10.75 | tok/s 8720
step    110 | loss 3.7703 | lr 3.27e-05 | grad 10.38 | tok/s 8595
step    120 | loss 3.7844 | lr 3.57e-05 | grad 7.03 | tok/s 8466
step    130 | loss 3.6656 | lr 3.87e-05 | grad 5.25 | tok/s 8664
step    140 | loss 3.4026 | lr 4.17e-05 | grad 5.59 | tok/s 8681
step    150 | loss 3.2534 | lr 4.47e-05 | grad 7.66 | tok/s 8281
step    160 | loss 3.1707 | lr 4.77e-05 | grad 5.84 | tok/s 8353
step    170 | loss 3.3112 | lr 5.07e-05 | grad 9.69 | tok/s 8645
step    180 | loss 3.4182 | lr 5.37e-05 | grad 5.62 | tok/s 8673
step    190 | loss 3.2423 | lr 5.67e-05 | grad 9.88 | tok/s 8810
step    200 | loss 3.1679 | lr 5.97e-05 | grad 3.56 | tok/s 9015
step    210 | loss 3.1929 | lr 6.27e-05 | grad 8.56 | tok/s 8628
step    220 | loss 3.2575 | lr 6.57e-05 | grad 4.19 | tok/s 8916
step    230 | loss 2.9843 | lr 6.87e-05 | grad 8.38 | tok/s 8583
step    240 | loss 3.1289 | lr 7.17e-05 | grad 5.97 | tok/s 8759
step    250 | loss 2.9644 | lr 7.47e-05 | grad 6.62 | tok/s 8635
step    260 | loss 2.9433 | lr 7.77e-05 | grad 4.16 | tok/s 8286
step    270 | loss 2.8491 | lr 8.07e-05 | grad 6.25 | tok/s 8599
step    280 | loss 2.7616 | lr 8.37e-05 | grad 6.28 | tok/s 8582
step    290 | loss 2.7376 | lr 8.67e-05 | grad 3.91 | tok/s 9054
step    300 | loss 2.6336 | lr 8.97e-05 | grad 5.16 | tok/s 9055
step    310 | loss 2.5549 | lr 9.27e-05 | grad 4.31 | tok/s 9053
step    320 | loss 2.6142 | lr 9.57e-05 | grad 8.50 | tok/s 8716
step    330 | loss 2.7358 | lr 9.87e-05 | grad 6.12 | tok/s 8500

Training complete! Final step: 330
