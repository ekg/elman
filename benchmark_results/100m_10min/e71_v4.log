Using device: cuda
Output directory: benchmark_results/100m_10min/e71_v4/level71_100m_20260115_140312
Auto r_h_mode: none (level 71 is matrix state - gated update is bounded)
Model: Level 71, 104,022,656 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.8462 | lr 2.70e-06 | grad 98.50 | tok/s 6607
step     20 | loss 5.8184 | lr 5.70e-06 | grad 95.00 | tok/s 8133
step     30 | loss 5.8273 | lr 8.70e-06 | grad 85.00 | tok/s 8614
step     40 | loss 5.8096 | lr 1.17e-05 | grad 85.00 | tok/s 8610
step     50 | loss 5.7533 | lr 1.47e-05 | grad 86.50 | tok/s 8609
step     60 | loss 5.6918 | lr 1.77e-05 | grad 94.00 | tok/s 8412
step     70 | loss 5.4176 | lr 2.07e-05 | grad 75.50 | tok/s 8117
step     80 | loss 5.0639 | lr 2.37e-05 | grad 45.25 | tok/s 8435
step     90 | loss 4.5576 | lr 2.67e-05 | grad 112.50 | tok/s 8149
step    100 | loss 4.0830 | lr 2.97e-05 | grad 16.00 | tok/s 8227
step    110 | loss 3.7502 | lr 3.27e-05 | grad 18.50 | tok/s 8109
step    120 | loss 3.7530 | lr 3.57e-05 | grad 21.12 | tok/s 7995
step    130 | loss 3.6585 | lr 3.87e-05 | grad 11.44 | tok/s 8181
step    140 | loss 3.4150 | lr 4.17e-05 | grad 19.75 | tok/s 8197
step    150 | loss 3.2435 | lr 4.47e-05 | grad 8.75 | tok/s 7805
step    160 | loss 3.1554 | lr 4.77e-05 | grad 6.00 | tok/s 7873
step    170 | loss 3.2736 | lr 5.07e-05 | grad 20.75 | tok/s 8147
step    180 | loss 3.3738 | lr 5.37e-05 | grad 7.31 | tok/s 8175
step    190 | loss 3.1906 | lr 5.67e-05 | grad 23.38 | tok/s 8302
step    200 | loss 3.1384 | lr 5.97e-05 | grad 5.56 | tok/s 8500
step    210 | loss 3.1722 | lr 6.27e-05 | grad 9.62 | tok/s 8136
step    220 | loss 3.2128 | lr 6.57e-05 | grad 7.69 | tok/s 8406
step    230 | loss 2.9592 | lr 6.87e-05 | grad 6.66 | tok/s 8116
step    240 | loss 3.0675 | lr 7.17e-05 | grad 8.88 | tok/s 8285
step    250 | loss 2.9241 | lr 7.47e-05 | grad 7.16 | tok/s 8165
step    260 | loss 2.8735 | lr 7.77e-05 | grad 11.50 | tok/s 7837
step    270 | loss 2.7738 | lr 8.07e-05 | grad 9.81 | tok/s 8132
step    280 | loss 2.6813 | lr 8.37e-05 | grad 9.81 | tok/s 8116
step    290 | loss 2.6642 | lr 8.67e-05 | grad 6.28 | tok/s 8564
step    300 | loss 2.5711 | lr 8.97e-05 | grad 6.28 | tok/s 8564
step    310 | loss 2.5077 | lr 9.27e-05 | grad 7.44 | tok/s 8563

Training complete! Final step: 312
