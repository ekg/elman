Using device: cuda
Output directory: benchmark_results/e87_highblock_20260119_154311/87b32k8/level87b32k8_100m_20260119_154317
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b32k8, 74,092,800 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 6.2234 | lr 1.00e-06 | grad 38912.00 | tok/s 17787
step     20 | loss 5.5631 | lr 1.00e-06 | grad 4832.00 | tok/s 39592
step     30 | loss 5.8816 | lr 1.00e-06 | grad 532.00 | tok/s 42981
step     40 | loss 5.4838 | lr 1.00e-06 | grad 149.00 | tok/s 44350
step     50 | loss 4.9588 | lr 1.00e-06 | grad 83.00 | tok/s 44964
step     60 | loss 4.5679 | lr 1.00e-06 | grad 97.00 | tok/s 44080
step     70 | loss 3.8571 | lr 1.00e-06 | grad 38.25 | tok/s 42858
step     80 | loss 3.8168 | lr 1.00e-06 | grad 4.59 | tok/s 44449
step     90 | loss 3.7998 | lr 1.00e-06 | grad 7840.00 | tok/s 42813
step    100 | loss 3.5939 | lr 1.00e-06 | grad 8768.00 | tok/s 43507
step    110 | loss 3.4392 | lr 1.00e-06 | grad 3152.00 | tok/s 42783
step    120 | loss 3.9581 | lr 1.00e-06 | grad 13696.00 | tok/s 42234
step    130 | loss 3.6049 | lr 1.00e-06 | grad 6.97 | tok/s 43120
step    140 | loss 3.4020 | lr 1.00e-06 | grad 58.75 | tok/s 42870
step    150 | loss 3.4328 | lr 1.00e-06 | grad 42.75 | tok/s 40829
step    160 | loss 3.2880 | lr 1.00e-06 | grad 748.00 | tok/s 41003
step    170 | loss 3.3437 | lr 1.00e-06 | grad 49.25 | tok/s 42443
step    180 | loss 3.4269 | lr 1.00e-06 | grad 108.00 | tok/s 42589
step    190 | loss 3.2942 | lr 1.00e-06 | grad 536.00 | tok/s 43295
step    200 | loss 3.3439 | lr 1.00e-06 | grad 232.00 | tok/s 44088
step    210 | loss 3.2848 | lr 1.00e-06 | grad 684.00 | tok/s 42188
step    220 | loss 3.3743 | lr 1.00e-06 | grad 172.00 | tok/s 43476
step    230 | loss 3.1735 | lr 1.00e-06 | grad 12288.00 | tok/s 42012
step    240 | loss 3.3379 | lr 1.00e-06 | grad 230.00 | tok/s 42844
step    250 | loss 3.1708 | lr 1.00e-06 | grad 7648.00 | tok/s 42187
step    260 | loss 3.1551 | lr 1.00e-06 | grad 1240.00 | tok/s 40464
step    270 | loss 3.1733 | lr 1.00e-06 | grad 3872.00 | tok/s 42002
step    280 | loss 3.3147 | lr 1.00e-06 | grad 1816.00 | tok/s 42347
step    290 | loss 3.2297 | lr 1.00e-06 | grad 14784.00 | tok/s 44767
step    300 | loss 3.1693 | lr 1.00e-06 | grad 34560.00 | tok/s 44638
step    310 | loss 3.1699 | lr 1.00e-06 | grad 25728.00 | tok/s 44541
step    320 | loss 3.3308 | lr 1.00e-06 | grad 9536.00 | tok/s 42796
step    330 | loss 3.4565 | lr 1.00e-06 | grad 77824.00 | tok/s 41272
step    340 | loss 3.4166 | lr 1.00e-06 | grad 20608.00 | tok/s 42019
step    350 | loss 3.5424 | lr 1.00e-06 | grad 7328.00 | tok/s 40949
step    360 | loss 3.5328 | lr 1.00e-06 | grad 8256.00 | tok/s 41701
step    370 | loss 3.3202 | lr 1.00e-06 | grad 9472.00 | tok/s 42794
step    380 | loss 3.5759 | lr 1.00e-06 | grad 8960.00 | tok/s 43819
step    390 | loss 3.2320 | lr 1.00e-06 | grad 137216.00 | tok/s 42205
step    400 | loss 3.5409 | lr 1.00e-06 | grad 606208.00 | tok/s 43184
step    410 | loss 3.6376 | lr 1.00e-06 | grad 64000.00 | tok/s 41826
step    420 | loss 3.4097 | lr 1.00e-06 | grad 39936.00 | tok/s 41391
step    430 | loss 3.4071 | lr 1.00e-06 | grad 55552.00 | tok/s 41368
step    440 | loss 3.5993 | lr 1.00e-06 | grad 194560.00 | tok/s 42909
step    450 | loss 3.1870 | lr 1.00e-06 | grad 350208.00 | tok/s 41755
step    460 | loss 3.3190 | lr 1.00e-06 | grad 428032.00 | tok/s 41799
step    470 | loss 3.4351 | lr 1.00e-06 | grad 337920.00 | tok/s 42128
step    480 | loss 3.4161 | lr 1.00e-06 | grad 82944.00 | tok/s 40671
step    490 | loss 3.3996 | lr 1.00e-06 | grad 53504.00 | tok/s 41374
step    500 | loss 3.9371 | lr 1.00e-06 | grad 128512.00 | tok/s 42613
step    510 | loss 3.2673 | lr 1.00e-06 | grad 2981888.00 | tok/s 41699
step    520 | loss 3.3813 | lr 1.00e-06 | grad 335872.00 | tok/s 42924
step    530 | loss 3.7070 | lr 1.00e-06 | grad 1073152.00 | tok/s 41920
step    540 | loss 3.2463 | lr 1.00e-06 | grad 1220608.00 | tok/s 41712
step    550 | loss 3.2486 | lr 1.00e-06 | grad 71680.00 | tok/s 43041
step    560 | loss 3.1610 | lr 1.00e-06 | grad 138240.00 | tok/s 43705
step    570 | loss 3.3581 | lr 1.00e-06 | grad 168960.00 | tok/s 42692
step    580 | loss 3.4880 | lr 1.00e-06 | grad 937984.00 | tok/s 41816
step    590 | loss 3.6029 | lr 1.00e-06 | grad 440320.00 | tok/s 40939
step    600 | loss 3.2855 | lr 1.00e-06 | grad 133120.00 | tok/s 40892
step    610 | loss 3.4902 | lr 1.00e-06 | grad 34560.00 | tok/s 42940
step    620 | loss 3.3855 | lr 1.00e-06 | grad 164864.00 | tok/s 40900
step    630 | loss 3.4968 | lr 1.00e-06 | grad 2752512.00 | tok/s 42425
step    640 | loss 3.8906 | lr 1.00e-06 | grad 205824.00 | tok/s 42648
step    650 | loss 3.2914 | lr 1.00e-06 | grad 189440.00 | tok/s 41772
step    660 | loss 3.4904 | lr 1.00e-06 | grad 811008.00 | tok/s 41254
step    670 | loss 3.3460 | lr 1.00e-06 | grad 195584.00 | tok/s 42553
step    680 | loss 3.3121 | lr 1.00e-06 | grad 270336.00 | tok/s 40987
step    690 | loss 3.2563 | lr 1.00e-06 | grad 112640.00 | tok/s 41255
step    700 | loss 3.3520 | lr 1.00e-06 | grad 501760.00 | tok/s 41573
step    710 | loss 3.3942 | lr 1.00e-06 | grad 473088.00 | tok/s 41800
step    720 | loss 3.2878 | lr 1.00e-06 | grad 134144.00 | tok/s 41791
step    730 | loss 3.2931 | lr 1.00e-06 | grad 569344.00 | tok/s 42117
step    740 | loss 3.2245 | lr 1.00e-06 | grad 92160.00 | tok/s 41816
step    750 | loss 3.1376 | lr 1.00e-06 | grad 79360.00 | tok/s 41295
step    760 | loss 3.4774 | lr 1.00e-06 | grad 872448.00 | tok/s 41853
step    770 | loss 3.0969 | lr 1.00e-06 | grad 1769472.00 | tok/s 41568
step    780 | loss 3.1776 | lr 1.00e-06 | grad 1695744.00 | tok/s 42015
step    790 | loss 3.2519 | lr 1.00e-06 | grad 974848.00 | tok/s 42343
step    800 | loss 3.2726 | lr 1.00e-06 | grad 487424.00 | tok/s 42365
step    810 | loss 3.1547 | lr 1.00e-06 | grad 356352.00 | tok/s 41968
step    820 | loss 3.4055 | lr 1.00e-06 | grad 62208.00 | tok/s 42920
step    830 | loss 3.4203 | lr 1.00e-06 | grad 147456.00 | tok/s 43727
step    840 | loss 3.3928 | lr 1.00e-06 | grad 438272.00 | tok/s 43743
step    850 | loss 3.2190 | lr 1.00e-06 | grad 2752512.00 | tok/s 41544
step    860 | loss 3.1703 | lr 1.00e-06 | grad 651264.00 | tok/s 40446
step    870 | loss 3.2795 | lr 1.00e-06 | grad 8355840.00 | tok/s 41822
step    880 | loss 3.2717 | lr 1.00e-06 | grad 1335296.00 | tok/s 41511
step    890 | loss 3.2064 | lr 1.00e-06 | grad 4259840.00 | tok/s 41641
step    900 | loss 3.5294 | lr 1.00e-06 | grad 33030144.00 | tok/s 40590
step    910 | loss 3.4047 | lr 1.00e-06 | grad 34340864.00 | tok/s 41274
step    920 | loss 3.4170 | lr 1.00e-06 | grad 1308622848.00 | tok/s 41079
step    930 | loss 3.5431 | lr 1.00e-06 | grad 12683575296.00 | tok/s 41015
step    940 | loss 3.3150 | lr 1.00e-06 | grad 77070336.00 | tok/s 40481
step    950 | loss 3.4786 | lr 1.00e-06 | grad 20185088.00 | tok/s 41521
step    960 | loss 3.4383 | lr 1.00e-06 | grad 17563648.00 | tok/s 43483
step    970 | loss 3.4263 | lr 1.00e-06 | grad 47710208.00 | tok/s 43559
step    980 | loss 3.3775 | lr 1.00e-06 | grad 140509184.00 | tok/s 42463
step    990 | loss 3.5919 | lr 1.00e-06 | grad 14221312.00 | tok/s 41414
step   1000 | loss 3.4169 | lr 1.00e-06 | grad 59506688.00 | tok/s 40269
  >>> saved checkpoint: checkpoint_step_001000_loss_3.4169.pt
step   1010 | loss 3.7419 | lr 1.00e-06 | grad 5242880.00 | tok/s 36794
step   1020 | loss 3.9166 | lr 1.00e-06 | grad 1630208.00 | tok/s 41315
step   1030 | loss 4.3649 | lr 1.00e-06 | grad 3817472.00 | tok/s 40872
step   1040 | loss 3.9766 | lr 1.00e-06 | grad 5570560.00 | tok/s 41935
step   1050 | loss 4.0873 | lr 1.00e-06 | grad 11534336.00 | tok/s 42165
step   1060 | loss 4.2463 | lr 1.00e-06 | grad 4030464.00 | tok/s 42264
step   1070 | loss 4.2553 | lr 1.00e-06 | grad 41418752.00 | tok/s 42407
step   1080 | loss 3.8545 | lr 1.00e-06 | grad 1098907648.00 | tok/s 41924
step   1090 | loss 3.9096 | lr 1.00e-06 | grad 763363328.00 | tok/s 41892
step   1100 | loss 3.9165 | lr 1.00e-06 | grad 402653184.00 | tok/s 41717
step   1110 | loss 4.1040 | lr 1.00e-06 | grad 1619001344.00 | tok/s 42510
step   1120 | loss 4.1969 | lr 1.00e-06 | grad 79691776.00 | tok/s 42863
step   1130 | loss 3.5757 | lr 1.00e-06 | grad 116391936.00 | tok/s 40521
step   1140 | loss 3.5346 | lr 1.00e-06 | grad 633339904.00 | tok/s 41328
step   1150 | loss 3.6991 | lr 1.00e-06 | grad 2030043136.00 | tok/s 41333
step   1160 | loss 3.5316 | lr 1.00e-06 | grad 14898167808.00 | tok/s 41005
step   1170 | loss 3.7047 | lr 1.00e-06 | grad 513802240.00 | tok/s 41592
step   1180 | loss 3.3871 | lr 1.00e-06 | grad 44302336.00 | tok/s 43696
step   1190 | loss 3.7254 | lr 1.00e-06 | grad 110100480.00 | tok/s 43582
step   1200 | loss 3.8199 | lr 1.00e-06 | grad 98566144.00 | tok/s 43919
step   1210 | loss 3.9876 | lr 1.00e-06 | grad 306184192.00 | tok/s 43982
step   1220 | loss 3.9694 | lr 1.00e-06 | grad 10804527104.00 | tok/s 43702
step   1230 | loss 3.9224 | lr 1.00e-06 | grad 444596224.00 | tok/s 42578
step   1240 | loss 3.9334 | lr 1.00e-06 | grad 6777995264.00 | tok/s 41743
step   1250 | loss 4.0718 | lr 1.00e-06 | grad 5771362304.00 | tok/s 42780
step   1260 | loss 4.0781 | lr 1.00e-06 | grad 20669530112.00 | tok/s 42698
step   1270 | loss 4.2181 | lr 1.00e-06 | grad 7818182656.00 | tok/s 42101
step   1280 | loss 4.1008 | lr 1.00e-06 | grad 6274678784.00 | tok/s 41574
step   1290 | loss 4.0146 | lr 1.00e-06 | grad 700448768.00 | tok/s 41597
step   1300 | loss 4.0005 | lr 1.00e-06 | grad 6945767424.00 | tok/s 41344
step   1310 | loss 4.1109 | lr 1.00e-06 | grad 14361296896.00 | tok/s 41403
step   1320 | loss 4.1353 | lr 1.00e-06 | grad 77594624.00 | tok/s 42266
step   1330 | loss 4.2468 | lr 1.00e-06 | grad 763363328.00 | tok/s 42525
step   1340 | loss 4.1026 | lr 1.00e-06 | grad 1468006400.00 | tok/s 42664
step   1350 | loss 4.1424 | lr 1.00e-06 | grad 973078528.00 | tok/s 43824
step   1360 | loss 4.0938 | lr 1.00e-06 | grad 254803968.00 | tok/s 41512
step   1370 | loss 4.3217 | lr 1.00e-06 | grad 1744830464.00 | tok/s 41963
step   1380 | loss 4.3839 | lr 1.00e-06 | grad 381681664.00 | tok/s 42218
step   1390 | loss 4.1936 | lr 1.00e-06 | grad 142606336.00 | tok/s 41276
step   1400 | loss 4.5094 | lr 1.00e-06 | grad 27525120.00 | tok/s 43275
step   1410 | loss 4.6009 | lr 1.00e-06 | grad 23199744.00 | tok/s 43608
step   1420 | loss 4.3826 | lr 1.00e-06 | grad 145752064.00 | tok/s 41598
step   1430 | loss 4.1827 | lr 1.00e-06 | grad 40894464.00 | tok/s 40655
step   1440 | loss 4.3583 | lr 1.00e-06 | grad 15007744.00 | tok/s 42898
step   1450 | loss 4.3835 | lr 1.00e-06 | grad 23724032.00 | tok/s 43845
step   1460 | loss 4.1759 | lr 1.00e-06 | grad 3375104.00 | tok/s 40982
step   1470 | loss 4.3503 | lr 1.00e-06 | grad 258998272.00 | tok/s 42437
step   1480 | loss 4.3345 | lr 1.00e-06 | grad 71827456.00 | tok/s 42963
step   1490 | loss 4.3179 | lr 1.00e-06 | grad 88080384.00 | tok/s 42794
step   1500 | loss 4.2432 | lr 1.00e-06 | grad 62914560.00 | tok/s 41400
step   1510 | loss 4.4222 | lr 1.00e-06 | grad 70778880.00 | tok/s 43883
step   1520 | loss 4.3594 | lr 1.00e-06 | grad 40632320.00 | tok/s 43357
step   1530 | loss 4.3328 | lr 1.00e-06 | grad 48758784.00 | tok/s 43050
step   1540 | loss 4.2780 | lr 1.00e-06 | grad 263192576.00 | tok/s 42434
step   1550 | loss 4.3689 | lr 1.00e-06 | grad 893386752.00 | tok/s 44364
step   1560 | loss 4.6072 | lr 1.00e-06 | grad 113770496.00 | tok/s 43251
step   1570 | loss 4.1986 | lr 1.00e-06 | grad 876609536.00 | tok/s 42656
step   1580 | loss 4.4421 | lr 1.00e-06 | grad 255852544.00 | tok/s 43941
step   1590 | loss 4.2750 | lr 1.00e-06 | grad 55836672.00 | tok/s 42944
step   1600 | loss 4.1655 | lr 1.00e-06 | grad 1769996288.00 | tok/s 42251

Training complete! Final step: 1600
