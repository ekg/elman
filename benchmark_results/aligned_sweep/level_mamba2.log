Using device: cuda
Output directory: output/levelmamba2_100m_20260114_021645
Created Mamba2 model: dim=736, depth=30, expand=2, params=101,359,638
Model: Level mamba2, 101,359,638 parameters

Starting training from step 0...
Batch size: 8, Chunk size: 256
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 5.6107 | lr 9.00e-07 | grad 45.62 | tok/s 360
step     20 | loss 5.5210 | lr 1.90e-06 | grad 14.55 | tok/s 11315
step     30 | loss 5.3615 | lr 2.90e-06 | grad 6.77 | tok/s 11744
step     40 | loss 4.9053 | lr 3.90e-06 | grad 7.54 | tok/s 11230
step     50 | loss 4.5754 | lr 4.90e-06 | grad 6.36 | tok/s 11523
step     60 | loss 4.0606 | lr 5.90e-06 | grad 4.78 | tok/s 11537
step     70 | loss 3.6441 | lr 6.90e-06 | grad 3.57 | tok/s 11627
step     80 | loss 3.4784 | lr 7.90e-06 | grad 3.04 | tok/s 11475
step     90 | loss 3.0092 | lr 8.90e-06 | grad 2.86 | tok/s 11569
step    100 | loss 4.0752 | lr 9.90e-06 | grad 9.25 | tok/s 11571
step    110 | loss 4.2738 | lr 1.09e-05 | grad 4.25 | tok/s 11969
step    120 | loss 2.7797 | lr 1.19e-05 | grad 3.04 | tok/s 11879
step    130 | loss 2.6366 | lr 1.29e-05 | grad 4.43 | tok/s 11368
step    140 | loss 2.4851 | lr 1.39e-05 | grad 2.67 | tok/s 12163
step    150 | loss 3.4979 | lr 1.49e-05 | grad 5.53 | tok/s 11816
step    160 | loss 5.1912 | lr 1.59e-05 | grad 4.66 | tok/s 11731
step    170 | loss 4.6195 | lr 1.69e-05 | grad 6.10 | tok/s 12577
step    180 | loss 4.5019 | lr 1.79e-05 | grad 7.90 | tok/s 12533
step    190 | loss 3.6171 | lr 1.89e-05 | grad 15.79 | tok/s 12774
step    200 | loss 3.5049 | lr 1.99e-05 | grad 7.14 | tok/s 12350
step    210 | loss 3.0907 | lr 2.09e-05 | grad 13.37 | tok/s 12619
step    220 | loss 3.0354 | lr 2.19e-05 | grad 3.53 | tok/s 12561
step    230 | loss 2.9265 | lr 2.29e-05 | grad 3.06 | tok/s 12920
step    240 | loss 3.0395 | lr 2.39e-05 | grad 6.13 | tok/s 13014
step    250 | loss 2.7053 | lr 2.49e-05 | grad 5.59 | tok/s 12951
step    260 | loss 2.7054 | lr 2.59e-05 | grad 4.13 | tok/s 12994
step    270 | loss 2.3836 | lr 2.69e-05 | grad 3.74 | tok/s 13085
step    280 | loss 2.2803 | lr 2.79e-05 | grad 4.08 | tok/s 13014
step    290 | loss 2.6059 | lr 2.89e-05 | grad 3.98 | tok/s 13030
step    300 | loss 2.4907 | lr 2.99e-05 | grad 3.30 | tok/s 13004
step    310 | loss 2.4727 | lr 3.09e-05 | grad 3.18 | tok/s 12849
step    320 | loss 2.0460 | lr 3.19e-05 | grad 2.50 | tok/s 11937
step    330 | loss 2.3649 | lr 3.29e-05 | grad 3.33 | tok/s 11212
step    340 | loss 2.2813 | lr 3.39e-05 | grad 9.68 | tok/s 10924
step    350 | loss 2.2459 | lr 3.49e-05 | grad 4.22 | tok/s 11115
step    360 | loss 2.3703 | lr 3.59e-05 | grad 3.24 | tok/s 11072
step    370 | loss 2.1500 | lr 3.69e-05 | grad 4.16 | tok/s 10381
step    380 | loss 2.2916 | lr 3.79e-05 | grad 6.16 | tok/s 10491
step    390 | loss 2.2542 | lr 3.89e-05 | grad 5.49 | tok/s 10148
step    400 | loss 1.8862 | lr 3.99e-05 | grad 3.60 | tok/s 10233
step    410 | loss 2.0295 | lr 4.09e-05 | grad 2.72 | tok/s 9158
step    420 | loss 2.0915 | lr 4.19e-05 | grad 3.16 | tok/s 10068
step    430 | loss 2.1178 | lr 4.29e-05 | grad 2.32 | tok/s 10092
step    440 | loss 2.4504 | lr 4.39e-05 | grad 3.00 | tok/s 10147
step    450 | loss 2.1406 | lr 4.49e-05 | grad 2.60 | tok/s 10124
step    460 | loss 3.1609 | lr 4.59e-05 | grad 4.45 | tok/s 9924
step    470 | loss 2.5139 | lr 4.69e-05 | grad 3.08 | tok/s 9817
step    480 | loss 2.4311 | lr 4.79e-05 | grad 3.42 | tok/s 9594
step    490 | loss 2.4473 | lr 4.89e-05 | grad 2.64 | tok/s 9961
step    500 | loss 2.2786 | lr 4.99e-05 | grad 3.02 | tok/s 9990
step    510 | loss 1.8816 | lr 5.09e-05 | grad 2.41 | tok/s 10573
step    520 | loss 1.6174 | lr 5.19e-05 | grad 2.09 | tok/s 10928
step    530 | loss 2.3702 | lr 5.29e-05 | grad 4.83 | tok/s 10854
step    540 | loss 2.0239 | lr 5.39e-05 | grad 3.09 | tok/s 10920
step    550 | loss 2.0317 | lr 5.49e-05 | grad 3.46 | tok/s 10745
step    560 | loss 2.7494 | lr 5.59e-05 | grad 4.62 | tok/s 11185
step    570 | loss 2.4586 | lr 5.69e-05 | grad 5.41 | tok/s 10803
step    580 | loss 1.1675 | lr 5.79e-05 | grad 1.44 | tok/s 10895
step    590 | loss 0.6439 | lr 5.89e-05 | grad 1.47 | tok/s 11461
step    600 | loss 0.4542 | lr 5.99e-05 | grad 1.01 | tok/s 11389
step    610 | loss 2.6607 | lr 6.09e-05 | grad 4.33 | tok/s 10896
step    620 | loss 2.4078 | lr 6.19e-05 | grad 2.36 | tok/s 11414
step    630 | loss 1.8304 | lr 6.29e-05 | grad 2.60 | tok/s 11398
step    640 | loss 1.9503 | lr 6.39e-05 | grad 4.43 | tok/s 11304
step    650 | loss 2.4866 | lr 6.49e-05 | grad 2.37 | tok/s 11092
step    660 | loss 2.1436 | lr 6.59e-05 | grad 2.94 | tok/s 11331
step    670 | loss 2.5517 | lr 6.69e-05 | grad 2.71 | tok/s 11312
step    680 | loss 2.1401 | lr 6.79e-05 | grad 2.18 | tok/s 10810
step    690 | loss 1.9309 | lr 6.89e-05 | grad 2.32 | tok/s 10884
step    700 | loss 2.1871 | lr 6.99e-05 | grad 2.18 | tok/s 11350
step    710 | loss 1.4787 | lr 7.09e-05 | grad 3.27 | tok/s 11538
step    720 | loss 1.6104 | lr 7.19e-05 | grad 30.53 | tok/s 11204
step    730 | loss 2.2258 | lr 7.29e-05 | grad 3.28 | tok/s 10603
step    740 | loss 1.9906 | lr 7.39e-05 | grad 2.65 | tok/s 11010
step    750 | loss 1.9718 | lr 7.49e-05 | grad 2.49 | tok/s 11063
step    760 | loss 1.8063 | lr 7.59e-05 | grad 2.09 | tok/s 11199
step    770 | loss 1.5547 | lr 7.69e-05 | grad 1.75 | tok/s 11345
step    780 | loss 1.5084 | lr 7.79e-05 | grad 1.83 | tok/s 11322
step    790 | loss 1.4946 | lr 7.89e-05 | grad 3.05 | tok/s 11286
step    800 | loss 1.6630 | lr 7.99e-05 | grad 2.66 | tok/s 11448
step    810 | loss 1.3049 | lr 8.09e-05 | grad 1.97 | tok/s 11279
step    820 | loss 1.5302 | lr 8.19e-05 | grad 2.67 | tok/s 10685
step    830 | loss 1.9263 | lr 8.29e-05 | grad 1.98 | tok/s 9347
step    840 | loss 2.2588 | lr 8.39e-05 | grad 2.76 | tok/s 10080
step    850 | loss 2.0870 | lr 8.49e-05 | grad 4.54 | tok/s 9954
step    860 | loss 2.1980 | lr 8.59e-05 | grad 5.13 | tok/s 9948
step    870 | loss 2.4114 | lr 8.69e-05 | grad 2.04 | tok/s 9947
step    880 | loss 2.1659 | lr 8.79e-05 | grad 3.30 | tok/s 10683
step    890 | loss 2.4348 | lr 8.89e-05 | grad 2.70 | tok/s 10055
step    900 | loss 1.9069 | lr 8.99e-05 | grad 2.02 | tok/s 10449
step    910 | loss 2.2537 | lr 9.09e-05 | grad 2.03 | tok/s 11382
step    920 | loss 1.6339 | lr 9.19e-05 | grad 4.28 | tok/s 10711
step    930 | loss 2.2358 | lr 9.29e-05 | grad 2.15 | tok/s 9815
step    940 | loss 1.7660 | lr 9.39e-05 | grad 8.14 | tok/s 10158
step    950 | loss 2.4683 | lr 9.49e-05 | grad 4.17 | tok/s 10260
step    960 | loss 2.3261 | lr 9.59e-05 | grad 2.64 | tok/s 10045
step    970 | loss 2.0041 | lr 9.69e-05 | grad 1.96 | tok/s 10309
step    980 | loss 1.6936 | lr 9.79e-05 | grad 1.60 | tok/s 10566
step    990 | loss 1.7167 | lr 9.89e-05 | grad 2.96 | tok/s 10380
step   1000 | loss 1.8821 | lr 9.99e-05 | grad 1.63 | tok/s 10600
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8821.pt
step   1010 | loss 1.8054 | lr 1.02e-06 | grad 1.94 | tok/s 4372
step   1020 | loss 1.9370 | lr 1.09e-06 | grad 1.90 | tok/s 9402
step   1030 | loss 1.9873 | lr 1.21e-06 | grad 2.33 | tok/s 9431
step   1040 | loss 2.0763 | lr 1.37e-06 | grad 3.23 | tok/s 9574
step   1050 | loss 2.2563 | lr 1.59e-06 | grad 2.27 | tok/s 9771
step   1060 | loss 1.8725 | lr 1.85e-06 | grad 2.22 | tok/s 9132
step   1070 | loss 1.9178 | lr 2.16e-06 | grad 2.10 | tok/s 8287
step   1080 | loss 1.6380 | lr 2.52e-06 | grad 2.14 | tok/s 8506
step   1090 | loss 1.7312 | lr 2.92e-06 | grad 2.46 | tok/s 8241
step   1100 | loss 1.8291 | lr 3.37e-06 | grad 1.59 | tok/s 8038
step   1110 | loss 2.0077 | lr 3.87e-06 | grad 2.13 | tok/s 8875
step   1120 | loss 1.7972 | lr 4.42e-06 | grad 2.10 | tok/s 9226
step   1130 | loss 1.7431 | lr 5.01e-06 | grad 1.57 | tok/s 8939
step   1140 | loss 1.7140 | lr 5.65e-06 | grad 1.84 | tok/s 7954
step   1150 | loss 1.6180 | lr 6.32e-06 | grad 1.34 | tok/s 7824
step   1160 | loss 1.6543 | lr 7.05e-06 | grad 1.88 | tok/s 8054
step   1170 | loss 2.3404 | lr 7.81e-06 | grad 3.52 | tok/s 8265
step   1180 | loss 1.9133 | lr 8.62e-06 | grad 1.77 | tok/s 7959
step   1190 | loss 1.9678 | lr 9.47e-06 | grad 2.12 | tok/s 8213
step   1200 | loss 1.8049 | lr 1.04e-05 | grad 2.59 | tok/s 8054
step   1210 | loss 1.7828 | lr 1.13e-05 | grad 1.73 | tok/s 8374
step   1220 | loss 1.6106 | lr 1.23e-05 | grad 1.57 | tok/s 8426
step   1230 | loss 1.5874 | lr 1.33e-05 | grad 1.78 | tok/s 7889
step   1240 | loss 1.6790 | lr 1.43e-05 | grad 1.90 | tok/s 7716
step   1250 | loss 1.8981 | lr 1.54e-05 | grad 4.06 | tok/s 8538
step   1260 | loss 1.8170 | lr 1.65e-05 | grad 9.24 | tok/s 8983
step   1270 | loss 1.9683 | lr 1.76e-05 | grad 1.98 | tok/s 8486
step   1280 | loss 2.1972 | lr 1.88e-05 | grad 2.78 | tok/s 8074
step   1290 | loss 1.5995 | lr 2.00e-05 | grad 2.19 | tok/s 8092
step   1300 | loss 1.9359 | lr 2.13e-05 | grad 1.96 | tok/s 8179
step   1310 | loss 1.6628 | lr 2.25e-05 | grad 1.89 | tok/s 7786
step   1320 | loss 1.6942 | lr 2.38e-05 | grad 1.90 | tok/s 8052
step   1330 | loss 2.3460 | lr 2.52e-05 | grad 2.63 | tok/s 8236
step   1340 | loss 1.7998 | lr 2.65e-05 | grad 1.66 | tok/s 8794
step   1350 | loss 1.8525 | lr 2.79e-05 | grad 2.92 | tok/s 9097
step   1360 | loss 2.1318 | lr 2.93e-05 | grad 2.35 | tok/s 9194
step   1370 | loss 1.9470 | lr 3.07e-05 | grad 1.74 | tok/s 9579
step   1380 | loss 1.7063 | lr 3.21e-05 | grad 1.45 | tok/s 9354
step   1390 | loss 1.5172 | lr 3.36e-05 | grad 1.79 | tok/s 9607
step   1400 | loss 1.8602 | lr 3.51e-05 | grad 1.74 | tok/s 9566
step   1410 | loss 1.6121 | lr 3.65e-05 | grad 1.89 | tok/s 8970
step   1420 | loss 1.5331 | lr 3.80e-05 | grad 3.64 | tok/s 8941
step   1430 | loss 1.6627 | lr 3.96e-05 | grad 1.77 | tok/s 9648
step   1440 | loss 1.3971 | lr 4.11e-05 | grad 1.16 | tok/s 9849
step   1450 | loss 1.4559 | lr 4.26e-05 | grad 2.12 | tok/s 9651
step   1460 | loss 1.7173 | lr 4.41e-05 | grad 1.79 | tok/s 9463
step   1470 | loss 1.4871 | lr 4.57e-05 | grad 1.59 | tok/s 9876
step   1480 | loss 1.3457 | lr 4.72e-05 | grad 1.79 | tok/s 9904
step   1490 | loss 1.2842 | lr 4.88e-05 | grad 1.80 | tok/s 9927
step   1500 | loss 1.1375 | lr 5.03e-05 | grad 1.24 | tok/s 9980
step   1510 | loss 1.1563 | lr 5.19e-05 | grad 1.49 | tok/s 10016
step   1520 | loss 1.0690 | lr 5.35e-05 | grad 1.37 | tok/s 10257
step   1530 | loss 1.1143 | lr 5.50e-05 | grad 1.32 | tok/s 10525
step   1540 | loss 1.0401 | lr 5.65e-05 | grad 1.80 | tok/s 10537
step   1550 | loss 1.0149 | lr 5.81e-05 | grad 1.27 | tok/s 10472
step   1560 | loss 1.0440 | lr 5.96e-05 | grad 2.08 | tok/s 10752
step   1570 | loss 1.6782 | lr 6.11e-05 | grad 2.72 | tok/s 9979
step   1580 | loss 2.1795 | lr 6.27e-05 | grad 2.83 | tok/s 9809
step   1590 | loss 2.1536 | lr 6.42e-05 | grad 2.16 | tok/s 9796
step   1600 | loss 1.7747 | lr 6.56e-05 | grad 1.80 | tok/s 9544
step   1610 | loss 1.7808 | lr 6.71e-05 | grad 2.10 | tok/s 9615
step   1620 | loss 1.8677 | lr 6.86e-05 | grad 2.13 | tok/s 9568
step   1630 | loss 1.5861 | lr 7.00e-05 | grad 2.00 | tok/s 9660
step   1640 | loss 1.9117 | lr 7.14e-05 | grad 1.70 | tok/s 9871
step   1650 | loss 1.4629 | lr 7.28e-05 | grad 1.77 | tok/s 9943
step   1660 | loss 1.4343 | lr 7.42e-05 | grad 2.43 | tok/s 9791
step   1670 | loss 1.7128 | lr 7.56e-05 | grad 3.40 | tok/s 9720
step   1680 | loss 1.8295 | lr 7.69e-05 | grad 1.58 | tok/s 9782
step   1690 | loss 1.5259 | lr 7.82e-05 | grad 1.98 | tok/s 9749
step   1700 | loss 1.4715 | lr 7.95e-05 | grad 1.47 | tok/s 10184
step   1710 | loss 1.6766 | lr 8.07e-05 | grad 2.40 | tok/s 10078
step   1720 | loss 1.6589 | lr 8.19e-05 | grad 2.28 | tok/s 9587
step   1730 | loss 1.6260 | lr 8.31e-05 | grad 1.89 | tok/s 9214
step   1740 | loss 2.0289 | lr 8.43e-05 | grad 2.57 | tok/s 9666
step   1750 | loss 1.8358 | lr 8.54e-05 | grad 2.23 | tok/s 10327
step   1760 | loss 1.7844 | lr 8.65e-05 | grad 1.88 | tok/s 9971
step   1770 | loss 1.7090 | lr 8.75e-05 | grad 2.25 | tok/s 10089
step   1780 | loss 1.5741 | lr 8.85e-05 | grad 2.28 | tok/s 10335
step   1790 | loss 1.5189 | lr 8.95e-05 | grad 1.71 | tok/s 10316
step   1800 | loss 1.5797 | lr 9.05e-05 | grad 2.30 | tok/s 10119
step   1810 | loss 1.7295 | lr 9.14e-05 | grad 1.70 | tok/s 10496
step   1820 | loss 1.5575 | lr 9.22e-05 | grad 2.40 | tok/s 10637
step   1830 | loss 1.6574 | lr 9.30e-05 | grad 3.47 | tok/s 10526
step   1840 | loss 1.7497 | lr 9.38e-05 | grad 1.96 | tok/s 9702
step   1850 | loss 1.5917 | lr 9.45e-05 | grad 2.22 | tok/s 10500
step   1860 | loss 1.4674 | lr 9.52e-05 | grad 1.39 | tok/s 10609
step   1870 | loss 1.6052 | lr 9.59e-05 | grad 3.41 | tok/s 10568
step   1880 | loss 1.6172 | lr 9.65e-05 | grad 1.71 | tok/s 10870
step   1890 | loss 1.1776 | lr 9.70e-05 | grad 1.87 | tok/s 10733
step   1900 | loss 1.4797 | lr 9.75e-05 | grad 2.36 | tok/s 10848
step   1910 | loss 1.8419 | lr 9.80e-05 | grad 2.90 | tok/s 10684
step   1920 | loss 1.8444 | lr 9.84e-05 | grad 2.56 | tok/s 10686
step   1930 | loss 1.9948 | lr 9.88e-05 | grad 1.79 | tok/s 10935
step   1940 | loss 1.5052 | lr 9.91e-05 | grad 2.12 | tok/s 10833
step   1950 | loss 1.6353 | lr 9.94e-05 | grad 1.68 | tok/s 10765
step   1960 | loss 1.5475 | lr 9.96e-05 | grad 1.65 | tok/s 11190
step   1970 | loss 1.9415 | lr 9.98e-05 | grad 1.89 | tok/s 10877
step   1980 | loss 2.1291 | lr 9.99e-05 | grad 8.31 | tok/s 10460
step   1990 | loss 1.8054 | lr 1.00e-04 | grad 1.59 | tok/s 10594
step   2000 | loss 2.0216 | lr 1.00e-04 | grad 2.37 | tok/s 10631
  >>> saved checkpoint: checkpoint_step_002000_loss_2.0216.pt
step   2010 | loss 1.6864 | lr 1.00e-04 | grad 2.29 | tok/s 5067
step   2020 | loss 1.9716 | lr 9.99e-05 | grad 1.87 | tok/s 10249
step   2030 | loss 1.5845 | lr 9.98e-05 | grad 2.45 | tok/s 10308
step   2040 | loss 1.5762 | lr 9.96e-05 | grad 1.52 | tok/s 10580
step   2050 | loss 1.4663 | lr 9.94e-05 | grad 1.18 | tok/s 10917
step   2060 | loss 1.7050 | lr 9.92e-05 | grad 6.84 | tok/s 10904
step   2070 | loss 1.5696 | lr 9.88e-05 | grad 2.06 | tok/s 10610
step   2080 | loss 1.6978 | lr 9.85e-05 | grad 1.90 | tok/s 10295
step   2090 | loss 1.7912 | lr 9.81e-05 | grad 1.47 | tok/s 11066
step   2100 | loss 1.8244 | lr 9.76e-05 | grad 2.74 | tok/s 10718
step   2110 | loss 1.8430 | lr 9.71e-05 | grad 1.96 | tok/s 10951
step   2120 | loss 1.8112 | lr 9.66e-05 | grad 1.39 | tok/s 10940
step   2130 | loss 1.8312 | lr 9.60e-05 | grad 2.89 | tok/s 10388
step   2140 | loss 1.5599 | lr 9.54e-05 | grad 2.59 | tok/s 10505
step   2150 | loss 1.5266 | lr 9.47e-05 | grad 1.47 | tok/s 10290
step   2160 | loss 1.0784 | lr 9.40e-05 | grad 1.67 | tok/s 11061
step   2170 | loss 1.2223 | lr 9.32e-05 | grad 2.33 | tok/s 10941
step   2180 | loss 1.5363 | lr 9.24e-05 | grad 1.59 | tok/s 10572
step   2190 | loss 1.7072 | lr 9.15e-05 | grad 1.36 | tok/s 11104
step   2200 | loss 1.4463 | lr 9.06e-05 | grad 1.30 | tok/s 11096
step   2210 | loss 1.3460 | lr 8.97e-05 | grad 1.20 | tok/s 11125
step   2220 | loss 1.3404 | lr 8.87e-05 | grad 1.09 | tok/s 10921
step   2230 | loss 1.3159 | lr 8.77e-05 | grad 1.19 | tok/s 10928
step   2240 | loss 1.2444 | lr 8.67e-05 | grad 1.06 | tok/s 10989
step   2250 | loss 1.2584 | lr 8.56e-05 | grad 1.14 | tok/s 11099
step   2260 | loss 1.2751 | lr 8.45e-05 | grad 1.23 | tok/s 11326
step   2270 | loss 1.2938 | lr 8.34e-05 | grad 1.08 | tok/s 11205
step   2280 | loss 1.2903 | lr 8.22e-05 | grad 1.10 | tok/s 10689
step   2290 | loss 1.2111 | lr 8.10e-05 | grad 0.99 | tok/s 11209
step   2300 | loss 1.1668 | lr 7.97e-05 | grad 1.35 | tok/s 11179
step   2310 | loss 1.2142 | lr 7.85e-05 | grad 1.14 | tok/s 11169
step   2320 | loss 1.2246 | lr 7.72e-05 | grad 0.94 | tok/s 11111
step   2330 | loss 1.3858 | lr 7.58e-05 | grad 1.13 | tok/s 11006
step   2340 | loss 1.3512 | lr 7.45e-05 | grad 1.09 | tok/s 11044
step   2350 | loss 1.2286 | lr 7.31e-05 | grad 1.10 | tok/s 10995
step   2360 | loss 1.1969 | lr 7.17e-05 | grad 1.43 | tok/s 11093
step   2370 | loss 1.1855 | lr 7.03e-05 | grad 1.27 | tok/s 11159
step   2380 | loss 1.2253 | lr 6.89e-05 | grad 1.05 | tok/s 11290
step   2390 | loss 1.2502 | lr 6.74e-05 | grad 1.07 | tok/s 11423
step   2400 | loss 1.2389 | lr 6.59e-05 | grad 1.28 | tok/s 11427
step   2410 | loss 1.3114 | lr 6.45e-05 | grad 1.21 | tok/s 10885
step   2420 | loss 1.2683 | lr 6.30e-05 | grad 1.11 | tok/s 10901
step   2430 | loss 1.1806 | lr 6.15e-05 | grad 1.14 | tok/s 10793
step   2440 | loss 1.2763 | lr 5.99e-05 | grad 1.04 | tok/s 10786
step   2450 | loss 1.1602 | lr 5.84e-05 | grad 1.11 | tok/s 10983
step   2460 | loss 1.2027 | lr 5.69e-05 | grad 0.91 | tok/s 11027
step   2470 | loss 2.1776 | lr 5.53e-05 | grad 3.29 | tok/s 10728
step   2480 | loss 1.7694 | lr 5.38e-05 | grad 1.72 | tok/s 10791
step   2490 | loss 1.7368 | lr 5.22e-05 | grad 2.59 | tok/s 10497
step   2500 | loss 1.3670 | lr 5.07e-05 | grad 2.57 | tok/s 10856
step   2510 | loss 1.6323 | lr 4.91e-05 | grad 1.61 | tok/s 10844
step   2520 | loss 1.7583 | lr 4.75e-05 | grad 4.36 | tok/s 10388
step   2530 | loss 1.8763 | lr 4.60e-05 | grad 1.54 | tok/s 10330
step   2540 | loss 1.6043 | lr 4.45e-05 | grad 1.23 | tok/s 11115
step   2550 | loss 1.6149 | lr 4.29e-05 | grad 1.39 | tok/s 11047
step   2560 | loss 1.4987 | lr 4.14e-05 | grad 1.49 | tok/s 10711
step   2570 | loss 1.7045 | lr 3.99e-05 | grad 1.74 | tok/s 11155
step   2580 | loss 1.5636 | lr 3.83e-05 | grad 1.60 | tok/s 10557
step   2590 | loss 1.5683 | lr 3.68e-05 | grad 1.62 | tok/s 11003
step   2600 | loss 1.5130 | lr 3.54e-05 | grad 1.55 | tok/s 10800
step   2610 | loss 1.5904 | lr 3.39e-05 | grad 2.10 | tok/s 11017
step   2620 | loss 1.7635 | lr 3.24e-05 | grad 1.72 | tok/s 11364
step   2630 | loss 1.8674 | lr 3.10e-05 | grad 1.60 | tok/s 11256
step   2640 | loss 1.4769 | lr 2.96e-05 | grad 1.47 | tok/s 10757
step   2650 | loss 1.4102 | lr 2.82e-05 | grad 3.23 | tok/s 10176
step   2660 | loss 1.9604 | lr 2.68e-05 | grad 1.93 | tok/s 10515
step   2670 | loss 1.7445 | lr 2.54e-05 | grad 1.31 | tok/s 10143
step   2680 | loss 1.8040 | lr 2.41e-05 | grad 1.57 | tok/s 10699
step   2690 | loss 1.7255 | lr 2.28e-05 | grad 1.32 | tok/s 10728
step   2700 | loss 1.5136 | lr 2.15e-05 | grad 1.55 | tok/s 10806
step   2710 | loss 1.4838 | lr 2.03e-05 | grad 1.33 | tok/s 10562
step   2720 | loss 1.7636 | lr 1.91e-05 | grad 2.74 | tok/s 10802
step   2730 | loss 1.6348 | lr 1.79e-05 | grad 4.12 | tok/s 10403
step   2740 | loss 1.7274 | lr 1.67e-05 | grad 2.86 | tok/s 10584
step   2750 | loss 1.5096 | lr 1.56e-05 | grad 1.58 | tok/s 10733
step   2760 | loss 1.4290 | lr 1.45e-05 | grad 1.29 | tok/s 10691
step   2770 | loss 1.6119 | lr 1.35e-05 | grad 1.69 | tok/s 9531

Training complete! Final step: 2779
