Using device: cuda
Output directory: output/level56_100m_20260114_020230
Auto r_h_mode: spectral_norm (level 56 has full W_h)
Created Level 56 model: dim=768, depth=19, params=101,093,760
Model: Level 56, 101,093,760 parameters

Starting training from step 0...
Batch size: 8, Chunk size: 256
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 5.6467 | lr 9.00e-07 | grad 66.56 | tok/s 2716
step     20 | loss 5.5690 | lr 1.90e-06 | grad 22.19 | tok/s 4842
step     30 | loss 5.4936 | lr 2.90e-06 | grad 7.89 | tok/s 4647
step     40 | loss 5.1875 | lr 3.90e-06 | grad 8.52 | tok/s 4153
step     50 | loss 5.1465 | lr 4.90e-06 | grad 7.14 | tok/s 4549
step     60 | loss 4.8286 | lr 5.90e-06 | grad 6.78 | tok/s 4892
step     70 | loss 4.4797 | lr 6.90e-06 | grad 6.14 | tok/s 4808
step     80 | loss 4.1035 | lr 7.90e-06 | grad 5.33 | tok/s 4703
step     90 | loss 3.4899 | lr 8.90e-06 | grad 4.42 | tok/s 4471
step    100 | loss 4.1286 | lr 9.90e-06 | grad 8.54 | tok/s 4929
step    110 | loss 4.3065 | lr 1.09e-05 | grad 4.26 | tok/s 3809
step    120 | loss 2.8966 | lr 1.19e-05 | grad 3.33 | tok/s 3878
step    130 | loss 2.7104 | lr 1.29e-05 | grad 5.26 | tok/s 4581
step    140 | loss 2.5285 | lr 1.39e-05 | grad 2.78 | tok/s 4483
step    150 | loss 3.4544 | lr 1.49e-05 | grad 5.45 | tok/s 4312
step    160 | loss 5.1228 | lr 1.59e-05 | grad 4.57 | tok/s 4484
step    170 | loss 4.6488 | lr 1.69e-05 | grad 6.23 | tok/s 4692
step    180 | loss 4.6322 | lr 1.79e-05 | grad 7.64 | tok/s 4712
step    190 | loss 3.8163 | lr 1.89e-05 | grad 13.99 | tok/s 4672
step    200 | loss 3.6256 | lr 1.99e-05 | grad 6.56 | tok/s 5139
step    210 | loss 3.3032 | lr 2.09e-05 | grad 15.18 | tok/s 4894
step    220 | loss 3.1467 | lr 2.19e-05 | grad 2.89 | tok/s 4656
step    230 | loss 3.0389 | lr 2.29e-05 | grad 4.81 | tok/s 4776
step    240 | loss 3.1908 | lr 2.39e-05 | grad 8.80 | tok/s 4817
step    250 | loss 2.8335 | lr 2.49e-05 | grad 5.62 | tok/s 6643
step    260 | loss 2.7540 | lr 2.59e-05 | grad 4.82 | tok/s 7426
step    270 | loss 2.4221 | lr 2.69e-05 | grad 4.09 | tok/s 7298
step    280 | loss 2.3437 | lr 2.79e-05 | grad 3.11 | tok/s 7397
step    290 | loss 2.6362 | lr 2.89e-05 | grad 4.15 | tok/s 7468
step    300 | loss 2.5702 | lr 2.99e-05 | grad 3.59 | tok/s 7590
step    310 | loss 2.5382 | lr 3.09e-05 | grad 4.74 | tok/s 7566
step    320 | loss 2.1283 | lr 3.19e-05 | grad 2.39 | tok/s 7559
step    330 | loss 2.3453 | lr 3.29e-05 | grad 3.55 | tok/s 7484
step    340 | loss 2.2347 | lr 3.39e-05 | grad 6.60 | tok/s 7527
step    350 | loss 2.3562 | lr 3.49e-05 | grad 4.77 | tok/s 7613
step    360 | loss 2.3846 | lr 3.59e-05 | grad 3.12 | tok/s 7665
step    370 | loss 2.1373 | lr 3.69e-05 | grad 4.02 | tok/s 7789
step    380 | loss 2.3049 | lr 3.79e-05 | grad 8.92 | tok/s 7851
step    390 | loss 2.2357 | lr 3.89e-05 | grad 8.72 | tok/s 7926
step    400 | loss 1.9248 | lr 3.99e-05 | grad 3.46 | tok/s 8000
step    410 | loss 2.0914 | lr 4.09e-05 | grad 2.63 | tok/s 8077
step    420 | loss 2.1642 | lr 4.19e-05 | grad 3.11 | tok/s 7854
step    430 | loss 2.1296 | lr 4.29e-05 | grad 2.43 | tok/s 7945
step    440 | loss 2.4874 | lr 4.39e-05 | grad 2.56 | tok/s 8101
step    450 | loss 2.1337 | lr 4.49e-05 | grad 2.57 | tok/s 7413
step    460 | loss 3.1998 | lr 4.59e-05 | grad 5.19 | tok/s 7412
step    470 | loss 2.5300 | lr 4.69e-05 | grad 3.02 | tok/s 7679
step    480 | loss 2.4251 | lr 4.79e-05 | grad 3.60 | tok/s 7554
step    490 | loss 2.4548 | lr 4.89e-05 | grad 2.80 | tok/s 7744
step    500 | loss 2.2736 | lr 4.99e-05 | grad 3.01 | tok/s 7612
step    510 | loss 1.8657 | lr 5.09e-05 | grad 2.35 | tok/s 8034
step    520 | loss 1.6068 | lr 5.19e-05 | grad 2.19 | tok/s 8075
step    530 | loss 2.3582 | lr 5.29e-05 | grad 5.06 | tok/s 7383
step    540 | loss 2.0326 | lr 5.39e-05 | grad 3.32 | tok/s 6854
step    550 | loss 2.0263 | lr 5.49e-05 | grad 3.41 | tok/s 5410
step    560 | loss 2.7251 | lr 5.59e-05 | grad 4.99 | tok/s 4801
step    570 | loss 2.3533 | lr 5.69e-05 | grad 5.88 | tok/s 4610
step    580 | loss 1.0732 | lr 5.79e-05 | grad 1.41 | tok/s 4229
step    590 | loss 0.6115 | lr 5.89e-05 | grad 1.65 | tok/s 4158
step    600 | loss 0.4392 | lr 5.99e-05 | grad 0.96 | tok/s 4424
step    610 | loss 2.6642 | lr 6.09e-05 | grad 4.16 | tok/s 4845
step    620 | loss 2.3692 | lr 6.19e-05 | grad 2.52 | tok/s 6898
step    630 | loss 1.8016 | lr 6.29e-05 | grad 2.58 | tok/s 6673
step    640 | loss 1.9653 | lr 6.39e-05 | grad 4.58 | tok/s 6229
step    650 | loss 2.5210 | lr 6.49e-05 | grad 2.47 | tok/s 6243
step    660 | loss 2.1601 | lr 6.59e-05 | grad 2.88 | tok/s 6505
step    670 | loss 2.5708 | lr 6.69e-05 | grad 2.94 | tok/s 6279
step    680 | loss 2.1545 | lr 6.79e-05 | grad 2.18 | tok/s 5952
step    690 | loss 1.9584 | lr 6.89e-05 | grad 2.31 | tok/s 6219
step    700 | loss 2.2174 | lr 6.99e-05 | grad 2.32 | tok/s 6276
step    710 | loss 1.3969 | lr 7.09e-05 | grad 2.93 | tok/s 6345
step    720 | loss 1.6107 | lr 7.19e-05 | grad 29.05 | tok/s 6383
step    730 | loss 2.2624 | lr 7.29e-05 | grad 3.19 | tok/s 6024
step    740 | loss 2.0087 | lr 7.39e-05 | grad 2.54 | tok/s 6340
step    750 | loss 1.9927 | lr 7.49e-05 | grad 2.82 | tok/s 6499
step    760 | loss 1.8113 | lr 7.59e-05 | grad 1.99 | tok/s 6970
step    770 | loss 1.5629 | lr 7.69e-05 | grad 1.77 | tok/s 7043
step    780 | loss 1.5215 | lr 7.79e-05 | grad 1.80 | tok/s 6019
step    790 | loss 1.5149 | lr 7.89e-05 | grad 3.16 | tok/s 5980
step    800 | loss 1.6669 | lr 7.99e-05 | grad 2.71 | tok/s 6081
step    810 | loss 1.3023 | lr 8.09e-05 | grad 1.92 | tok/s 6242
step    820 | loss 1.5534 | lr 8.19e-05 | grad 2.66 | tok/s 6207
step    830 | loss 1.9521 | lr 8.29e-05 | grad 1.93 | tok/s 5953
step    840 | loss 2.2587 | lr 8.39e-05 | grad 2.80 | tok/s 6213
step    850 | loss 2.1102 | lr 8.49e-05 | grad 5.00 | tok/s 6169
step    860 | loss 2.2171 | lr 8.59e-05 | grad 5.00 | tok/s 5989
step    870 | loss 2.4274 | lr 8.69e-05 | grad 1.90 | tok/s 6155
step    880 | loss 2.1633 | lr 8.79e-05 | grad 3.05 | tok/s 6239
step    890 | loss 2.4944 | lr 8.89e-05 | grad 2.61 | tok/s 6056
step    900 | loss 1.9324 | lr 8.99e-05 | grad 1.81 | tok/s 5997
step    910 | loss 2.2913 | lr 9.09e-05 | grad 1.96 | tok/s 5847
step    920 | loss 1.6509 | lr 9.19e-05 | grad 4.54 | tok/s 5964
step    930 | loss 2.2692 | lr 9.29e-05 | grad 2.49 | tok/s 5666
step    940 | loss 1.7907 | lr 9.39e-05 | grad 12.46 | tok/s 5785
step    950 | loss 2.5308 | lr 9.49e-05 | grad 5.03 | tok/s 5780
step    960 | loss 2.3074 | lr 9.59e-05 | grad 2.75 | tok/s 5935
step    970 | loss 2.0027 | lr 9.69e-05 | grad 1.89 | tok/s 6212
step    980 | loss 1.7260 | lr 9.79e-05 | grad 1.56 | tok/s 6015
step    990 | loss 1.7495 | lr 9.89e-05 | grad 3.02 | tok/s 6578
step   1000 | loss 1.9097 | lr 9.99e-05 | grad 1.69 | tok/s 6871
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9097.pt
step   1010 | loss 1.8271 | lr 1.02e-06 | grad 1.91 | tok/s 3866
step   1020 | loss 1.9639 | lr 1.09e-06 | grad 1.83 | tok/s 7128
step   1030 | loss 2.0145 | lr 1.21e-06 | grad 2.16 | tok/s 7345
step   1040 | loss 2.1140 | lr 1.37e-06 | grad 3.23 | tok/s 7116
step   1050 | loss 2.3060 | lr 1.59e-06 | grad 2.26 | tok/s 6903
step   1060 | loss 1.8963 | lr 1.85e-06 | grad 2.18 | tok/s 6541
step   1070 | loss 1.9356 | lr 2.16e-06 | grad 2.09 | tok/s 6488
step   1080 | loss 1.6633 | lr 2.52e-06 | grad 2.11 | tok/s 6694
step   1090 | loss 1.7544 | lr 2.92e-06 | grad 2.18 | tok/s 6722
step   1100 | loss 1.8407 | lr 3.37e-06 | grad 1.58 | tok/s 6434
step   1110 | loss 1.9998 | lr 3.87e-06 | grad 2.24 | tok/s 6850
step   1120 | loss 1.8372 | lr 4.42e-06 | grad 2.06 | tok/s 6811
step   1130 | loss 1.7741 | lr 5.01e-06 | grad 1.58 | tok/s 6732
step   1140 | loss 1.7295 | lr 5.65e-06 | grad 1.79 | tok/s 6338
step   1150 | loss 1.6315 | lr 6.32e-06 | grad 1.33 | tok/s 5951
step   1160 | loss 1.6856 | lr 7.05e-06 | grad 1.75 | tok/s 5972
step   1170 | loss 2.3972 | lr 7.81e-06 | grad 3.56 | tok/s 6221
step   1180 | loss 1.9339 | lr 8.62e-06 | grad 1.74 | tok/s 5935
step   1190 | loss 1.9854 | lr 9.47e-06 | grad 2.05 | tok/s 6161
step   1200 | loss 1.8181 | lr 1.04e-05 | grad 2.27 | tok/s 6214
step   1210 | loss 1.7936 | lr 1.13e-05 | grad 1.61 | tok/s 6216
step   1220 | loss 1.6227 | lr 1.23e-05 | grad 1.51 | tok/s 6068
step   1230 | loss 1.6012 | lr 1.33e-05 | grad 1.70 | tok/s 5805
step   1240 | loss 1.6940 | lr 1.43e-05 | grad 1.77 | tok/s 5750
step   1250 | loss 1.9080 | lr 1.54e-05 | grad 3.79 | tok/s 5917
step   1260 | loss 1.8642 | lr 1.65e-05 | grad 10.05 | tok/s 5883
step   1270 | loss 2.0018 | lr 1.76e-05 | grad 1.92 | tok/s 6201
step   1280 | loss 2.1495 | lr 1.88e-05 | grad 2.38 | tok/s 6308
step   1290 | loss 1.6396 | lr 2.00e-05 | grad 2.39 | tok/s 6094
step   1300 | loss 1.9640 | lr 2.13e-05 | grad 1.95 | tok/s 6316
step   1310 | loss 1.6835 | lr 2.25e-05 | grad 1.90 | tok/s 5891
step   1320 | loss 1.7172 | lr 2.38e-05 | grad 1.88 | tok/s 6299
step   1330 | loss 2.4243 | lr 2.52e-05 | grad 2.51 | tok/s 6247
step   1340 | loss 1.8148 | lr 2.65e-05 | grad 1.60 | tok/s 6365
step   1350 | loss 1.8851 | lr 2.79e-05 | grad 2.79 | tok/s 6353
step   1360 | loss 2.1537 | lr 2.93e-05 | grad 2.37 | tok/s 6642
step   1370 | loss 1.9451 | lr 3.07e-05 | grad 1.74 | tok/s 7065
step   1380 | loss 1.6891 | lr 3.21e-05 | grad 1.50 | tok/s 7105
step   1390 | loss 1.5061 | lr 3.36e-05 | grad 1.99 | tok/s 7144
step   1400 | loss 1.8622 | lr 3.51e-05 | grad 1.69 | tok/s 7077
step   1410 | loss 1.6244 | lr 3.65e-05 | grad 1.82 | tok/s 6367
step   1420 | loss 1.5489 | lr 3.80e-05 | grad 3.63 | tok/s 5875
step   1430 | loss 1.6645 | lr 3.96e-05 | grad 1.81 | tok/s 6170
step   1440 | loss 1.3802 | lr 4.11e-05 | grad 1.21 | tok/s 6322
step   1450 | loss 1.4732 | lr 4.26e-05 | grad 2.16 | tok/s 6398
step   1460 | loss 1.7163 | lr 4.41e-05 | grad 1.82 | tok/s 6495
step   1470 | loss 1.4500 | lr 4.57e-05 | grad 1.58 | tok/s 6511
step   1480 | loss 1.3317 | lr 4.72e-05 | grad 1.87 | tok/s 6577
step   1490 | loss 1.2745 | lr 4.88e-05 | grad 1.83 | tok/s 6419
step   1500 | loss 1.1261 | lr 5.03e-05 | grad 1.28 | tok/s 6592
step   1510 | loss 1.1473 | lr 5.19e-05 | grad 1.49 | tok/s 6564
step   1520 | loss 1.0650 | lr 5.35e-05 | grad 1.37 | tok/s 6643
step   1530 | loss 1.1186 | lr 5.50e-05 | grad 1.40 | tok/s 6418
step   1540 | loss 1.0497 | lr 5.65e-05 | grad 1.72 | tok/s 6456
step   1550 | loss 1.0207 | lr 5.81e-05 | grad 1.20 | tok/s 6510
step   1560 | loss 1.0479 | lr 5.96e-05 | grad 2.25 | tok/s 6495
step   1570 | loss 1.6826 | lr 6.11e-05 | grad 2.69 | tok/s 6120
step   1580 | loss 2.1690 | lr 6.27e-05 | grad 2.80 | tok/s 6417
step   1590 | loss 2.1891 | lr 6.42e-05 | grad 2.09 | tok/s 6212
step   1600 | loss 1.7754 | lr 6.56e-05 | grad 1.60 | tok/s 6256
step   1610 | loss 1.8118 | lr 6.71e-05 | grad 2.14 | tok/s 6359
step   1620 | loss 1.9178 | lr 6.86e-05 | grad 2.19 | tok/s 6158
step   1630 | loss 1.6002 | lr 7.00e-05 | grad 1.97 | tok/s 6216
step   1640 | loss 1.9251 | lr 7.14e-05 | grad 1.64 | tok/s 6540
step   1650 | loss 1.4407 | lr 7.28e-05 | grad 1.71 | tok/s 6668
step   1660 | loss 1.4282 | lr 7.42e-05 | grad 2.39 | tok/s 6718
step   1670 | loss 1.7187 | lr 7.56e-05 | grad 3.24 | tok/s 6516
step   1680 | loss 1.6798 | lr 7.69e-05 | grad 1.56 | tok/s 6671
step   1690 | loss 1.5423 | lr 7.82e-05 | grad 2.05 | tok/s 6619
step   1700 | loss 1.4686 | lr 7.95e-05 | grad 1.46 | tok/s 6675
step   1710 | loss 1.6908 | lr 8.07e-05 | grad 2.16 | tok/s 6369
step   1720 | loss 1.6794 | lr 8.19e-05 | grad 2.33 | tok/s 6463
step   1730 | loss 1.6412 | lr 8.31e-05 | grad 1.88 | tok/s 6518
step   1740 | loss 2.0706 | lr 8.43e-05 | grad 2.67 | tok/s 6522
step   1750 | loss 1.8636 | lr 8.54e-05 | grad 2.03 | tok/s 6563
step   1760 | loss 1.8149 | lr 8.65e-05 | grad 1.91 | tok/s 6395
step   1770 | loss 1.7513 | lr 8.75e-05 | grad 2.23 | tok/s 6731
step   1780 | loss 1.5969 | lr 8.85e-05 | grad 2.42 | tok/s 6815
step   1790 | loss 1.5407 | lr 8.95e-05 | grad 1.76 | tok/s 6816
step   1800 | loss 1.5823 | lr 9.05e-05 | grad 2.47 | tok/s 6907

Training complete! Final step: 1809
