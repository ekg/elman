Using device: cuda
Output directory: output/level51_100m_20260114_020544
Auto r_h_mode: spectral_norm (level 51 has full W_h)
Created Level 51 model: dim=768, depth=32, params=99,349,248
Model: Level 51, 99,349,248 parameters

Starting training from step 0...
Batch size: 8, Chunk size: 256
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 5.6452 | lr 9.00e-07 | grad 29.21 | tok/s 3633
step     20 | loss 5.2068 | lr 1.90e-06 | grad 14.76 | tok/s 7475
step     30 | loss 4.4631 | lr 2.90e-06 | grad 7.22 | tok/s 7549
step     40 | loss 4.0324 | lr 3.90e-06 | grad 7.08 | tok/s 7569
step     50 | loss 3.6915 | lr 4.90e-06 | grad 9.63 | tok/s 7843
step     60 | loss 3.6852 | lr 5.90e-06 | grad 3.94 | tok/s 7627
step     70 | loss 3.5782 | lr 6.90e-06 | grad 3.16 | tok/s 7306
step     80 | loss 3.7149 | lr 7.90e-06 | grad 3.34 | tok/s 7445
step     90 | loss 3.3465 | lr 8.90e-06 | grad 2.49 | tok/s 7623
step    100 | loss 4.3662 | lr 9.90e-06 | grad 17.46 | tok/s 7332
step    110 | loss 4.5372 | lr 1.09e-05 | grad 15.89 | tok/s 7585
step    120 | loss 3.8055 | lr 1.19e-05 | grad 3.21 | tok/s 7647
step    130 | loss 3.3783 | lr 1.29e-05 | grad 6.12 | tok/s 7308
step    140 | loss 3.3558 | lr 1.39e-05 | grad 2.28 | tok/s 7618
step    150 | loss 4.1901 | lr 1.49e-05 | grad 4.72 | tok/s 7618
step    160 | loss 5.6137 | lr 1.59e-05 | grad 4.60 | tok/s 7910
step    170 | loss 4.7425 | lr 1.69e-05 | grad 11.21 | tok/s 7339
step    180 | loss 4.7173 | lr 1.79e-05 | grad 16.65 | tok/s 7326
step    190 | loss 4.0403 | lr 1.89e-05 | grad 13.26 | tok/s 7073
step    200 | loss 4.2156 | lr 1.99e-05 | grad 8.59 | tok/s 7118
step    210 | loss 3.9182 | lr 2.09e-05 | grad 12.77 | tok/s 7089
step    220 | loss 3.8682 | lr 2.19e-05 | grad 4.05 | tok/s 7106
step    230 | loss 3.9459 | lr 2.29e-05 | grad 13.82 | tok/s 7338
step    240 | loss 4.1803 | lr 2.39e-05 | grad 18.05 | tok/s 7414
step    250 | loss 3.8769 | lr 2.49e-05 | grad 13.48 | tok/s 7259
step    260 | loss 3.7320 | lr 2.59e-05 | grad 17.63 | tok/s 7201
step    270 | loss 3.6064 | lr 2.69e-05 | grad 8.19 | tok/s 7259
step    280 | loss 3.4556 | lr 2.79e-05 | grad 15.06 | tok/s 7294
step    290 | loss 3.9479 | lr 2.89e-05 | grad 3.68 | tok/s 7259
step    300 | loss 3.8240 | lr 2.99e-05 | grad 8.52 | tok/s 7301
step    310 | loss 3.5724 | lr 3.09e-05 | grad 10.88 | tok/s 7174
step    320 | loss 3.3130 | lr 3.19e-05 | grad 7.11 | tok/s 7133
step    330 | loss 3.5414 | lr 3.29e-05 | grad 3.17 | tok/s 7313
step    340 | loss 3.2294 | lr 3.39e-05 | grad 17.96 | tok/s 7365
step    350 | loss 3.6621 | lr 3.49e-05 | grad 10.07 | tok/s 7390
step    360 | loss 3.4546 | lr 3.59e-05 | grad 4.27 | tok/s 7400
step    370 | loss 3.4391 | lr 3.69e-05 | grad 8.34 | tok/s 7392
step    380 | loss 3.7272 | lr 3.79e-05 | grad 20.90 | tok/s 7257
step    390 | loss 3.4641 | lr 3.89e-05 | grad 10.01 | tok/s 7215
step    400 | loss 3.0744 | lr 3.99e-05 | grad 15.42 | tok/s 7351
step    410 | loss 3.4365 | lr 4.09e-05 | grad 7.75 | tok/s 7434
step    420 | loss 3.2417 | lr 4.19e-05 | grad 4.74 | tok/s 7369
step    430 | loss 3.3793 | lr 4.29e-05 | grad 1.57 | tok/s 7316
step    440 | loss 3.3202 | lr 4.39e-05 | grad 1.44 | tok/s 7268
step    450 | loss 3.1668 | lr 4.49e-05 | grad 1.51 | tok/s 7427
step    460 | loss 3.9802 | lr 4.59e-05 | grad 4.57 | tok/s 7175
step    470 | loss 3.3026 | lr 4.69e-05 | grad 2.65 | tok/s 7019
step    480 | loss 3.1926 | lr 4.79e-05 | grad 2.33 | tok/s 6989
step    490 | loss 3.2371 | lr 4.89e-05 | grad 2.35 | tok/s 7808
step    500 | loss 3.0298 | lr 4.99e-05 | grad 2.09 | tok/s 7587
step    510 | loss 2.8156 | lr 5.09e-05 | grad 1.95 | tok/s 7705
step    520 | loss 2.6251 | lr 5.19e-05 | grad 2.12 | tok/s 7643
step    530 | loss 3.0662 | lr 5.29e-05 | grad 3.21 | tok/s 7474
step    540 | loss 2.9783 | lr 5.39e-05 | grad 3.38 | tok/s 7485
step    550 | loss 2.7473 | lr 5.49e-05 | grad 2.24 | tok/s 7473
step    560 | loss 3.3260 | lr 5.59e-05 | grad 4.22 | tok/s 7850
step    570 | loss 3.4607 | lr 5.69e-05 | grad 6.00 | tok/s 7734
step    580 | loss 2.2734 | lr 5.79e-05 | grad 1.70 | tok/s 7824
step    590 | loss 1.2861 | lr 5.89e-05 | grad 1.18 | tok/s 7982
step    600 | loss 0.8360 | lr 5.99e-05 | grad 0.77 | tok/s 7966
step    610 | loss 2.9357 | lr 6.09e-05 | grad 2.27 | tok/s 7300
step    620 | loss 3.1357 | lr 6.19e-05 | grad 1.84 | tok/s 7340
step    630 | loss 2.8225 | lr 6.29e-05 | grad 1.57 | tok/s 7269
step    640 | loss 2.7281 | lr 6.39e-05 | grad 2.27 | tok/s 6993
step    650 | loss 2.9761 | lr 6.49e-05 | grad 1.65 | tok/s 7089
step    660 | loss 2.7340 | lr 6.59e-05 | grad 1.78 | tok/s 7336
step    670 | loss 3.0731 | lr 6.69e-05 | grad 2.54 | tok/s 7138
step    680 | loss 2.7817 | lr 6.79e-05 | grad 1.48 | tok/s 6778
step    690 | loss 2.5237 | lr 6.89e-05 | grad 1.60 | tok/s 7319
step    700 | loss 2.8054 | lr 6.99e-05 | grad 1.56 | tok/s 7616
step    710 | loss 2.4000 | lr 7.09e-05 | grad 2.20 | tok/s 7510
step    720 | loss 2.4935 | lr 7.19e-05 | grad 11.36 | tok/s 7682
step    730 | loss 2.8778 | lr 7.29e-05 | grad 1.62 | tok/s 6797
step    740 | loss 2.6138 | lr 7.39e-05 | grad 1.59 | tok/s 7061
step    750 | loss 2.6820 | lr 7.49e-05 | grad 1.60 | tok/s 7392
step    760 | loss 2.3963 | lr 7.59e-05 | grad 1.40 | tok/s 7460
step    770 | loss 2.2317 | lr 7.69e-05 | grad 1.45 | tok/s 7443
step    780 | loss 2.1429 | lr 7.79e-05 | grad 1.31 | tok/s 7254
step    790 | loss 2.1362 | lr 7.89e-05 | grad 2.36 | tok/s 7122
step    800 | loss 2.3515 | lr 7.99e-05 | grad 1.70 | tok/s 7091
step    810 | loss 2.1074 | lr 8.09e-05 | grad 1.43 | tok/s 7126
step    820 | loss 2.1465 | lr 8.19e-05 | grad 1.75 | tok/s 6795
step    830 | loss 2.4493 | lr 8.29e-05 | grad 1.35 | tok/s 6672
step    840 | loss 2.7360 | lr 8.39e-05 | grad 2.13 | tok/s 7166
step    850 | loss 2.6432 | lr 8.49e-05 | grad 2.52 | tok/s 7447
step    860 | loss 2.6605 | lr 8.59e-05 | grad 3.05 | tok/s 7456
step    870 | loss 2.8947 | lr 8.69e-05 | grad 1.50 | tok/s 7445
step    880 | loss 2.7809 | lr 8.79e-05 | grad 2.21 | tok/s 7688
step    890 | loss 4.4383 | lr 8.89e-05 | grad 9.22 | tok/s 7168
step    900 | loss 2.5369 | lr 8.99e-05 | grad 1.79 | tok/s 7467
step    910 | loss 2.8617 | lr 9.09e-05 | grad 1.56 | tok/s 7315
step    920 | loss 2.2929 | lr 9.19e-05 | grad 4.96 | tok/s 7101
step    930 | loss 2.7864 | lr 9.29e-05 | grad 2.03 | tok/s 6635
step    940 | loss 2.3304 | lr 9.39e-05 | grad 4.75 | tok/s 7107
step    950 | loss 2.9562 | lr 9.49e-05 | grad 3.47 | tok/s 7080
step    960 | loss 2.9900 | lr 9.59e-05 | grad 2.17 | tok/s 7121
step    970 | loss 2.5540 | lr 9.69e-05 | grad 1.68 | tok/s 7150
step    980 | loss 2.2478 | lr 9.79e-05 | grad 1.16 | tok/s 7141
step    990 | loss 2.2508 | lr 9.89e-05 | grad 2.31 | tok/s 7149
step   1000 | loss 2.4294 | lr 9.99e-05 | grad 1.44 | tok/s 7486
  >>> saved checkpoint: checkpoint_step_001000_loss_2.4294.pt
step   1010 | loss 2.3137 | lr 1.02e-06 | grad 1.41 | tok/s 4280
step   1020 | loss 2.4072 | lr 1.09e-06 | grad 1.23 | tok/s 7404
step   1030 | loss 2.4098 | lr 1.21e-06 | grad 1.35 | tok/s 7297
step   1040 | loss 2.5488 | lr 1.37e-06 | grad 2.57 | tok/s 7234
step   1050 | loss 2.7003 | lr 1.59e-06 | grad 1.59 | tok/s 7067
step   1060 | loss 2.2491 | lr 1.85e-06 | grad 1.22 | tok/s 6752
step   1070 | loss 2.2782 | lr 2.16e-06 | grad 1.64 | tok/s 6938
step   1080 | loss 2.0849 | lr 2.52e-06 | grad 1.06 | tok/s 7086
step   1090 | loss 2.1408 | lr 2.92e-06 | grad 1.24 | tok/s 6828
step   1100 | loss 2.2145 | lr 3.37e-06 | grad 0.79 | tok/s 6455
step   1110 | loss 2.4999 | lr 3.87e-06 | grad 1.27 | tok/s 7030
step   1120 | loss 2.2913 | lr 4.42e-06 | grad 1.15 | tok/s 6902
step   1130 | loss 2.1905 | lr 5.01e-06 | grad 1.09 | tok/s 6518
step   1140 | loss 2.1780 | lr 5.65e-06 | grad 1.06 | tok/s 6752
step   1150 | loss 2.0552 | lr 6.32e-06 | grad 0.94 | tok/s 6677
step   1160 | loss 2.1232 | lr 7.05e-06 | grad 1.13 | tok/s 6618
step   1170 | loss 2.8425 | lr 7.81e-06 | grad 2.47 | tok/s 6991
step   1180 | loss 2.3200 | lr 8.62e-06 | grad 0.98 | tok/s 6668
step   1190 | loss 2.3640 | lr 9.47e-06 | grad 1.24 | tok/s 6757
step   1200 | loss 2.1662 | lr 1.04e-05 | grad 1.37 | tok/s 6758
step   1210 | loss 2.1903 | lr 1.13e-05 | grad 1.00 | tok/s 6925
step   1220 | loss 2.0225 | lr 1.23e-05 | grad 1.06 | tok/s 6893
step   1230 | loss 2.0186 | lr 1.33e-05 | grad 0.89 | tok/s 6790
step   1240 | loss 2.0885 | lr 1.43e-05 | grad 1.05 | tok/s 6766
step   1250 | loss 2.2630 | lr 1.54e-05 | grad 1.96 | tok/s 6989
step   1260 | loss 2.3106 | lr 1.65e-05 | grad 3.22 | tok/s 6809
step   1270 | loss 2.4312 | lr 1.76e-05 | grad 1.39 | tok/s 6768
step   1280 | loss 2.6448 | lr 1.88e-05 | grad 2.09 | tok/s 6909
step   1290 | loss 2.0979 | lr 2.00e-05 | grad 1.46 | tok/s 6942
step   1300 | loss 2.2956 | lr 2.13e-05 | grad 1.02 | tok/s 7078
step   1310 | loss 2.1299 | lr 2.25e-05 | grad 1.29 | tok/s 6728
step   1320 | loss 2.1752 | lr 2.38e-05 | grad 1.17 | tok/s 6985
step   1330 | loss 2.9227 | lr 2.52e-05 | grad 2.14 | tok/s 6848
step   1340 | loss 2.2611 | lr 2.65e-05 | grad 1.28 | tok/s 6993
step   1350 | loss 2.3331 | lr 2.79e-05 | grad 2.47 | tok/s 6547
step   1360 | loss 2.6492 | lr 2.93e-05 | grad 1.59 | tok/s 6873
step   1370 | loss 2.5161 | lr 3.07e-05 | grad 1.18 | tok/s 7071
step   1380 | loss 2.3114 | lr 3.21e-05 | grad 1.12 | tok/s 7272
step   1390 | loss 2.1576 | lr 3.36e-05 | grad 1.17 | tok/s 7331
step   1400 | loss 2.3046 | lr 3.51e-05 | grad 1.31 | tok/s 7280
step   1410 | loss 2.0392 | lr 3.65e-05 | grad 1.25 | tok/s 6926
step   1420 | loss 1.9789 | lr 3.80e-05 | grad 2.06 | tok/s 7074
step   1430 | loss 2.1627 | lr 3.96e-05 | grad 1.27 | tok/s 7567
step   1440 | loss 2.0302 | lr 4.11e-05 | grad 1.11 | tok/s 7577
step   1450 | loss 1.9537 | lr 4.26e-05 | grad 1.42 | tok/s 7177
step   1460 | loss 2.1561 | lr 4.41e-05 | grad 1.39 | tok/s 7130
step   1470 | loss 2.0556 | lr 4.57e-05 | grad 1.29 | tok/s 7394
step   1480 | loss 1.8694 | lr 4.72e-05 | grad 1.22 | tok/s 7295
step   1490 | loss 1.7680 | lr 4.88e-05 | grad 1.39 | tok/s 7526
step   1500 | loss 1.6344 | lr 5.03e-05 | grad 0.98 | tok/s 7516
step   1510 | loss 1.6533 | lr 5.19e-05 | grad 1.17 | tok/s 7567
step   1520 | loss 1.5637 | lr 5.35e-05 | grad 1.11 | tok/s 7696
step   1530 | loss 1.5780 | lr 5.50e-05 | grad 1.41 | tok/s 7755
step   1540 | loss 1.5671 | lr 5.65e-05 | grad 1.56 | tok/s 7368
step   1550 | loss 1.4583 | lr 5.81e-05 | grad 1.05 | tok/s 7207
step   1560 | loss 1.5238 | lr 5.96e-05 | grad 1.79 | tok/s 7130
step   1570 | loss 2.0977 | lr 6.11e-05 | grad 1.74 | tok/s 6682
step   1580 | loss 2.7682 | lr 6.27e-05 | grad 2.50 | tok/s 6994
step   1590 | loss 2.6768 | lr 6.42e-05 | grad 1.40 | tok/s 6997
step   1600 | loss 2.1274 | lr 6.56e-05 | grad 1.48 | tok/s 6877
step   1610 | loss 2.1991 | lr 6.71e-05 | grad 1.52 | tok/s 6996
step   1620 | loss 2.2650 | lr 6.86e-05 | grad 1.71 | tok/s 6959
step   1630 | loss 2.1240 | lr 7.00e-05 | grad 1.39 | tok/s 7072
step   1640 | loss 2.4274 | lr 7.14e-05 | grad 1.27 | tok/s 7259
step   1650 | loss 2.0911 | lr 7.28e-05 | grad 1.42 | tok/s 7237
step   1660 | loss 2.0154 | lr 7.42e-05 | grad 1.49 | tok/s 7260
step   1670 | loss 2.1769 | lr 7.56e-05 | grad 1.43 | tok/s 7088
step   1680 | loss 2.2157 | lr 7.69e-05 | grad 1.62 | tok/s 7127
step   1690 | loss 2.0862 | lr 7.82e-05 | grad 1.47 | tok/s 7124
step   1700 | loss 2.0461 | lr 7.95e-05 | grad 1.16 | tok/s 7239
step   1710 | loss 2.1144 | lr 8.07e-05 | grad 1.23 | tok/s 7043
step   1720 | loss 2.0911 | lr 8.19e-05 | grad 1.41 | tok/s 6875
step   1730 | loss 2.1298 | lr 8.31e-05 | grad 1.48 | tok/s 7063
step   1740 | loss 2.5178 | lr 8.43e-05 | grad 2.17 | tok/s 6997
step   1750 | loss 2.2484 | lr 8.54e-05 | grad 1.59 | tok/s 7116
step   1760 | loss 2.2203 | lr 8.65e-05 | grad 1.33 | tok/s 7062
step   1770 | loss 2.2102 | lr 8.75e-05 | grad 1.99 | tok/s 6951
step   1780 | loss 1.9563 | lr 8.85e-05 | grad 1.66 | tok/s 7151
step   1790 | loss 1.9287 | lr 8.95e-05 | grad 1.36 | tok/s 6956
step   1800 | loss 1.9924 | lr 9.05e-05 | grad 1.85 | tok/s 7138
step   1810 | loss 2.1977 | lr 9.14e-05 | grad 1.59 | tok/s 7258
step   1820 | loss 2.0248 | lr 9.22e-05 | grad 1.71 | tok/s 7356
step   1830 | loss 2.1242 | lr 9.30e-05 | grad 1.68 | tok/s 7356
step   1840 | loss 2.1440 | lr 9.38e-05 | grad 1.63 | tok/s 7058
step   1850 | loss 2.0725 | lr 9.45e-05 | grad 1.51 | tok/s 7429
step   1860 | loss 1.9909 | lr 9.52e-05 | grad 1.22 | tok/s 7807
step   1870 | loss 2.0935 | lr 9.59e-05 | grad 2.55 | tok/s 7849
step   1880 | loss 2.0454 | lr 9.65e-05 | grad 1.46 | tok/s 7904
step   1890 | loss 1.6903 | lr 9.70e-05 | grad 1.46 | tok/s 7932
step   1900 | loss 1.9181 | lr 9.75e-05 | grad 1.79 | tok/s 7768
step   1910 | loss 2.2326 | lr 9.80e-05 | grad 2.34 | tok/s 7288
step   1920 | loss 2.2350 | lr 9.84e-05 | grad 1.96 | tok/s 7332
step   1930 | loss 2.3866 | lr 9.88e-05 | grad 1.82 | tok/s 7565
step   1940 | loss 1.9462 | lr 9.91e-05 | grad 1.41 | tok/s 7149
step   1950 | loss 2.0126 | lr 9.94e-05 | grad 1.28 | tok/s 6819
step   1960 | loss 1.9718 | lr 9.96e-05 | grad 1.31 | tok/s 7104
step   1970 | loss 2.3437 | lr 9.98e-05 | grad 1.42 | tok/s 6974
step   1980 | loss 2.5208 | lr 9.99e-05 | grad 5.48 | tok/s 6893
step   1990 | loss 2.3061 | lr 1.00e-04 | grad 1.57 | tok/s 7056
step   2000 | loss 2.4127 | lr 1.00e-04 | grad 1.96 | tok/s 6267
  >>> saved checkpoint: checkpoint_step_002000_loss_2.4127.pt
step   2010 | loss 2.1017 | lr 1.00e-04 | grad 1.51 | tok/s 3592
step   2020 | loss 2.3167 | lr 9.99e-05 | grad 1.21 | tok/s 6070
step   2030 | loss 1.9403 | lr 9.98e-05 | grad 1.86 | tok/s 5985
step   2040 | loss 2.0039 | lr 9.96e-05 | grad 1.50 | tok/s 6075
step   2050 | loss 1.9180 | lr 9.94e-05 | grad 0.92 | tok/s 6608
step   2060 | loss 2.0594 | lr 9.92e-05 | grad 2.37 | tok/s 6388
step   2070 | loss 2.0074 | lr 9.88e-05 | grad 1.28 | tok/s 6494
step   2080 | loss 2.0454 | lr 9.85e-05 | grad 1.38 | tok/s 6305
step   2090 | loss 2.1285 | lr 9.81e-05 | grad 1.01 | tok/s 7105
step   2100 | loss 2.2332 | lr 9.76e-05 | grad 2.10 | tok/s 7154
step   2110 | loss 2.3018 | lr 9.71e-05 | grad 1.36 | tok/s 7436

Training complete! Final step: 2112
