Using device: cuda
Output directory: output/level42_100m_20260114_020231
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Created Level 42 model: dim=768, depth=32, params=99,349,248
Model: Level 42, 99,349,248 parameters

Starting training from step 0...
Batch size: 8, Chunk size: 256
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 5.5700 | lr 9.00e-07 | grad 207.03 | tok/s 2231
step     20 | loss 4.6664 | lr 1.90e-06 | grad 42.19 | tok/s 3873
step     30 | loss 3.7538 | lr 2.90e-06 | grad 14.93 | tok/s 4223
step     40 | loss 3.1320 | lr 3.90e-06 | grad 15.50 | tok/s 4114
step     50 | loss 2.8959 | lr 4.90e-06 | grad 13.16 | tok/s 4231
step     60 | loss 2.9433 | lr 5.90e-06 | grad 16.06 | tok/s 4252
step     70 | loss 2.6365 | lr 6.90e-06 | grad 14.30 | tok/s 3610
step     80 | loss 2.8256 | lr 7.90e-06 | grad 14.94 | tok/s 4027
step     90 | loss 2.3276 | lr 8.90e-06 | grad 17.54 | tok/s 3460
step    100 | loss 3.4517 | lr 9.90e-06 | grad 34.76 | tok/s 4120
step    110 | loss 3.5576 | lr 1.09e-05 | grad 43.05 | tok/s 3701
step    120 | loss 2.5875 | lr 1.19e-05 | grad 16.43 | tok/s 3994
step    130 | loss 2.5121 | lr 1.29e-05 | grad 19.53 | tok/s 3686
step    140 | loss 2.0032 | lr 1.39e-05 | grad 12.36 | tok/s 3638
step    150 | loss 3.3006 | lr 1.49e-05 | grad 25.49 | tok/s 3814
step    160 | loss 4.8389 | lr 1.59e-05 | grad 21.05 | tok/s 3458
step    170 | loss 4.1118 | lr 1.69e-05 | grad 30.32 | tok/s 3838
step    180 | loss 3.9820 | lr 1.79e-05 | grad 30.75 | tok/s 3514
step    190 | loss 3.2929 | lr 1.89e-05 | grad 50.42 | tok/s 4474
step    200 | loss 3.3216 | lr 1.99e-05 | grad 20.46 | tok/s 4066
step    210 | loss 2.9659 | lr 2.09e-05 | grad 38.27 | tok/s 5203
step    220 | loss 3.1152 | lr 2.19e-05 | grad 9.61 | tok/s 6467
step    230 | loss 2.9658 | lr 2.29e-05 | grad 33.95 | tok/s 6445
step    240 | loss 3.1610 | lr 2.39e-05 | grad 30.92 | tok/s 6463
step    250 | loss 2.7020 | lr 2.49e-05 | grad 20.61 | tok/s 6485
step    260 | loss 2.7496 | lr 2.59e-05 | grad 21.91 | tok/s 6486
step    270 | loss 2.7799 | lr 2.69e-05 | grad 13.96 | tok/s 6455
step    280 | loss 2.3990 | lr 2.79e-05 | grad 15.55 | tok/s 6497
step    290 | loss 2.6944 | lr 2.89e-05 | grad 8.50 | tok/s 6602
step    300 | loss 2.6411 | lr 2.99e-05 | grad 11.90 | tok/s 6625
step    310 | loss 2.7635 | lr 3.09e-05 | grad 47.41 | tok/s 6599
step    320 | loss 2.3847 | lr 3.19e-05 | grad 16.99 | tok/s 6524
step    330 | loss 2.4668 | lr 3.29e-05 | grad 8.56 | tok/s 6784
step    340 | loss 2.7267 | lr 3.39e-05 | grad 18.69 | tok/s 6800
step    350 | loss 2.3953 | lr 3.49e-05 | grad 9.93 | tok/s 6820
step    360 | loss 2.4971 | lr 3.59e-05 | grad 8.00 | tok/s 6736
step    370 | loss 2.2653 | lr 3.69e-05 | grad 9.01 | tok/s 6891
step    380 | loss 2.4209 | lr 3.79e-05 | grad 36.17 | tok/s 6877
step    390 | loss 2.3383 | lr 3.89e-05 | grad 16.17 | tok/s 6705
step    400 | loss 2.0466 | lr 3.99e-05 | grad 23.03 | tok/s 6710
step    410 | loss 2.3258 | lr 4.09e-05 | grad 5.94 | tok/s 6631
step    420 | loss 2.2837 | lr 4.19e-05 | grad 10.16 | tok/s 6585
step    430 | loss 2.2951 | lr 4.29e-05 | grad 6.94 | tok/s 6438
step    440 | loss 2.4797 | lr 4.39e-05 | grad 6.40 | tok/s 6364
step    450 | loss 1.9862 | lr 4.49e-05 | grad 6.95 | tok/s 6240
step    460 | loss 2.9174 | lr 4.59e-05 | grad 9.32 | tok/s 6055
step    470 | loss 2.4058 | lr 4.69e-05 | grad 8.32 | tok/s 4307
step    480 | loss 2.3913 | lr 4.79e-05 | grad 6.55 | tok/s 4218
step    490 | loss 2.3828 | lr 4.89e-05 | grad 5.88 | tok/s 4477
step    500 | loss 2.2519 | lr 4.99e-05 | grad 6.20 | tok/s 4380
step    510 | loss 1.7538 | lr 5.09e-05 | grad 5.40 | tok/s 4735
step    520 | loss 1.5319 | lr 5.19e-05 | grad 5.07 | tok/s 6191
step    530 | loss 2.3307 | lr 5.29e-05 | grad 10.70 | tok/s 6213
step    540 | loss 2.0032 | lr 5.39e-05 | grad 5.96 | tok/s 6449
step    550 | loss 1.9885 | lr 5.49e-05 | grad 6.77 | tok/s 6426
step    560 | loss 2.6738 | lr 5.59e-05 | grad 9.02 | tok/s 5947
step    570 | loss 2.1242 | lr 5.69e-05 | grad 12.02 | tok/s 5810
step    580 | loss 0.8209 | lr 5.79e-05 | grad 2.88 | tok/s 5892
step    590 | loss 0.5364 | lr 5.89e-05 | grad 3.33 | tok/s 5787
step    600 | loss 0.4184 | lr 5.99e-05 | grad 2.67 | tok/s 5828
step    610 | loss 2.5602 | lr 6.09e-05 | grad 7.29 | tok/s 5624
step    620 | loss 2.2298 | lr 6.19e-05 | grad 6.22 | tok/s 5780
step    630 | loss 1.7193 | lr 6.29e-05 | grad 5.51 | tok/s 5738
step    640 | loss 1.9138 | lr 6.39e-05 | grad 8.71 | tok/s 5583
step    650 | loss 2.3467 | lr 6.49e-05 | grad 3.88 | tok/s 5433
step    660 | loss 2.0957 | lr 6.59e-05 | grad 6.11 | tok/s 5702
step    670 | loss 2.5672 | lr 6.69e-05 | grad 5.36 | tok/s 5501
step    680 | loss 2.1523 | lr 6.79e-05 | grad 3.91 | tok/s 5246
step    690 | loss 1.9378 | lr 6.89e-05 | grad 3.83 | tok/s 5146
step    700 | loss 2.0967 | lr 6.99e-05 | grad 4.28 | tok/s 5196
step    710 | loss 1.1858 | lr 7.09e-05 | grad 5.14 | tok/s 5321
step    720 | loss 1.5470 | lr 7.19e-05 | grad 70.02 | tok/s 5389
step    730 | loss 2.1801 | lr 7.29e-05 | grad 5.00 | tok/s 5013
step    740 | loss 1.9800 | lr 7.39e-05 | grad 4.64 | tok/s 5195
step    750 | loss 1.9775 | lr 7.49e-05 | grad 5.09 | tok/s 5118
step    760 | loss 1.7432 | lr 7.59e-05 | grad 2.82 | tok/s 5118
step    770 | loss 1.5395 | lr 7.69e-05 | grad 2.75 | tok/s 5378
step    780 | loss 1.5151 | lr 7.79e-05 | grad 2.85 | tok/s 5376
step    790 | loss 1.5307 | lr 7.89e-05 | grad 5.65 | tok/s 5295
step    800 | loss 1.6951 | lr 7.99e-05 | grad 5.00 | tok/s 5447
step    810 | loss 1.3006 | lr 8.09e-05 | grad 3.30 | tok/s 5312
step    820 | loss 1.5455 | lr 8.19e-05 | grad 4.07 | tok/s 5118
step    830 | loss 1.9559 | lr 8.29e-05 | grad 3.00 | tok/s 4876
step    840 | loss 2.2740 | lr 8.39e-05 | grad 5.96 | tok/s 5200
step    850 | loss 2.0716 | lr 8.49e-05 | grad 8.33 | tok/s 5386
step    860 | loss 2.2641 | lr 8.59e-05 | grad 7.68 | tok/s 5297
step    870 | loss 2.4864 | lr 8.69e-05 | grad 2.80 | tok/s 5064
step    880 | loss 2.1437 | lr 8.79e-05 | grad 4.98 | tok/s 5330
step    890 | loss 3.3400 | lr 8.89e-05 | grad 4.17 | tok/s 5137
step    900 | loss 1.9478 | lr 8.99e-05 | grad 2.66 | tok/s 5378
step    910 | loss 2.2978 | lr 9.09e-05 | grad 3.54 | tok/s 5927
step    920 | loss 1.6798 | lr 9.19e-05 | grad 6.55 | tok/s 5844
step    930 | loss 2.2796 | lr 9.29e-05 | grad 3.59 | tok/s 5531
step    940 | loss 1.8324 | lr 9.39e-05 | grad 17.15 | tok/s 5797
step    950 | loss 2.5406 | lr 9.49e-05 | grad 5.94 | tok/s 5812
step    960 | loss 2.3056 | lr 9.59e-05 | grad 4.35 | tok/s 6129
step    970 | loss 2.0495 | lr 9.69e-05 | grad 3.03 | tok/s 5921
step    980 | loss 1.7789 | lr 9.79e-05 | grad 2.28 | tok/s 6096
step    990 | loss 1.8022 | lr 9.89e-05 | grad 4.41 | tok/s 6214
step   1000 | loss 1.9293 | lr 9.99e-05 | grad 2.22 | tok/s 5832
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9293.pt
step   1010 | loss 1.8608 | lr 1.02e-06 | grad 2.66 | tok/s 3430
step   1020 | loss 1.9823 | lr 1.09e-06 | grad 2.43 | tok/s 5185
step   1030 | loss 2.0372 | lr 1.21e-06 | grad 2.95 | tok/s 5022
step   1040 | loss 2.1390 | lr 1.37e-06 | grad 5.72 | tok/s 5185
step   1050 | loss 2.3034 | lr 1.59e-06 | grad 3.37 | tok/s 5346
step   1060 | loss 1.9088 | lr 1.85e-06 | grad 2.81 | tok/s 5476
step   1070 | loss 1.9638 | lr 2.16e-06 | grad 2.73 | tok/s 5435
step   1080 | loss 1.6787 | lr 2.52e-06 | grad 3.60 | tok/s 5473
step   1090 | loss 1.7713 | lr 2.92e-06 | grad 2.95 | tok/s 5272
step   1100 | loss 1.8661 | lr 3.37e-06 | grad 2.14 | tok/s 5260
step   1110 | loss 2.0491 | lr 3.87e-06 | grad 3.01 | tok/s 5486
step   1120 | loss 1.8675 | lr 4.42e-06 | grad 3.00 | tok/s 5120
step   1130 | loss 1.7792 | lr 5.01e-06 | grad 2.07 | tok/s 4977
step   1140 | loss 1.7684 | lr 5.65e-06 | grad 2.46 | tok/s 5243
step   1150 | loss 1.6403 | lr 6.32e-06 | grad 1.86 | tok/s 5061
step   1160 | loss 1.6941 | lr 7.05e-06 | grad 2.61 | tok/s 5164
step   1170 | loss 2.4125 | lr 7.81e-06 | grad 5.42 | tok/s 5506
step   1180 | loss 1.9246 | lr 8.62e-06 | grad 2.45 | tok/s 5351
step   1190 | loss 1.9814 | lr 9.47e-06 | grad 2.96 | tok/s 5693
step   1200 | loss 1.8213 | lr 1.04e-05 | grad 3.47 | tok/s 5705
step   1210 | loss 1.7993 | lr 1.13e-05 | grad 2.24 | tok/s 5734
step   1220 | loss 1.6272 | lr 1.23e-05 | grad 2.15 | tok/s 5096
step   1230 | loss 1.6172 | lr 1.33e-05 | grad 2.41 | tok/s 4753
step   1240 | loss 1.7006 | lr 1.43e-05 | grad 2.57 | tok/s 4565
step   1250 | loss 1.8805 | lr 1.54e-05 | grad 5.33 | tok/s 4823
step   1260 | loss 1.8599 | lr 1.65e-05 | grad 13.83 | tok/s 4805
step   1270 | loss 1.9971 | lr 1.76e-05 | grad 3.19 | tok/s 4853
step   1280 | loss 2.0747 | lr 1.88e-05 | grad 3.25 | tok/s 4950
step   1290 | loss 1.6561 | lr 2.00e-05 | grad 3.34 | tok/s 5147
step   1300 | loss 1.9606 | lr 2.13e-05 | grad 2.73 | tok/s 5371
step   1310 | loss 1.6963 | lr 2.25e-05 | grad 2.81 | tok/s 5242
step   1320 | loss 1.7218 | lr 2.38e-05 | grad 2.74 | tok/s 5172
step   1330 | loss 2.3765 | lr 2.52e-05 | grad 4.22 | tok/s 5035
step   1340 | loss 1.8187 | lr 2.65e-05 | grad 2.33 | tok/s 5046
step   1350 | loss 1.8895 | lr 2.79e-05 | grad 4.42 | tok/s 4959
step   1360 | loss 2.1710 | lr 2.93e-05 | grad 3.58 | tok/s 5224
step   1370 | loss 1.9518 | lr 3.07e-05 | grad 2.53 | tok/s 5544
step   1380 | loss 1.6394 | lr 3.21e-05 | grad 2.33 | tok/s 5385
step   1390 | loss 1.4665 | lr 3.36e-05 | grad 2.99 | tok/s 5057
step   1400 | loss 1.8706 | lr 3.51e-05 | grad 2.51 | tok/s 5104
step   1410 | loss 1.6351 | lr 3.65e-05 | grad 2.49 | tok/s 4645
step   1420 | loss 1.5705 | lr 3.80e-05 | grad 5.21 | tok/s 4676
step   1430 | loss 1.6828 | lr 3.96e-05 | grad 2.96 | tok/s 4968
step   1440 | loss 1.3785 | lr 4.11e-05 | grad 2.02 | tok/s 5023
step   1450 | loss 1.4844 | lr 4.26e-05 | grad 3.45 | tok/s 5383
step   1460 | loss 1.7311 | lr 4.41e-05 | grad 2.60 | tok/s 5549
step   1470 | loss 1.4528 | lr 4.57e-05 | grad 2.45 | tok/s 5697
step   1480 | loss 1.3337 | lr 4.72e-05 | grad 3.01 | tok/s 5390
step   1490 | loss 1.2672 | lr 4.88e-05 | grad 2.85 | tok/s 5337
step   1500 | loss 1.1274 | lr 5.03e-05 | grad 2.03 | tok/s 5294
step   1510 | loss 1.1471 | lr 5.19e-05 | grad 2.35 | tok/s 5344
step   1520 | loss 1.0782 | lr 5.35e-05 | grad 2.02 | tok/s 5358
step   1530 | loss 1.1213 | lr 5.50e-05 | grad 2.19 | tok/s 5342
step   1540 | loss 1.0691 | lr 5.65e-05 | grad 2.76 | tok/s 5408

Training complete! Final step: 1540
