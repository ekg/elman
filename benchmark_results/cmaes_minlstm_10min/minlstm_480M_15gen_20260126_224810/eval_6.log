Using device: cuda
Output directory: benchmark_results/cmaes_minlstm_10min/minlstm_480M_15gen_20260126_224810/eval_6/levelminlstm_100m_20260126_224816
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 233,918,208 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3837 | lr 3.00e-04 | grad 2.36 | tok/s 18582
step     20 | loss 3.1541 | lr 3.00e-04 | grad 1.66 | tok/s 19926
step     30 | loss 3.5981 | lr 3.00e-04 | grad 3.25 | tok/s 20087
step     40 | loss 3.8090 | lr 3.00e-04 | grad 6.53 | tok/s 20747
step     50 | loss 5.0531 | lr 3.00e-04 | grad 3.27 | tok/s 21290
step     60 | loss 4.0295 | lr 3.00e-04 | grad 4.38 | tok/s 21273
step     70 | loss 3.8177 | lr 3.00e-04 | grad 4.34 | tok/s 21277
step     80 | loss 3.7383 | lr 3.00e-04 | grad 5.06 | tok/s 21266
step     90 | loss 3.5020 | lr 3.00e-04 | grad 2.98 | tok/s 21257
step    100 | loss 3.4629 | lr 3.00e-04 | grad 1.77 | tok/s 21247
step    110 | loss 3.2618 | lr 3.00e-04 | grad 4.41 | tok/s 21106
step    120 | loss 3.4459 | lr 3.00e-04 | grad 1.66 | tok/s 20405
step    130 | loss 2.7940 | lr 3.00e-04 | grad 2.23 | tok/s 20511
step    140 | loss 2.8042 | lr 3.00e-04 | grad 3.11 | tok/s 20505
step    150 | loss 2.9651 | lr 3.00e-04 | grad 3.09 | tok/s 20942
step    160 | loss 2.9839 | lr 3.00e-04 | grad 2.34 | tok/s 21049
step    170 | loss 2.8399 | lr 3.00e-04 | grad 1.97 | tok/s 19744
step    180 | loss 2.9915 | lr 3.00e-04 | grad 2.50 | tok/s 20657
step    190 | loss 2.6862 | lr 3.00e-04 | grad 1.61 | tok/s 19680
step    200 | loss 2.4458 | lr 3.00e-04 | grad 2.27 | tok/s 20998
step    210 | loss 2.4312 | lr 3.00e-04 | grad 1.62 | tok/s 20275
step    220 | loss 2.7891 | lr 3.00e-04 | grad 1.33 | tok/s 20420
step    230 | loss 2.9310 | lr 3.00e-04 | grad 3.69 | tok/s 20077
step    240 | loss 2.6087 | lr 3.00e-04 | grad 2.05 | tok/s 20410
step    250 | loss 2.7341 | lr 3.00e-04 | grad 1.71 | tok/s 20208
step    260 | loss 2.5499 | lr 3.00e-04 | grad 2.36 | tok/s 20974
step    270 | loss 2.5631 | lr 3.00e-04 | grad 2.30 | tok/s 20512
step    280 | loss 2.3090 | lr 3.00e-04 | grad 2.20 | tok/s 19362
step    290 | loss 2.3590 | lr 3.00e-04 | grad 2.03 | tok/s 20045
step    300 | loss 2.5149 | lr 3.00e-04 | grad 1.48 | tok/s 19857
step    310 | loss 2.3065 | lr 3.00e-04 | grad 1.03 | tok/s 20047
step    320 | loss 2.4160 | lr 3.00e-04 | grad 2.34 | tok/s 19814
step    330 | loss 2.4281 | lr 3.00e-04 | grad 1.82 | tok/s 20400
step    340 | loss 2.5566 | lr 3.00e-04 | grad 1.42 | tok/s 20474
step    350 | loss 2.6518 | lr 3.00e-04 | grad 1.45 | tok/s 20506
step    360 | loss 2.2942 | lr 3.00e-04 | grad 1.80 | tok/s 19640
step    370 | loss 2.3793 | lr 3.00e-04 | grad 1.73 | tok/s 21015
step    380 | loss 2.2629 | lr 3.00e-04 | grad 1.54 | tok/s 21171
step    390 | loss 2.2130 | lr 3.00e-04 | grad 2.05 | tok/s 21183
step    400 | loss 2.2443 | lr 3.00e-04 | grad 3.05 | tok/s 20490
step    410 | loss 2.4376 | lr 3.00e-04 | grad 2.09 | tok/s 20209
step    420 | loss 2.4522 | lr 3.00e-04 | grad 1.09 | tok/s 20818
step    430 | loss 2.4391 | lr 3.00e-04 | grad 1.55 | tok/s 21036
step    440 | loss 2.3177 | lr 3.00e-04 | grad 1.69 | tok/s 20174
step    450 | loss 2.3463 | lr 3.00e-04 | grad 1.12 | tok/s 20512
step    460 | loss 2.2790 | lr 3.00e-04 | grad 1.45 | tok/s 20455
step    470 | loss 2.3038 | lr 3.00e-04 | grad 1.84 | tok/s 20265
step    480 | loss 2.4170 | lr 3.00e-04 | grad 2.27 | tok/s 21135
step    490 | loss 2.3664 | lr 3.00e-04 | grad 2.11 | tok/s 20314
step    500 | loss 2.2245 | lr 3.00e-04 | grad 1.65 | tok/s 20215
step    510 | loss 2.4734 | lr 3.00e-04 | grad 1.30 | tok/s 20070
step    520 | loss 2.1809 | lr 3.00e-04 | grad 1.46 | tok/s 19913
step    530 | loss 2.2036 | lr 3.00e-04 | grad 1.38 | tok/s 20146
step    540 | loss 2.4713 | lr 3.00e-04 | grad 1.29 | tok/s 20374
step    550 | loss 1.8904 | lr 3.00e-04 | grad 1.73 | tok/s 19994
step    560 | loss 2.2134 | lr 3.00e-04 | grad 2.02 | tok/s 20895
step    570 | loss 2.1522 | lr 3.00e-04 | grad 1.88 | tok/s 21187
step    580 | loss 2.0951 | lr 3.00e-04 | grad 1.81 | tok/s 21188
step    590 | loss 2.0517 | lr 3.00e-04 | grad 1.31 | tok/s 21185
step    600 | loss 2.1264 | lr 3.00e-04 | grad 1.87 | tok/s 21194
step    610 | loss 2.0757 | lr 3.00e-04 | grad 1.34 | tok/s 21188
step    620 | loss 2.0232 | lr 3.00e-04 | grad 1.46 | tok/s 21192
step    630 | loss 2.2395 | lr 3.00e-04 | grad 1.82 | tok/s 20374
step    640 | loss 2.0851 | lr 3.00e-04 | grad 1.54 | tok/s 19762
step    650 | loss 2.2522 | lr 3.00e-04 | grad 1.83 | tok/s 20648
step    660 | loss 2.1272 | lr 3.00e-04 | grad 1.96 | tok/s 20516
step    670 | loss 2.2939 | lr 3.00e-04 | grad 1.45 | tok/s 20792
step    680 | loss 2.2642 | lr 3.00e-04 | grad 1.86 | tok/s 19540
step    690 | loss 2.2359 | lr 3.00e-04 | grad 1.89 | tok/s 20374
step    700 | loss 2.0782 | lr 3.00e-04 | grad 1.58 | tok/s 19667
step    710 | loss 2.2796 | lr 3.00e-04 | grad 1.75 | tok/s 20238
step    720 | loss 2.1546 | lr 3.00e-04 | grad 2.27 | tok/s 20090
step    730 | loss 2.1232 | lr 3.00e-04 | grad 4.25 | tok/s 21055
step    740 | loss 2.1835 | lr 3.00e-04 | grad 1.56 | tok/s 20274
step    750 | loss 2.7339 | lr 3.00e-04 | grad 1.88 | tok/s 21069
step    760 | loss 2.1851 | lr 3.00e-04 | grad 2.31 | tok/s 21043
step    770 | loss 2.0568 | lr 3.00e-04 | grad 1.37 | tok/s 20438
step    780 | loss 2.1587 | lr 3.00e-04 | grad 1.72 | tok/s 20623
step    790 | loss 2.1673 | lr 3.00e-04 | grad 1.86 | tok/s 20507
step    800 | loss 2.5481 | lr 3.00e-04 | grad 1.63 | tok/s 20397
step    810 | loss 1.6441 | lr 3.00e-04 | grad 1.27 | tok/s 20412
step    820 | loss 2.2096 | lr 3.00e-04 | grad 1.12 | tok/s 20384
step    830 | loss 2.1009 | lr 3.00e-04 | grad 1.40 | tok/s 19406
step    840 | loss 2.2584 | lr 3.00e-04 | grad 2.25 | tok/s 20534
step    850 | loss 2.1939 | lr 3.00e-04 | grad 2.03 | tok/s 20210
step    860 | loss 2.2000 | lr 3.00e-04 | grad 1.52 | tok/s 20207
step    870 | loss 2.4758 | lr 3.00e-04 | grad 1.44 | tok/s 21198
step    880 | loss 2.1660 | lr 3.00e-04 | grad 1.91 | tok/s 20425
step    890 | loss 2.0788 | lr 3.00e-04 | grad 1.54 | tok/s 20187
step    900 | loss 2.0660 | lr 3.00e-04 | grad 1.41 | tok/s 20435
step    910 | loss 2.1758 | lr 3.00e-04 | grad 2.17 | tok/s 19990
step    920 | loss 2.1137 | lr 3.00e-04 | grad 1.45 | tok/s 20559
step    930 | loss 2.1469 | lr 3.00e-04 | grad 1.64 | tok/s 20439
step    940 | loss 1.9972 | lr 3.00e-04 | grad 1.10 | tok/s 20302
step    950 | loss 2.0762 | lr 3.00e-04 | grad 1.81 | tok/s 19349
step    960 | loss 1.9960 | lr 3.00e-04 | grad 1.58 | tok/s 19847
step    970 | loss 2.0058 | lr 3.00e-04 | grad 1.71 | tok/s 20332
step    980 | loss 2.5956 | lr 3.00e-04 | grad 2.45 | tok/s 21015
step    990 | loss 2.5547 | lr 3.00e-04 | grad 2.09 | tok/s 20757
step   1000 | loss 2.1730 | lr 3.00e-04 | grad 0.89 | tok/s 20041
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1730.pt
step   1010 | loss 1.7699 | lr 3.00e-04 | grad 1.66 | tok/s 11801
step   1020 | loss 1.8603 | lr 3.00e-04 | grad 1.34 | tok/s 21094
step   1030 | loss 2.1983 | lr 3.00e-04 | grad 1.70 | tok/s 20576
step   1040 | loss 2.3795 | lr 3.00e-04 | grad 2.56 | tok/s 20257
step   1050 | loss 2.3783 | lr 3.00e-04 | grad 1.62 | tok/s 20801
step   1060 | loss 2.1548 | lr 3.00e-04 | grad 2.28 | tok/s 20474
step   1070 | loss 1.7452 | lr 3.00e-04 | grad 1.88 | tok/s 20528
step   1080 | loss 1.9294 | lr 3.00e-04 | grad 1.98 | tok/s 21192
step   1090 | loss 1.8874 | lr 3.00e-04 | grad 1.49 | tok/s 21190
step   1100 | loss 1.8757 | lr 3.00e-04 | grad 1.91 | tok/s 21191
step   1110 | loss 1.8302 | lr 3.00e-04 | grad 1.89 | tok/s 21190
step   1120 | loss 1.9365 | lr 3.00e-04 | grad 1.83 | tok/s 20984
step   1130 | loss 2.3844 | lr 3.00e-04 | grad 2.47 | tok/s 20812
step   1140 | loss 2.3570 | lr 3.00e-04 | grad 1.23 | tok/s 20710
step   1150 | loss 2.2866 | lr 3.00e-04 | grad 3.41 | tok/s 20652
step   1160 | loss 2.4065 | lr 3.00e-04 | grad 2.36 | tok/s 20420
step   1170 | loss 2.1192 | lr 3.00e-04 | grTraceback (most recent call last):
ad 1.76 | tok/s 19706
step   1180 | loss 2.0834 | lr 3.00e-04 | grad 1.70 | tok/s 20578
step   1190 | loss 2.2057 | lr 3.00e-04 | grad 2.45 | tok/s 20967
step   1200 | loss 2.0874 | lr 3.00e-04 | grad 1.48 | tok/s 21181
step   1210 | loss 1.8303 | lr 3.00e-04 | grad 1.66 | tok/s 20427
step   1220 | loss 2.0176 | lr 3.00e-04 | grad 2.20 | tok/s 20459
step   1230 | loss 2.0705 | lr 3.00e-04 | grad 2.03 | tok/s 20370
step   1240 | loss 1.9308 | lr 3.00e-04 | grad 1.58 | tok/s 21062
step   1250 | loss 2.0213 | lr 3.00e-04 | grad 1.60 | tok/s 20388
step   1260 | loss 2.4110 | lr 3.00e-04 | grad 2.39 | tok/s 21189
step   1270 | loss 2.0317 | lr 3.00e-04 | grad 2.05 | tok/s 20504
step   1280 | loss 1.9455 | lr 3.00e-04 | grad 1.66 | tok/s 20534
step   1290 | loss 2.0756 | lr 3.00e-04 | grad 1.56 | tok/s 19627
step   1300 | loss 2.1057 | lr 3.00e-04 | grad 1.83 | tok/s 20094
step   1310 | loss 2.4345 | lr 3.00e-04 | grad 1.57 | tok/s 20577
step   1320 | loss 2.0501 | lr 3.00e-04 | grad 1.58 | tok/s 20890
step   1330 | loss 2.2237 | lr 3.00e-04 | grad 2.28 | tok/s 20642
step   1340 | loss 1.9399 | lr 3.00e-04 | grad 1.49 | tok/s 19953
step   1350 | loss 2.2194 | lr 3.00e-04 | grad 1.95 | tok/s 20930
step   1360 | loss 2.1439 | lr 3.00e-04 | grad 1.81 | tok/s 19729
step   1370 | loss 2.0080 | lr 3.00e-04 | grad 2.14 | tok/s 20350
step   1380 | loss 2.2733 | lr 3.00e-04 | grad 1.20 | tok/s 20712
step   1390 | loss 2.0607 | lr 3.00e-04 | grad 1.71 | tok/s 20133
step   1400 | loss 2.2617 | lr 3.00e-04 | grad 1.55 | tok/s 19998
step   1410 | loss 1.8935 | lr 3.00e-04 | grad 2.14 | tok/s 20215
step   1420 | loss 2.0972 | lr 3.00e-04 | grad 2.09 | tok/s 20633
step   1430 | loss 2.2861 | lr 3.00e-04 | grad 1.57 | tok/s 20446
step   1440 | loss 2.0203 | lr 3.00e-04 | grad 1.90 | tok/s 20366
step   1450 | loss 2.1312 | lr 3.00e-04 | grad 1.32 | tok/s 20649
step   1460 | loss 2.0176 | lr 3.00e-04 | grad 1.86 | tok/s 20435
step   1470 | loss 2.0174 | lr 3.00e-04 | grad 1.98 | tok/s 20066
step   1480 | loss 1.9498 | lr 3.00e-04 | grad 1.18 | tok/s 20014
step   1490 | loss 2.0949 | lr 3.00e-04 | grad 1.45 | tok/s 20397
step   1500 | loss 2.3297 | lr 3.00e-04 | grad 1.77 | tok/s 20814
step   1510 | loss 1.8603 | lr 3.00e-04 | grad 1.48 | tok/s 20284
step   1520 | loss 1.9691 | lr 3.00e-04 | grad 1.77 | tok/s 20267
step   1530 | loss 2.0002 | lr 3.00e-04 | grad 2.08 | tok/s 20454
