Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_41/levelE88_100m_20260127_173939
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,900,728 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.1082 | lr 3.00e-04 | grad 17.88 | tok/s 5104
step     20 | loss 2.9688 | lr 3.00e-04 | grad 11.44 | tok/s 16030
step     30 | loss 2.7158 | lr 3.00e-04 | grad 7.16 | tok/s 15992
step     40 | loss 2.4613 | lr 3.00e-04 | grad 19.88 | tok/s 15356
step     50 | loss 3.0805 | lr 3.00e-04 | grad 13.06 | tok/s 15983
step     60 | loss 2.0093 | lr 3.00e-04 | grad 5.91 | tok/s 16147
step     70 | loss 1.8054 | lr 3.00e-04 | grad 3.48 | tok/s 16315
step     80 | loss 6.7491 | lr 3.00e-04 | grad 80.50 | tok/s 16775
step     90 | loss 5.0904 | lr 3.00e-04 | grad 16.75 | tok/s 16769
step    100 | loss 4.2060 | lr 3.00e-04 | grad 23.12 | tok/s 16756
step    110 | loss 3.5157 | lr 3.00e-04 | grad 19.25 | tok/s 16655
step    120 | loss 3.0384 | lr 3.00e-04 | grad 16.00 | tok/s 16607
step    130 | loss 2.9604 | lr 3.00e-04 | grad 6.19 | tok/s 16690
step    140 | loss 2.7142 | lr 3.00e-04 | grad 17.00 | tok/s 16581
step    150 | loss 2.6671 | lr 3.00e-04 | grad 6.78 | tok/s 16603
step    160 | loss 2.3339 | lr 3.00e-04 | grad 10.00 | tok/s 15511
step    170 | loss 2.4579 | lr 3.00e-04 | grad 7.69 | tok/s 16480
step    180 | loss 2.1572 | lr 3.00e-04 | grad 5.12 | tok/s 16609
step    190 | loss 2.3650 | lr 3.00e-04 | grad 6.00 | tok/s 16519
step    200 | loss 2.0866 | lr 3.00e-04 | grad 6.62 | tok/s 15682
step    210 | loss 2.1161 | lr 3.00e-04 | grad 4.34 | tok/s 16451
step    220 | loss 2.1105 | lr 3.00e-04 | grad 4.06 | tok/s 16306
step    230 | loss 2.1218 | lr 3.00e-04 | grad 3.23 | tok/s 15071
step    240 | loss 2.3016 | lr 3.00e-04 | grad 3.75 | tok/s 11189
step    250 | loss 2.0344 | lr 3.00e-04 | grad 2.80 | tok/s 15545
step    260 | loss 1.5478 | lr 3.00e-04 | grad 3.48 | tok/s 16102
step    270 | loss 2.0349 | lr 3.00e-04 | grad 2.48 | tok/s 15414
step    280 | loss 2.4355 | lr 3.00e-04 | grad 13.69 | tok/s 15601
step    290 | loss 1.1304 | lr 3.00e-04 | grad 3.19 | tok/s 16531
step    300 | loss 0.7608 | lr 3.00e-04 | grad 4.69 | tok/s 16457
step    310 | loss 2.3522 | lr 3.00e-04 | grad 3.80 | tok/s 16408
step    320 | loss 1.9160 | lr 3.00e-04 | grad 3.98 | tok/s 15346
step    330 | loss 1.9565 | lr 3.00e-04 | grad 3.14 | tok/s 15282
step    340 | loss 2.2788 | lr 3.00e-04 | grad 2.98 | tok/s 15572
step    350 | loss 1.7824 | lr 3.00e-04 | grad 5.59 | tok/s 16039
step    360 | loss 1.2574 | lr 3.00e-04 | grad 4.94 | tok/s 16317
step    370 | loss 1.7811 | lr 3.00e-04 | grad 3.12 | tok/s 14495
step    380 | loss 1.7553 | lr 3.00e-04 | grad 3.08 | tok/s 15903
step    390 | loss 1.5080 | lr 3.00e-04 | grad 2.58 | tok/s 16545
step    400 | loss 1.4936 | lr 3.00e-04 | grad 3.55 | tok/s 16345
step    410 | loss 1.2908 | lr 3.00e-04 | grad 3.36 | tok/s 11534
step    420 | loss 1.9516 | lr 3.00e-04 | grad 13.06 | tok/s 14498
step    430 | loss 2.0048 | lr 3.00e-04 | grad 2.42 | tok/s 15732
step    440 | loss 2.1891 | lr 3.00e-04 | grad 6.22 | tok/s 15436
step    450 | loss 1.9913 | lr 3.00e-04 | grad 2.25 | tok/s 15834
step    460 | loss 1.6704 | lr 3.00e-04 | grad 4.03 | tok/s 7595
step    470 | loss 1.8803 | lr 3.00e-04 | grad 3.14 | tok/s 15796
step    480 | loss 2.3118 | lr 3.00e-04 | grad 5.03 | tok/s 7547
step    490 | loss 1.7396 | lr 3.00e-04 | grad 2.48 | tok/s 14944
step    500 | loss 1.7047 | lr 3.00e-04 | grad 2.95 | tok/s 16318
step    510 | loss 1.6678 | lr 3.00e-04 | grad 2.78 | tok/s 16502
step    520 | loss 1.6520 | lr 3.00e-04 | grad 2.52 | tok/s 7434
step    530 | loss 1.9136 | lr 3.00e-04 | grad 2.30 | tok/s 7491
step    540 | loss 1.7214 | lr 3.00e-04 | grad 2.70 | tok/s 8492
step    550 | loss 1.6153 | lr 3.00e-04 | grad 2.94 | tok/s 7915
step    560 | loss 1.7153 | lr 3.00e-04 | grad 3.31 | tok/s 14391
step    570 | loss 1.5970 | lr 3.00e-04 | grad 2.12 | tok/s 14960
step    580 | loss 1.5744 | lr 3.00e-04 | grad 3.00 | tok/s 9528
step    590 | loss 1.9555 | lr 3.00e-04 | grad 5.62 | tok/s 5849
step    600 | loss 1.7180 | lr 3.00e-04 | grad 2.80 | tok/s 6984
step    610 | loss 1.5917 | lr 3.00e-04 | grad 3.05 | tok/s 7713
step    620 | loss 1.6047 | lr 3.00e-04 | grad 3.33 | tok/s 6516
step    630 | loss 1.5900 | lr 3.00e-04 | grad 7.91 | tok/s 7869
step    640 | loss 1.9425 | lr 3.00e-04 | grad 2.97 | tok/s 12186
step    650 | loss 1.6429 | lr 3.00e-04 | grad 2.38 | tok/s 9977
step    660 | loss 1.6101 | lr 3.00e-04 | grad 2.38 | tok/s 11949
step    670 | loss 2.0069 | lr 3.00e-04 | grad 3.14 | tok/s 7718
step    680 | loss 1.8428 | lr 3.00e-04 | grad 4.28 | tok/s 15609
step    690 | loss 1.6282 | lr 3.00e-04 | grad 2.33 | tok/s 4740
step    700 | loss 1.5607 | lr 3.00e-04 | grad 2.64 | tok/s 13019
step    710 | loss 1.4532 | lr 3.00e-04 | grad 2.78 | tok/s 13377
step    720 | loss 1.4455 | lr 3.00e-04 | grad 1.85 | tok/s 16252
step    730 | loss 1.4490 | lr 3.00e-04 | grad 2.41 | tok/s 16036
step    740 | loss 1.2431 | lr 3.00e-04 | grad 2.05 | tok/s 16939
step    750 | loss 1.1423 | lr 3.00e-04 | grad 2.27 | tok/s 16830
step    760 | loss 1.0733 | lr 3.00e-04 | grad 1.85 | tok/s 16304
step    770 | loss 1.0246 | lr 3.00e-04 | grad 2.39 | tok/s 15552
step    780 | loss 0.9724 | lr 3.00e-04 | grad 1.98 | tok/s 16717
step    790 | loss 1.6170 | lr 3.00e-04 | grad 4.25 | tok/s 15768
step    800 | loss 1.8050 | lr 3.00e-04 | grad 2.75 | tok/s 16021
step    810 | loss 1.6868 | lr 3.00e-04 | grad 5.72 | tok/s 15533
step    820 | loss 1.6224 | lr 3.00e-04 | grad 2.98 | tok/s 10339
step    830 | loss 1.3407 | lr 3.00e-04 | grad 2.62 | tok/s 8913
step    840 | loss 1.5815 | lr 3.00e-04 | grad 20.38 | tok/s 13082
step    850 | loss 1.4627 | lr 3.00e-04 | grad 2.62 | tok/s 15526
step    860 | loss 1.5099 | lr 3.00e-04 | grad 2.27 | tok/s 12839
step    870 | loss 1.5696 | lr 3.00e-04 | grad 5.41 | tok/s 11521
step    880 | loss 1.7016 | lr 3.00e-04 | grad 2.69 | tok/s 14149
step    890 | loss 1.6553 | lr 3.00e-04 | grad 2.19 | tok/s 15680
step    900 | loss 1.4036 | lr 3.00e-04 | grad 2.14 | tok/s 8650
step    910 | loss 1.5735 | lr 3.00e-04 | grad 4.28 | tok/s 15746
step    920 | loss 1.6052 | lr 3.00e-04 | grad 5.16 | tok/s 10904
step    930 | loss 1.4681 | lr 3.00e-04 | grad 2.80 | tok/s 15283
step    940 | loss 1.4519 | lr 3.00e-04 | grad 2.61 | tok/s 16096
step    950 | loss 1.2834 | lr 3.00e-04 | grad 2.95 | tok/s 16327
step    960 | loss 1.6542 | lr 3.00e-04 | grad 3.47 | tok/s 15501
step    970 | loss 1.7509 | lr 3.00e-04 | grad 2.38 | tok/s 12301
step    980 | loss 1.4525 | lr 3.00e-04 | grad 2.33 | tok/s 9272
step    990 | loss 1.6837 | lr 3.00e-04 | grad 3.05 | tok/s 14744
step   1000 | loss 1.7737 | lr 3.00e-04 | grad 5.38 | tok/s 16073
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7737.pt
step   1010 | loss 1.5873 | lr 3.00e-04 | grad 2.56 | tok/s 3349
step   1020 | loss 1.4107 | lr 3.00e-04 | grad 2.31 | tok/s 15931
step   1030 | loss 1.5176 | lr 3.00e-04 | grad 2.36 | tok/s 15681
step   1040 | loss 1.6386 | lr 3.00e-04 | grad 2.88 | tok/s 15406
step   1050 | loss 1.6859 | lr 3.00e-04 | grad 2.39 | tok/s 16224
step   1060 | loss 1.6688 | lr 3.00e-04 | grad 2.62 | tok/s 15651
step   1070 | loss 1.4059 | lr 3.00e-04 | grad 2.69 | tok/s 14903
step   1080 | loss 1.0246 | lr 3.00e-04 | grad 5.31 | tok/s 16655
step   1090 | loss 1.5550 | lr 3.00e-04 | grad 2.69 | tok/s 15991
step   1100 | loss 1.4051 | lr 3.00e-04 | grad 2.11 | tok/s 16648
step   1110 | loss 1.3326 | lr 3.00e-04 | grad 2.42 | tok/s 16696
step   1120 | loss 1.2737 | lr 3.00e-04 | grad 2.33 | tok/s 16691
step   1130 | loss 1.2851 | lr 3.00e-04 | grad 2.14 | tok/s 16222
step   1140 | loss 1.2763 | lr 3.00e-04 | grad 2.05 | tok/s 14559
step   1150 | loss 1.2062 | lr 3.00e-04 | grad 2.11 | tok/s 16694
step   1160 | loss 1.2767 | lr 3.00e-04 | grad 2.16 | tok/s 16786
step   1170 | loss 1.2940 | lr 3.00e-04 | grad 2.67 | tok/s 12463
step   1180 | loss 1.2057 | lr 3.00e-04 | grad 1.99 | tok/s 15997
step   1190 | loss 1.2253 | lr 3.00e-04 | grad 1.91 | tok/s 14938
step   1200 | loss 1.2581 | lr 3.00e-04 | grad 2.09 | tok/s 16668
step   1210 | loss 1.2729 | lr 3.00e-04 | grad 2.12 | tok/s 14861
step   1220 | loss 1.2529 | lr 3.00e-04 | grad 1.82 | tok/s 15744
step   1230 | loss 1.3536 | lr 3.00e-04 | grad 6.38 | tok/s 16308
step   1240 | loss 1.7304 | lr 3.00e-04 | grad 2.28 | tok/s 15853
step   1250 | loss 1.3457 | lr 3.00e-04 | grad 2.44 | tok/s 14422
step   1260 | loss 1.7480 | lr 3.00e-04 | grad 2.53 | tok/s 15252
step   1270 | loss 1.5830 | lr 3.00e-04 | grad 2.17 | tok/s 16259
step   1280 | loss 1.5320 | lr 3.00e-04 | grad 2.62 | tok/s 15832
step   1290 | loss 1.5067 | lr 3.00e-04 | grad 3.58 | tok/s 15680
step   1300 | loss 1.4518 | lr 3.00e-04 | grad 3.88 | tok/s 14819
step   1310 | loss 1.6360 | lr 3.00e-04 | grad 3.86 | tok/s 16172
step   1320 | loss 1.4120 | lr 3.00e-04 | grad 2.47 | tok/s 16008
step   1330 | loss 1.6445 | lr 3.00e-04 | grad 2.19 | tok/s 14928
step   1340 | loss 1.7723 | lr 3.00e-04 | grad 3.31 | tok/s 15356
step   1350 | loss 1.4449 | lr 3.00e-04 | grad 1.91 | tok/s 16181
step   1360 | loss 1.5763 | lr 3.00e-04 | grad 4.81 | tok/s 15855
step   1370 | loss 1.5787 | lr 3.00e-04 | grad 2.83 | tok/s 15177
step   1380 | loss 1.4386 | lr 3.00e-04 | grad 2.05 | tok/s 15745
step   1390 | loss 1.3874 | lr 3.00e-04 | grad 2.09 | tok/s 15886
step   1400 | loss 1.5630 | lr 3.00e-04 | grad 4.03 | tok/s 15148
step   1410 | loss 1.6182 | lr 3.00e-04 | grad 2.64 | tok/s 15502
step   1420 | loss 1.3255 | lr 3.00e-04 | grad 2.34 | tok/s 15505
step   1430 | loss 1.1337 | lr 3.00e-04 | grad 2.14 | tok/s 16424
step   1440 | loss 1.2853 | lr 3.00e-04 | grad 6.88 | tok/s 16024
step   1450 | loss 1.6929 | lr 3.00e-04 | grad 6.06 | tok/s 14813
step   1460 | loss 1.4285 | lr 3.00e-04 | grad 1.96 | tok/s 14990
step   1470 | loss 1.8638 | lr 3.00e-04 | grad 3.89 | tok/s 16316
step   1480 | loss 1.5574 | lr 3.00e-04 | grad 5.56 | tok/s 16595
step   1490 | loss 1.2748 | lr 3.00e-04 | grad 2.19 | tok/s 16602
step   1500 | loss 1.5686 | lr 3.00e-04 | grad 2.22 | tok/s 16456
step   1510 | loss 1.4594 | lr 3.00e-04 | grad 2.62 | tok/s 16154
step   1520 | loss 1.4244 | lr 3.00e-04 | grad 2.19 | tok/s 16202
step   1530 | loss 1.6531 | lr 3.00e-04 | grad 2.47 | tok/s 15797
step   1540 | loss 1.2472 | lr 3.00e-04 | grad 2.38 | tok/s 16475
step   1550 | loss 1.6091 | lr 3.00e-04 | grad 2.42 | tok/s 15566
step   1560 | loss 1.3331 | lr 3.00e-04 | grad 3.30 | tok/s 16362
step   1570 | loss 1.6907 | lr 3.00e-04 | grad 3.91 | tok/s 16259
step   1580 | loss 1.5531 | lr 3.00e-04 | grad 2.06 | tok/s 15426
step   1590 | loss 0.8555 | lr 3.00e-04 | grad 1.73 | tok/s 15845
step   1600 | loss 1.1936 | lr 3.00e-04 | grad 2.08 | tok/s 15232
step   1610 | loss 1.3481 | lr 3.00e-04 | grad 2.88 | tok/s 15627
step   1620 | loss 1.3759 | lr 3.00e-04 | grad 2.42 | tok/s 15935
step   1630 | loss 1.4575 | lr 3.00e-04 | grad 4.97 | tok/s 11383
step   1640 | loss 1.5676 | lr 3.00e-04 | grad 4.03 | tok/s 12398
step   1650 | loss 1.2305 | lr 3.00e-04 | grad 1.88 | tok/s 14372
step   1660 | loss 1.6292 | lr 3.00e-04 | grad 7.91 | tok/s 15197
step   1670 | loss 1.5506 | lr 3.00e-04 | grad 2.61 | tok/s 14130
step   1680 | loss 1.4514 | lr 3.00e-04 | grad 3.42 | tok/s 15360
step   1690 | loss 1.5441 | lr 3.00e-04 | grad 2.28 | tok/s 15288
step   1700 | loss 1.4530 | lr 3.00e-04 | grad 2.41 | tok/s 16068
step   1710 | loss 1.5086 | lr 3.00e-04 | grad 3.08 | tok/s 16576
step   1720 | loss 1.2008 | lr 3.00e-04 | grad 3.78 | tok/s 16573
step   1730 | loss 1.4058 | lr 3.00e-04 | grad 2.62 | tok/s 15863
step   1740 | loss 1.5179 | lr 3.00e-04 | grad 2.45 | tok/s 16111
step   1750 | loss 1.5998 | lr 3.00e-04 | grad 2.77 | tok/s 14431
step   1760 | loss 1.4368 | lr 3.00e-04 | grad 1.92 | tok/s 15651
step   1770 | loss 1.4968 | lr 3.00e-04 | grad 2.86 | tok/s 16030
step   1780 | loss 1.4283 | lr 3.00e-04 | grad 2.08 | tok/s 15913
step   1790 | loss 1.5646 | lr 3.00e-04 | grad 2.42 | tok/s 15569
step   1800 | loss 1.4818 | lr 3.00e-04 | grad 3.27 | tok/s 15760
step   1810 | loss 1.4852 | lr 3.00e-04 | grad 3.75 | tok/s 15885
step   1820 | loss 1.4408 | lr 3.00e-04 | grad 2.47 | tok/s 16204
step   1830 | loss 1.4409 | lr 3.00e-04 | grad 1.74 | tok/s 15443
step   1840 | loss 1.2859 | lr 3.00e-04 | grad 2.00 | tok/s 16496
step   1850 | loss 1.3687 | lr 3.00e-04 | grad 2.36 | tok/s 15453
step   1860 | loss 1.3387 | lr 3.00e-04 | grad 1.55 | tok/s 16266
step   1870 | loss 1.3098 | lr 3.00e-04 | grad 2.47 | tok/s 14739
step   1880 | loss 1.5570 | lr 3.00e-04 | grad 2.03 | tok/s 15571
step   1890 | loss 1.3715 | lr 3.00e-04 | grad 1.98 | tok/s 15789
step   1900 | loss 1.5021 | lr 3.00e-04 | grad 1.94 | tok/s 15346
step   1910 | loss 1.3599 | lr 3.00e-04 | grad 1.98 | tok/s 16415
step   1920 | loss 1.4862 | lr 3.00e-04 | grad 2.19 | tok/s 15408
step   1930 | loss 1.4760 | lr 3.00e-04 | grad 3.14 | tok/s 16320
step   1940 | loss 1.8224 | lr 3.00e-04 | grad 3.48 | tok/s 16629
step   1950 | loss 1.4458 | lr 3.00e-04 | grad 3.86 | tok/s 16641
step   1960 | loss 1.5376 | lr 3.00e-04 | grad 5.56 | tok/s 16079
step   1970 | loss 1.5009 | lr 3.00e-04 | grad 1.95 | tok/s 15614
step   1980 | loss 1.6393 | lr 3.00e-04 | grad 2.06 | tok/s 15744
step   1990 | loss 1.5076 | lr 3.00e-04 | grad 2.33 | tok/s 15973
step   2000 | loss 1.0203 | lr 3.00e-04 | grad 1.77 | tok/s 16503
  >>> saved checkpoint: checkpoint_step_002000_loss_1.0203.pt
step   2010 | loss 1.3165 | lr 3.00e-04 | grad 2.84 | tok/s 6076
step   2020 | loss 0.9759 | lr 3.00e-04 | grad 4.84 | tok/s 16755
step   2030 | loss 1.2573 | lr 3.00e-04 | grad 1.70 | tok/s 16713
step   2040 | loss 1.3139 | lr 3.00e-04 | grad 2.98 | tok/s 16025
step   2050 | loss 1.6627 | lr 3.00e-04 | grad 2.50 | tok/s 15757
step   2060 | loss 1.9038 | lr 3.00e-04 | grad 8.12 | tok/s 15827

Training complete! Final step: 2061
