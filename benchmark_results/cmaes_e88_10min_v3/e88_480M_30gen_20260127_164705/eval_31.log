Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_31/levelE88_100m_20260127_171846
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,036,416 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.4690 | lr 3.00e-04 | grad 17.38 | tok/s 9172
step     20 | loss 3.5607 | lr 3.00e-04 | grad 9.44 | tok/s 18761
step     30 | loss 3.3276 | lr 3.00e-04 | grad 10.12 | tok/s 19738
step     40 | loss 4.9235 | lr 3.00e-04 | grad 36.00 | tok/s 20063
step     50 | loss 4.5301 | lr 3.00e-04 | grad 18.38 | tok/s 20198
step     60 | loss 3.5481 | lr 3.00e-04 | grad 9.62 | tok/s 20074
step     70 | loss 3.0424 | lr 3.00e-04 | grad 6.72 | tok/s 19959
step     80 | loss 2.6560 | lr 3.00e-04 | grad 11.38 | tok/s 19888
step     90 | loss 2.4910 | lr 3.00e-04 | grad 7.25 | tok/s 19802
step    100 | loss 2.3074 | lr 3.00e-04 | grad 4.41 | tok/s 19749
step    110 | loss 2.3081 | lr 3.00e-04 | grad 5.78 | tok/s 19533
step    120 | loss 2.7440 | lr 3.00e-04 | grad 3.55 | tok/s 18582
step    130 | loss 2.0854 | lr 3.00e-04 | grad 6.81 | tok/s 18993
step    140 | loss 2.3854 | lr 3.00e-04 | grad 8.94 | tok/s 19068
step    150 | loss 1.3615 | lr 3.00e-04 | grad 7.41 | tok/s 19536
step    160 | loss 2.3113 | lr 3.00e-04 | grad 3.38 | tok/s 18919
step    170 | loss 2.3080 | lr 3.00e-04 | grad 2.84 | tok/s 18637
step    180 | loss 1.8137 | lr 3.00e-04 | grad 4.28 | tok/s 19000
step    190 | loss 1.8930 | lr 3.00e-04 | grad 3.97 | tok/s 18617
step    200 | loss 1.6209 | lr 3.00e-04 | grad 2.61 | tok/s 19481
step    210 | loss 1.8831 | lr 3.00e-04 | grad 7.12 | tok/s 18521
step    220 | loss 2.1911 | lr 3.00e-04 | grad 5.16 | tok/s 18662
step    230 | loss 2.0368 | lr 3.00e-04 | grad 3.61 | tok/s 18663
step    240 | loss 2.2786 | lr 3.00e-04 | grad 7.03 | tok/s 18917
step    250 | loss 1.7524 | lr 3.00e-04 | grad 2.34 | tok/s 18663
step    260 | loss 1.8836 | lr 3.00e-04 | grad 4.53 | tok/s 19185
step    270 | loss 1.8214 | lr 3.00e-04 | grad 2.94 | tok/s 18810
step    280 | loss 1.7762 | lr 3.00e-04 | grad 2.50 | tok/s 17691
step    290 | loss 1.6673 | lr 3.00e-04 | grad 2.98 | tok/s 18268
step    300 | loss 1.9769 | lr 3.00e-04 | grad 3.31 | tok/s 18448
step    310 | loss 1.6675 | lr 3.00e-04 | grad 2.44 | tok/s 18295
step    320 | loss 1.8944 | lr 3.00e-04 | grad 5.72 | tok/s 18515
step    330 | loss 1.7294 | lr 3.00e-04 | grad 2.64 | tok/s 18367
step    340 | loss 2.0514 | lr 3.00e-04 | grad 2.78 | tok/s 18670
step    350 | loss 1.7054 | lr 3.00e-04 | grad 2.78 | tok/s 19202
step    360 | loss 1.5883 | lr 3.00e-04 | grad 2.42 | tok/s 18337
step    370 | loss 1.4804 | lr 3.00e-04 | grad 2.52 | tok/s 19348
step    380 | loss 1.2069 | lr 3.00e-04 | grad 2.20 | tok/s 19537
step    390 | loss 1.1227 | lr 3.00e-04 | grad 1.93 | tok/s 19562
step    400 | loss 1.7528 | lr 3.00e-04 | grad 2.23 | tok/s 18508
step    410 | loss 1.7902 | lr 3.00e-04 | grad 3.03 | tok/s 18690
step    420 | loss 1.5891 | lr 3.00e-04 | grad 4.31 | tok/s 19464
step    430 | loss 1.6061 | lr 3.00e-04 | grad 2.53 | tok/s 19115
step    440 | loss 1.7174 | lr 3.00e-04 | grad 3.19 | tok/s 18501
step    450 | loss 1.6463 | lr 3.00e-04 | grad 2.30 | tok/s 18751
step    460 | loss 1.6052 | lr 3.00e-04 | grad 2.69 | tok/s 19020
step    470 | loss 1.5721 | lr 3.00e-04 | grad 4.62 | tok/s 18892
step    480 | loss 1.5716 | lr 3.00e-04 | grad 3.89 | tok/s 19315
step    490 | loss 1.7217 | lr 3.00e-04 | grad 3.20 | tok/s 18545
step    500 | loss 1.8165 | lr 3.00e-04 | grad 2.23 | tok/s 18852
step    510 | loss 1.6952 | lr 3.00e-04 | grad 2.16 | tok/s 18041
step    520 | loss 1.5532 | lr 3.00e-04 | grad 2.77 | tok/s 18847
step    530 | loss 1.7242 | lr 3.00e-04 | grad 2.38 | tok/s 18514
step    540 | loss 1.5935 | lr 3.00e-04 | grad 2.11 | tok/s 18178
step    550 | loss 1.3825 | lr 3.00e-04 | grad 4.31 | tok/s 18961
step    560 | loss 1.4558 | lr 3.00e-04 | grad 2.59 | tok/s 19494
step    570 | loss 1.3584 | lr 3.00e-04 | grad 2.50 | tok/s 19436
step    580 | loss 1.3164 | lr 3.00e-04 | grad 2.03 | tok/s 18292
step    590 | loss 1.3447 | lr 3.00e-04 | grad 1.94 | tok/s 19467
step    600 | loss 1.2810 | lr 3.00e-04 | grad 2.27 | tok/s 19489
step    610 | loss 1.3161 | lr 3.00e-04 | grad 2.31 | tok/s 19510
step    620 | loss 1.3050 | lr 3.00e-04 | grad 2.38 | tok/s 19421
step    630 | loss 1.7424 | lr 3.00e-04 | grad 6.41 | tok/s 18369
step    640 | loss 1.7568 | lr 3.00e-04 | grad 3.12 | tok/s 18585
step    650 | loss 1.5659 | lr 3.00e-04 | grad 2.34 | tok/s 18576
step    660 | loss 1.6058 | lr 3.00e-04 | grad 2.42 | tok/s 19268
step    670 | loss 1.6546 | lr 3.00e-04 | grad 6.00 | tok/s 18708
step    680 | loss 1.6593 | lr 3.00e-04 | grad 2.95 | tok/s 18333
step    690 | loss 1.6197 | lr 3.00e-04 | grad 2.34 | tok/s 18224
step    700 | loss 1.4920 | lr 3.00e-04 | grad 1.83 | tok/s 18614
step    710 | loss 1.6769 | lr 3.00e-04 | grad 3.69 | tok/s 18294
step    720 | loss 1.3237 | lr 3.00e-04 | grad 2.36 | tok/s 19044
step    730 | loss 1.5063 | lr 3.00e-04 | grad 1.97 | tok/s 18749
step    740 | loss 1.7779 | lr 3.00e-04 | grad 4.34 | tok/s 19212
step    750 | loss 1.5207 | lr 3.00e-04 | grad 2.27 | tok/s 19430
step    760 | loss 1.5657 | lr 3.00e-04 | grad 4.78 | tok/s 18993
step    770 | loss 1.6136 | lr 3.00e-04 | grad 2.55 | tok/s 18661
step    780 | loss 1.5071 | lr 3.00e-04 | grad 2.52 | tok/s 18743
step    790 | loss 1.6496 | lr 3.00e-04 | grad 6.88 | tok/s 19136
step    800 | loss 1.3400 | lr 3.00e-04 | grad 1.59 | tok/s 18908
step    810 | loss 1.3383 | lr 3.00e-04 | grad 3.14 | tok/s 18164
step    820 | loss 1.4420 | lr 3.00e-04 | grad 2.34 | tok/s 18373
step    830 | loss 1.5165 | lr 3.00e-04 | grad 1.77 | tok/s 18044
step    840 | loss 1.6557 | lr 3.00e-04 | grad 2.28 | tok/s 18203
step    850 | loss 1.5778 | lr 3.00e-04 | grad 1.91 | tok/s 18639
step    860 | loss 1.6254 | lr 3.00e-04 | grad 4.91 | tok/s 19006
step    870 | loss 1.4355 | lr 3.00e-04 | grad 2.30 | tok/s 19049
step    880 | loss 1.6167 | lr 3.00e-04 | grad 2.44 | tok/s 18747
step    890 | loss 1.5111 | lr 3.00e-04 | grad 2.03 | tok/s 18637
step    900 | loss 1.5600 | lr 3.00e-04 | grad 1.95 | tok/s 16790
step    910 | loss 1.5592 | lr 3.00e-04 | grad 7.72 | tok/s 18419
step    920 | loss 1.5197 | lr 3.00e-04 | grad 2.59 | tok/s 18254
step    930 | loss 1.4034 | lr 3.00e-04 | grad 2.70 | tok/s 18850
step    940 | loss 1.3817 | lr 3.00e-04 | grad 2.36 | tok/s 18507
step    950 | loss 1.5164 | lr 3.00e-04 | grad 3.12 | tok/s 18145
step    960 | loss 1.4649 | lr 3.00e-04 | grad 2.36 | tok/s 18696
step    970 | loss 1.4978 | lr 3.00e-04 | grad 2.34 | tok/s 18523
step    980 | loss 1.9392 | lr 3.00e-04 | grad 3.69 | tok/s 19286
step    990 | loss 1.6064 | lr 3.00e-04 | grad 2.36 | tok/s 18620
step   1000 | loss 1.6202 | lr 3.00e-04 | grad 2.69 | tok/s 18701
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6202.pt
step   1010 | loss 1.4621 | lr 3.00e-04 | grad 2.14 | tok/s 3721
step   1020 | loss 1.7833 | lr 3.00e-04 | grad 10.38 | tok/s 18847
step   1030 | loss 2.0981 | lr 3.00e-04 | grad 1.84 | tok/s 19694
step   1040 | loss 1.5175 | lr 3.00e-04 | grad 3.45 | tok/s 19013
step   1050 | loss 1.1499 | lr 3.00e-04 | grad 3.45 | tok/s 12021
step   1060 | loss 1.4086 | lr 3.00e-04 | grad 2.45 | tok/s 5857
step   1070 | loss 1.2920 | lr 3.00e-04 | grad 2.33 | tok/s 19938
step   1080 | loss 1.2175 | lr 3.00e-04 | grad 1.78 | tok/s 16931
step   1090 | loss 1.3458 | lr 3.00e-04 | grad 1.99 | tok/s 19522
step   1100 | loss 1.5599 | lr 3.00e-04 | grad 5.66 | tok/s 19454
step   1110 | loss 1.8971 | lr 3.00e-04 | grad 3.12 | tok/s 19298
step   1120 | loss 1.5183 | lr 3.00e-04 | grad 2.14 | tok/s 18439
step   1130 | loss 1.6161 | lr 3.00e-04 | grad 2.70 | tok/s 18262
step   1140 | loss 1.7358 | lr 3.00e-04 | grad 2.36 | tok/s 17817
step   1150 | loss 1.5052 | lr 3.00e-04 | grad 3.03 | tok/s 18942
step   1160 | loss 1.5100 | lr 3.00e-04 | grad 3.45 | tok/s 19338
step   1170 | loss 1.3525 | lr 3.00e-04 | grad 3.06 | tok/s 19591
step   1180 | loss 1.1991 | lr 3.00e-04 | grad 2.50 | tok/s 18500
step   1190 | loss 1.4383 | lr 3.00e-04 | grad 2.34 | tok/s 18517
step   1200 | loss 1.5296 | lr 3.00e-04 | grad 2.80 | tok/s 18611
step   1210 | loss 1.2415 | lr 3.00e-04 | grad 2.19 | tok/s 19321
step   1220 | loss 1.4316 | lr 3.00e-04 | grad 2.86 | tok/s 18805
step   1230 | loss 1.3882 | lr 3.00e-04 | grad 2.94 | tok/s 19398
step   1240 | loss 1.5151 | lr 3.00e-04 | grad 3.72 | tok/s 18860
step   1250 | loss 1.3260 | lr 3.00e-04 | grad 2.81 | tok/s 19057
step   1260 | loss 1.5039 | lr 3.00e-04 | grad 1.98 | tok/s 18056
step   1270 | loss 1.4512 | lr 3.00e-04 | grad 1.98 | tok/s 18360
step   1280 | loss 1.6431 | lr 3.00e-04 | grad 2.89 | tok/s 18924
step   1290 | loss 1.5108 | lr 3.00e-04 | grad 3.09 | tok/s 19152
step   1300 | loss 1.5827 | lr 3.00e-04 | grad 2.17 | tok/s 18901
step   1310 | loss 1.4321 | lr 3.00e-04 | grad 2.64 | tok/s 18449
step   1320 | loss 1.5721 | lr 3.00e-04 | grad 3.09 | tok/s 19019
step   1330 | loss 1.5429 | lr 3.00e-04 | grad 4.66 | tok/s 18356

Training complete! Final step: 1336
