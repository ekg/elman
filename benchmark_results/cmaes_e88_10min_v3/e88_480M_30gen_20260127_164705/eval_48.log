Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_48/levelE88_100m_20260127_173939
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,651,016 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1134 | lr 3.00e-04 | grad 65.50 | tok/s 8180
step     20 | loss 3.1293 | lr 3.00e-04 | grad 24.75 | tok/s 17397
step     30 | loss 3.0176 | lr 3.00e-04 | grad 5.34 | tok/s 18464
step     40 | loss 5.1011 | lr 3.00e-04 | grad 59.25 | tok/s 18601
step     50 | loss 4.3720 | lr 3.00e-04 | grad 15.31 | tok/s 18783
step     60 | loss 3.6048 | lr 3.00e-04 | grad 12.25 | tok/s 18694
step     70 | loss 2.8857 | lr 3.00e-04 | grad 9.12 | tok/s 18665
step     80 | loss 2.6275 | lr 3.00e-04 | grad 4.59 | tok/s 18312
step     90 | loss 2.5038 | lr 3.00e-04 | grad 6.47 | tok/s 18166
step    100 | loss 2.3997 | lr 3.00e-04 | grad 5.00 | tok/s 18518
step    110 | loss 2.5212 | lr 3.00e-04 | grad 14.50 | tok/s 18218
step    120 | loss 2.5097 | lr 3.00e-04 | grad 3.72 | tok/s 17309
step    130 | loss 2.1197 | lr 3.00e-04 | grad 3.55 | tok/s 17910
step    140 | loss 2.4195 | lr 3.00e-04 | grad 9.31 | tok/s 17836
step    150 | loss 1.5052 | lr 3.00e-04 | grad 4.56 | tok/s 18201
step    160 | loss 2.2395 | lr 3.00e-04 | grad 2.89 | tok/s 17475
step    170 | loss 2.3264 | lr 3.00e-04 | grad 2.89 | tok/s 17578
step    180 | loss 1.8120 | lr 3.00e-04 | grad 3.33 | tok/s 17655
step    190 | loss 1.8694 | lr 3.00e-04 | grad 3.06 | tok/s 17662
step    200 | loss 1.6362 | lr 3.00e-04 | grad 2.50 | tok/s 18298
step    210 | loss 1.9680 | lr 3.00e-04 | grad 2.97 | tok/s 17190
step    220 | loss 2.2646 | lr 3.00e-04 | grad 13.81 | tok/s 17532
step    230 | loss 1.9716 | lr 3.00e-04 | grad 4.44 | tok/s 17343
step    240 | loss 2.2195 | lr 3.00e-04 | grad 2.47 | tok/s 17073
step    250 | loss 1.8073 | lr 3.00e-04 | grad 3.23 | tok/s 17006
step    260 | loss 1.9253 | lr 3.00e-04 | grad 3.34 | tok/s 18091
step    270 | loss 1.7789 | lr 3.00e-04 | grad 2.17 | tok/s 15827
step    280 | loss 1.7854 | lr 3.00e-04 | grad 1.98 | tok/s 16750
step    290 | loss 1.6684 | lr 3.00e-04 | grad 2.55 | tok/s 16793
step    300 | loss 2.0183 | lr 3.00e-04 | grad 2.61 | tok/s 17307
step    310 | loss 1.6803 | lr 3.00e-04 | grad 3.14 | tok/s 13472
step    320 | loss 1.8615 | lr 3.00e-04 | grad 2.91 | tok/s 11845
step    330 | loss 1.7373 | lr 3.00e-04 | grad 2.28 | tok/s 17674
step    340 | loss 2.0921 | lr 3.00e-04 | grad 2.95 | tok/s 14724
step    350 | loss 1.7074 | lr 3.00e-04 | grad 2.25 | tok/s 17012
step    360 | loss 1.5696 | lr 3.00e-04 | grad 1.74 | tok/s 13720
step    370 | loss 1.4709 | lr 3.00e-04 | grad 1.89 | tok/s 13569
step    380 | loss 1.2173 | lr 3.00e-04 | grad 2.25 | tok/s 14615
step    390 | loss 1.1131 | lr 3.00e-04 | grad 1.86 | tok/s 15562
step    400 | loss 1.8724 | lr 3.00e-04 | grad 2.52 | tok/s 11704
step    410 | loss 1.7451 | lr 3.00e-04 | grad 2.80 | tok/s 11974
step    420 | loss 1.6097 | lr 3.00e-04 | grad 2.27 | tok/s 16357
step    430 | loss 1.6081 | lr 3.00e-04 | grad 2.69 | tok/s 11835
step    440 | loss 1.7817 | lr 3.00e-04 | grad 3.22 | tok/s 14246
step    450 | loss 1.5759 | lr 3.00e-04 | grad 3.41 | tok/s 17591
step    460 | loss 1.6473 | lr 3.00e-04 | grad 2.84 | tok/s 17693
step    470 | loss 1.5882 | lr 3.00e-04 | grad 2.52 | tok/s 17582
step    480 | loss 1.6555 | lr 3.00e-04 | grad 3.12 | tok/s 17809
step    490 | loss 1.6392 | lr 3.00e-04 | grad 1.99 | tok/s 17978
step    500 | loss 1.8918 | lr 3.00e-04 | grad 2.56 | tok/s 17460
step    510 | loss 1.6269 | lr 3.00e-04 | grad 2.03 | tok/s 16956
step    520 | loss 1.5502 | lr 3.00e-04 | grad 2.20 | tok/s 14522
step    530 | loss 1.8152 | lr 3.00e-04 | grad 2.38 | tok/s 10918
step    540 | loss 1.3927 | lr 3.00e-04 | grad 10.88 | tok/s 17071
step    550 | loss 1.4996 | lr 3.00e-04 | grad 2.02 | tok/s 12501
step    560 | loss 1.3907 | lr 3.00e-04 | grad 1.74 | tok/s 17393
step    570 | loss 1.3632 | lr 3.00e-04 | grad 1.88 | tok/s 18403
step    580 | loss 1.2751 | lr 3.00e-04 | grad 1.67 | tok/s 12346
step    590 | loss 1.3512 | lr 3.00e-04 | grad 1.88 | tok/s 13635
step    600 | loss 1.2897 | lr 3.00e-04 | grad 1.77 | tok/s 17963
step    610 | loss 1.3264 | lr 3.00e-04 | grad 1.95 | tok/s 11508
step    620 | loss 1.5922 | lr 3.00e-04 | grad 2.78 | tok/s 14373
step    630 | loss 1.5702 | lr 3.00e-04 | grad 1.77 | tok/s 14086
step    640 | loss 1.7099 | lr 3.00e-04 | grad 2.33 | tok/s 14806
step    650 | loss 1.5599 | lr 3.00e-04 | grad 2.34 | tok/s 14503
step    660 | loss 1.7254 | lr 3.00e-04 | grad 3.78 | tok/s 15339
step    670 | loss 1.6714 | lr 3.00e-04 | grad 2.58 | tok/s 17133
step    680 | loss 1.5669 | lr 3.00e-04 | grad 3.55 | tok/s 17498
step    690 | loss 1.5980 | lr 3.00e-04 | grad 1.82 | tok/s 17131
step    700 | loss 1.5630 | lr 3.00e-04 | grad 8.75 | tok/s 17257
step    710 | loss 1.5583 | lr 3.00e-04 | grad 1.91 | tok/s 17412
step    720 | loss 1.2204 | lr 3.00e-04 | grad 4.03 | tok/s 18330
step    730 | loss 1.6960 | lr 3.00e-04 | grad 1.73 | tok/s 16731
step    740 | loss 1.8514 | lr 3.00e-04 | grad 1.92 | tok/s 17621
step    750 | loss 1.5111 | lr 3.00e-04 | grad 1.91 | tok/s 17677
step    760 | loss 1.5292 | lr 3.00e-04 | grad 2.14 | tok/s 15911
step    770 | loss 1.5165 | lr 3.00e-04 | grad 2.86 | tok/s 17733
step    780 | loss 1.5079 | lr 3.00e-04 | grad 1.90 | tok/s 15851
step    790 | loss 1.8307 | lr 3.00e-04 | grad 2.38 | tok/s 16930
step    800 | loss 1.0716 | lr 3.00e-04 | grad 1.82 | tok/s 15255
step    810 | loss 1.4943 | lr 3.00e-04 | grad 2.31 | tok/s 16851
step    820 | loss 1.5201 | lr 3.00e-04 | grad 2.17 | tok/s 16890
step    830 | loss 1.4855 | lr 3.00e-04 | grad 3.59 | tok/s 18076
step    840 | loss 1.6510 | lr 3.00e-04 | grad 4.38 | tok/s 17297
step    850 | loss 1.5839 | lr 3.00e-04 | grad 1.78 | tok/s 15794
step    860 | loss 1.5362 | lr 3.00e-04 | grad 3.77 | tok/s 17767
step    870 | loss 1.5531 | lr 3.00e-04 | grad 2.47 | tok/s 17710
step    880 | loss 1.5624 | lr 3.00e-04 | grad 2.03 | tok/s 16642
step    890 | loss 1.5181 | lr 3.00e-04 | grad 2.33 | tok/s 17775
step    900 | loss 1.5746 | lr 3.00e-04 | grad 2.28 | tok/s 17209
step    910 | loss 1.5529 | lr 3.00e-04 | grad 2.41 | tok/s 17751
step    920 | loss 1.4662 | lr 3.00e-04 | grad 1.56 | tok/s 17714
step    930 | loss 1.4266 | lr 3.00e-04 | grad 2.30 | tok/s 17543
step    940 | loss 1.4620 | lr 3.00e-04 | grad 1.74 | tok/s 15974
step    950 | loss 1.4750 | lr 3.00e-04 | grad 2.11 | tok/s 15635
step    960 | loss 1.4656 | lr 3.00e-04 | grad 2.23 | tok/s 17460
step    970 | loss 1.8369 | lr 3.00e-04 | grad 4.41 | tok/s 18185
step    980 | loss 1.6760 | lr 3.00e-04 | grad 2.11 | tok/s 18095
step    990 | loss 1.6382 | lr 3.00e-04 | grad 3.83 | tok/s 14945
step   1000 | loss 1.3018 | lr 3.00e-04 | grad 1.73 | tok/s 18016
  >>> saved checkpoint: checkpoint_step_001000_loss_1.3018.pt
step   1010 | loss 1.2642 | lr 3.00e-04 | grad 1.53 | tok/s 6805
step   1020 | loss 1.5704 | lr 3.00e-04 | grad 2.17 | tok/s 17854
step   1030 | loss 2.1822 | lr 3.00e-04 | grad 3.91 | tok/s 18214
step   1040 | loss 1.5343 | lr 3.00e-04 | grad 3.22 | tok/s 18346
step   1050 | loss 1.1846 | lr 3.00e-04 | grad 1.74 | tok/s 18094
step   1060 | loss 1.4731 | lr 3.00e-04 | grad 2.11 | tok/s 18011
step   1070 | loss 1.2937 | lr 3.00e-04 | grad 1.62 | tok/s 18597
step   1080 | loss 1.2496 | lr 3.00e-04 | grad 1.53 | tok/s 18572
step   1090 | loss 1.2327 | lr 3.00e-04 | grad 1.49 | tok/s 18572
step   1100 | loss 1.1717 | lr 3.00e-04 | grad 1.55 | tok/s 18564
step   1110 | loss 1.4734 | lr 3.00e-04 | grad 4.31 | tok/s 18053
step   1120 | loss 1.7032 | lr 3.00e-04 | grad 1.73 | tok/s 18197
step   1130 | loss 1.7875 | lr 3.00e-04 | grad 2.02 | tok/s 18408
step   1140 | loss 1.6556 | lr 3.00e-04 | grad 2.36 | tok/s 17810
step   1150 | loss 1.7714 | lr 3.00e-04 | grad 2.59 | tok/s 17545
step   1160 | loss 1.5261 | lr 3.00e-04 | grad 2.22 | tok/s 17341
step   1170 | loss 1.3650 | lr 3.00e-04 | grad 3.23 | tok/s 18207
step   1180 | loss 1.6445 | lr 3.00e-04 | grad 2.83 | tok/s 18386
step   1190 | loss 1.1398 | lr 3.00e-04 | grad 2.59 | tok/s 18483
step   1200 | loss 1.4234 | lr 3.00e-04 | grad 1.99 | tok/s 17186
step   1210 | loss 1.4140 | lr 3.00e-04 | grad 2.44 | tok/s 18073
step   1220 | loss 1.3682 | lr 3.00e-04 | grad 1.47 | tok/s 18093

Training complete! Final step: 1221
