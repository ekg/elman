Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_34/levelE88_100m_20260127_172917
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 474,917,672 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2931 | lr 3.00e-04 | grad 17.12 | tok/s 9574
step     20 | loss 3.0965 | lr 3.00e-04 | grad 8.81 | tok/s 19367
step     30 | loss 3.1775 | lr 3.00e-04 | grad 10.69 | tok/s 20387
step     40 | loss 4.9371 | lr 3.00e-04 | grad 37.50 | tok/s 20734
step     50 | loss 4.6220 | lr 3.00e-04 | grad 18.25 | tok/s 20969
step     60 | loss 3.4896 | lr 3.00e-04 | grad 11.56 | tok/s 20844
step     70 | loss 2.9325 | lr 3.00e-04 | grad 7.78 | tok/s 20781
step     80 | loss 2.5959 | lr 3.00e-04 | grad 7.78 | tok/s 20735
step     90 | loss 2.4812 | lr 3.00e-04 | grad 5.81 | tok/s 20691
step    100 | loss 2.3693 | lr 3.00e-04 | grad 7.97 | tok/s 20672
step    110 | loss 2.3187 | lr 3.00e-04 | grad 4.25 | tok/s 20458
step    120 | loss 2.7827 | lr 3.00e-04 | grad 3.30 | tok/s 19548
step    130 | loss 2.0933 | lr 3.00e-04 | grad 7.25 | tok/s 19988
step    140 | loss 2.3785 | lr 3.00e-04 | grad 8.88 | tok/s 20027
step    150 | loss 1.4936 | lr 3.00e-04 | grad 7.34 | tok/s 20441
step    160 | loss 2.3129 | lr 3.00e-04 | grad 3.20 | tok/s 19761
step    170 | loss 2.3200 | lr 3.00e-04 | grad 2.66 | tok/s 19520
step    180 | loss 1.7945 | lr 3.00e-04 | grad 4.16 | tok/s 19989
step    190 | loss 1.9022 | lr 3.00e-04 | grad 4.00 | tok/s 19612
step    200 | loss 1.6217 | lr 3.00e-04 | grad 2.58 | tok/s 20526
step    210 | loss 1.8791 | lr 3.00e-04 | grad 8.06 | tok/s 19457
step    220 | loss 2.2097 | lr 3.00e-04 | grad 4.88 | tok/s 19689
step    230 | loss 2.0198 | lr 3.00e-04 | grad 3.44 | tok/s 19619
step    240 | loss 2.2674 | lr 3.00e-04 | grad 7.22 | tok/s 19867
step    250 | loss 1.7571 | lr 3.00e-04 | grad 2.27 | tok/s 19791
step    260 | loss 1.8880 | lr 3.00e-04 | grad 3.98 | tok/s 20316
step    270 | loss 1.8175 | lr 3.00e-04 | grad 2.94 | tok/s 19874
step    280 | loss 1.7690 | lr 3.00e-04 | grad 2.39 | tok/s 18688
step    290 | loss 1.6670 | lr 3.00e-04 | grad 2.97 | tok/s 19336
step    300 | loss 1.9798 | lr 3.00e-04 | grad 2.84 | tok/s 19492
step    310 | loss 1.6650 | lr 3.00e-04 | grad 2.44 | tok/s 19392
step    320 | loss 1.8846 | lr 3.00e-04 | grad 4.56 | tok/s 19602
step    330 | loss 1.7239 | lr 3.00e-04 | grad 2.59 | tok/s 19787
step    340 | loss 2.0584 | lr 3.00e-04 | grad 2.77 | tok/s 19644
step    350 | loss 1.6997 | lr 3.00e-04 | grad 2.61 | tok/s 20185
step    360 | loss 1.5840 | lr 3.00e-04 | grad 2.50 | tok/s 19370
step    370 | loss 1.4727 | lr 3.00e-04 | grad 2.34 | tok/s 20379
step    380 | loss 1.1930 | lr 3.00e-04 | grad 1.98 | tok/s 20568
step    390 | loss 1.1028 | lr 3.00e-04 | grad 1.99 | tok/s 20585
step    400 | loss 1.7633 | lr 3.00e-04 | grad 2.34 | tok/s 19485
step    410 | loss 1.7830 | lr 3.00e-04 | grad 3.00 | tok/s 19655
step    420 | loss 1.5927 | lr 3.00e-04 | grad 4.31 | tok/s 20461
step    430 | loss 1.5999 | lr 3.00e-04 | grad 2.72 | tok/s 20096
step    440 | loss 1.7159 | lr 3.00e-04 | grad 3.02 | tok/s 19482
step    450 | loss 1.6463 | lr 3.00e-04 | grad 1.99 | tok/s 19690
step    460 | loss 1.6038 | lr 3.00e-04 | grad 2.64 | tok/s 20026
step    470 | loss 1.5768 | lr 3.00e-04 | grad 4.38 | tok/s 19672
step    480 | loss 1.5965 | lr 3.00e-04 | grad 3.62 | tok/s 20063
step    490 | loss 1.7205 | lr 3.00e-04 | grad 3.06 | tok/s 17418
step    500 | loss 1.8265 | lr 3.00e-04 | grad 2.20 | tok/s 19640
step    510 | loss 1.6867 | lr 3.00e-04 | grad 2.22 | tok/s 17996
step    520 | loss 1.5434 | lr 3.00e-04 | grad 2.78 | tok/s 19790
step    530 | loss 1.7208 | lr 3.00e-04 | grad 2.38 | tok/s 19434
step    540 | loss 1.5954 | lr 3.00e-04 | grad 2.16 | tok/s 16533
step    550 | loss 1.4341 | lr 3.00e-04 | grad 2.33 | tok/s 19731
step    560 | loss 1.4218 | lr 3.00e-04 | grad 2.70 | tok/s 20473
step    570 | loss 1.3401 | lr 3.00e-04 | grad 2.09 | tok/s 20500
step    580 | loss 1.2929 | lr 3.00e-04 | grad 2.14 | tok/s 20436
step    590 | loss 1.3490 | lr 3.00e-04 | grad 2.62 | tok/s 20470
step    600 | loss 1.2728 | lr 3.00e-04 | grad 2.31 | tok/s 20499
step    610 | loss 1.3011 | lr 3.00e-04 | grad 1.93 | tok/s 20484
step    620 | loss 1.3976 | lr 3.00e-04 | grad 5.06 | tok/s 20143
step    630 | loss 1.7052 | lr 3.00e-04 | grad 3.92 | tok/s 18607
step    640 | loss 1.7049 | lr 3.00e-04 | grad 2.81 | tok/s 19557
step    650 | loss 1.5616 | lr 3.00e-04 | grad 3.50 | tok/s 19541
step    660 | loss 1.6734 | lr 3.00e-04 | grad 4.88 | tok/s 20229
step    670 | loss 1.5972 | lr 3.00e-04 | grad 3.73 | tok/s 18948
step    680 | loss 1.6322 | lr 3.00e-04 | grad 1.80 | tok/s 18146
step    690 | loss 1.6459 | lr 3.00e-04 | grad 3.55 | tok/s 17818
step    700 | loss 1.4604 | lr 3.00e-04 | grad 1.83 | tok/s 18605
step    710 | loss 1.7045 | lr 3.00e-04 | grad 2.98 | tok/s 16736
step    720 | loss 1.2861 | lr 3.00e-04 | grad 2.41 | tok/s 20034
step    730 | loss 1.5695 | lr 3.00e-04 | grad 4.66 | tok/s 18422
step    740 | loss 1.7742 | lr 3.00e-04 | grad 5.16 | tok/s 20273
step    750 | loss 1.4808 | lr 3.00e-04 | grad 2.27 | tok/s 20407
step    760 | loss 1.6100 | lr 3.00e-04 | grad 3.11 | tok/s 19923
step    770 | loss 1.5937 | lr 3.00e-04 | grad 2.33 | tok/s 19668
step    780 | loss 1.4852 | lr 3.00e-04 | grad 2.05 | tok/s 19647
step    790 | loss 1.6815 | lr 3.00e-04 | grad 3.47 | tok/s 20167
step    800 | loss 1.2037 | lr 3.00e-04 | grad 4.59 | tok/s 19928
step    810 | loss 1.4241 | lr 3.00e-04 | grad 2.80 | tok/s 19149
step    820 | loss 1.4920 | lr 3.00e-04 | grad 5.19 | tok/s 19535
step    830 | loss 1.4347 | lr 3.00e-04 | grad 2.30 | tok/s 19268
step    840 | loss 1.6855 | lr 3.00e-04 | grad 2.48 | tok/s 19020
step    850 | loss 1.5648 | lr 3.00e-04 | grad 2.69 | tok/s 19583
step    860 | loss 1.6155 | lr 3.00e-04 | grad 3.33 | tok/s 20066
step    870 | loss 1.4124 | lr 3.00e-04 | grad 2.81 | tok/s 20014
step    880 | loss 1.6049 | lr 3.00e-04 | grad 2.11 | tok/s 19677
step    890 | loss 1.5181 | lr 3.00e-04 | grad 2.97 | tok/s 18426
step    900 | loss 1.5727 | lr 3.00e-04 | grad 2.73 | tok/s 15922
step    910 | loss 1.5162 | lr 3.00e-04 | grad 2.80 | tok/s 19579
step    920 | loss 1.5352 | lr 3.00e-04 | grad 2.02 | tok/s 19546
step    930 | loss 1.4076 | lr 3.00e-04 | grad 2.47 | tok/s 19544
step    940 | loss 1.3895 | lr 3.00e-04 | grad 2.20 | tok/s 16369
step    950 | loss 1.4757 | lr 3.00e-04 | grad 2.02 | tok/s 19233
step    960 | loss 1.4622 | lr 3.00e-04 | grad 2.61 | tok/s 19327
step    970 | loss 1.6943 | lr 3.00e-04 | grad 6.16 | tok/s 15843
step    980 | loss 1.8024 | lr 3.00e-04 | grad 4.41 | tok/s 20004
step    990 | loss 1.5284 | lr 3.00e-04 | grad 2.44 | tok/s 19568
step   1000 | loss 1.6307 | lr 3.00e-04 | grad 2.62 | tok/s 19047
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6307.pt
step   1010 | loss 1.1370 | lr 3.00e-04 | grad 2.78 | tok/s 8703
step   1020 | loss 1.3933 | lr 3.00e-04 | grad 2.09 | tok/s 15720
step   1030 | loss 1.7920 | lr 3.00e-04 | grad 9.75 | tok/s 16000
step   1040 | loss 2.1049 | lr 3.00e-04 | grad 1.92 | tok/s 19821
step   1050 | loss 1.5090 | lr 3.00e-04 | grad 3.23 | tok/s 19836
step   1060 | loss 1.1451 | lr 3.00e-04 | grad 3.19 | tok/s 20330
step   1070 | loss 1.4136 | lr 3.00e-04 | grad 3.34 | tok/s 20405
step   1080 | loss 1.2573 | lr 3.00e-04 | grad 2.00 | tok/s 20622
step   1090 | loss 1.2369 | lr 3.00e-04 | grad 1.98 | tok/s 20602
step   1100 | loss 1.1962 | lr 3.00e-04 | grad 2.05 | tok/s 20637
step   1110 | loss 1.3064 | lr 3.00e-04 | grad 2.42 | tok/s 20369
step   1120 | loss 1.5644 | lr 3.00e-04 | grad 7.12 | tok/s 20176
step   1130 | loss 1.8822 | lr 3.00e-04 | grad 3.47 | tok/s 20095
step   1140 | loss 1.5108 | lr 3.00e-04 | grad 9.31 | tok/s 20150
step   1150 | loss 1.6000 | lr 3.00e-04 | grad 3.03 | tok/s 19833
step   1160 | loss 1.7620 | lr 3.00e-04 | grad 2.42 | tok/s 19376
step   1170 | loss 1.4932 | lr 3.00e-04 | grad 2.25 | tok/s 19579
step   1180 | loss 1.4917 | lr 3.00e-04 | grad 3.09 | tok/s 20286
step   1190 | loss 1.4092 | lr 3.00e-04 | grad 1.61 | tok/s 20483
step   1200 | loss 1.1709 | lr 3.00e-04 | grad 2.59 | tok/s 18378
step   1210 | loss 1.4591 | lr 3.00e-04 | grad 2.39 | tok/s 19575
step   1220 | loss 1.4684 | lr 3.00e-04 | grad 2.12 | tok/s 19879
step   1230 | loss 1.2791 | lr 3.00e-04 | grad 1.76 | tok/s 20361
step   1240 | loss 1.3641 | lr 3.00e-04 | grad 2.41 | tok/s 19881
step   1250 | loss 1.4505 | lr 3.00e-04 | grad 3.03 | tok/s 20346
step   1260 | loss 1.4729 | lr 3.00e-04 | grad 2.14 | tok/s 19943
step   1270 | loss 1.3489 | lr 3.00e-04 | grad 1.80 | tok/s 20059
step   1280 | loss 1.5039 | lr 3.00e-04 | grad 1.90 | tok/s 19359
step   1290 | loss 1.4342 | lr 3.00e-04 | grad 4.16 | tok/s 19232
step   1300 | loss 1.6070 | lr 3.00e-04 | grad 2.12 | tok/s 19660
step   1310 | loss 1.4960 | lr 3.00e-04 | grad 2.75 | tok/s 20051
step   1320 | loss 1.6007 | lr 3.00e-04 | grad 2.38 | tok/s 19925
step   1330 | loss 1.4181 | lr 3.00e-04 | grad 2.30 | tok/s 19417
step   1340 | loss 1.5600 | lr 3.00e-04 | grad 2.16 | tok/s 20015
step   1350 | loss 1.5522 | lr 3.00e-04 | grad 2.20 | tok/s 19463
step   1360 | loss 1.3063 | lr 3.00e-04 | grad 2.08 | tok/s 19577
step   1370 | loss 1.7851 | lr 3.00e-04 | grad 1.85 | tok/s 20116
step   1380 | loss 1.4453 | lr 3.00e-04 | grad 2.75 | tok/s 19362
step   1390 | loss 1.5542 | lr 3.00e-04 | grad 3.97 | tok/s 19731
step   1400 | loss 1.3083 | lr 3.00e-04 | grad 2.08 | tok/s 19551
step   1410 | loss 1.3436 | lr 3.00e-04 | grad 5.31 | tok/s 19778
step   1420 | loss 1.4271 | lr 3.00e-04 | grad 1.86 | tok/s 20051
step   1430 | loss 1.5638 | lr 3.00e-04 | grad 2.27 | tok/s 19755
step   1440 | loss 1.4551 | lr 3.00e-04 | grad 2.91 | tok/s 20374

Training complete! Final step: 1447
