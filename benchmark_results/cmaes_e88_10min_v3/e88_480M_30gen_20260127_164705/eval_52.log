Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_52/levelE88_100m_20260127_175005
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 474,988,672 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2804 | lr 3.00e-04 | grad 18.62 | tok/s 9227
step     20 | loss 3.3298 | lr 3.00e-04 | grad 9.75 | tok/s 19234
step     30 | loss 3.3216 | lr 3.00e-04 | grad 11.31 | tok/s 20299
step     40 | loss 4.9437 | lr 3.00e-04 | grad 46.25 | tok/s 20665
step     50 | loss 4.7160 | lr 3.00e-04 | grad 19.88 | tok/s 20886
step     60 | loss 3.5147 | lr 3.00e-04 | grad 10.81 | tok/s 20773
step     70 | loss 2.9661 | lr 3.00e-04 | grad 7.16 | tok/s 20687
step     80 | loss 2.7144 | lr 3.00e-04 | grad 7.38 | tok/s 20660
step     90 | loss 2.6487 | lr 3.00e-04 | grad 6.84 | tok/s 20650
step    100 | loss 2.3434 | lr 3.00e-04 | grad 4.66 | tok/s 20586
step    110 | loss 2.3415 | lr 3.00e-04 | grad 4.50 | tok/s 20419
step    120 | loss 2.7550 | lr 3.00e-04 | grad 3.48 | tok/s 19415
step    130 | loss 2.1406 | lr 3.00e-04 | grad 7.00 | tok/s 19856
step    140 | loss 2.3996 | lr 3.00e-04 | grad 9.50 | tok/s 19902
step    150 | loss 1.3888 | lr 3.00e-04 | grad 7.78 | tok/s 20382
step    160 | loss 2.3464 | lr 3.00e-04 | grad 3.38 | tok/s 19697
step    170 | loss 2.3306 | lr 3.00e-04 | grad 2.98 | tok/s 19393
step    180 | loss 1.8006 | lr 3.00e-04 | grad 4.38 | tok/s 19818
step    190 | loss 1.9216 | lr 3.00e-04 | grad 3.70 | tok/s 19465
step    200 | loss 1.6444 | lr 3.00e-04 | grad 2.86 | tok/s 20353
step    210 | loss 1.8889 | lr 3.00e-04 | grad 9.12 | tok/s 19325
step    220 | loss 2.2132 | lr 3.00e-04 | grad 4.50 | tok/s 19518
step    230 | loss 2.0123 | lr 3.00e-04 | grad 3.52 | tok/s 19503
step    240 | loss 2.2866 | lr 3.00e-04 | grad 8.00 | tok/s 19767
step    250 | loss 1.7757 | lr 3.00e-04 | grad 2.58 | tok/s 19655
step    260 | loss 1.8985 | lr 3.00e-04 | grad 4.06 | tok/s 20164
step    270 | loss 1.8275 | lr 3.00e-04 | grad 3.02 | tok/s 19696
step    280 | loss 1.7836 | lr 3.00e-04 | grad 2.70 | tok/s 18508
step    290 | loss 1.6813 | lr 3.00e-04 | grad 3.11 | tok/s 19131
step    300 | loss 1.9974 | lr 3.00e-04 | grad 2.81 | tok/s 19282
step    310 | loss 1.6746 | lr 3.00e-04 | grad 2.62 | tok/s 19224
step    320 | loss 1.8843 | lr 3.00e-04 | grad 4.44 | tok/s 19449
step    330 | loss 1.7306 | lr 3.00e-04 | grad 2.72 | tok/s 19653
step    340 | loss 2.0642 | lr 3.00e-04 | grad 2.91 | tok/s 19538
step    350 | loss 1.7049 | lr 3.00e-04 | grad 2.67 | tok/s 20119
step    360 | loss 1.5967 | lr 3.00e-04 | grad 2.56 | tok/s 19274
step    370 | loss 1.4789 | lr 3.00e-04 | grad 2.58 | tok/s 20268
step    380 | loss 1.1948 | lr 3.00e-04 | grad 2.09 | tok/s 20460
step    390 | loss 1.1040 | lr 3.00e-04 | grad 2.08 | tok/s 20443
step    400 | loss 1.7717 | lr 3.00e-04 | grad 2.47 | tok/s 19384
step    410 | loss 1.7791 | lr 3.00e-04 | grad 3.08 | tok/s 19565
step    420 | loss 1.5905 | lr 3.00e-04 | grad 4.56 | tok/s 20395
step    430 | loss 1.6050 | lr 3.00e-04 | grad 2.92 | tok/s 20063
step    440 | loss 1.7145 | lr 3.00e-04 | grad 3.14 | tok/s 19409
step    450 | loss 1.6495 | lr 3.00e-04 | grad 2.12 | tok/s 19654
step    460 | loss 1.6167 | lr 3.00e-04 | grad 2.72 | tok/s 19939
step    470 | loss 1.5877 | lr 3.00e-04 | grad 4.72 | tok/s 19811
step    480 | loss 1.6191 | lr 3.00e-04 | grad 4.12 | tok/s 20237
step    490 | loss 1.7203 | lr 3.00e-04 | grad 3.12 | tok/s 19436
step    500 | loss 1.8232 | lr 3.00e-04 | grad 2.38 | tok/s 19765
step    510 | loss 1.6917 | lr 3.00e-04 | grad 2.47 | tok/s 18866
step    520 | loss 1.5407 | lr 3.00e-04 | grad 2.78 | tok/s 19756
step    530 | loss 1.7327 | lr 3.00e-04 | grad 2.48 | tok/s 19436
step    540 | loss 1.6018 | lr 3.00e-04 | grad 2.16 | tok/s 18193
step    550 | loss 1.3959 | lr 3.00e-04 | grad 4.31 | tok/s 19880
step    560 | loss 1.4621 | lr 3.00e-04 | grad 2.55 | tok/s 20431
step    570 | loss 1.3556 | lr 3.00e-04 | grad 2.66 | tok/s 20454
step    580 | loss 1.3044 | lr 3.00e-04 | grad 1.95 | tok/s 20459
step    590 | loss 1.3419 | lr 3.00e-04 | grad 2.02 | tok/s 20463
step    600 | loss 1.2761 | lr 3.00e-04 | grad 2.28 | tok/s 20463
step    610 | loss 1.3131 | lr 3.00e-04 | grad 2.23 | tok/s 20434
step    620 | loss 1.3032 | lr 3.00e-04 | grad 2.66 | tok/s 20349
step    630 | loss 1.7218 | lr 3.00e-04 | grad 7.56 | tok/s 19226
step    640 | loss 1.7636 | lr 3.00e-04 | grad 2.44 | tok/s 19505
step    650 | loss 1.5679 | lr 3.00e-04 | grad 2.42 | tok/s 19483
step    660 | loss 1.6156 | lr 3.00e-04 | grad 2.36 | tok/s 20250
step    670 | loss 1.6596 | lr 3.00e-04 | grad 6.12 | tok/s 19542
step    680 | loss 1.6737 | lr 3.00e-04 | grad 3.06 | tok/s 19229
step    690 | loss 1.6183 | lr 3.00e-04 | grad 2.50 | tok/s 19095
step    700 | loss 1.4993 | lr 3.00e-04 | grad 1.82 | tok/s 19508
step    710 | loss 1.6657 | lr 3.00e-04 | grad 3.64 | tok/s 19176
step    720 | loss 1.3239 | lr 3.00e-04 | grad 2.45 | tok/s 19978
step    730 | loss 1.5035 | lr 3.00e-04 | grad 1.99 | tok/s 19605
step    740 | loss 1.8046 | lr 3.00e-04 | grad 4.84 | tok/s 20119
step    750 | loss 1.5414 | lr 3.00e-04 | grad 2.36 | tok/s 20372
step    760 | loss 1.5652 | lr 3.00e-04 | grad 4.53 | tok/s 19939
step    770 | loss 1.6054 | lr 3.00e-04 | grad 2.62 | tok/s 19603
step    780 | loss 1.5077 | lr 3.00e-04 | grad 2.50 | tok/s 19735
step    790 | loss 1.6376 | lr 3.00e-04 | grad 5.53 | tok/s 20173
step    800 | loss 1.3421 | lr 3.00e-04 | grad 1.60 | tok/s 19834
step    810 | loss 1.3369 | lr 3.00e-04 | grad 3.33 | tok/s 19125
step    820 | loss 1.4359 | lr 3.00e-04 | grad 2.69 | tok/s 19497
step    830 | loss 1.5143 | lr 3.00e-04 | grad 1.94 | tok/s 19257
step    840 | loss 1.6523 | lr 3.00e-04 | grad 2.47 | tok/s 19196
step    850 | loss 1.5733 | lr 3.00e-04 | grad 1.98 | tok/s 19499
step    860 | loss 1.6221 | lr 3.00e-04 | grad 3.38 | tok/s 19757
step    870 | loss 1.4201 | lr 3.00e-04 | grad 2.41 | tok/s 19964
step    880 | loss 1.6208 | lr 3.00e-04 | grad 2.66 | tok/s 19578
step    890 | loss 1.5143 | lr 3.00e-04 | grad 1.98 | tok/s 19303
step    900 | loss 1.6121 | lr 3.00e-04 | grad 2.75 | tok/s 15864
step    910 | loss 1.5238 | lr 3.00e-04 | grad 4.34 | tok/s 19644
step    920 | loss 1.5119 | lr 3.00e-04 | grad 2.33 | tok/s 19413
step    930 | loss 1.4249 | lr 3.00e-04 | grad 2.73 | tok/s 19553
step    940 | loss 1.3878 | lr 3.00e-04 | grad 4.69 | tok/s 19077
step    950 | loss 1.4874 | lr 3.00e-04 | grad 2.67 | tok/s 19303
step    960 | loss 1.4634 | lr 3.00e-04 | grad 2.80 | tok/s 19467
step    970 | loss 1.6028 | lr 3.00e-04 | grad 6.69 | tok/s 19581
step    980 | loss 1.9149 | lr 3.00e-04 | grad 3.59 | tok/s 20164
step    990 | loss 1.5986 | lr 3.00e-04 | grad 2.09 | tok/s 19523
step   1000 | loss 1.6331 | lr 3.00e-04 | grad 2.03 | tok/s 19363
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6331.pt
step   1010 | loss 1.1847 | lr 3.00e-04 | grad 3.30 | tok/s 8532
step   1020 | loss 1.3857 | lr 3.00e-04 | grad 2.14 | tok/s 20142
step   1030 | loss 1.7910 | lr 3.00e-04 | grad 9.62 | tok/s 19467
step   1040 | loss 2.1011 | lr 3.00e-04 | grad 1.99 | tok/s 20355
step   1050 | loss 1.5209 | lr 3.00e-04 | grad 3.62 | tok/s 19739
step   1060 | loss 1.1838 | lr 3.00e-04 | grad 3.44 | tok/s 20079
step   1070 | loss 1.4178 | lr 3.00e-04 | grad 3.41 | tok/s 20239
step   1080 | loss 1.2601 | lr 3.00e-04 | grad 2.14 | tok/s 19969
step   1090 | loss 1.2422 | lr 3.00e-04 | grad 1.95 | tok/s 19878
step   1100 | loss 1.2015 | lr 3.00e-04 | grad 2.09 | tok/s 20455
step   1110 | loss 1.3098 | lr 3.00e-04 | grad 2.47 | tok/s 20177
step   1120 | loss 1.5753 | lr 3.00e-04 | grad 7.41 | tok/s 20179
step   1130 | loss 1.8917 | lr 3.00e-04 | grad 3.56 | tok/s 19962
step   1140 | loss 1.5174 | lr 3.00e-04 | grad 9.50 | tok/s 20098
step   1150 | loss 1.6131 | lr 3.00e-04 | grad 3.06 | tok/s 19636
step   1160 | loss 1.7728 | lr 3.00e-04 | grad 2.41 | tok/s 19174
step   1170 | loss 1.5035 | lr 3.00e-04 | grad 2.33 | tok/s 19512
step   1180 | loss 1.5000 | lr 3.00e-04 | grad 3.17 | tok/s 20116
step   1190 | loss 1.4073 | lr 3.00e-04 | grad 1.70 | tok/s 20429
step   1200 | loss 1.1729 | lr 3.00e-04 | grad 2.64 | tok/s 19968
step   1210 | loss 1.4104 | lr 3.00e-04 | grad 2.11 | tok/s 16063
step   1220 | loss 1.5257 | lr 3.00e-04 | grad 2.80 | tok/s 19634
step   1230 | loss 1.2357 | lr 3.00e-04 | grad 2.23 | tok/s 20329
step   1240 | loss 1.4232 | lr 3.00e-04 | grad 3.02 | tok/s 19782
step   1250 | loss 1.3768 | lr 3.00e-04 | grad 3.08 | tok/s 20256
step   1260 | loss 1.5195 | lr 3.00e-04 | grad 3.70 | tok/s 19715
step   1270 | loss 1.3297 | lr 3.00e-04 | grad 2.11 | tok/s 19951
step   1280 | loss 1.5036 | lr 3.00e-04 | grad 2.05 | tok/s 18788
step   1290 | loss 1.4378 | lr 3.00e-04 | grad 1.80 | tok/s 16459
step   1300 | loss 1.6567 | lr 3.00e-04 | grad 3.02 | tok/s 19430
step   1310 | loss 1.5054 | lr 3.00e-04 | grad 3.20 | tok/s 19926
step   1320 | loss 1.5907 | lr 3.00e-04 | grad 2.47 | tok/s 18061
step   1330 | loss 1.4396 | lr 3.00e-04 | grad 2.66 | tok/s 16916
step   1340 | loss 1.5523 | lr 3.00e-04 | grad 3.12 | tok/s 19373
step   1350 | loss 1.5516 | lr 3.00e-04 | grad 4.72 | tok/s 19289
step   1360 | loss 1.3022 | lr 3.00e-04 | grad 2.88 | tok/s 19588
step   1370 | loss 1.8013 | lr 3.00e-04 | grad 2.62 | tok/s 20045
step   1380 | loss 1.4122 | lr 3.00e-04 | grad 2.67 | tok/s 19333
step   1390 | loss 1.6299 | lr 3.00e-04 | grad 2.52 | tok/s 19292
step   1400 | loss 1.2957 | lr 3.00e-04 | grad 2.25 | tok/s 19604
step   1410 | loss 1.2881 | lr 3.00e-04 | grad 4.22 | tok/s 19742
step   1420 | loss 1.5113 | lr 3.00e-04 | grad 5.31 | tok/s 19640
step   1430 | loss 1.5301 | lr 3.00e-04 | grad 2.73 | tok/s 19575
step   1440 | loss 1.4543 | lr 3.00e-04 | grad 2.72 | tok/s 19980
step   1450 | loss 1.4845 | lr 3.00e-04 | grad 2.45 | tok/s 19371

Training complete! Final step: 1451
