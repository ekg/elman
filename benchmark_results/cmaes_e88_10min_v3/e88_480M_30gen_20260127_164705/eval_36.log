Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_36/levelE88_100m_20260127_172916
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,388,628 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3307 | lr 3.00e-04 | grad 16.12 | tok/s 9433
step     20 | loss 3.2702 | lr 3.00e-04 | grad 8.94 | tok/s 19295
step     30 | loss 3.1948 | lr 3.00e-04 | grad 11.56 | tok/s 20309
step     40 | loss 4.9334 | lr 3.00e-04 | grad 41.00 | tok/s 20644
step     50 | loss 4.7890 | lr 3.00e-04 | grad 22.62 | tok/s 20844
step     60 | loss 3.4310 | lr 3.00e-04 | grad 10.56 | tok/s 20702
step     70 | loss 2.9473 | lr 3.00e-04 | grad 9.31 | tok/s 20683
step     80 | loss 2.6657 | lr 3.00e-04 | grad 6.50 | tok/s 20582
step     90 | loss 2.4722 | lr 3.00e-04 | grad 5.88 | tok/s 20533
step    100 | loss 2.2342 | lr 3.00e-04 | grad 4.53 | tok/s 20502
step    110 | loss 2.2749 | lr 3.00e-04 | grad 4.12 | tok/s 20364
step    120 | loss 2.7620 | lr 3.00e-04 | grad 3.28 | tok/s 19419
step    130 | loss 2.1238 | lr 3.00e-04 | grad 7.16 | tok/s 19858
step    140 | loss 2.3913 | lr 3.00e-04 | grad 10.12 | tok/s 19875
step    150 | loss 1.4840 | lr 3.00e-04 | grad 7.69 | tok/s 20313
step    160 | loss 2.3115 | lr 3.00e-04 | grad 3.34 | tok/s 19691
step    170 | loss 2.3204 | lr 3.00e-04 | grad 2.97 | tok/s 19350
step    180 | loss 1.7956 | lr 3.00e-04 | grad 4.06 | tok/s 19825
step    190 | loss 1.9082 | lr 3.00e-04 | grad 3.95 | tok/s 19478
step    200 | loss 1.6277 | lr 3.00e-04 | grad 2.77 | tok/s 20380
step    210 | loss 1.8760 | lr 3.00e-04 | grad 9.50 | tok/s 19321
step    220 | loss 2.1921 | lr 3.00e-04 | grad 5.06 | tok/s 19517
step    230 | loss 1.9672 | lr 3.00e-04 | grad 3.44 | tok/s 19540
step    240 | loss 2.2512 | lr 3.00e-04 | grad 7.19 | tok/s 19764
step    250 | loss 1.7550 | lr 3.00e-04 | grad 2.23 | tok/s 19645
step    260 | loss 1.8925 | lr 3.00e-04 | grad 4.28 | tok/s 20196
step    270 | loss 1.8186 | lr 3.00e-04 | grad 3.31 | tok/s 19733
step    280 | loss 1.7703 | lr 3.00e-04 | grad 2.30 | tok/s 18520
step    290 | loss 1.6664 | lr 3.00e-04 | grad 2.94 | tok/s 19132
step    300 | loss 1.9895 | lr 3.00e-04 | grad 2.97 | tok/s 19304
step    310 | loss 1.6690 | lr 3.00e-04 | grad 2.55 | tok/s 19187
step    320 | loss 1.8850 | lr 3.00e-04 | grad 5.38 | tok/s 19387
step    330 | loss 1.7246 | lr 3.00e-04 | grad 2.69 | tok/s 19604
step    340 | loss 2.0520 | lr 3.00e-04 | grad 2.75 | tok/s 19511
step    350 | loss 1.6946 | lr 3.00e-04 | grad 2.70 | tok/s 20043
step    360 | loss 1.5803 | lr 3.00e-04 | grad 2.44 | tok/s 19235
step    370 | loss 1.4675 | lr 3.00e-04 | grad 2.41 | tok/s 20239
step    380 | loss 1.1906 | lr 3.00e-04 | grad 2.05 | tok/s 20404
step    390 | loss 1.0994 | lr 3.00e-04 | grad 2.02 | tok/s 20381
step    400 | loss 1.7523 | lr 3.00e-04 | grad 2.41 | tok/s 19308
step    410 | loss 1.7728 | lr 3.00e-04 | grad 3.17 | tok/s 19493
step    420 | loss 1.5973 | lr 3.00e-04 | grad 4.19 | tok/s 20331
step    430 | loss 1.5910 | lr 3.00e-04 | grad 3.00 | tok/s 19963
step    440 | loss 1.7114 | lr 3.00e-04 | grad 3.17 | tok/s 19387
step    450 | loss 1.6397 | lr 3.00e-04 | grad 2.14 | tok/s 19587
step    460 | loss 1.6126 | lr 3.00e-04 | grad 2.67 | tok/s 19911
step    470 | loss 1.5844 | lr 3.00e-04 | grad 4.56 | tok/s 19634
step    480 | loss 1.6252 | lr 3.00e-04 | grad 3.97 | tok/s 20096
step    490 | loss 1.7168 | lr 3.00e-04 | grad 3.22 | tok/s 19306
step    500 | loss 1.8186 | lr 3.00e-04 | grad 2.28 | tok/s 19593
step    510 | loss 1.6833 | lr 3.00e-04 | grad 2.06 | tok/s 18728
step    520 | loss 1.5331 | lr 3.00e-04 | grad 2.66 | tok/s 18950
step    530 | loss 1.7234 | lr 3.00e-04 | grad 2.47 | tok/s 17688
step    540 | loss 1.5932 | lr 3.00e-04 | grad 2.16 | tok/s 14990
step    550 | loss 1.4267 | lr 3.00e-04 | grad 2.42 | tok/s 19739
step    560 | loss 1.4232 | lr 3.00e-04 | grad 2.61 | tok/s 20352
step    570 | loss 1.3428 | lr 3.00e-04 | grad 2.17 | tok/s 20374
step    580 | loss 1.2927 | lr 3.00e-04 | grad 2.28 | tok/s 20383
step    590 | loss 1.3466 | lr 3.00e-04 | grad 2.69 | tok/s 20227
step    600 | loss 1.2723 | lr 3.00e-04 | grad 2.48 | tok/s 20432
step    610 | loss 1.3014 | lr 3.00e-04 | grad 2.00 | tok/s 19694
step    620 | loss 1.4096 | lr 3.00e-04 | grad 6.00 | tok/s 19918
step    630 | loss 1.6959 | lr 3.00e-04 | grad 3.97 | tok/s 19270
step    640 | loss 1.7022 | lr 3.00e-04 | grad 2.86 | tok/s 19479
step    650 | loss 1.5633 | lr 3.00e-04 | grad 3.59 | tok/s 19421
step    660 | loss 1.6704 | lr 3.00e-04 | grad 5.41 | tok/s 20166
step    670 | loss 1.5900 | lr 3.00e-04 | grad 4.25 | tok/s 19137
step    680 | loss 1.6358 | lr 3.00e-04 | grad 1.91 | tok/s 18883
step    690 | loss 1.6476 | lr 3.00e-04 | grad 3.50 | tok/s 19212
step    700 | loss 1.4625 | lr 3.00e-04 | grad 1.87 | tok/s 19298
step    710 | loss 1.6720 | lr 3.00e-04 | grad 3.28 | tok/s 19177
step    720 | loss 1.2871 | lr 3.00e-04 | grad 2.56 | tok/s 19892
step    730 | loss 1.5752 | lr 3.00e-04 | grad 4.88 | tok/s 19397
step    740 | loss 1.7685 | lr 3.00e-04 | grad 5.31 | tok/s 20126
step    750 | loss 1.4714 | lr 3.00e-04 | grad 2.41 | tok/s 20022
step    760 | loss 1.6229 | lr 3.00e-04 | grad 3.39 | tok/s 19662
step    770 | loss 1.5998 | lr 3.00e-04 | grad 2.36 | tok/s 19469
step    780 | loss 1.4838 | lr 3.00e-04 | grad 2.14 | tok/s 19578
step    790 | loss 1.6801 | lr 3.00e-04 | grad 3.45 | tok/s 19989
step    800 | loss 1.1995 | lr 3.00e-04 | grad 3.44 | tok/s 19741
step    810 | loss 1.4167 | lr 3.00e-04 | grad 2.55 | tok/s 18965
step    820 | loss 1.4923 | lr 3.00e-04 | grad 5.66 | tok/s 19224
step    830 | loss 1.4380 | lr 3.00e-04 | grad 2.34 | tok/s 19070
step    840 | loss 1.6810 | lr 3.00e-04 | grad 2.55 | tok/s 18851
step    850 | loss 1.5629 | lr 3.00e-04 | grad 2.61 | tok/s 18084
step    860 | loss 1.6168 | lr 3.00e-04 | grad 3.22 | tok/s 19355
step    870 | loss 1.4132 | lr 3.00e-04 | grad 2.91 | tok/s 19854
step    880 | loss 1.6005 | lr 3.00e-04 | grad 2.16 | tok/s 19491
step    890 | loss 1.5192 | lr 3.00e-04 | grad 2.92 | tok/s 17115
step    900 | loss 1.6058 | lr 3.00e-04 | grad 3.00 | tok/s 16003
step    910 | loss 1.5163 | lr 3.00e-04 | grad 3.95 | tok/s 19437
step    920 | loss 1.5032 | lr 3.00e-04 | grad 2.34 | tok/s 19385
step    930 | loss 1.4222 | lr 3.00e-04 | grad 2.69 | tok/s 19402
step    940 | loss 1.3849 | lr 3.00e-04 | grad 4.31 | tok/s 19012
step    950 | loss 1.4795 | lr 3.00e-04 | grad 2.56 | tok/s 19152
step    960 | loss 1.4561 | lr 3.00e-04 | grad 2.62 | tok/s 18469
step    970 | loss 1.5932 | lr 3.00e-04 | grad 6.25 | tok/s 16546
step    980 | loss 1.8155 | lr 3.00e-04 | grad 3.39 | tok/s 19806
step    990 | loss 1.5840 | lr 3.00e-04 | grad 2.05 | tok/s 19357
step   1000 | loss 1.6203 | lr 3.00e-04 | grad 2.02 | tok/s 19362
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6203.pt
step   1010 | loss 1.2203 | lr 3.00e-04 | grad 2.33 | tok/s 9014
step   1020 | loss 1.3204 | lr 3.00e-04 | grad 1.91 | tok/s 20254
step   1030 | loss 1.6616 | lr 3.00e-04 | grad 1.83 | tok/s 19232
step   1040 | loss 2.2167 | lr 3.00e-04 | grad 3.56 | tok/s 20328
step   1050 | loss 1.5390 | lr 3.00e-04 | grad 10.69 | tok/s 19789
step   1060 | loss 1.1213 | lr 3.00e-04 | grad 4.06 | tok/s 20211
step   1070 | loss 1.4573 | lr 3.00e-04 | grad 2.31 | tok/s 19997
step   1080 | loss 1.2675 | lr 3.00e-04 | grad 2.31 | tok/s 20505
step   1090 | loss 1.2374 | lr 3.00e-04 | grad 2.03 | tok/s 20457
step   1100 | loss 1.2033 | lr 3.00e-04 | grad 2.47 | tok/s 20476
step   1110 | loss 1.2816 | lr 3.00e-04 | grad 2.64 | tok/s 20241
step   1120 | loss 1.5373 | lr 3.00e-04 | grad 4.19 | tok/s 20027
step   1130 | loss 1.8626 | lr 3.00e-04 | grad 4.03 | tok/s 20027
step   1140 | loss 1.4556 | lr 3.00e-04 | grad 2.72 | tok/s 19893
step   1150 | loss 1.7257 | lr 3.00e-04 | grad 2.28 | tok/s 19629
step   1160 | loss 1.7813 | lr 3.00e-04 | grad 2.20 | tok/s 19338
step   1170 | loss 1.5021 | lr 3.00e-04 | grad 2.89 | tok/s 19424
step   1180 | loss 1.4608 | lr 3.00e-04 | grad 3.70 | tok/s 20034
step   1190 | loss 1.4649 | lr 3.00e-04 | grad 1.87 | tok/s 20373
step   1200 | loss 1.1471 | lr 3.00e-04 | grad 2.33 | tok/s 20041
step   1210 | loss 1.4954 | lr 3.00e-04 | grad 2.30 | tok/s 18070
step   1220 | loss 1.4127 | lr 3.00e-04 | grad 2.95 | tok/s 19736
step   1230 | loss 1.3153 | lr 3.00e-04 | grad 2.45 | tok/s 20205
step   1240 | loss 1.3335 | lr 3.00e-04 | grad 3.58 | tok/s 19841
step   1250 | loss 1.4805 | lr 3.00e-04 | grad 5.28 | tok/s 20103
step   1260 | loss 1.4562 | lr 3.00e-04 | grad 2.11 | tok/s 19925
step   1270 | loss 1.3641 | lr 3.00e-04 | grad 1.73 | tok/s 19823
step   1280 | loss 1.4835 | lr 3.00e-04 | grad 2.27 | tok/s 19432
step   1290 | loss 1.3822 | lr 3.00e-04 | grad 2.28 | tok/s 19084
step   1300 | loss 1.6761 | lr 3.00e-04 | grad 2.78 | tok/s 19534
step   1310 | loss 1.5210 | lr 3.00e-04 | grad 2.22 | tok/s 19888
step   1320 | loss 1.5900 | lr 3.00e-04 | grad 3.09 | tok/s 20054
step   1330 | loss 1.4089 | lr 3.00e-04 | grad 2.97 | tok/s 19276
step   1340 | loss 1.5594 | lr 3.00e-04 | grad 1.82 | tok/s 19790
step   1350 | loss 1.5685 | lr 3.00e-04 | grad 3.25 | tok/s 19576
step   1360 | loss 1.3191 | lr 3.00e-04 | grad 1.59 | tok/s 19325
step   1370 | loss 1.7728 | lr 3.00e-04 | grad 2.66 | tok/s 20051
step   1380 | loss 1.4487 | lr 3.00e-04 | grad 2.78 | tok/s 19173
step   1390 | loss 1.4990 | lr 3.00e-04 | grad 3.84 | tok/s 19753
step   1400 | loss 1.3783 | lr 3.00e-04 | grad 2.06 | tok/s 19544
step   1410 | loss 1.3809 | lr 3.00e-04 | grad 4.66 | tok/s 19475
step   1420 | loss 1.3793 | lr 3.00e-04 | grad 2.81 | tok/s 20103
step   1430 | loss 1.5369 | lr 3.00e-04 | grad 1.82 | tok/s 19507
step   1440 | loss 1.4624 | lr 3.00e-04 | grad 2.64 | tok/s 20145

Training complete! Final step: 1447
