Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v3/e88_480M_30gen_20260127_164705/eval_45/levelE88_100m_20260127_173939
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,351,656 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.0211 | lr 3.00e-04 | grad 20.88 | tok/s 5237
step     20 | loss 2.9938 | lr 3.00e-04 | grad 11.94 | tok/s 16157
step     30 | loss 2.7225 | lr 3.00e-04 | grad 7.34 | tok/s 10725
step     40 | loss 2.5417 | lr 3.00e-04 | grad 28.88 | tok/s 15703
step     50 | loss 3.1633 | lr 3.00e-04 | grad 15.75 | tok/s 16072
step     60 | loss 2.0674 | lr 3.00e-04 | grad 6.88 | tok/s 16532
step     70 | loss 1.8586 | lr 3.00e-04 | grad 3.52 | tok/s 16669
step     80 | loss 7.1945 | lr 3.00e-04 | grad 135.00 | tok/s 17300
step     90 | loss 5.7652 | lr 3.00e-04 | grad 29.25 | tok/s 17236
step    100 | loss 4.4830 | lr 3.00e-04 | grad 39.75 | tok/s 16124
step    110 | loss 3.7929 | lr 3.00e-04 | grad 27.62 | tok/s 12736
step    120 | loss 3.2054 | lr 3.00e-04 | grad 20.25 | tok/s 16978
step    130 | loss 3.1071 | lr 3.00e-04 | grad 7.69 | tok/s 17045
step    140 | loss 2.9024 | lr 3.00e-04 | grad 23.38 | tok/s 16475
step    150 | loss 2.8512 | lr 3.00e-04 | grad 9.06 | tok/s 13179
step    160 | loss 2.3921 | lr 3.00e-04 | grad 9.31 | tok/s 16839
step    170 | loss 2.5559 | lr 3.00e-04 | grad 10.88 | tok/s 17018
step    180 | loss 2.2785 | lr 3.00e-04 | grad 6.34 | tok/s 15222
step    190 | loss 2.4268 | lr 3.00e-04 | grad 7.25 | tok/s 16480
step    200 | loss 2.1918 | lr 3.00e-04 | grad 12.88 | tok/s 16890
step    210 | loss 2.1834 | lr 3.00e-04 | grad 5.97 | tok/s 17013
step    220 | loss 2.1612 | lr 3.00e-04 | grad 4.38 | tok/s 13895
step    230 | loss 2.1881 | lr 3.00e-04 | grad 3.28 | tok/s 7719
step    240 | loss 2.4031 | lr 3.00e-04 | grad 4.16 | tok/s 14750
step    250 | loss 1.9926 | lr 3.00e-04 | grad 3.14 | tok/s 15515
step    260 | loss 1.5812 | lr 3.00e-04 | grad 3.11 | tok/s 15876
step    270 | loss 2.1004 | lr 3.00e-04 | grad 3.44 | tok/s 16149
step    280 | loss 2.4016 | lr 3.00e-04 | grad 6.50 | tok/s 16595
step    290 | loss 1.0342 | lr 3.00e-04 | grad 4.84 | tok/s 17119
step    300 | loss 0.9037 | lr 3.00e-04 | grad 4.09 | tok/s 15191
step    310 | loss 2.2919 | lr 3.00e-04 | grad 4.53 | tok/s 14475
step    320 | loss 1.9433 | lr 3.00e-04 | grad 4.22 | tok/s 15246
step    330 | loss 1.9725 | lr 3.00e-04 | grad 2.81 | tok/s 16269
step    340 | loss 2.2876 | lr 3.00e-04 | grad 2.70 | tok/s 15982
step    350 | loss 1.7418 | lr 3.00e-04 | grad 5.34 | tok/s 15567
step    360 | loss 1.3440 | lr 3.00e-04 | grad 3.50 | tok/s 16136
step    370 | loss 1.7951 | lr 3.00e-04 | grad 2.77 | tok/s 15352
step    380 | loss 1.7685 | lr 3.00e-04 | grad 2.91 | tok/s 16775
step    390 | loss 1.5141 | lr 3.00e-04 | grad 2.78 | tok/s 17035
step    400 | loss 1.4877 | lr 3.00e-04 | grad 2.48 | tok/s 16888
step    410 | loss 1.3441 | lr 3.00e-04 | grad 2.72 | tok/s 16321
step    420 | loss 2.0711 | lr 3.00e-04 | grad 5.53 | tok/s 10824
step    430 | loss 2.2396 | lr 3.00e-04 | grad 5.47 | tok/s 16522
step    440 | loss 1.8674 | lr 3.00e-04 | grad 3.17 | tok/s 14338
step    450 | loss 2.0753 | lr 3.00e-04 | grad 2.62 | tok/s 11337
step    460 | loss 1.7805 | lr 3.00e-04 | grad 4.97 | tok/s 16187
step    470 | loss 1.8497 | lr 3.00e-04 | grad 3.48 | tok/s 7933
step    480 | loss 2.2559 | lr 3.00e-04 | grad 2.83 | tok/s 14062
step    490 | loss 1.5954 | lr 3.00e-04 | grad 3.05 | tok/s 16099
step    500 | loss 1.7593 | lr 3.00e-04 | grad 3.27 | tok/s 12436
step    510 | loss 1.7868 | lr 3.00e-04 | grad 3.62 | tok/s 15136
step    520 | loss 1.7808 | lr 3.00e-04 | grad 5.22 | tok/s 11167
step    530 | loss 1.8838 | lr 3.00e-04 | grad 5.78 | tok/s 8484
step    540 | loss 1.5588 | lr 3.00e-04 | grad 3.00 | tok/s 10212
step    550 | loss 1.6510 | lr 3.00e-04 | grad 3.64 | tok/s 12100
step    560 | loss 1.7519 | lr 3.00e-04 | grad 2.50 | tok/s 12815
step    570 | loss 1.6286 | lr 3.00e-04 | grad 3.97 | tok/s 15802
step    580 | loss 1.5798 | lr 3.00e-04 | grad 2.98 | tok/s 16015
step    590 | loss 2.0282 | lr 3.00e-04 | grad 3.03 | tok/s 16103
step    600 | loss 1.6921 | lr 3.00e-04 | grad 3.44 | tok/s 15822
step    610 | loss 1.6115 | lr 3.00e-04 | grad 2.25 | tok/s 13390
step    620 | loss 1.6598 | lr 3.00e-04 | grad 5.50 | tok/s 7255
step    630 | loss 1.7570 | lr 3.00e-04 | grad 7.00 | tok/s 10554
step    640 | loss 1.7674 | lr 3.00e-04 | grad 3.27 | tok/s 9944
step    650 | loss 1.6980 | lr 3.00e-04 | grad 2.52 | tok/s 13466
step    660 | loss 1.6113 | lr 3.00e-04 | grad 2.66 | tok/s 15691
step    670 | loss 2.0426 | lr 3.00e-04 | grad 2.33 | tok/s 11498
step    680 | loss 1.8261 | lr 3.00e-04 | grad 3.06 | tok/s 12189
step    690 | loss 1.5628 | lr 3.00e-04 | grad 3.09 | tok/s 8394
step    700 | loss 1.5705 | lr 3.00e-04 | grad 2.86 | tok/s 11420
step    710 | loss 1.4878 | lr 3.00e-04 | grad 5.12 | tok/s 7321
step    720 | loss 1.3869 | lr 3.00e-04 | grad 2.14 | tok/s 16998
step    730 | loss 1.4897 | lr 3.00e-04 | grad 2.48 | tok/s 14233
step    740 | loss 1.2483 | lr 3.00e-04 | grad 2.78 | tok/s 5988
step    750 | loss 1.1426 | lr 3.00e-04 | grad 2.41 | tok/s 7839
step    760 | loss 1.0775 | lr 3.00e-04 | grad 2.34 | tok/s 14646
step    770 | loss 1.0204 | lr 3.00e-04 | grad 2.11 | tok/s 15119
step    780 | loss 1.0098 | lr 3.00e-04 | grad 2.62 | tok/s 13429
step    790 | loss 1.7334 | lr 3.00e-04 | grad 3.17 | tok/s 13069
step    800 | loss 1.8105 | lr 3.00e-04 | grad 2.62 | tok/s 7588
step    810 | loss 1.7218 | lr 3.00e-04 | grad 3.02 | tok/s 5774
step    820 | loss 1.5965 | lr 3.00e-04 | grad 3.17 | tok/s 4443
step    830 | loss 1.4835 | lr 3.00e-04 | grad 4.25 | tok/s 7805
step    840 | loss 1.6355 | lr 3.00e-04 | grad 4.69 | tok/s 7664
step    850 | loss 1.5342 | lr 3.00e-04 | grad 2.78 | tok/s 7657
step    860 | loss 1.5094 | lr 3.00e-04 | grad 2.75 | tok/s 11454
step    870 | loss 1.7605 | lr 3.00e-04 | grad 3.12 | tok/s 16017
step    880 | loss 1.7074 | lr 3.00e-04 | grad 3.67 | tok/s 8336
step    890 | loss 1.4494 | lr 3.00e-04 | grad 2.67 | tok/s 7904
step    900 | loss 1.5113 | lr 3.00e-04 | grad 3.22 | tok/s 17100
step    910 | loss 1.5625 | lr 3.00e-04 | grad 2.45 | tok/s 16522
step    920 | loss 1.5881 | lr 3.00e-04 | grad 2.94 | tok/s 16686
step    930 | loss 1.4541 | lr 3.00e-04 | grad 2.61 | tok/s 8372
step    940 | loss 1.2668 | lr 3.00e-04 | grad 2.67 | tok/s 5939
step    950 | loss 1.6070 | lr 3.00e-04 | grad 2.61 | tok/s 16453
step    960 | loss 1.7951 | lr 3.00e-04 | grad 2.39 | tok/s 16152
step    970 | loss 1.4796 | lr 3.00e-04 | grad 2.05 | tok/s 16288
step    980 | loss 1.6586 | lr 3.00e-04 | grad 2.42 | tok/s 15700
step    990 | loss 1.8155 | lr 3.00e-04 | grad 6.38 | tok/s 11967
step   1000 | loss 1.6423 | lr 3.00e-04 | grad 3.27 | tok/s 15879
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6423.pt
step   1010 | loss 1.4708 | lr 3.00e-04 | grad 2.41 | tok/s 3753
step   1020 | loss 1.5238 | lr 3.00e-04 | grad 2.39 | tok/s 15848
step   1030 | loss 1.6566 | lr 3.00e-04 | grad 2.86 | tok/s 15844
step   1040 | loss 1.7074 | lr 3.00e-04 | grad 2.38 | tok/s 16466
step   1050 | loss 1.6884 | lr 3.00e-04 | grad 2.61 | tok/s 16276
step   1060 | loss 1.4205 | lr 3.00e-04 | grad 2.73 | tok/s 15091
step   1070 | loss 1.0370 | lr 3.00e-04 | grad 5.97 | tok/s 16540
step   1080 | loss 1.5769 | lr 3.00e-04 | grad 2.62 | tok/s 13155
step   1090 | loss 1.4169 | lr 3.00e-04 | grad 2.02 | tok/s 17313
step   1100 | loss 1.3443 | lr 3.00e-04 | grad 2.52 | tok/s 17303
step   1110 | loss 1.2863 | lr 3.00e-04 | grad 2.31 | tok/s 17222
step   1120 | loss 1.2947 | lr 3.00e-04 | grad 2.14 | tok/s 16716
step   1130 | loss 1.2886 | lr 3.00e-04 | grad 1.97 | tok/s 16440
step   1140 | loss 1.2153 | lr 3.00e-04 | grad 2.00 | tok/s 14478
step   1150 | loss 1.2919 | lr 3.00e-04 | grad 2.19 | tok/s 14506
step   1160 | loss 1.3044 | lr 3.00e-04 | grad 2.61 | tok/s 16846
step   1170 | loss 1.2200 | lr 3.00e-04 | grad 1.93 | tok/s 12011
step   1180 | loss 1.2362 | lr 3.00e-04 | grad 1.94 | tok/s 17286
step   1190 | loss 1.2665 | lr 3.00e-04 | grad 2.05 | tok/s 17206
step   1200 | loss 1.2831 | lr 3.00e-04 | grad 2.12 | tok/s 16876
step   1210 | loss 1.2631 | lr 3.00e-04 | grad 1.88 | tok/s 13830
step   1220 | loss 1.3860 | lr 3.00e-04 | grad 7.41 | tok/s 15289
step   1230 | loss 1.7357 | lr 3.00e-04 | grad 2.41 | tok/s 14295
step   1240 | loss 1.3497 | lr 3.00e-04 | grad 2.53 | tok/s 15784
step   1250 | loss 1.7596 | lr 3.00e-04 | grad 2.59 | tok/s 15836
step   1260 | loss 1.5965 | lr 3.00e-04 | grad 2.20 | tok/s 16415
step   1270 | loss 1.5407 | lr 3.00e-04 | grad 2.73 | tok/s 16296
step   1280 | loss 1.5194 | lr 3.00e-04 | grad 3.55 | tok/s 16257
step   1290 | loss 1.4611 | lr 3.00e-04 | grad 3.70 | tok/s 17007
step   1300 | loss 1.6531 | lr 3.00e-04 | grad 3.84 | tok/s 16912
step   1310 | loss 1.4371 | lr 3.00e-04 | grad 2.38 | tok/s 16907
step   1320 | loss 1.6616 | lr 3.00e-04 | grad 2.30 | tok/s 15721
step   1330 | loss 1.7832 | lr 3.00e-04 | grad 3.38 | tok/s 15692
step   1340 | loss 1.4558 | lr 3.00e-04 | grad 2.00 | tok/s 16616
step   1350 | loss 1.5831 | lr 3.00e-04 | grad 5.06 | tok/s 15778
step   1360 | loss 1.5855 | lr 3.00e-04 | grad 2.98 | tok/s 15287
step   1370 | loss 1.4492 | lr 3.00e-04 | grad 2.12 | tok/s 16059
step   1380 | loss 1.3973 | lr 3.00e-04 | grad 2.14 | tok/s 16330
step   1390 | loss 1.5964 | lr 3.00e-04 | grad 4.09 | tok/s 15281
step   1400 | loss 1.6269 | lr 3.00e-04 | grad 2.66 | tok/s 14223
step   1410 | loss 1.3387 | lr 3.00e-04 | grad 2.48 | tok/s 15694
step   1420 | loss 1.1450 | lr 3.00e-04 | grad 2.22 | tok/s 13239
step   1430 | loss 1.2994 | lr 3.00e-04 | grad 7.06 | tok/s 14227
step   1440 | loss 1.7006 | lr 3.00e-04 | grad 5.97 | tok/s 15585
step   1450 | loss 1.4385 | lr 3.00e-04 | grad 1.96 | tok/s 16782
step   1460 | loss 1.8862 | lr 3.00e-04 | grad 3.91 | tok/s 16838
step   1470 | loss 1.5724 | lr 3.00e-04 | grad 5.25 | tok/s 17094
step   1480 | loss 1.2834 | lr 3.00e-04 | grad 2.11 | tok/s 17083
step   1490 | loss 1.5771 | lr 3.00e-04 | grad 2.27 | tok/s 16775
step   1500 | loss 1.4746 | lr 3.00e-04 | grad 2.67 | tok/s 12653
step   1510 | loss 1.4271 | lr 3.00e-04 | grad 2.17 | tok/s 12233
step   1520 | loss 1.6705 | lr 3.00e-04 | grad 2.42 | tok/s 15954
step   1530 | loss 1.2566 | lr 3.00e-04 | grad 2.38 | tok/s 16691
step   1540 | loss 1.6149 | lr 3.00e-04 | grad 2.38 | tok/s 15872
step   1550 | loss 1.3414 | lr 3.00e-04 | grad 3.33 | tok/s 16717
step   1560 | loss 1.7086 | lr 3.00e-04 | grad 3.66 | tok/s 16852
step   1570 | loss 1.5727 | lr 3.00e-04 | grad 2.05 | tok/s 15861
step   1580 | loss 0.8596 | lr 3.00e-04 | grad 1.40 | tok/s 16613
step   1590 | loss 1.2015 | lr 3.00e-04 | grad 2.02 | tok/s 15367
step   1600 | loss 1.3710 | lr 3.00e-04 | grad 2.83 | tok/s 15872
step   1610 | loss 1.3812 | lr 3.00e-04 | grad 2.25 | tok/s 16342
step   1620 | loss 1.4676 | lr 3.00e-04 | grad 4.62 | tok/s 15862
step   1630 | loss 1.5867 | lr 3.00e-04 | grad 4.38 | tok/s 14985
step   1640 | loss 1.2429 | lr 3.00e-04 | grad 1.84 | tok/s 16980
step   1650 | loss 1.6245 | lr 3.00e-04 | grad 7.84 | tok/s 16209
step   1660 | loss 1.5677 | lr 3.00e-04 | grad 2.69 | tok/s 15908
step   1670 | loss 1.4601 | lr 3.00e-04 | grad 3.50 | tok/s 16512
step   1680 | loss 1.5503 | lr 3.00e-04 | grad 2.33 | tok/s 14329
step   1690 | loss 1.4619 | lr 3.00e-04 | grad 2.45 | tok/s 16548
step   1700 | loss 1.5298 | lr 3.00e-04 | grad 3.06 | tok/s 17143
step   1710 | loss 1.2120 | lr 3.00e-04 | grad 3.61 | tok/s 17142
step   1720 | loss 1.4196 | lr 3.00e-04 | grad 2.70 | tok/s 16399
step   1730 | loss 1.5296 | lr 3.00e-04 | grad 2.42 | tok/s 16683
step   1740 | loss 1.6154 | lr 3.00e-04 | grad 2.69 | tok/s 16459
step   1750 | loss 1.4474 | lr 3.00e-04 | grad 2.02 | tok/s 16127
step   1760 | loss 1.5082 | lr 3.00e-04 | grad 2.83 | tok/s 16543
step   1770 | loss 1.4334 | lr 3.00e-04 | grad 2.09 | tok/s 16426
step   1780 | loss 1.5822 | lr 3.00e-04 | grad 2.38 | tok/s 16037
step   1790 | loss 1.4999 | lr 3.00e-04 | grad 3.11 | tok/s 16244
step   1800 | loss 1.5015 | lr 3.00e-04 | grad 4.91 | tok/s 16369
step   1810 | loss 1.4629 | lr 3.00e-04 | grad 2.48 | tok/s 16700
step   1820 | loss 1.4545 | lr 3.00e-04 | grad 1.73 | tok/s 15882
step   1830 | loss 1.2982 | lr 3.00e-04 | grad 1.95 | tok/s 16994
step   1840 | loss 1.3757 | lr 3.00e-04 | grad 2.42 | tok/s 15919
step   1850 | loss 1.3508 | lr 3.00e-04 | grad 1.59 | tok/s 16775
step   1860 | loss 1.3262 | lr 3.00e-04 | grad 2.56 | tok/s 15190
step   1870 | loss 1.5723 | lr 3.00e-04 | grad 2.05 | tok/s 14971
step   1880 | loss 1.3816 | lr 3.00e-04 | grad 2.03 | tok/s 16229
step   1890 | loss 1.5108 | lr 3.00e-04 | grad 1.98 | tok/s 15848
step   1900 | loss 1.3731 | lr 3.00e-04 | grad 2.06 | tok/s 16960
step   1910 | loss 1.4979 | lr 3.00e-04 | grad 2.19 | tok/s 15909
step   1920 | loss 1.4856 | lr 3.00e-04 | grad 3.20 | tok/s 16847
step   1930 | loss 1.8529 | lr 3.00e-04 | grad 3.64 | tok/s 17131
step   1940 | loss 1.4704 | lr 3.00e-04 | grad 3.92 | tok/s 17122
step   1950 | loss 1.5655 | lr 3.00e-04 | grad 5.09 | tok/s 16583
step   1960 | loss 1.5011 | lr 3.00e-04 | grad 1.88 | tok/s 16134
step   1970 | loss 1.6503 | lr 3.00e-04 | grad 2.11 | tok/s 16238
step   1980 | loss 1.5192 | lr 3.00e-04 | grad 2.33 | tok/s 16498
step   1990 | loss 1.0247 | lr 3.00e-04 | grad 1.84 | tok/s 17060
step   2000 | loss 1.3078 | lr 3.00e-04 | grad 2.03 | tok/s 16486
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3078.pt
step   2010 | loss 1.0422 | lr 3.00e-04 | grad 4.31 | tok/s 6178
step   2020 | loss 1.2567 | lr 3.00e-04 | grad 1.76 | tok/s 17255
step   2030 | loss 1.3193 | lr 3.00e-04 | grad 2.89 | tok/s 16534

Training complete! Final step: 2030
