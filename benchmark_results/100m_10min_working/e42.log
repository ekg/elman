Using device: cuda
Output directory: benchmark_results/100m_10min_working/e42/level42_100m_20260114_185052
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 94,615,296 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4013 | lr 2.70e-06 | grad 68.00 | tok/s 13501
step     20 | loss 4.6681 | lr 5.70e-06 | grad 33.50 | tok/s 31606
step     30 | loss 4.9279 | lr 8.70e-06 | grad 22.38 | tok/s 36396
step     40 | loss 4.1187 | lr 1.17e-05 | grad 15.44 | tok/s 36149
step     50 | loss 3.2255 | lr 1.47e-05 | grad 10.12 | tok/s 35532
step     60 | loss 3.1503 | lr 1.77e-05 | grad 19.00 | tok/s 34515
step     70 | loss 2.7936 | lr 2.07e-05 | grad 18.00 | tok/s 27158
step     80 | loss 3.0709 | lr 2.37e-05 | grad 15.44 | tok/s 25951
step     90 | loss 3.0379 | lr 2.67e-05 | grad 22.75 | tok/s 28160
step    100 | loss 2.6074 | lr 2.97e-05 | grad 5.94 | tok/s 29811
step    110 | loss 2.4778 | lr 3.27e-05 | grad 8.62 | tok/s 32712
step    120 | loss 2.6375 | lr 3.57e-05 | grad 6.69 | tok/s 31529
step    130 | loss 2.4494 | lr 3.87e-05 | grad 5.88 | tok/s 32185
step    140 | loss 2.2527 | lr 4.17e-05 | grad 5.59 | tok/s 33011
step    150 | loss 2.1127 | lr 4.47e-05 | grad 8.88 | tok/s 30795
step    160 | loss 2.0154 | lr 4.77e-05 | grad 6.81 | tok/s 33194
step    170 | loss 2.2056 | lr 5.07e-05 | grad 12.81 | tok/s 29676
step    180 | loss 2.2025 | lr 5.37e-05 | grad 5.81 | tok/s 31363
step    190 | loss 1.9136 | lr 5.67e-05 | grad 5.91 | tok/s 33470
step    200 | loss 1.5109 | lr 5.97e-05 | grad 5.09 | tok/s 35396
step    210 | loss 2.2082 | lr 6.27e-05 | grad 6.38 | tok/s 32331
step    220 | loss 2.0461 | lr 6.57e-05 | grad 4.28 | tok/s 30370
step    230 | loss 1.9202 | lr 6.87e-05 | grad 6.59 | tok/s 30229
step    240 | loss 1.9126 | lr 7.17e-05 | grad 5.88 | tok/s 29216
step    250 | loss 1.9188 | lr 7.47e-05 | grad 4.56 | tok/s 24210
step    260 | loss 2.0034 | lr 7.77e-05 | grad 3.03 | tok/s 28168
step    270 | loss 1.8756 | lr 8.07e-05 | grad 3.61 | tok/s 35249
step    280 | loss 1.7576 | lr 8.37e-05 | grad 4.47 | tok/s 30020
step    290 | loss 1.5800 | lr 8.67e-05 | grad 2.84 | tok/s 33832
step    300 | loss 1.4876 | lr 8.97e-05 | grad 2.52 | tok/s 32027
step    310 | loss 1.4490 | lr 9.27e-05 | grad 1.84 | tok/s 33823
step    320 | loss 1.7795 | lr 9.57e-05 | grad 4.97 | tok/s 35948
step    330 | loss 1.9029 | lr 9.87e-05 | grad 2.91 | tok/s 35227
step    340 | loss 1.8695 | lr 1.02e-04 | grad 5.88 | tok/s 36158
step    350 | loss 1.8753 | lr 1.05e-04 | grad 2.97 | tok/s 35148
step    360 | loss 1.8338 | lr 1.08e-04 | grad 5.69 | tok/s 35761
step    370 | loss 1.6259 | lr 1.11e-04 | grad 2.62 | tok/s 34767
step    380 | loss 2.1379 | lr 1.14e-04 | grad 2.53 | tok/s 31337
step    390 | loss 1.7791 | lr 1.17e-04 | grad 2.44 | tok/s 29867
step    400 | loss 1.8829 | lr 1.20e-04 | grad 6.00 | tok/s 30170
step    410 | loss 1.6458 | lr 1.23e-04 | grad 4.66 | tok/s 27311
step    420 | loss 1.7842 | lr 1.26e-04 | grad 2.47 | tok/s 24794
step    430 | loss 1.9257 | lr 1.29e-04 | grad 2.86 | tok/s 29233
step    440 | loss 1.9330 | lr 1.32e-04 | grad 2.53 | tok/s 30582
step    450 | loss 1.7731 | lr 1.35e-04 | grad 1.64 | tok/s 32048
step    460 | loss 1.7348 | lr 1.38e-04 | grad 1.87 | tok/s 33012
step    470 | loss 1.7291 | lr 1.41e-04 | grad 2.20 | tok/s 35205
step    480 | loss 1.6452 | lr 1.44e-04 | grad 1.26 | tok/s 34453
step    490 | loss 1.6002 | lr 1.47e-04 | grad 1.34 | tok/s 35059
step    500 | loss 2.2922 | lr 1.50e-04 | grad 2.38 | tok/s 31693
step    510 | loss 1.6663 | lr 1.53e-04 | grad 1.37 | tok/s 29065
step    520 | loss 1.6029 | lr 1.56e-04 | grad 1.53 | tok/s 30444
step    530 | loss 2.2009 | lr 1.59e-04 | grad 3.89 | tok/s 30160
step    540 | loss 1.6819 | lr 1.62e-04 | grad 2.09 | tok/s 30639
step    550 | loss 1.5198 | lr 1.65e-04 | grad 1.05 | tok/s 31910
step    560 | loss 1.3367 | lr 1.68e-04 | grad 0.97 | tok/s 34601
step    570 | loss 1.7321 | lr 1.71e-04 | grad 3.23 | tok/s 33511
step    580 | loss 2.0446 | lr 1.74e-04 | grad 1.46 | tok/s 34705
step    590 | loss 2.2916 | lr 1.77e-04 | grad 2.20 | tok/s 34501
step    600 | loss 1.7424 | lr 1.80e-04 | grad 2.06 | tok/s 34239
step    610 | loss 1.7399 | lr 1.83e-04 | grad 1.64 | tok/s 36691
step    620 | loss 1.7110 | lr 1.86e-04 | grad 1.21 | tok/s 35807
step    630 | loss 1.5821 | lr 1.89e-04 | grad 1.13 | tok/s 37024
step    640 | loss 1.9215 | lr 1.92e-04 | grad 1.24 | tok/s 37203
step    650 | loss 1.6686 | lr 1.95e-04 | grad 1.77 | tok/s 35601
step    660 | loss 1.9384 | lr 1.98e-04 | grad 4.59 | tok/s 33946
step    670 | loss 1.8469 | lr 2.01e-04 | grad 2.11 | tok/s 34500
step    680 | loss 1.8280 | lr 2.04e-04 | grad 1.41 | tok/s 31166
step    690 | loss 1.8041 | lr 2.07e-04 | grad 1.77 | tok/s 34468
step    700 | loss 1.8929 | lr 2.10e-04 | grad 2.30 | tok/s 34582
step    710 | loss 1.7974 | lr 2.13e-04 | grad 1.18 | tok/s 34220
step    720 | loss 1.7307 | lr 2.16e-04 | grad 4.59 | tok/s 34037
step    730 | loss 1.9602 | lr 2.19e-04 | grad 1.64 | tok/s 35787
step    740 | loss 1.8783 | lr 2.22e-04 | grad 2.02 | tok/s 34156
step    750 | loss 1.6346 | lr 2.25e-04 | grad 1.59 | tok/s 35046
step    760 | loss 2.0222 | lr 2.28e-04 | grad 0.98 | tok/s 36299
step    770 | loss 1.6691 | lr 2.31e-04 | grad 1.33 | tok/s 34995
step    780 | loss 1.7224 | lr 2.34e-04 | grad 1.02 | tok/s 36398
step    790 | loss 1.6157 | lr 2.37e-04 | grad 0.79 | tok/s 36993
step    800 | loss 1.5797 | lr 2.40e-04 | grad 1.17 | tok/s 36359
step    810 | loss 1.6984 | lr 2.43e-04 | grad 1.77 | tok/s 33859
step    820 | loss 2.3876 | lr 2.46e-04 | grad 1.26 | tok/s 35565
step    830 | loss 1.8007 | lr 2.49e-04 | grad 0.82 | tok/s 36491
step    840 | loss 1.4962 | lr 2.52e-04 | grad 0.70 | tok/s 36265
step    850 | loss 1.8733 | lr 2.55e-04 | grad 1.41 | tok/s 33746
step    860 | loss 1.6898 | lr 2.58e-04 | grad 1.02 | tok/s 29974
step    870 | loss 1.6301 | lr 2.61e-04 | grad 1.05 | tok/s 35580
step    880 | loss 1.7199 | lr 2.64e-04 | grad 1.15 | tok/s 31421
step    890 | loss 1.6064 | lr 2.67e-04 | grad 0.93 | tok/s 34265
step    900 | loss 2.0873 | lr 2.70e-04 | grad 0.93 | tok/s 32282
step    910 | loss 1.6658 | lr 2.73e-04 | grad 0.93 | tok/s 32371
step    920 | loss 1.6501 | lr 2.76e-04 | grad 0.80 | tok/s 32422
step    930 | loss 1.7166 | lr 2.79e-04 | grad 1.26 | tok/s 32282
step    940 | loss 1.6469 | lr 2.82e-04 | grad 1.62 | tok/s 32629
step    950 | loss 1.7648 | lr 2.85e-04 | grad 1.18 | tok/s 32339
step    960 | loss 1.4189 | lr 2.88e-04 | grad 0.52 | tok/s 33965
step    970 | loss 1.2741 | lr 2.91e-04 | grad 0.45 | tok/s 34171
step    980 | loss 1.4735 | lr 2.94e-04 | grad 1.91 | tok/s 32280
step    990 | loss 1.8386 | lr 2.97e-04 | grad 0.80 | tok/s 31687
step   1000 | loss 1.6915 | lr 3.00e-04 | grad 0.63 | tok/s 30154
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6915.pt
step   1010 | loss 1.9771 | lr 1.06e-06 | grad 1.39 | tok/s 25031
step   1020 | loss 1.6516 | lr 1.27e-06 | grad 0.68 | tok/s 32409
step   1030 | loss 1.9517 | lr 1.62e-06 | grad 0.79 | tok/s 31780
step   1040 | loss 1.6229 | lr 2.12e-06 | grad 1.27 | tok/s 32622
step   1050 | loss 1.6550 | lr 2.77e-06 | grad 0.73 | tok/s 32598
step   1060 | loss 1.9579 | lr 3.56e-06 | grad 2.06 | tok/s 33027
step   1070 | loss 2.1136 | lr 4.50e-06 | grad 0.88 | tok/s 32166
step   1080 | loss 2.3799 | lr 5.58e-06 | grad 1.07 | tok/s 33607
step   1090 | loss 2.0727 | lr 6.81e-06 | grad 1.00 | tok/s 34544
step   1100 | loss 1.7008 | lr 8.17e-06 | grad 0.77 | tok/s 34913
step   1110 | loss 1.7515 | lr 9.68e-06 | grad 0.90 | tok/s 35460
step   1120 | loss 2.0594 | lr 1.13e-05 | grad 0.77 | tok/s 35673
step   1130 | loss 1.7125 | lr 1.31e-05 | grad 0.73 | tok/s 34185
step   1140 | loss 1.5900 | lr 1.50e-05 | grad 0.71 | tok/s 35826
step   1150 | loss 1.8622 | lr 1.71e-05 | grad 1.13 | tok/s 35838
step   1160 | loss 1.5215 | lr 1.93e-05 | grad 0.51 | tok/s 35713
step   1170 | loss 1.8866 | lr 2.16e-05 | grad 0.70 | tok/s 36417
step   1180 | loss 1.6599 | lr 2.40e-05 | grad 0.70 | tok/s 37763
step   1190 | loss 1.5848 | lr 2.66e-05 | grad 0.51 | tok/s 37293
step   1200 | loss 1.4986 | lr 2.93e-05 | grad 0.48 | tok/s 37440
step   1210 | loss 1.4544 | lr 3.21e-05 | grad 0.51 | tok/s 37508
step   1220 | loss 1.4465 | lr 3.50e-05 | grad 0.75 | tok/s 37292
step   1230 | loss 1.4796 | lr 3.80e-05 | grad 0.66 | tok/s 35401
step   1240 | loss 1.5905 | lr 4.12e-05 | grad 0.66 | tok/s 34880

Training complete! Final step: 1243
