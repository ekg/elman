Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_12/levelE88_100m_20260126_043951
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 496,855,196 parameters
Using schedule-free AdamW (lr=0.0005786230110934947)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1932 | lr 5.79e-04 | grad 7.16 | tok/s 5075
step     20 | loss 2.6817 | lr 5.79e-04 | grad 2.41 | tok/s 10301
step     30 | loss 2.5627 | lr 5.79e-04 | grad 2.56 | tok/s 10384
step     40 | loss 2.3791 | lr 5.79e-04 | grad 2.23 | tok/s 9950
step     50 | loss 2.8884 | lr 5.79e-04 | grad 5.31 | tok/s 10164
step     60 | loss 2.0414 | lr 5.79e-04 | grad 4.91 | tok/s 10438
step     70 | loss 1.8357 | lr 5.79e-04 | grad 2.53 | tok/s 10550
step     80 | loss 5.2727 | lr 5.79e-04 | grad 24.88 | tok/s 10603
step     90 | loss 4.2984 | lr 5.79e-04 | grad 3.62 | tok/s 10783
step    100 | loss 3.5961 | lr 5.79e-04 | grad 2.59 | tok/s 10775
step    110 | loss 3.1068 | lr 5.79e-04 | grad 7.59 | tok/s 10747
step    120 | loss 2.6487 | lr 5.79e-04 | grad 2.72 | tok/s 10725
step    130 | loss 2.4563 | lr 5.79e-04 | grad 3.52 | tok/s 10311
step    140 | loss 2.3082 | lr 5.79e-04 | grad 2.89 | tok/s 10755
step    150 | loss 2.1740 | lr 5.79e-04 | grad 3.05 | tok/s 10757
step    160 | loss 1.9982 | lr 5.79e-04 | grad 4.50 | tok/s 10734
step    170 | loss 2.0192 | lr 5.79e-04 | grad 4.34 | tok/s 10733
step    180 | loss 1.8703 | lr 5.79e-04 | grad 3.28 | tok/s 10730
step    190 | loss 2.0359 | lr 5.79e-04 | grad 2.61 | tok/s 10768
step    200 | loss 1.7876 | lr 5.79e-04 | grad 1.85 | tok/s 10751
step    210 | loss 1.8379 | lr 5.79e-04 | grad 2.91 | tok/s 10750
step    220 | loss 1.9536 | lr 5.79e-04 | grad 1.77 | tok/s 10612
step    230 | loss 2.0086 | lr 5.79e-04 | grad 1.57 | tok/s 10478
step    240 | loss 2.1970 | lr 5.79e-04 | grad 2.38 | tok/s 9970
step    250 | loss 1.9931 | lr 5.79e-04 | grad 1.32 | tok/s 10234
step    260 | loss 1.4653 | lr 5.79e-04 | grad 1.55 | tok/s 10565
step    270 | loss 1.9805 | lr 5.79e-04 | grad 1.41 | tok/s 10413
step    280 | loss 2.1933 | lr 5.79e-04 | grad 2.86 | tok/s 9798
step    290 | loss 1.3285 | lr 5.79e-04 | grad 1.58 | tok/s 10765
step    300 | loss 0.5251 | lr 5.79e-04 | grad 1.59 | tok/s 10762
step    310 | loss 2.2769 | lr 5.79e-04 | grad 2.08 | tok/s 10577
step    320 | loss 1.7822 | lr 5.79e-04 | grad 2.66 | tok/s 10340
step    330 | loss 1.8311 | lr 5.79e-04 | grad 1.44 | tok/s 10003
step    340 | loss 2.1252 | lr 5.79e-04 | grad 1.39 | tok/s 10159
step    350 | loss 1.7356 | lr 5.79e-04 | grad 1.51 | tok/s 10403
step    360 | loss 1.1113 | lr 5.79e-04 | grad 3.95 | tok/s 10639
step    370 | loss 1.7082 | lr 5.79e-04 | grad 1.23 | tok/s 9652
step    380 | loss 1.6515 | lr 5.79e-04 | grad 1.24 | tok/s 10284
step    390 | loss 1.4476 | lr 5.79e-04 | grad 1.07 | tok/s 10742
step    400 | loss 1.4118 | lr 5.79e-04 | grad 1.26 | tok/s 10639
step    410 | loss 1.2128 | lr 5.79e-04 | grad 0.92 | tok/s 10397
step    420 | loss 1.7146 | lr 5.79e-04 | grad 2.05 | tok/s 9727
step    430 | loss 2.0107 | lr 5.79e-04 | grad 1.45 | tok/s 10584
step    440 | loss 2.0486 | lr 5.79e-04 | grad 1.86 | tok/s 10002
step    450 | loss 1.7420 | lr 5.79e-04 | grad 1.20 | tok/s 10361

Training complete! Final step: 457
