Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_11/levelE88_100m_20260126_043951
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 500,113,810 parameters
Using schedule-free AdamW (lr=0.00033935812140081274)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.5889 | lr 3.39e-04 | grad 9.38 | tok/s 4444
step     20 | loss 2.6139 | lr 3.39e-04 | grad 4.06 | tok/s 7952
step     30 | loss 2.5491 | lr 3.39e-04 | grad 1.93 | tok/s 8060
step     40 | loss 2.3429 | lr 3.39e-04 | grad 2.06 | tok/s 7679
step     50 | loss 3.1146 | lr 3.39e-04 | grad 10.56 | tok/s 7798
step     60 | loss 2.1350 | lr 3.39e-04 | grad 2.42 | tok/s 8029
step     70 | loss 2.0153 | lr 3.39e-04 | grad 2.81 | tok/s 8122
step     80 | loss 4.9653 | lr 3.39e-04 | grad 95.00 | tok/s 7866
step     90 | loss 5.1449 | lr 3.39e-04 | grad 8.62 | tok/s 8287
step    100 | loss 4.5929 | lr 3.39e-04 | grad 11.06 | tok/s 8282
step    110 | loss 4.2125 | lr 3.39e-04 | grad 18.62 | tok/s 8277
step    120 | loss 3.7373 | lr 3.39e-04 | grad 16.62 | tok/s 8264
step    130 | loss 3.4015 | lr 3.39e-04 | grad 19.00 | tok/s 8262
step    140 | loss 2.7650 | lr 3.39e-04 | grad 10.44 | tok/s 8257
step    150 | loss 2.9913 | lr 3.39e-04 | grad 9.94 | tok/s 8251
step    160 | loss 2.3775 | lr 3.39e-04 | grad 8.31 | tok/s 7946
step    170 | loss 2.4683 | lr 3.39e-04 | grad 7.75 | tok/s 8238
step    180 | loss 2.2552 | lr 3.39e-04 | grad 4.34 | tok/s 8233
step    190 | loss 2.3637 | lr 3.39e-04 | grad 3.80 | tok/s 8232
step    200 | loss 2.1127 | lr 3.39e-04 | grad 3.69 | tok/s 8222
step    210 | loss 2.0444 | lr 3.39e-04 | grad 3.08 | tok/s 8225
step    220 | loss 2.1953 | lr 3.39e-04 | grad 1.71 | tok/s 8145
step    230 | loss 2.1800 | lr 3.39e-04 | grad 1.82 | tok/s 7843
step    240 | loss 2.2525 | lr 3.39e-04 | grad 2.38 | tok/s 7607
step    250 | loss 2.0943 | lr 3.39e-04 | grad 1.34 | tok/s 7820
step    260 | loss 1.6184 | lr 3.39e-04 | grad 1.61 | tok/s 8077
step    270 | loss 2.0835 | lr 3.39e-04 | grad 1.41 | tok/s 7956
step    280 | loss 2.2496 | lr 3.39e-04 | grad 3.38 | tok/s 7813
step    290 | loss 1.4773 | lr 3.39e-04 | grad 2.42 | tok/s 8222
step    300 | loss 0.6038 | lr 3.39e-04 | grad 2.25 | tok/s 8220
step    310 | loss 2.3995 | lr 3.39e-04 | grad 2.28 | tok/s 7815
step    320 | loss 1.9921 | lr 3.39e-04 | grad 3.31 | tok/s 7908
step    330 | loss 1.9308 | lr 3.39e-04 | grad 1.56 | tok/s 7627
step    340 | loss 2.2314 | lr 3.39e-04 | grad 1.43 | tok/s 7746
step    350 | loss 1.9044 | lr 3.39e-04 | grad 2.69 | tok/s 7947

Training complete! Final step: 351
