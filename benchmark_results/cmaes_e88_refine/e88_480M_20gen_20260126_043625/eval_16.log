Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_16/levelE88_100m_20260126_043950
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,257,566 parameters
Using schedule-free AdamW (lr=0.0003099586065483192)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.6560 | lr 3.10e-04 | grad 24.00 | tok/s 5644
step     20 | loss 2.6955 | lr 3.10e-04 | grad 9.88 | tok/s 12603
step     30 | loss 2.5548 | lr 3.10e-04 | grad 5.09 | tok/s 12754
step     40 | loss 2.4120 | lr 3.10e-04 | grad 4.38 | tok/s 12199
step     50 | loss 3.2216 | lr 3.10e-04 | grad 21.50 | tok/s 12370
step     60 | loss 2.1611 | lr 3.10e-04 | grad 4.84 | tok/s 12750
step     70 | loss 1.9908 | lr 3.10e-04 | grad 5.72 | tok/s 12902
step     80 | loss 5.6891 | lr 3.10e-04 | grad 183.00 | tok/s 12302
step     90 | loss 5.8106 | lr 3.10e-04 | grad 17.50 | tok/s 13170
step    100 | loss 4.9940 | lr 3.10e-04 | grad 22.75 | tok/s 13176
step    110 | loss 4.6903 | lr 3.10e-04 | grad 35.50 | tok/s 13162
step    120 | loss 4.1998 | lr 3.10e-04 | grad 39.25 | tok/s 13128
step    130 | loss 3.8722 | lr 3.10e-04 | grad 43.50 | tok/s 13141
step    140 | loss 3.0709 | lr 3.10e-04 | grad 25.38 | tok/s 13101
step    150 | loss 3.6102 | lr 3.10e-04 | grad 32.50 | tok/s 13121
step    160 | loss 2.6574 | lr 3.10e-04 | grad 28.00 | tok/s 12583
step    170 | loss 2.7336 | lr 3.10e-04 | grad 25.62 | tok/s 13095
step    180 | loss 2.4797 | lr 3.10e-04 | grad 5.88 | tok/s 13061
step    190 | loss 2.7335 | lr 3.10e-04 | grad 7.34 | tok/s 13108
step    200 | loss 2.3166 | lr 3.10e-04 | grad 16.50 | tok/s 13067
step    210 | loss 2.3238 | lr 3.10e-04 | grad 11.50 | tok/s 13082
step    220 | loss 2.3678 | lr 3.10e-04 | grad 2.75 | tok/s 12922
step    230 | loss 2.1707 | lr 3.10e-04 | grad 3.19 | tok/s 12780
step    240 | loss 2.3022 | lr 3.10e-04 | grad 4.34 | tok/s 11531
step    250 | loss 2.1363 | lr 3.10e-04 | grad 2.31 | tok/s 12454
step    260 | loss 1.6232 | lr 3.10e-04 | grad 2.61 | tok/s 12850
step    270 | loss 2.1444 | lr 3.10e-04 | grad 2.30 | tok/s 12664
step    280 | loss 2.3175 | lr 3.10e-04 | grad 5.72 | tok/s 12445
step    290 | loss 1.5199 | lr 3.10e-04 | grad 4.59 | tok/s 13099
step    300 | loss 0.6334 | lr 3.10e-04 | grad 2.41 | tok/s 13094
step    310 | loss 2.4278 | lr 3.10e-04 | grad 3.00 | tok/s 12866
step    320 | loss 1.9817 | lr 3.10e-04 | grad 5.25 | tok/s 12100
step    330 | loss 1.9879 | lr 3.10e-04 | grad 2.56 | tok/s 12193
step    340 | loss 2.3104 | lr 3.10e-04 | grad 2.39 | tok/s 12365
step    350 | loss 1.9423 | lr 3.10e-04 | grad 6.31 | tok/s 12652
step    360 | loss 1.2830 | lr 3.10e-04 | grad 4.91 | tok/s 12942
step    370 | loss 1.8410 | lr 3.10e-04 | grad 2.20 | tok/s 11710
step    380 | loss 1.8224 | lr 3.10e-04 | grad 2.08 | tok/s 12485
step    390 | loss 1.5776 | lr 3.10e-04 | grad 1.71 | tok/s 13063
step    400 | loss 1.5317 | lr 3.10e-04 | grad 2.16 | tok/s 12417
step    410 | loss 1.3213 | lr 3.10e-04 | grad 1.76 | tok/s 12658
step    420 | loss 1.8478 | lr 3.10e-04 | grad 3.81 | tok/s 12049
step    430 | loss 2.1917 | lr 3.10e-04 | grad 2.48 | tok/s 12845
step    440 | loss 2.1855 | lr 3.10e-04 | grad 3.66 | tok/s 12157
step    450 | loss 1.9509 | lr 3.10e-04 | grad 2.36 | tok/s 12582
step    460 | loss 1.7422 | lr 3.10e-04 | grad 2.27 | tok/s 12303
step    470 | loss 1.8701 | lr 3.10e-04 | grad 1.97 | tok/s 12708
step    480 | loss 2.2943 | lr 3.10e-04 | grad 5.78 | tok/s 12149
step    490 | loss 1.8184 | lr 3.10e-04 | grad 2.23 | tok/s 12004
step    500 | loss 1.7205 | lr 3.10e-04 | grad 2.81 | tok/s 12835
step    510 | loss 1.7351 | lr 3.10e-04 | grad 1.95 | tok/s 13009
step    520 | loss 1.6958 | lr 3.10e-04 | grad 1.73 | tok/s 12954
step    530 | loss 1.9391 | lr 3.10e-04 | grad 2.16 | tok/s 12464
step    540 | loss 1.7656 | lr 3.10e-04 | grad 1.87 | tok/s 12493
step    550 | loss 1.5932 | lr 3.10e-04 | grad 2.52 | tok/s 12187

Training complete! Final step: 554
