Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_14/levelE88_100m_20260126_043951
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 497,915,506 parameters
Using schedule-free AdamW (lr=0.00034697308134300057)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.6184 | lr 3.47e-04 | grad 7.16 | tok/s 4232
step     20 | loss 2.6778 | lr 3.47e-04 | grad 4.00 | tok/s 7258
step     30 | loss 2.5752 | lr 3.47e-04 | grad 1.53 | tok/s 7347
step     40 | loss 2.3616 | lr 3.47e-04 | grad 1.84 | tok/s 6994
step     50 | loss 3.0873 | lr 3.47e-04 | grad 6.75 | tok/s 7112
step     60 | loss 2.1249 | lr 3.47e-04 | grad 4.03 | tok/s 7341
step     70 | loss 2.0444 | lr 3.47e-04 | grad 2.72 | tok/s 7426
step     80 | loss 4.9525 | lr 3.47e-04 | grad 78.00 | tok/s 7144
step     90 | loss 5.1349 | lr 3.47e-04 | grad 9.00 | tok/s 7608
step    100 | loss 4.5244 | lr 3.47e-04 | grad 11.19 | tok/s 7593
step    110 | loss 4.1975 | lr 3.47e-04 | grad 13.81 | tok/s 7605
step    120 | loss 3.7172 | lr 3.47e-04 | grad 17.75 | tok/s 7600
step    130 | loss 3.3167 | lr 3.47e-04 | grad 16.62 | tok/s 7604
step    140 | loss 2.6703 | lr 3.47e-04 | grad 7.16 | tok/s 7597
step    150 | loss 2.9152 | lr 3.47e-04 | grad 8.31 | tok/s 7448
step    160 | loss 2.3344 | lr 3.47e-04 | grad 7.19 | tok/s 7597
step    170 | loss 2.4499 | lr 3.47e-04 | grad 6.75 | tok/s 7628
step    180 | loss 2.2469 | lr 3.47e-04 | grad 4.66 | tok/s 7593
step    190 | loss 2.3173 | lr 3.47e-04 | grad 2.00 | tok/s 7603
step    200 | loss 2.0797 | lr 3.47e-04 | grad 3.19 | tok/s 7590
step    210 | loss 2.0801 | lr 3.47e-04 | grad 3.12 | tok/s 7595
step    220 | loss 2.1924 | lr 3.47e-04 | grad 1.58 | tok/s 7504
step    230 | loss 2.1783 | lr 3.47e-04 | grad 2.12 | tok/s 7220
step    240 | loss 2.2572 | lr 3.47e-04 | grad 2.19 | tok/s 7030
step    250 | loss 2.1018 | lr 3.47e-04 | grad 1.19 | tok/s 7237
step    260 | loss 1.6446 | lr 3.47e-04 | grad 1.52 | tok/s 7465
step    270 | loss 2.1010 | lr 3.47e-04 | grad 1.30 | tok/s 7372
step    280 | loss 2.2704 | lr 3.47e-04 | grad 2.78 | tok/s 7222
step    290 | loss 1.5841 | lr 3.47e-04 | grad 2.61 | tok/s 7608
step    300 | loss 0.6052 | lr 3.47e-04 | grad 1.45 | tok/s 7608
step    310 | loss 2.4243 | lr 3.47e-04 | grad 2.41 | tok/s 7293
step    320 | loss 2.0154 | lr 3.47e-04 | grad 3.12 | tok/s 7317

Training complete! Final step: 324
