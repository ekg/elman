Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_6/levelE88_100m_20260126_043632
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 489,755,740 parameters
Using schedule-free AdamW (lr=0.00032072116232665136)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.7332 | lr 3.21e-04 | grad 29.50 | tok/s 5537
step     20 | loss 2.6910 | lr 3.21e-04 | grad 12.00 | tok/s 12515
step     30 | loss 2.5973 | lr 3.21e-04 | grad 5.28 | tok/s 12642
step     40 | loss 2.4319 | lr 3.21e-04 | grad 4.19 | tok/s 12110
step     50 | loss 3.1921 | lr 3.21e-04 | grad 21.88 | tok/s 12279
step     60 | loss 2.1710 | lr 3.21e-04 | grad 4.81 | tok/s 12678
step     70 | loss 2.0048 | lr 3.21e-04 | grad 6.03 | tok/s 12844
step     80 | loss 5.9525 | lr 3.21e-04 | grad 240.00 | tok/s 12124
step     90 | loss 6.1668 | lr 3.21e-04 | grad 19.38 | tok/s 13110
step    100 | loss 5.3090 | lr 3.21e-04 | grad 20.12 | tok/s 13072
step    110 | loss 4.8765 | lr 3.21e-04 | grad 47.25 | tok/s 13053
step    120 | loss 4.3049 | lr 3.21e-04 | grad 42.25 | tok/s 13023
step    130 | loss 4.0164 | lr 3.21e-04 | grad 54.75 | tok/s 13008
step    140 | loss 3.2362 | lr 3.21e-04 | grad 36.25 | tok/s 12962
step    150 | loss 3.7152 | lr 3.21e-04 | grad 38.50 | tok/s 12532
step    160 | loss 2.7754 | lr 3.21e-04 | grad 33.25 | tok/s 12939
step    170 | loss 2.8860 | lr 3.21e-04 | grad 32.00 | tok/s 12914
step    180 | loss 2.6620 | lr 3.21e-04 | grad 6.16 | tok/s 12908
step    190 | loss 2.8701 | lr 3.21e-04 | grad 7.34 | tok/s 12891
step    200 | loss 2.4246 | lr 3.21e-04 | grad 18.25 | tok/s 12890
step    210 | loss 2.4047 | lr 3.21e-04 | grad 12.38 | tok/s 12871
step    220 | loss 2.4004 | lr 3.21e-04 | grad 2.77 | tok/s 12698
step    230 | loss 2.1913 | lr 3.21e-04 | grad 3.58 | tok/s 12001
step    240 | loss 2.3150 | lr 3.21e-04 | grad 4.19 | tok/s 11909
step    250 | loss 2.1372 | lr 3.21e-04 | grad 2.17 | tok/s 12229
step    260 | loss 1.6085 | lr 3.21e-04 | grad 2.47 | tok/s 12618
step    270 | loss 2.1528 | lr 3.21e-04 | grad 2.28 | tok/s 12430
step    280 | loss 2.2978 | lr 3.21e-04 | grad 4.47 | tok/s 12192
step    290 | loss 1.5568 | lr 3.21e-04 | grad 3.95 | tok/s 12877
step    300 | loss 0.6223 | lr 3.21e-04 | grad 2.23 | tok/s 12832
step    310 | loss 2.4205 | lr 3.21e-04 | grad 3.06 | tok/s 12187
step    320 | loss 1.9893 | lr 3.21e-04 | grad 5.19 | tok/s 12400
step    330 | loss 1.9728 | lr 3.21e-04 | grad 2.56 | tok/s 11929
step    340 | loss 2.3149 | lr 3.21e-04 | grad 2.47 | tok/s 12139
step    350 | loss 1.9414 | lr 3.21e-04 | grad 5.28 | tok/s 12420
step    360 | loss 1.2738 | lr 3.21e-04 | grad 5.28 | tok/s 12694
step    370 | loss 1.8397 | lr 3.21e-04 | grad 2.16 | tok/s 11508
step    380 | loss 1.8195 | lr 3.21e-04 | grad 2.02 | tok/s 12269
step    390 | loss 1.5711 | lr 3.21e-04 | grad 1.59 | tok/s 12425
step    400 | loss 1.5275 | lr 3.21e-04 | grad 2.11 | tok/s 12680
step    410 | loss 1.3131 | lr 3.21e-04 | grad 1.74 | tok/s 12418
step    420 | loss 1.8507 | lr 3.21e-04 | grad 3.73 | tok/s 11854
step    430 | loss 2.1891 | lr 3.21e-04 | grad 2.41 | tok/s 12642
step    440 | loss 2.2028 | lr 3.21e-04 | grad 3.50 | tok/s 11912
step    450 | loss 1.9691 | lr 3.21e-04 | grad 2.34 | tok/s 12339
step    460 | loss 1.7469 | lr 3.21e-04 | grad 2.38 | tok/s 12094
step    470 | loss 1.8691 | lr 3.21e-04 | grad 1.95 | tok/s 12110
step    480 | loss 2.2928 | lr 3.21e-04 | grad 5.75 | tok/s 12442
step    490 | loss 1.8234 | lr 3.21e-04 | grad 2.06 | tok/s 11751
step    500 | loss 1.7144 | lr 3.21e-04 | grad 2.80 | tok/s 12561
step    510 | loss 1.7325 | lr 3.21e-04 | grad 1.94 | tok/s 12692
step    520 | loss 1.6891 | lr 3.21e-04 | grad 1.71 | tok/s 12690
step    530 | loss 1.9439 | lr 3.21e-04 | grad 2.12 | tok/s 12203
step    540 | loss 1.7661 | lr 3.21e-04 | grad 1.84 | tok/s 12198

Training complete! Final step: 546
