Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_7/levelE88_100m_20260126_043632
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,733,312 parameters
Using schedule-free AdamW (lr=0.00044847119441087003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.3450 | lr 4.48e-04 | grad 4.91 | tok/s 6351
step     20 | loss 2.7611 | lr 4.48e-04 | grad 1.83 | tok/s 8715
step     30 | loss 3.1247 | lr 4.48e-04 | grad 3.81 | tok/s 9150
step     40 | loss 4.2068 | lr 4.48e-04 | grad 34.25 | tok/s 9293
step     50 | loss 4.5185 | lr 4.48e-04 | grad 12.06 | tok/s 9309
step     60 | loss 3.5160 | lr 4.48e-04 | grad 6.22 | tok/s 9204
step     70 | loss 2.7192 | lr 4.48e-04 | grad 3.06 | tok/s 9131
step     80 | loss 2.3759 | lr 4.48e-04 | grad 2.11 | tok/s 9054
step     90 | loss 2.2238 | lr 4.48e-04 | grad 1.97 | tok/s 8884
step    100 | loss 2.0418 | lr 4.48e-04 | grad 1.20 | tok/s 8933
step    110 | loss 2.1723 | lr 4.48e-04 | grad 1.74 | tok/s 8832
step    120 | loss 2.6507 | lr 4.48e-04 | grad 0.96 | tok/s 8379
step    130 | loss 2.1239 | lr 4.48e-04 | grad 3.02 | tok/s 8567
step    140 | loss 2.3375 | lr 4.48e-04 | grad 4.44 | tok/s 8571
step    150 | loss 1.3845 | lr 4.48e-04 | grad 3.00 | tok/s 8756
step    160 | loss 2.3140 | lr 4.48e-04 | grad 1.09 | tok/s 8462
step    170 | loss 2.2534 | lr 4.48e-04 | grad 0.86 | tok/s 8327
step    180 | loss 1.8181 | lr 4.48e-04 | grad 1.55 | tok/s 8453
step    190 | loss 1.8876 | lr 4.48e-04 | grad 1.43 | tok/s 8335

Training complete! Final step: 194
