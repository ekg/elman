Using device: cuda
Output directory: benchmark_results/cmaes_e88_refine/e88_480M_20gen_20260126_043625/eval_4/levelE88_100m_20260126_043632
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 506,304,848 parameters
Using schedule-free AdamW (lr=0.0002461149084469088)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 5.0108 | lr 2.46e-04 | grad 28.12 | tok/s 4942
step     20 | loss 2.7809 | lr 2.46e-04 | grad 7.94 | tok/s 9720
step     30 | loss 2.6072 | lr 2.46e-04 | grad 4.03 | tok/s 9806
step     40 | loss 2.4462 | lr 2.46e-04 | grad 3.83 | tok/s 9389
step     50 | loss 3.3521 | lr 2.46e-04 | grad 31.12 | tok/s 9479
step     60 | loss 2.2720 | lr 2.46e-04 | grad 5.78 | tok/s 9786
step     70 | loss 2.1217 | lr 2.46e-04 | grad 7.09 | tok/s 9887
step     80 | loss 5.2398 | lr 2.46e-04 | grad 158.00 | tok/s 9967
step     90 | loss 5.3217 | lr 2.46e-04 | grad 14.00 | tok/s 10074
step    100 | loss 4.8904 | lr 2.46e-04 | grad 25.62 | tok/s 10037
step    110 | loss 4.7696 | lr 2.46e-04 | grad 52.50 | tok/s 10016
step    120 | loss 4.4932 | lr 2.46e-04 | grad 53.75 | tok/s 9995
step    130 | loss 4.3912 | lr 2.46e-04 | grad 57.75 | tok/s 9317
step    140 | loss 3.5396 | lr 2.46e-04 | grad 40.75 | tok/s 9948
step    150 | loss 4.1678 | lr 2.46e-04 | grad 44.75 | tok/s 9936
step    160 | loss 3.2278 | lr 2.46e-04 | grad 39.50 | tok/s 9912
step    170 | loss 3.1822 | lr 2.46e-04 | grad 39.50 | tok/s 9918
step    180 | loss 2.9145 | lr 2.46e-04 | grad 6.91 | tok/s 9894
step    190 | loss 3.1005 | lr 2.46e-04 | grad 11.06 | tok/s 9877
step    200 | loss 2.5718 | lr 2.46e-04 | grad 24.25 | tok/s 9891
step    210 | loss 2.6117 | lr 2.46e-04 | grad 19.88 | tok/s 9879
step    220 | loss 2.5437 | lr 2.46e-04 | grad 2.80 | tok/s 9731
step    230 | loss 2.5044 | lr 2.46e-04 | grad 4.38 | tok/s 9651
step    240 | loss 2.3477 | lr 2.46e-04 | grad 4.16 | tok/s 9125
step    250 | loss 2.1990 | lr 2.46e-04 | grad 2.09 | tok/s 9384
step    260 | loss 1.7573 | lr 2.46e-04 | grad 2.53 | tok/s 9684
step    270 | loss 2.2201 | lr 2.46e-04 | grad 2.00 | tok/s 9548
step    280 | loss 2.3965 | lr 2.46e-04 | grad 4.56 | tok/s 9093
step    290 | loss 1.8234 | lr 2.46e-04 | grad 11.44 | tok/s 9874
step    300 | loss 0.8238 | lr 2.46e-04 | grad 4.25 | tok/s 9866
step    310 | loss 2.5183 | lr 2.46e-04 | grad 3.14 | tok/s 9725
step    320 | loss 2.1351 | lr 2.46e-04 | grad 7.03 | tok/s 9500
step    330 | loss 2.0457 | lr 2.46e-04 | grad 2.59 | tok/s 9165
step    340 | loss 2.4057 | lr 2.46e-04 | grad 2.50 | tok/s 9308
step    350 | loss 2.1283 | lr 2.46e-04 | grad 8.50 | tok/s 9554
step    360 | loss 1.7163 | lr 2.46e-04 | grad 6.59 | tok/s 9759
step    370 | loss 1.9377 | lr 2.46e-04 | grad 2.16 | tok/s 8845
step    380 | loss 1.9230 | lr 2.46e-04 | grad 1.96 | tok/s 9457
step    390 | loss 1.6689 | lr 2.46e-04 | grad 1.60 | tok/s 9871
step    400 | loss 1.6378 | lr 2.46e-04 | grad 2.16 | tok/s 9805
step    410 | loss 1.4575 | lr 2.46e-04 | grad 1.74 | tok/s 9549
step    420 | loss 1.9214 | lr 2.46e-04 | grad 4.28 | tok/s 8848

Training complete! Final step: 423
