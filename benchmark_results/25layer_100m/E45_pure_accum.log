Using device: cuda
Output directory: /home/erikg/elman/benchmark_results/25layer_100m/E45_pure_accum/level45_100m_20260113_190745
Model: Level 45, 99,802,620 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.3057 | lr 4.90e-06 | grad 17.68 | tok/s 23482
step    100 | loss 4.2689 | lr 9.90e-06 | grad 6.99 | tok/s 31222
step    150 | loss 3.7652 | lr 1.49e-05 | grad 3.97 | tok/s 29363
step    200 | loss 3.5711 | lr 1.99e-05 | grad 2.40 | tok/s 29115
step    250 | loss 3.5115 | lr 2.49e-05 | grad 3.05 | tok/s 27772
step    300 | loss 3.2769 | lr 2.99e-05 | grad 3.35 | tok/s 27838
step    350 | loss 3.2967 | lr 3.49e-05 | grad 2.52 | tok/s 25950
step    400 | loss 3.3277 | lr 3.99e-05 | grad 5.29 | tok/s 27880
step    450 | loss 3.3923 | lr 4.49e-05 | grad 1.80 | tok/s 27873
step    500 | loss 3.2978 | lr 4.99e-05 | grad 2.49 | tok/s 28164
step    550 | loss 3.2571 | lr 5.49e-05 | grad 3.17 | tok/s 27335
step    600 | loss 3.1485 | lr 5.99e-05 | grad 1.10 | tok/s 29154
step    650 | loss 3.2279 | lr 6.49e-05 | grad 2.97 | tok/s 28516
step    700 | loss 3.2502 | lr 6.99e-05 | grad 2.15 | tok/s 27629
step    750 | loss 3.2977 | lr 7.49e-05 | grad 3.90 | tok/s 28008
step    800 | loss 3.2891 | lr 7.99e-05 | grad 2.71 | tok/s 28197
step    850 | loss 3.1251 | lr 8.49e-05 | grad 1.36 | tok/s 27448
step    900 | loss 3.2791 | lr 8.99e-05 | grad 1.78 | tok/s 27961
step    950 | loss 3.1972 | lr 9.49e-05 | grad 1.46 | tok/s 27888
step   1000 | loss 3.2918 | lr 9.99e-05 | grad 2.13 | tok/s 27919
  >>> saved checkpoint: checkpoint_step_001000_loss_3.2918.pt
step   1050 | loss 3.2271 | lr 1.59e-06 | grad 7.21 | tok/s 25505
step   1100 | loss 3.2267 | lr 3.37e-06 | grad 1.00 | tok/s 28778
step   1150 | loss 3.2383 | lr 6.32e-06 | grad 3.48 | tok/s 29048
step   1200 | loss 3.3188 | lr 1.04e-05 | grad 2.63 | tok/s 27969
step   1250 | loss 3.2300 | lr 1.54e-05 | grad 0.94 | tok/s 28607
step   1300 | loss 3.3133 | lr 2.13e-05 | grad 1.28 | tok/s 28393
step   1350 | loss 3.2790 | lr 2.79e-05 | grad 1.10 | tok/s 28232
step   1400 | loss 3.2853 | lr 3.51e-05 | grad 2.42 | tok/s 28055
step   1450 | loss 3.3003 | lr 4.26e-05 | grad 1.72 | tok/s 28208
step   1500 | loss 3.2075 | lr 5.03e-05 | grad 2.03 | tok/s 28351
step   1550 | loss 3.1892 | lr 5.81e-05 | grad 1.48 | tok/s 28332
step   1600 | loss 3.2735 | lr 6.56e-05 | grad 1.12 | tok/s 28779
step   1650 | loss 3.2546 | lr 7.28e-05 | grad 1.10 | tok/s 29030
step   1700 | loss 3.2143 | lr 7.95e-05 | grad 1.32 | tok/s 29130
step   1750 | loss 3.1666 | lr 8.54e-05 | grad 2.69 | tok/s 28309
step   1800 | loss 3.2266 | lr 9.05e-05 | grad 1.71 | tok/s 28126
step   1850 | loss 3.1428 | lr 9.45e-05 | grad 1.40 | tok/s 28156
step   1900 | loss 3.1345 | lr 9.75e-05 | grad 2.19 | tok/s 28196
step   1950 | loss 3.1488 | lr 9.94e-05 | grad 0.71 | tok/s 29859
step   2000 | loss 3.2085 | lr 1.00e-04 | grad 1.26 | tok/s 27913
  >>> saved checkpoint: checkpoint_step_002000_loss_3.2085.pt
step   2050 | loss 3.2544 | lr 9.94e-05 | grad 1.34 | tok/s 26150
step   2100 | loss 3.1531 | lr 9.76e-05 | grad 2.48 | tok/s 28358

Training complete! Final step: 2140
