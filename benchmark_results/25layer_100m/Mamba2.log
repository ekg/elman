Using device: cuda
Output directory: benchmark_results/25layer_100m/Mamba2/levelmamba2_100m_20260113_192529
Model: Level mamba2, 102,140,275 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.2192 | lr 4.90e-06 | grad 8.57 | tok/s 13444
step    100 | loss 4.2276 | lr 9.90e-06 | grad 3.94 | tok/s 22775
step    150 | loss 3.1664 | lr 1.49e-05 | grad 5.09 | tok/s 21701
step    200 | loss 2.7871 | lr 1.99e-05 | grad 1.91 | tok/s 21342
step    250 | loss 2.4841 | lr 2.49e-05 | grad 2.32 | tok/s 20707
step    300 | loss 2.0982 | lr 2.99e-05 | grad 3.33 | tok/s 20663
step    350 | loss 2.0484 | lr 3.49e-05 | grad 3.15 | tok/s 20497
step    400 | loss 1.7086 | lr 3.99e-05 | grad 2.02 | tok/s 21338
step    450 | loss 1.9989 | lr 4.49e-05 | grad 1.72 | tok/s 21023
step    500 | loss 1.8053 | lr 4.99e-05 | grad 1.98 | tok/s 20873
step    550 | loss 1.8410 | lr 5.49e-05 | grad 1.80 | tok/s 20315
step    600 | loss 1.4828 | lr 5.99e-05 | grad 1.80 | tok/s 21694
step    650 | loss 1.5925 | lr 6.49e-05 | grad 2.00 | tok/s 21198
step    700 | loss 1.7754 | lr 6.99e-05 | grad 2.04 | tok/s 20591
step    750 | loss 1.7481 | lr 7.49e-05 | grad 3.52 | tok/s 20898
step    800 | loss 1.7869 | lr 7.99e-05 | grad 3.91 | tok/s 21083
step    850 | loss 1.6617 | lr 8.49e-05 | grad 1.37 | tok/s 20541
step    900 | loss 1.7378 | lr 8.99e-05 | grad 1.24 | tok/s 20859
step    950 | loss 1.6085 | lr 9.49e-05 | grad 1.21 | tok/s 20761
step   1000 | loss 1.7644 | lr 9.99e-05 | grad 1.25 | tok/s 20806
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7644.pt
step   1050 | loss 1.7238 | lr 1.59e-06 | grad 5.21 | tok/s 19379
step   1100 | loss 1.7429 | lr 3.37e-06 | grad 1.04 | tok/s 21323
step   1150 | loss 1.6540 | lr 6.32e-06 | grad 2.37 | tok/s 21487
step   1200 | loss 1.8007 | lr 1.04e-05 | grad 2.21 | tok/s 20740
step   1250 | loss 1.6004 | lr 1.54e-05 | grad 1.05 | tok/s 21052
step   1300 | loss 1.6070 | lr 2.13e-05 | grad 1.49 | tok/s 21017
step   1350 | loss 1.6768 | lr 2.79e-05 | grad 1.23 | tok/s 20824
step   1400 | loss 1.6821 | lr 3.51e-05 | grad 3.23 | tok/s 20635
step   1450 | loss 1.6361 | lr 4.26e-05 | grad 1.70 | tok/s 20797
step   1500 | loss 1.5793 | lr 5.03e-05 | grad 1.52 | tok/s 20812
step   1550 | loss 1.6070 | lr 5.81e-05 | grad 1.16 | tok/s 20701

Training complete! Final step: 1569
