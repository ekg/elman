Using device: cuda
Output directory: benchmark_results/500m_state_matched_v3/mamba2/levelmamba2_500m_20260121_143744
Created Mamba2 model: dim=1600, depth=32, expand=2, params=508,362,560
Model: Level mamba2, 508,362,560 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8370 | lr 9.00e-07 | grad 32.50 | tok/s 4477
step     20 | loss 5.8174 | lr 1.90e-06 | grad 26.88 | tok/s 17928
step     30 | loss 5.7503 | lr 2.90e-06 | grad 12.50 | tok/s 17734
step     40 | loss 5.6423 | lr 3.90e-06 | grad 15.38 | tok/s 18268
step     50 | loss 5.7940 | lr 4.90e-06 | grad 15.38 | tok/s 18808
step     60 | loss 5.6741 | lr 5.90e-06 | grad 8.19 | tok/s 18665
step     70 | loss 5.4840 | lr 6.90e-06 | grad 21.12 | tok/s 18523
step     80 | loss 5.3326 | lr 7.90e-06 | grad 9.81 | tok/s 18386
step     90 | loss 5.0259 | lr 8.90e-06 | grad 7.53 | tok/s 18234
step    100 | loss 4.7502 | lr 9.90e-06 | grad 8.31 | tok/s 18095
step    110 | loss 4.3647 | lr 1.09e-05 | grad 11.56 | tok/s 17903
step    120 | loss 4.3423 | lr 1.19e-05 | grad 7.81 | tok/s 17243
step    130 | loss 3.4822 | lr 1.29e-05 | grad 6.41 | tok/s 16774
step    140 | loss 3.0413 | lr 1.39e-05 | grad 7.47 | tok/s 16754
step    150 | loss 3.3308 | lr 1.49e-05 | grad 12.19 | tok/s 17222
step    160 | loss 3.0499 | lr 1.59e-05 | grad 6.28 | tok/s 17235
step    170 | loss 3.0376 | lr 1.69e-05 | grad 8.19 | tok/s 16229
step    180 | loss 3.0833 | lr 1.79e-05 | grad 10.31 | tok/s 16779
step    190 | loss 2.8673 | lr 1.89e-05 | grad 4.34 | tok/s 16003
step    200 | loss 2.4142 | lr 1.99e-05 | grad 3.66 | tok/s 17055
step    210 | loss 2.2967 | lr 2.09e-05 | grad 4.50 | tok/s 16508
step    220 | loss 2.6739 | lr 2.19e-05 | grad 6.25 | tok/s 15880
step    230 | loss 2.9376 | lr 2.29e-05 | grad 3.50 | tok/s 15861
step    240 | loss 2.4017 | lr 2.39e-05 | grad 5.00 | tok/s 15879
step    250 | loss 2.6285 | lr 2.49e-05 | grad 4.44 | tok/s 15888
step    260 | loss 2.1897 | lr 2.59e-05 | grad 3.47 | tok/s 16389
step    270 | loss 2.3674 | lr 2.69e-05 | grad 3.55 | tok/s 16387
step    280 | loss 2.0098 | lr 2.79e-05 | grad 4.19 | tok/s 15873
step    290 | loss 2.0118 | lr 2.89e-05 | grad 7.22 | tok/s 15231
step    300 | loss 2.1065 | lr 2.99e-05 | grad 5.44 | tok/s 15441
step    310 | loss 2.0761 | lr 3.09e-05 | grad 3.39 | tok/s 15766
step    320 | loss 1.8520 | lr 3.19e-05 | grad 5.31 | tok/s 15088
step    330 | loss 2.1145 | lr 3.29e-05 | grad 3.09 | tok/s 15776
step    340 | loss 2.1775 | lr 3.39e-05 | grad 6.56 | tok/s 16063
step    350 | loss 2.1978 | lr 3.49e-05 | grad 5.50 | tok/s 15772
step    360 | loss 2.0827 | lr 3.59e-05 | grad 4.28 | tok/s 16138
step    370 | loss 1.7976 | lr 3.69e-05 | grad 3.33 | tok/s 15805
step    380 | loss 1.8345 | lr 3.79e-05 | grad 3.80 | tok/s 16496
step    390 | loss 1.4259 | lr 3.89e-05 | grad 3.05 | tok/s 16638
step    400 | loss 1.3033 | lr 3.99e-05 | grad 3.17 | tok/s 16375
step    410 | loss 2.2309 | lr 4.09e-05 | grad 3.83 | tok/s 15826
step    420 | loss 2.0547 | lr 4.19e-05 | grad 4.53 | tok/s 15799
step    430 | loss 1.9879 | lr 4.29e-05 | grad 5.78 | tok/s 16552
step    440 | loss 1.8448 | lr 4.39e-05 | grad 4.28 | tok/s 16032
step    450 | loss 1.9868 | lr 4.49e-05 | grad 2.77 | tok/s 15809
step    460 | loss 1.7403 | lr 4.59e-05 | grad 7.41 | tok/s 15655
step    470 | loss 1.8381 | lr 4.69e-05 | grad 3.70 | tok/s 15668
step    480 | loss 1.8040 | lr 4.79e-05 | grad 4.34 | tok/s 16378
step    490 | loss 1.8276 | lr 4.89e-05 | grad 3.34 | tok/s 15824
step    500 | loss 1.8860 | lr 4.99e-05 | grad 3.14 | tok/s 15737
step    510 | loss 2.1046 | lr 5.09e-05 | grad 10.88 | tok/s 15468
step    520 | loss 1.8083 | lr 5.19e-05 | grad 3.17 | tok/s 14822
step    530 | loss 1.6763 | lr 5.29e-05 | grad 3.09 | tok/s 15752
step    540 | loss 1.9084 | lr 5.39e-05 | grad 2.94 | tok/s 15734
step    550 | loss 1.8118 | lr 5.49e-05 | grad 2.86 | tok/s 15357
step    560 | loss 1.5504 | lr 5.59e-05 | grad 3.42 | tok/s 16092
step    570 | loss 1.5915 | lr 5.69e-05 | grad 2.55 | tok/s 16558
step    580 | loss 1.4200 | lr 5.79e-05 | grad 2.20 | tok/s 16564
step    590 | loss 1.3679 | lr 5.89e-05 | grad 1.95 | tok/s 16539
step    600 | loss 1.4662 | lr 5.99e-05 | grad 2.50 | tok/s 16537
step    610 | loss 1.3674 | lr 6.09e-05 | grad 1.99 | tok/s 16535
step    620 | loss 1.3916 | lr 6.19e-05 | grad 1.68 | tok/s 16507
step    630 | loss 1.4968 | lr 6.29e-05 | grad 10.31 | tok/s 16290
step    640 | loss 1.9131 | lr 6.39e-05 | grad 4.09 | tok/s 15567
step    650 | loss 1.8829 | lr 6.49e-05 | grad 3.28 | tok/s 15482
step    660 | loss 1.7305 | lr 6.59e-05 | grad 3.52 | tok/s 15635
step    670 | loss 1.7923 | lr 6.69e-05 | grad 2.62 | tok/s 16153
step    680 | loss 1.8967 | lr 6.79e-05 | grad 3.56 | tok/s 15606
step    690 | loss 1.8511 | lr 6.89e-05 | grad 2.81 | tok/s 15505
step    700 | loss 1.8436 | lr 6.99e-05 | grad 2.66 | tok/s 15387
step    710 | loss 1.6800 | lr 7.09e-05 | grad 2.55 | tok/s 15814
step    720 | loss 1.8964 | lr 7.19e-05 | grad 4.03 | tok/s 15443
step    730 | loss 1.4771 | lr 7.29e-05 | grad 2.08 | tok/s 16149
step    740 | loss 1.6318 | lr 7.39e-05 | grad 1.98 | tok/s 15677
step    750 | loss 2.2014 | lr 7.49e-05 | grad 5.12 | tok/s 16300
step    760 | loss 1.8678 | lr 7.59e-05 | grad 2.06 | tok/s 16310
step    770 | loss 1.7472 | lr 7.69e-05 | grad 3.02 | tok/s 15932
step    780 | loss 1.7975 | lr 7.79e-05 | grad 2.31 | tok/s 15483
step    790 | loss 1.7308 | lr 7.89e-05 | grad 2.66 | tok/s 15877
step    800 | loss 1.9789 | lr 7.99e-05 | grad 5.50 | tok/s 16301
step    810 | loss 1.7278 | lr 8.09e-05 | grad 2.25 | tok/s 15832
step    820 | loss 1.4831 | lr 8.19e-05 | grad 4.72 | tok/s 15433
step    830 | loss 1.6887 | lr 8.29e-05 | grad 2.73 | tok/s 15689
step    840 | loss 1.7507 | lr 8.39e-05 | grad 1.66 | tok/s 15372
step    850 | loss 1.8604 | lr 8.49e-05 | grad 1.88 | tok/s 15387
step    860 | loss 1.8608 | lr 8.59e-05 | grad 1.99 | tok/s 15537
step    870 | loss 1.7924 | lr 8.69e-05 | grad 4.28 | tok/s 15685
step    880 | loss 1.6885 | lr 8.79e-05 | grad 2.80 | tok/s 16432
step    890 | loss 1.8274 | lr 8.89e-05 | grad 2.59 | tok/s 15652
step    900 | loss 1.6630 | lr 8.99e-05 | grad 1.78 | tok/s 15635
step    910 | loss 1.6204 | lr 9.09e-05 | grad 2.05 | tok/s 15818
step    920 | loss 1.7467 | lr 9.19e-05 | grad 1.65 | tok/s 15601
step    930 | loss 1.7281 | lr 9.29e-05 | grad 1.73 | tok/s 15685
step    940 | loss 1.5981 | lr 9.39e-05 | grad 1.85 | tok/s 16119
step    950 | loss 1.5498 | lr 9.49e-05 | grad 1.55 | tok/s 15420
step    960 | loss 1.6464 | lr 9.59e-05 | grad 1.45 | tok/s 15219
step    970 | loss 1.5619 | lr 9.69e-05 | grad 1.42 | tok/s 15387
step    980 | loss 1.5543 | lr 9.79e-05 | grad 1.52 | tok/s 15818
step    990 | loss 2.1557 | lr 9.89e-05 | grad 2.80 | tok/s 16436
step   1000 | loss 1.9102 | lr 9.99e-05 | grad 1.84 | tok/s 15726
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9102.pt
step   1010 | loss 1.8757 | lr 1.02e-06 | grad 2.28 | tok/s 9438
step   1020 | loss 1.5300 | lr 1.09e-06 | grad 1.85 | tok/s 16255
step   1030 | loss 1.5362 | lr 1.21e-06 | grad 3.27 | tok/s 16578
step   1040 | loss 1.9824 | lr 1.37e-06 | grad 1.78 | tok/s 16537
step   1050 | loss 2.0589 | lr 1.59e-06 | grad 6.25 | tok/s 15762
step   1060 | loss 2.7095 | lr 1.85e-06 | grad 1.55 | tok/s 16512
step   1070 | loss 1.8915 | lr 2.16e-06 | grad 8.56 | tok/s 15946
step   1080 | loss 1.3963 | lr 2.52e-06 | grad 2.64 | tok/s 16287
step   1090 | loss 1.6632 | lr 2.92e-06 | grad 1.97 | tok/s 16299
step   1100 | loss 1.5937 | lr 3.37e-06 | grad 1.73 | tok/s 16698
step   1110 | loss 1.5797 | lr 3.87e-06 | grad 1.58 | tok/s 16720
step   1120 | loss 1.5615 | lr 4.42e-06 | grad 1.55 | tok/s 16697
step   1130 | loss 1.5709 | lr 5.01e-06 | grad 1.66 | tok/s 16213
step   1140 | loss 2.0461 | lr 5.65e-06 | grad 4.06 | tok/s 16406
step   1150 | loss 2.2685 | lr 6.32e-06 | grad 3.28 | tok/s 16103
step   1160 | loss 1.7846 | lr 7.05e-06 | grad 2.06 | tok/s 16088
step   1170 | loss 2.3875 | lr 7.81e-06 | grad 3.25 | tok/s 15736
step   1180 | loss 2.0286 | lr 8.62e-06 | grad 1.86 | tok/s 15824
step   1190 | loss 1.7880 | lr 9.47e-06 | grad 1.58 | tok/s 15314

Training complete! Final step: 1196
