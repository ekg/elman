Using device: cuda
Output directory: benchmark_results/500m_state_matched_v3/e88_h40n64/levelE88_h40n64_100m_20260121_143744
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h40n64, 500,695,744 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.7932 | lr 9.00e-07 | grad 59.25 | tok/s 5291
step     20 | loss 5.7574 | lr 1.90e-06 | grad 52.50 | tok/s 7238
step     30 | loss 5.6465 | lr 2.90e-06 | grad 20.62 | tok/s 7101
step     40 | loss 5.7281 | lr 3.90e-06 | grad 20.12 | tok/s 7352
step     50 | loss 5.9790 | lr 4.90e-06 | grad 20.62 | tok/s 7597
step     60 | loss 5.9017 | lr 5.90e-06 | grad 15.88 | tok/s 7550
step     70 | loss 5.7784 | lr 6.90e-06 | grad 24.00 | tok/s 7525
step     80 | loss 5.6658 | lr 7.90e-06 | grad 17.12 | tok/s 7458
step     90 | loss 5.5009 | lr 8.90e-06 | grad 14.75 | tok/s 7397
step    100 | loss 5.2998 | lr 9.90e-06 | grad 15.62 | tok/s 7404
step    110 | loss 5.1019 | lr 1.09e-05 | grad 68.50 | tok/s 7334
step    120 | loss 4.8361 | lr 1.19e-05 | grad 58.50 | tok/s 7077
step    130 | loss 4.4074 | lr 1.29e-05 | grad 12.38 | tok/s 6930
step    140 | loss 4.0380 | lr 1.39e-05 | grad 46.50 | tok/s 6966
step    150 | loss 3.9442 | lr 1.49e-05 | grad 24.00 | tok/s 7190
step    160 | loss 3.5901 | lr 1.59e-05 | grad 9.94 | tok/s 7207
step    170 | loss 3.6873 | lr 1.69e-05 | grad 19.62 | tok/s 6794
step    180 | loss 3.7379 | lr 1.79e-05 | grad 14.50 | tok/s 7037
step    190 | loss 3.4802 | lr 1.89e-05 | grad 7.78 | tok/s 6724
step    200 | loss 3.1365 | lr 1.99e-05 | grad 6.06 | tok/s 7219
step    210 | loss 2.9617 | lr 2.09e-05 | grad 7.22 | tok/s 6993
step    220 | loss 3.2142 | lr 2.19e-05 | grad 8.56 | tok/s 6741
step    230 | loss 3.4602 | lr 2.29e-05 | grad 4.22 | tok/s 6753
step    240 | loss 2.9478 | lr 2.39e-05 | grad 8.62 | tok/s 6783
step    250 | loss 3.1837 | lr 2.49e-05 | grad 7.03 | tok/s 6811
step    260 | loss 2.8060 | lr 2.59e-05 | grad 4.12 | tok/s 7043
step    270 | loss 2.8868 | lr 2.69e-05 | grad 4.78 | tok/s 7038
step    280 | loss 2.5419 | lr 2.79e-05 | grad 4.19 | tok/s 6830
step    290 | loss 2.5278 | lr 2.89e-05 | grad 6.38 | tok/s 6560
step    300 | loss 2.5775 | lr 2.99e-05 | grad 5.72 | tok/s 6666
step    310 | loss 2.5579 | lr 3.09e-05 | grad 4.22 | tok/s 6834
step    320 | loss 2.3209 | lr 3.19e-05 | grad 4.94 | tok/s 6555
step    330 | loss 2.5755 | lr 3.29e-05 | grad 3.39 | tok/s 6840
step    340 | loss 2.6278 | lr 3.39e-05 | grad 18.38 | tok/s 6988
step    350 | loss 2.6231 | lr 3.49e-05 | grad 5.38 | tok/s 6855
step    360 | loss 2.6394 | lr 3.59e-05 | grad 4.69 | tok/s 7033
step    370 | loss 2.3319 | lr 3.69e-05 | grad 3.36 | tok/s 6906
step    380 | loss 2.3401 | lr 3.79e-05 | grad 3.89 | tok/s 7219
step    390 | loss 2.0704 | lr 3.89e-05 | grad 3.84 | tok/s 7246
step    400 | loss 1.9223 | lr 3.99e-05 | grad 3.61 | tok/s 7153
step    410 | loss 2.5431 | lr 4.09e-05 | grad 5.50 | tok/s 6863
step    420 | loss 2.3798 | lr 4.19e-05 | grad 4.97 | tok/s 6869
step    430 | loss 2.5230 | lr 4.29e-05 | grad 7.91 | tok/s 7205
step    440 | loss 2.2701 | lr 4.39e-05 | grad 4.56 | tok/s 7016
step    450 | loss 2.3390 | lr 4.49e-05 | grad 3.67 | tok/s 6930
step    460 | loss 2.0677 | lr 4.59e-05 | grad 6.44 | tok/s 6853
step    470 | loss 2.2241 | lr 4.69e-05 | grad 4.09 | tok/s 6868
step    480 | loss 2.2512 | lr 4.79e-05 | grad 4.53 | tok/s 7196
step    490 | loss 2.1968 | lr 4.89e-05 | grad 3.73 | tok/s 6882
step    500 | loss 2.2172 | lr 4.99e-05 | grad 3.69 | tok/s 6853
step    510 | loss 2.3820 | lr 5.09e-05 | grad 11.19 | tok/s 6790
step    520 | loss 2.0955 | lr 5.19e-05 | grad 2.80 | tok/s 6492
step    530 | loss 1.9856 | lr 5.29e-05 | grad 3.64 | tok/s 6899

Training complete! Final step: 531
