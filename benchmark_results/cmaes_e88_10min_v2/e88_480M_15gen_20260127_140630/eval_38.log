Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_38/levelE88_100m_20260127_144751
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 470,397,280 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0181 | lr 3.00e-04 | grad 32.50 | tok/s 7773
step     20 | loss 2.9196 | lr 3.00e-04 | grad 14.31 | tok/s 13798
step     30 | loss 2.8421 | lr 3.00e-04 | grad 3.36 | tok/s 14292
step     40 | loss 4.7228 | lr 3.00e-04 | grad 30.62 | tok/s 14336
step     50 | loss 4.0402 | lr 3.00e-04 | grad 7.59 | tok/s 14909
step     60 | loss 3.3086 | lr 3.00e-04 | grad 6.44 | tok/s 14523
step     70 | loss 2.7133 | lr 3.00e-04 | grad 5.47 | tok/s 14839
step     80 | loss 2.4361 | lr 3.00e-04 | grad 2.45 | tok/s 14646
step     90 | loss 2.2804 | lr 3.00e-04 | grad 4.00 | tok/s 14783
step    100 | loss 2.1475 | lr 3.00e-04 | grad 2.83 | tok/s 12671
step    110 | loss 2.4125 | lr 3.00e-04 | grad 11.69 | tok/s 14607
step    120 | loss 2.4939 | lr 3.00e-04 | grad 2.84 | tok/s 13834
step    130 | loss 2.1013 | lr 3.00e-04 | grad 4.22 | tok/s 14311
step    140 | loss 2.3606 | lr 3.00e-04 | grad 6.28 | tok/s 14390
step    150 | loss 1.4311 | lr 3.00e-04 | grad 3.34 | tok/s 14642
step    160 | loss 2.2386 | lr 3.00e-04 | grad 2.09 | tok/s 13093
step    170 | loss 2.2965 | lr 3.00e-04 | grad 1.98 | tok/s 14036
step    180 | loss 1.8097 | lr 3.00e-04 | grad 2.52 | tok/s 12516
step    190 | loss 1.8515 | lr 3.00e-04 | grad 2.12 | tok/s 14134
step    200 | loss 1.6220 | lr 3.00e-04 | grad 1.60 | tok/s 14652
step    210 | loss 1.9432 | lr 3.00e-04 | grad 1.91 | tok/s 13881
step    220 | loss 2.2620 | lr 3.00e-04 | grad 10.06 | tok/s 14069
step    230 | loss 1.9552 | lr 3.00e-04 | grad 3.30 | tok/s 13926
step    240 | loss 2.1854 | lr 3.00e-04 | grad 1.88 | tok/s 14225
step    250 | loss 1.7883 | lr 3.00e-04 | grad 2.12 | tok/s 14235
step    260 | loss 1.9189 | lr 3.00e-04 | grad 2.64 | tok/s 14492
step    270 | loss 1.7629 | lr 3.00e-04 | grad 1.65 | tok/s 14063
step    280 | loss 1.7717 | lr 3.00e-04 | grad 1.52 | tok/s 13438
step    290 | loss 1.6514 | lr 3.00e-04 | grad 1.92 | tok/s 13657
step    300 | loss 1.9893 | lr 3.00e-04 | grad 1.94 | tok/s 13962
step    310 | loss 1.6667 | lr 3.00e-04 | grad 2.28 | tok/s 13708
step    320 | loss 1.8489 | lr 3.00e-04 | grad 2.12 | tok/s 12597
step    330 | loss 1.7188 | lr 3.00e-04 | grad 1.67 | tok/s 13773
step    340 | loss 2.0475 | lr 3.00e-04 | grad 2.22 | tok/s 13905
step    350 | loss 1.6888 | lr 3.00e-04 | grad 1.55 | tok/s 13213
step    360 | loss 1.5534 | lr 3.00e-04 | grad 1.30 | tok/s 12598
step    370 | loss 1.4675 | lr 3.00e-04 | grad 1.57 | tok/s 13910
step    380 | loss 1.2224 | lr 3.00e-04 | grad 1.59 | tok/s 14658
step    390 | loss 1.1125 | lr 3.00e-04 | grad 1.38 | tok/s 14414
step    400 | loss 1.8128 | lr 3.00e-04 | grad 1.84 | tok/s 13949
step    410 | loss 1.7423 | lr 3.00e-04 | grad 1.95 | tok/s 14001
step    420 | loss 1.6184 | lr 3.00e-04 | grad 7.00 | tok/s 14406
step    430 | loss 1.5981 | lr 3.00e-04 | grad 1.89 | tok/s 14272
step    440 | loss 1.7051 | lr 3.00e-04 | grad 2.11 | tok/s 14052
step    450 | loss 1.6051 | lr 3.00e-04 | grad 2.30 | tok/s 14087
step    460 | loss 1.5884 | lr 3.00e-04 | grad 1.41 | tok/s 14287
step    470 | loss 1.5916 | lr 3.00e-04 | grad 2.44 | tok/s 14223
step    480 | loss 1.5569 | lr 3.00e-04 | grad 2.31 | tok/s 14376
step    490 | loss 1.6790 | lr 3.00e-04 | grad 1.59 | tok/s 13240
step    500 | loss 1.8264 | lr 3.00e-04 | grad 2.66 | tok/s 14135
step    510 | loss 1.6491 | lr 3.00e-04 | grad 1.76 | tok/s 13425
step    520 | loss 1.5201 | lr 3.00e-04 | grad 1.60 | tok/s 14185
step    530 | loss 1.7318 | lr 3.00e-04 | grad 1.70 | tok/s 14150
step    540 | loss 1.5779 | lr 3.00e-04 | grad 1.71 | tok/s 13664
step    550 | loss 1.3615 | lr 3.00e-04 | grad 1.71 | tok/s 12640
step    560 | loss 1.4163 | lr 3.00e-04 | grad 1.38 | tok/s 14720
step    570 | loss 1.3407 | lr 3.00e-04 | grad 1.24 | tok/s 14703
step    580 | loss 1.2886 | lr 3.00e-04 | grad 1.52 | tok/s 14720
step    590 | loss 1.3483 | lr 3.00e-04 | grad 1.35 | tok/s 14718
step    600 | loss 1.2803 | lr 3.00e-04 | grad 1.21 | tok/s 14640
step    610 | loss 1.3012 | lr 3.00e-04 | grad 1.36 | tok/s 14708
step    620 | loss 1.4951 | lr 3.00e-04 | grad 5.03 | tok/s 14466
step    630 | loss 1.5764 | lr 3.00e-04 | grad 1.87 | tok/s 13905
step    640 | loss 1.6748 | lr 3.00e-04 | grad 1.39 | tok/s 13918
step    650 | loss 1.5481 | lr 3.00e-04 | grad 1.80 | tok/s 14173
step    660 | loss 1.6740 | lr 3.00e-04 | grad 2.14 | tok/s 14514
step    670 | loss 1.5931 | lr 3.00e-04 | grad 2.58 | tok/s 12980
step    680 | loss 1.5677 | lr 3.00e-04 | grad 1.95 | tok/s 13582
step    690 | loss 1.6052 | lr 3.00e-04 | grad 1.36 | tok/s 13854
step    700 | loss 1.4740 | lr 3.00e-04 | grad 2.83 | tok/s 13947
step    710 | loss 1.6177 | lr 3.00e-04 | grad 1.66 | tok/s 14043
step    720 | loss 1.2464 | lr 3.00e-04 | grad 1.44 | tok/s 14386
step    730 | loss 1.5753 | lr 3.00e-04 | grad 1.55 | tok/s 14032
step    740 | loss 1.7950 | lr 3.00e-04 | grad 3.48 | tok/s 14568
step    750 | loss 1.4673 | lr 3.00e-04 | grad 2.77 | tok/s 14579
step    760 | loss 1.5606 | lr 3.00e-04 | grad 1.58 | tok/s 14428
step    770 | loss 1.5458 | lr 3.00e-04 | grad 1.38 | tok/s 14086
step    780 | loss 1.4724 | lr 3.00e-04 | grad 1.43 | tok/s 14195
step    790 | loss 1.7348 | lr 3.00e-04 | grad 2.28 | tok/s 14497
step    800 | loss 1.1176 | lr 3.00e-04 | grad 1.59 | tok/s 14249
step    810 | loss 1.4785 | lr 3.00e-04 | grad 2.30 | tok/s 13601
step    820 | loss 1.4069 | lr 3.00e-04 | grad 1.37 | tok/s 14063
step    830 | loss 1.4150 | lr 3.00e-04 | grad 1.33 | tok/s 14073
step    840 | loss 1.7158 | lr 3.00e-04 | grad 2.75 | tok/s 13747
step    850 | loss 1.5029 | lr 3.00e-04 | grad 1.77 | tok/s 14129
step    860 | loss 1.5815 | lr 3.00e-04 | grad 2.00 | tok/s 14551
step    870 | loss 1.4397 | lr 3.00e-04 | grad 2.00 | tok/s 14420
step    880 | loss 1.5940 | lr 3.00e-04 | grad 1.52 | tok/s 14174
step    890 | loss 1.4800 | lr 3.00e-04 | grad 1.67 | tok/s 14125
step    900 | loss 1.5565 | lr 3.00e-04 | grad 1.88 | tok/s 13475
step    910 | loss 1.4940 | lr 3.00e-04 | grad 1.90 | tok/s 14132
step    920 | loss 1.5087 | lr 3.00e-04 | grad 1.54 | tok/s 14082
step    930 | loss 1.3970 | lr 3.00e-04 | grad 1.57 | tok/s 14095
step    940 | loss 1.3691 | lr 3.00e-04 | grad 1.78 | tok/s 13735
step    950 | loss 1.4540 | lr 3.00e-04 | grad 1.43 | tok/s 13883
step    960 | loss 1.4442 | lr 3.00e-04 | grad 1.67 | tok/s 14234
step    970 | loss 1.6674 | lr 3.00e-04 | grad 4.00 | tok/s 14241
step    980 | loss 1.7597 | lr 3.00e-04 | grad 2.70 | tok/s 14593
step    990 | loss 1.5024 | lr 3.00e-04 | grad 1.55 | tok/s 14148
step   1000 | loss 1.5792 | lr 3.00e-04 | grad 1.57 | tok/s 14197
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5792.pt
step   1010 | loss 1.2672 | lr 3.00e-04 | grad 2.73 | tok/s 9158
step   1020 | loss 1.2177 | lr 3.00e-04 | grad 1.61 | tok/s 14867
step   1030 | loss 1.6389 | lr 3.00e-04 | grad 2.17 | tok/s 13965
step   1040 | loss 2.1024 | lr 3.00e-04 | grad 1.97 | tok/s 14603

Training complete! Final step: 1047
