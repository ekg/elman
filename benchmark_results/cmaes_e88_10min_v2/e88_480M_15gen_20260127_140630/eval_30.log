Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_30/levelE88_100m_20260127_143730
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 493,310,384 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.4029 | lr 3.00e-04 | grad 10.75 | tok/s 5218
step     20 | loss 2.6180 | lr 3.00e-04 | grad 4.38 | tok/s 10247
step     30 | loss 2.5152 | lr 3.00e-04 | grad 2.30 | tok/s 10387
step     40 | loss 2.3435 | lr 3.00e-04 | grad 2.47 | tok/s 9945
step     50 | loss 3.0124 | lr 3.00e-04 | grad 12.19 | tok/s 10107
step     60 | loss 2.1031 | lr 3.00e-04 | grad 2.47 | tok/s 10410
step     70 | loss 1.9528 | lr 3.00e-04 | grad 3.47 | tok/s 10530
step     80 | loss 5.1435 | lr 3.00e-04 | grad 82.50 | tok/s 10596
step     90 | loss 5.1675 | lr 3.00e-04 | grad 8.75 | tok/s 10769
step    100 | loss 4.4468 | lr 3.00e-04 | grad 10.69 | tok/s 10767
step    110 | loss 4.1416 | lr 3.00e-04 | grad 16.50 | tok/s 10754
step    120 | loss 3.6686 | lr 3.00e-04 | grad 17.50 | tok/s 10762
step    130 | loss 3.2907 | lr 3.00e-04 | grad 19.00 | tok/s 10753
step    140 | loss 2.7043 | lr 3.00e-04 | grad 10.69 | tok/s 10305
step    150 | loss 2.9245 | lr 3.00e-04 | grad 12.81 | tok/s 10768
step    160 | loss 2.3280 | lr 3.00e-04 | grad 9.31 | tok/s 10747
step    170 | loss 2.4657 | lr 3.00e-04 | grad 9.56 | tok/s 10785
step    180 | loss 2.2306 | lr 3.00e-04 | grad 4.41 | tok/s 10762
step    190 | loss 2.3955 | lr 3.00e-04 | grad 5.41 | tok/s 10743
step    200 | loss 2.1045 | lr 3.00e-04 | grad 5.12 | tok/s 10752
step    210 | loss 2.0940 | lr 3.00e-04 | grad 4.09 | tok/s 10750
step    220 | loss 2.1926 | lr 3.00e-04 | grad 2.14 | tok/s 10623
step    230 | loss 2.0645 | lr 3.00e-04 | grad 2.47 | tok/s 10478
step    240 | loss 2.2551 | lr 3.00e-04 | grad 3.12 | tok/s 9979
step    250 | loss 2.0973 | lr 3.00e-04 | grad 1.67 | tok/s 10240
step    260 | loss 1.5974 | lr 3.00e-04 | grad 1.98 | tok/s 10576
step    270 | loss 2.0912 | lr 3.00e-04 | grad 1.84 | tok/s 10425
step    280 | loss 2.2518 | lr 3.00e-04 | grad 3.88 | tok/s 9932
step    290 | loss 1.4164 | lr 3.00e-04 | grad 3.28 | tok/s 10770
step    300 | loss 0.6097 | lr 3.00e-04 | grad 2.47 | tok/s 10771
step    310 | loss 2.4020 | lr 3.00e-04 | grad 2.70 | tok/s 10587
step    320 | loss 1.9698 | lr 3.00e-04 | grad 3.88 | tok/s 10371
step    330 | loss 1.9367 | lr 3.00e-04 | grad 1.99 | tok/s 9999
step    340 | loss 2.2636 | lr 3.00e-04 | grad 1.82 | tok/s 10172
step    350 | loss 1.9006 | lr 3.00e-04 | grad 3.45 | tok/s 10419
step    360 | loss 1.2622 | lr 3.00e-04 | grad 4.12 | tok/s 10661
step    370 | loss 1.8137 | lr 3.00e-04 | grad 1.80 | tok/s 9658
step    380 | loss 1.7833 | lr 3.00e-04 | grad 1.69 | tok/s 10289
step    390 | loss 1.5465 | lr 3.00e-04 | grad 1.34 | tok/s 10748
step    400 | loss 1.5092 | lr 3.00e-04 | grad 1.73 | tok/s 10648
step    410 | loss 1.3027 | lr 3.00e-04 | grad 1.41 | tok/s 10415
step    420 | loss 1.8110 | lr 3.00e-04 | grad 3.11 | tok/s 9949
step    430 | loss 2.1378 | lr 3.00e-04 | grad 2.03 | tok/s 10387
step    440 | loss 2.1360 | lr 3.00e-04 | grad 3.03 | tok/s 10005
step    450 | loss 1.9153 | lr 3.00e-04 | grad 1.89 | tok/s 10360
step    460 | loss 1.7175 | lr 3.00e-04 | grad 2.09 | tok/s 10137
step    470 | loss 1.8251 | lr 3.00e-04 | grad 1.61 | tok/s 10455
step    480 | loss 2.2177 | lr 3.00e-04 | grad 4.75 | tok/s 10490
step    490 | loss 1.7743 | lr 3.00e-04 | grad 1.77 | tok/s 9881
step    500 | loss 1.6823 | lr 3.00e-04 | grad 2.38 | tok/s 10556
step    510 | loss 1.7054 | lr 3.00e-04 | grad 1.59 | tok/s 10703
step    520 | loss 1.6654 | lr 3.00e-04 | grad 1.48 | tok/s 10694
step    530 | loss 1.9022 | lr 3.00e-04 | grad 1.81 | tok/s 10298
step    540 | loss 1.7319 | lr 3.00e-04 | grad 1.54 | tok/s 10268
step    550 | loss 1.5684 | lr 3.00e-04 | grad 2.09 | tok/s 10053
step    560 | loss 1.7195 | lr 3.00e-04 | grad 1.86 | tok/s 9835
step    570 | loss 1.6537 | lr 3.00e-04 | grad 2.67 | tok/s 10060
step    580 | loss 1.5408 | lr 3.00e-04 | grad 1.52 | tok/s 9875
step    590 | loss 1.8469 | lr 3.00e-04 | grad 2.25 | tok/s 10320
step    600 | loss 1.8126 | lr 3.00e-04 | grad 1.62 | tok/s 9939
step    610 | loss 1.6188 | lr 3.00e-04 | grad 1.64 | tok/s 10452
step    620 | loss 1.5391 | lr 3.00e-04 | grad 1.71 | tok/s 9901
step    630 | loss 1.6549 | lr 3.00e-04 | grad 3.17 | tok/s 9984
step    640 | loss 1.7933 | lr 3.00e-04 | grad 1.77 | tok/s 10258
step    650 | loss 1.6521 | lr 3.00e-04 | grad 1.87 | tok/s 10308
step    660 | loss 1.6885 | lr 3.00e-04 | grad 1.66 | tok/s 10353
step    670 | loss 1.9046 | lr 3.00e-04 | grad 6.94 | tok/s 10428
step    680 | loss 1.7112 | lr 3.00e-04 | grad 1.77 | tok/s 10223
step    690 | loss 1.8161 | lr 3.00e-04 | grad 2.38 | tok/s 10577
step    700 | loss 1.4349 | lr 3.00e-04 | grad 2.19 | tok/s 10785
step    710 | loss 1.5724 | lr 3.00e-04 | grad 1.71 | tok/s 10069
step    720 | loss 1.4544 | lr 3.00e-04 | grad 2.86 | tok/s 9935
step    730 | loss 1.3079 | lr 3.00e-04 | grad 1.99 | tok/s 10493
step    740 | loss 1.4924 | lr 3.00e-04 | grad 1.75 | tok/s 10625
step    750 | loss 1.2065 | lr 3.00e-04 | grad 1.84 | tok/s 10800
step    760 | loss 1.1106 | lr 3.00e-04 | grad 1.62 | tok/s 10795
step    770 | loss 1.0502 | lr 3.00e-04 | grad 1.45 | tok/s 10789
step    780 | loss 0.9912 | lr 3.00e-04 | grad 1.48 | tok/s 10804
step    790 | loss 1.1164 | lr 3.00e-04 | grad 2.50 | tok/s 10452
step    800 | loss 1.7953 | lr 3.00e-04 | grad 4.19 | tok/s 10418
step    810 | loss 1.6845 | lr 3.00e-04 | grad 1.57 | tok/s 10357
step    820 | loss 1.6923 | lr 3.00e-04 | grad 2.94 | tok/s 9961
step    830 | loss 1.4838 | lr 3.00e-04 | grad 1.75 | tok/s 10680
step    840 | loss 1.3829 | lr 3.00e-04 | grad 1.69 | tok/s 10790
step    850 | loss 1.5825 | lr 3.00e-04 | grad 1.55 | tok/s 10743
step    860 | loss 1.4734 | lr 3.00e-04 | grad 2.67 | tok/s 10630
step    870 | loss 1.4915 | lr 3.00e-04 | grad 2.03 | tok/s 10245
step    880 | loss 1.6508 | lr 3.00e-04 | grad 1.95 | tok/s 10014
step    890 | loss 1.6590 | lr 3.00e-04 | grad 2.23 | tok/s 10426
step    900 | loss 1.5463 | lr 3.00e-04 | grad 1.92 | tok/s 10450
step    910 | loss 1.4101 | lr 3.00e-04 | grad 2.83 | tok/s 10224
step    920 | loss 1.5072 | lr 3.00e-04 | grad 2.83 | tok/s 10619
step    930 | loss 1.5811 | lr 3.00e-04 | grad 2.73 | tok/s 10144
step    940 | loss 1.3825 | lr 3.00e-04 | grad 1.39 | tok/s 10699
step    950 | loss 1.4724 | lr 3.00e-04 | grad 2.02 | tok/s 10735
step    960 | loss 1.3127 | lr 3.00e-04 | grad 1.86 | tok/s 10754
step    970 | loss 1.7086 | lr 3.00e-04 | grad 2.73 | tok/s 10115
step    980 | loss 1.6175 | lr 3.00e-04 | grad 1.74 | tok/s 10383
step    990 | loss 1.4447 | lr 3.00e-04 | grad 1.57 | tok/s 10586
step   1000 | loss 1.8138 | lr 3.00e-04 | grad 6.19 | tok/s 10140
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8138.pt
step   1010 | loss 1.6143 | lr 3.00e-04 | grad 2.48 | tok/s 5699
step   1020 | loss 1.6262 | lr 3.00e-04 | grad 1.40 | tok/s 9915
step   1030 | loss 1.4463 | lr 3.00e-04 | grad 1.50 | tok/s 10133
step   1040 | loss 1.4696 | lr 3.00e-04 | grad 1.59 | tok/s 10654
step   1050 | loss 1.5945 | lr 3.00e-04 | grad 2.39 | tok/s 9850
step   1060 | loss 1.7070 | lr 3.00e-04 | grad 2.55 | tok/s 10635
step   1070 | loss 1.6384 | lr 3.00e-04 | grad 2.00 | tok/s 10586
step   1080 | loss 1.3850 | lr 3.00e-04 | grad 1.45 | tok/s 9612
step   1090 | loss 1.0972 | lr 3.00e-04 | grad 1.05 | tok/s 10590
step   1100 | loss 1.4132 | lr 3.00e-04 | grad 2.53 | tok/s 10278
step   1110 | loss 1.4431 | lr 3.00e-04 | grad 1.47 | tok/s 10804
step   1120 | loss 1.3243 | lr 3.00e-04 | grad 1.52 | tok/s 10802
step   1130 | loss 1.2775 | lr 3.00e-04 | grad 1.41 | tok/s 10812
step   1140 | loss 1.2655 | lr 3.00e-04 | grad 1.58 | tok/s 10809
step   1150 | loss 1.2833 | lr 3.00e-04 | grad 1.34 | tok/s 10810
step   1160 | loss 1.2028 | lr 3.00e-04 | grad 1.34 | tok/s 10803
step   1170 | loss 1.2280 | lr 3.00e-04 | grad 1.58 | tok/s 10803
step   1180 | loss 1.3278 | lr 3.00e-04 | grad 1.26 | tok/s 10613
step   1190 | loss 1.2050 | lr 3.00e-04 | grad 1.59 | tok/s 10792
step   1200 | loss 1.2040 | lr 3.00e-04 | grad 1.51 | tok/s 10793
step   1210 | loss 1.2551 | lr 3.00e-04 | grad 1.54 | tok/s 10799
step   1220 | loss 1.2713 | lr 3.00e-04 | grad 1.52 | tok/s 10802
step   1230 | loss 1.2379 | lr 3.00e-04 | grad 1.34 | tok/s 10802
step   1240 | loss 1.2061 | lr 3.00e-04 | grad 1.20 | tok/s 10799
step   1250 | loss 1.7401 | lr 3.00e-04 | grad 2.62 | tok/s 10206
step   1260 | loss 1.3498 | lr 3.00e-04 | grad 3.39 | tok/s 10108
step   1270 | loss 1.6393 | lr 3.00e-04 | grad 3.77 | tok/s 10084
step   1280 | loss 1.5881 | lr 3.00e-04 | grad 1.41 | tok/s 10394
step   1290 | loss 1.4549 | lr 3.00e-04 | grad 1.53 | tok/s 10327
step   1300 | loss 1.5008 | lr 3.00e-04 | grad 1.91 | tok/s 10400
step   1310 | loss 1.4401 | lr 3.00e-04 | grad 1.74 | tok/s 10593
step   1320 | loss 1.5632 | lr 3.00e-04 | grad 1.70 | tok/s 10609
step   1330 | loss 1.5678 | lr 3.00e-04 | grad 1.77 | tok/s 10429
step   1340 | loss 1.4685 | lr 3.00e-04 | grad 7.78 | tok/s 10128
step   1350 | loss 1.6862 | lr 3.00e-04 | grad 1.81 | tok/s 9790
step   1360 | loss 1.4774 | lr 3.00e-04 | grad 1.76 | tok/s 10402
step   1370 | loss 1.3922 | lr 3.00e-04 | grad 1.22 | tok/s 10268
step   1380 | loss 1.5913 | lr 3.00e-04 | grad 1.80 | tok/s 9861
step   1390 | loss 1.5021 | lr 3.00e-04 | grad 1.27 | tok/s 10479
step   1400 | loss 1.3805 | lr 3.00e-04 | grad 1.33 | tok/s 10085
step   1410 | loss 1.4290 | lr 3.00e-04 | grad 2.39 | tok/s 10117
step   1420 | loss 1.6316 | lr 3.00e-04 | grad 4.16 | tok/s 10140
step   1430 | loss 1.3348 | lr 3.00e-04 | grad 1.53 | tok/s 10324
step   1440 | loss 1.1314 | lr 3.00e-04 | grad 1.32 | tok/s 10659
step   1450 | loss 1.1480 | lr 3.00e-04 | grad 3.38 | tok/s 10718
step   1460 | loss 1.6084 | lr 3.00e-04 | grad 1.58 | tok/s 10132
step   1470 | loss 1.4916 | lr 3.00e-04 | grad 1.44 | tok/s 10503
step   1480 | loss 1.7702 | lr 3.00e-04 | grad 2.89 | tok/s 10185
step   1490 | loss 1.5237 | lr 3.00e-04 | grad 1.34 | tok/s 10721
step   1500 | loss 1.3219 | lr 3.00e-04 | grad 1.31 | tok/s 10764
step   1510 | loss 1.4804 | lr 3.00e-04 | grad 1.55 | tok/s 10623
step   1520 | loss 1.4066 | lr 3.00e-04 | grad 3.03 | tok/s 10389
step   1530 | loss 1.4128 | lr 3.00e-04 | grad 1.26 | tok/s 10661
step   1540 | loss 1.5815 | lr 3.00e-04 | grad 1.89 | tok/s 9996

Training complete! Final step: 1544
