Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_94/levelE88_100m_20260127_160111
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 490,078,584 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.1998 | lr 3.00e-04 | grad 12.50 | tok/s 5414
step     20 | loss 2.6753 | lr 3.00e-04 | grad 3.94 | tok/s 11478
step     30 | loss 2.5070 | lr 3.00e-04 | grad 2.45 | tok/s 11540
step     40 | loss 2.3139 | lr 3.00e-04 | grad 3.30 | tok/s 11015
step     50 | loss 2.9173 | lr 3.00e-04 | grad 11.38 | tok/s 11194
step     60 | loss 2.0371 | lr 3.00e-04 | grad 2.48 | tok/s 11554
step     70 | loss 1.9031 | lr 3.00e-04 | grad 3.66 | tok/s 11693
step     80 | loss 5.1038 | lr 3.00e-04 | grad 52.00 | tok/s 11815
step     90 | loss 4.9120 | lr 3.00e-04 | grad 7.41 | tok/s 12028
step    100 | loss 4.0478 | lr 3.00e-04 | grad 6.75 | tok/s 11992
step    110 | loss 3.5341 | lr 3.00e-04 | grad 14.69 | tok/s 11933
step    120 | loss 3.1791 | lr 3.00e-04 | grad 10.25 | tok/s 11941
step    130 | loss 2.8299 | lr 3.00e-04 | grad 11.75 | tok/s 11906
step    140 | loss 2.6227 | lr 3.00e-04 | grad 7.66 | tok/s 11908
step    150 | loss 2.5781 | lr 3.00e-04 | grad 8.19 | tok/s 11895
step    160 | loss 2.2499 | lr 3.00e-04 | grad 7.38 | tok/s 11904
step    170 | loss 2.3093 | lr 3.00e-04 | grad 8.31 | tok/s 11135
step    180 | loss 2.1137 | lr 3.00e-04 | grad 4.81 | tok/s 11897
step    190 | loss 2.2704 | lr 3.00e-04 | grad 4.41 | tok/s 11896
step    200 | loss 1.9687 | lr 3.00e-04 | grad 3.89 | tok/s 11888
step    210 | loss 1.9804 | lr 3.00e-04 | grad 4.06 | tok/s 11881
step    220 | loss 2.1213 | lr 3.00e-04 | grad 2.31 | tok/s 11741
step    230 | loss 2.0037 | lr 3.00e-04 | grad 2.33 | tok/s 11580
step    240 | loss 2.2455 | lr 3.00e-04 | grad 3.28 | tok/s 11014
step    250 | loss 2.0812 | lr 3.00e-04 | grad 1.81 | tok/s 11328
step    260 | loss 1.5675 | lr 3.00e-04 | grad 2.05 | tok/s 11689
step    270 | loss 2.0617 | lr 3.00e-04 | grad 1.92 | tok/s 11520
step    280 | loss 2.2333 | lr 3.00e-04 | grad 3.78 | tok/s 11300
step    290 | loss 1.4095 | lr 3.00e-04 | grad 2.78 | tok/s 11915
step    300 | loss 0.5663 | lr 3.00e-04 | grad 2.14 | tok/s 11882
step    310 | loss 2.3860 | lr 3.00e-04 | grad 3.06 | tok/s 11077
step    320 | loss 1.9413 | lr 3.00e-04 | grad 3.94 | tok/s 11437
step    330 | loss 1.9184 | lr 3.00e-04 | grad 2.05 | tok/s 11045
step    340 | loss 2.2358 | lr 3.00e-04 | grad 1.94 | tok/s 11229
step    350 | loss 1.8809 | lr 3.00e-04 | grad 3.16 | tok/s 11522
step    360 | loss 1.2261 | lr 3.00e-04 | grad 5.88 | tok/s 11754
step    370 | loss 1.7963 | lr 3.00e-04 | grad 1.91 | tok/s 10672
step    380 | loss 1.7530 | lr 3.00e-04 | grad 1.84 | tok/s 11375
step    390 | loss 1.5288 | lr 3.00e-04 | grad 1.45 | tok/s 11872
step    400 | loss 1.4815 | lr 3.00e-04 | grad 1.84 | tok/s 11779
step    410 | loss 1.2818 | lr 3.00e-04 | grad 1.45 | tok/s 11478
step    420 | loss 1.7921 | lr 3.00e-04 | grad 3.22 | tok/s 10982
step    430 | loss 2.1282 | lr 3.00e-04 | grad 2.16 | tok/s 11683
step    440 | loss 2.1317 | lr 3.00e-04 | grad 3.06 | tok/s 11043
step    450 | loss 1.8686 | lr 3.00e-04 | grad 2.00 | tok/s 11434
step    460 | loss 1.7109 | lr 3.00e-04 | grad 2.31 | tok/s 11177
step    470 | loss 1.8116 | lr 3.00e-04 | grad 1.70 | tok/s 11538
step    480 | loss 2.1963 | lr 3.00e-04 | grad 5.00 | tok/s 11550
step    490 | loss 1.7618 | lr 3.00e-04 | grad 1.78 | tok/s 10900
step    500 | loss 1.6628 | lr 3.00e-04 | grad 2.45 | tok/s 11633
step    510 | loss 1.6953 | lr 3.00e-04 | grad 1.69 | tok/s 11791
step    520 | loss 1.6517 | lr 3.00e-04 | grad 1.52 | tok/s 11766
step    530 | loss 1.8853 | lr 3.00e-04 | grad 1.84 | tok/s 11322
step    540 | loss 1.7240 | lr 3.00e-04 | grad 1.59 | tok/s 11311
step    550 | loss 1.5595 | lr 3.00e-04 | grad 2.11 | tok/s 11094
step    560 | loss 1.7133 | lr 3.00e-04 | grad 1.95 | tok/s 10782
step    570 | loss 1.6385 | lr 3.00e-04 | grad 2.75 | tok/s 11078
step    580 | loss 1.5283 | lr 3.00e-04 | grad 1.57 | tok/s 11048
step    590 | loss 1.8354 | lr 3.00e-04 | grad 2.34 | tok/s 11324
step    600 | loss 1.7979 | lr 3.00e-04 | grad 1.70 | tok/s 10955
step    610 | loss 1.6086 | lr 3.00e-04 | grad 1.70 | tok/s 11517
step    620 | loss 1.5331 | lr 3.00e-04 | grad 1.80 | tok/s 10907
step    630 | loss 1.6493 | lr 3.00e-04 | grad 3.34 | tok/s 11020
step    640 | loss 1.7812 | lr 3.00e-04 | grad 1.82 | tok/s 11315
step    650 | loss 1.6414 | lr 3.00e-04 | grad 1.87 | tok/s 11356
step    660 | loss 1.6811 | lr 3.00e-04 | grad 1.68 | tok/s 11405
step    670 | loss 1.8726 | lr 3.00e-04 | grad 2.89 | tok/s 11481
step    680 | loss 1.7078 | lr 3.00e-04 | grad 1.82 | tok/s 11238
step    690 | loss 1.8040 | lr 3.00e-04 | grad 2.44 | tok/s 11646
step    700 | loss 1.4245 | lr 3.00e-04 | grad 2.30 | tok/s 11871
step    710 | loss 1.5666 | lr 3.00e-04 | grad 1.77 | tok/s 11064
step    720 | loss 1.4538 | lr 3.00e-04 | grad 2.58 | tok/s 10909
step    730 | loss 1.2881 | lr 3.00e-04 | grad 2.11 | tok/s 11846
step    740 | loss 1.4919 | lr 3.00e-04 | grad 1.77 | tok/s 11690
step    750 | loss 1.2000 | lr 3.00e-04 | grad 1.94 | tok/s 11866
step    760 | loss 1.1030 | lr 3.00e-04 | grad 1.66 | tok/s 10558
step    770 | loss 1.0496 | lr 3.00e-04 | grad 1.49 | tok/s 11927
step    780 | loss 0.9867 | lr 3.00e-04 | grad 1.55 | tok/s 11877
step    790 | loss 1.1115 | lr 3.00e-04 | grad 2.61 | tok/s 11503
step    800 | loss 1.7861 | lr 3.00e-04 | grad 4.22 | tok/s 11437
step    810 | loss 1.6832 | lr 3.00e-04 | grad 1.62 | tok/s 11394
step    820 | loss 1.6802 | lr 3.00e-04 | grad 2.95 | tok/s 10961
step    830 | loss 1.4665 | lr 3.00e-04 | grad 1.87 | tok/s 11762
step    840 | loss 1.3675 | lr 3.00e-04 | grad 1.71 | tok/s 11886
step    850 | loss 1.5683 | lr 3.00e-04 | grad 1.60 | tok/s 11829
step    860 | loss 1.4631 | lr 3.00e-04 | grad 2.75 | tok/s 11710
step    870 | loss 1.4881 | lr 3.00e-04 | grad 2.06 | tok/s 11283
step    880 | loss 1.6507 | lr 3.00e-04 | grad 1.97 | tok/s 11369
step    890 | loss 1.6540 | lr 3.00e-04 | grad 2.28 | tok/s 11501
step    900 | loss 1.5422 | lr 3.00e-04 | grad 2.02 | tok/s 11481
step    910 | loss 1.4011 | lr 3.00e-04 | grad 2.94 | tok/s 11221
step    920 | loss 1.4996 | lr 3.00e-04 | grad 2.86 | tok/s 11664
step    930 | loss 1.5753 | lr 3.00e-04 | grad 2.95 | tok/s 11156
step    940 | loss 1.3683 | lr 3.00e-04 | grad 1.41 | tok/s 11750
step    950 | loss 1.4559 | lr 3.00e-04 | grad 1.95 | tok/s 11822
step    960 | loss 1.3050 | lr 3.00e-04 | grad 1.88 | tok/s 11818
step    970 | loss 1.7094 | lr 3.00e-04 | grad 2.83 | tok/s 11106
step    980 | loss 1.6120 | lr 3.00e-04 | grad 1.89 | tok/s 11407
step    990 | loss 1.4329 | lr 3.00e-04 | grad 1.59 | tok/s 11598
step   1000 | loss 1.8102 | lr 3.00e-04 | grad 6.59 | tok/s 11152
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8102.pt
step   1010 | loss 1.6738 | lr 3.00e-04 | grad 2.73 | tok/s 4594
step   1020 | loss 1.6286 | lr 3.00e-04 | grad 1.80 | tok/s 10464
step   1030 | loss 1.3847 | lr 3.00e-04 | grad 1.60 | tok/s 11527
step   1040 | loss 1.4968 | lr 3.00e-04 | grad 1.72 | tok/s 11455
step   1050 | loss 1.6087 | lr 3.00e-04 | grad 2.11 | tok/s 11119
step   1060 | loss 1.6511 | lr 3.00e-04 | grad 1.68 | tok/s 11651
step   1070 | loss 1.6588 | lr 3.00e-04 | grad 1.92 | tok/s 11387
step   1080 | loss 1.3783 | lr 3.00e-04 | grad 1.89 | tok/s 10729
step   1090 | loss 0.9933 | lr 3.00e-04 | grad 3.92 | tok/s 11873
step   1100 | loss 1.5242 | lr 3.00e-04 | grad 2.06 | tok/s 11386
step   1110 | loss 1.3826 | lr 3.00e-04 | grad 1.34 | tok/s 11875
step   1120 | loss 1.3208 | lr 3.00e-04 | grad 1.74 | tok/s 11867
step   1130 | loss 1.2602 | lr 3.00e-04 | grad 1.50 | tok/s 11914
step   1140 | loss 1.2704 | lr 3.00e-04 | grad 1.59 | tok/s 11908
step   1150 | loss 1.2663 | lr 3.00e-04 | grad 1.34 | tok/s 11927
step   1160 | loss 1.1965 | lr 3.00e-04 | grad 1.33 | tok/s 11898
step   1170 | loss 1.2673 | lr 3.00e-04 | grad 1.52 | tok/s 11856
step   1180 | loss 1.2858 | lr 3.00e-04 | grad 1.94 | tok/s 11864
step   1190 | loss 1.1974 | lr 3.00e-04 | grad 1.44 | tok/s 11939
step   1200 | loss 1.2143 | lr 3.00e-04 | grad 1.43 | tok/s 11863
step   1210 | loss 1.2425 | lr 3.00e-04 | grad 1.55 | tok/s 10776
step   1220 | loss 1.2561 | lr 3.00e-04 | grad 1.48 | tok/s 11867
step   1230 | loss 1.2371 | lr 3.00e-04 | grad 1.32 | tok/s 11878
step   1240 | loss 1.3270 | lr 3.00e-04 | grad 5.59 | tok/s 11652
step   1250 | loss 1.6644 | lr 3.00e-04 | grad 1.81 | tok/s 11271
step   1260 | loss 1.2696 | lr 3.00e-04 | grad 1.82 | tok/s 11119
step   1270 | loss 1.6994 | lr 3.00e-04 | grad 1.78 | tok/s 10962
step   1280 | loss 1.5349 | lr 3.00e-04 | grad 1.43 | tok/s 11601
step   1290 | loss 1.5035 | lr 3.00e-04 | grad 2.02 | tok/s 11420
step   1300 | loss 1.4866 | lr 3.00e-04 | grad 2.30 | tok/s 11243
step   1310 | loss 1.4327 | lr 3.00e-04 | grad 2.56 | tok/s 11776
step   1320 | loss 1.6009 | lr 3.00e-04 | grad 2.58 | tok/s 11644
step   1330 | loss 1.3822 | lr 3.00e-04 | grad 1.82 | tok/s 11680
step   1340 | loss 1.6164 | lr 3.00e-04 | grad 1.66 | tok/s 10838
step   1350 | loss 1.7126 | lr 3.00e-04 | grad 2.34 | tok/s 9893
step   1360 | loss 1.4254 | lr 3.00e-04 | grad 1.40 | tok/s 11564
step   1370 | loss 1.5019 | lr 3.00e-04 | grad 2.91 | tok/s 11303
step   1380 | loss 1.5446 | lr 3.00e-04 | grad 2.23 | tok/s 10851
step   1390 | loss 1.4115 | lr 3.00e-04 | grad 1.46 | tok/s 11227
step   1400 | loss 1.3555 | lr 3.00e-04 | grad 1.56 | tok/s 11343
step   1410 | loss 1.5135 | lr 3.00e-04 | grad 2.91 | tok/s 11145
step   1420 | loss 1.5666 | lr 3.00e-04 | grad 1.80 | tok/s 11149
step   1430 | loss 1.3091 | lr 3.00e-04 | grad 1.76 | tok/s 11281
step   1440 | loss 1.1195 | lr 3.00e-04 | grad 1.45 | tok/s 11869
step   1450 | loss 1.2609 | lr 3.00e-04 | grad 5.72 | tok/s 11724
step   1460 | loss 1.6307 | lr 3.00e-04 | grad 4.28 | tok/s 11050
step   1470 | loss 1.4042 | lr 3.00e-04 | grad 1.38 | tok/s 11787
step   1480 | loss 1.7990 | lr 3.00e-04 | grad 2.97 | tok/s 11674
step   1490 | loss 1.5138 | lr 3.00e-04 | grad 2.75 | tok/s 11824
step   1500 | loss 1.2586 | lr 3.00e-04 | grad 1.41 | tok/s 11858
step   1510 | loss 1.5227 | lr 3.00e-04 | grad 1.71 | tok/s 11671
step   1520 | loss 1.4298 | lr 3.00e-04 | grad 1.80 | tok/s 11423
step   1530 | loss 1.3849 | lr 3.00e-04 | grad 1.54 | tok/s 11502
step   1540 | loss 1.6082 | lr 3.00e-04 | grad 1.73 | tok/s 11266
step   1550 | loss 1.2285 | lr 3.00e-04 | grad 1.69 | tok/s 11760
step   1560 | loss 1.5668 | lr 3.00e-04 | grad 1.63 | tok/s 11193
step   1570 | loss 1.3101 | lr 3.00e-04 | grad 2.36 | tok/s 11699
step   1580 | loss 1.6232 | lr 3.00e-04 | grad 2.84 | tok/s 11697
step   1590 | loss 1.5123 | lr 3.00e-04 | grad 1.49 | tok/s 11134
step   1600 | loss 0.8414 | lr 3.00e-04 | grad 1.03 | tok/s 11878
step   1610 | loss 1.1697 | lr 3.00e-04 | grad 1.49 | tok/s 10955
step   1620 | loss 1.3223 | lr 3.00e-04 | grad 3.09 | tok/s 11225
step   1630 | loss 1.3434 | lr 3.00e-04 | grad 1.60 | tok/s 11485
step   1640 | loss 1.4322 | lr 3.00e-04 | grad 3.44 | tok/s 11155
step   1650 | loss 1.5217 | lr 3.00e-04 | grad 3.22 | tok/s 10616
step   1660 | loss 1.2198 | lr 3.00e-04 | grad 1.31 | tok/s 11851
step   1670 | loss 1.5766 | lr 3.00e-04 | grad 5.84 | tok/s 11249
step   1680 | loss 1.5198 | lr 3.00e-04 | grad 2.03 | tok/s 11003
step   1690 | loss 1.4105 | lr 3.00e-04 | grad 2.80 | tok/s 11431

Training complete! Final step: 1693
