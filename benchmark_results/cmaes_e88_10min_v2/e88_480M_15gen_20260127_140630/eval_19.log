Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_19/levelE88_100m_20260127_142712
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 470,161,988 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.6001 | lr 3.00e-04 | grad 12.25 | tok/s 6580
step     20 | loss 2.7827 | lr 3.00e-04 | grad 2.86 | tok/s 9023
step     30 | loss 3.1446 | lr 3.00e-04 | grad 8.50 | tok/s 9514
step     40 | loss 4.2723 | lr 3.00e-04 | grad 54.00 | tok/s 9686
step     50 | loss 5.0587 | lr 3.00e-04 | grad 29.38 | tok/s 9771
step     60 | loss 4.3564 | lr 3.00e-04 | grad 23.88 | tok/s 9568
step     70 | loss 3.6071 | lr 3.00e-04 | grad 15.31 | tok/s 9732
step     80 | loss 3.1802 | lr 3.00e-04 | grad 10.94 | tok/s 9737
step     90 | loss 2.7104 | lr 3.00e-04 | grad 8.56 | tok/s 9723
step    100 | loss 2.4785 | lr 3.00e-04 | grad 3.06 | tok/s 9714
step    110 | loss 2.4136 | lr 3.00e-04 | grad 2.75 | tok/s 9645
step    120 | loss 2.8111 | lr 3.00e-04 | grad 1.50 | tok/s 9179
step    130 | loss 2.1909 | lr 3.00e-04 | grad 4.69 | tok/s 9390
step    140 | loss 2.4513 | lr 3.00e-04 | grad 7.00 | tok/s 9416
step    150 | loss 1.5650 | lr 3.00e-04 | grad 3.67 | tok/s 9627
step    160 | loss 2.3782 | lr 3.00e-04 | grad 1.75 | tok/s 9327
step    170 | loss 2.3266 | lr 3.00e-04 | grad 1.37 | tok/s 9173
step    180 | loss 2.1149 | lr 3.00e-04 | grad 2.34 | tok/s 9396
step    190 | loss 2.0007 | lr 3.00e-04 | grad 1.48 | tok/s 9230
step    200 | loss 1.7659 | lr 3.00e-04 | grad 1.45 | tok/s 9664
step    210 | loss 1.9508 | lr 3.00e-04 | grad 3.23 | tok/s 9153
step    220 | loss 2.2666 | lr 3.00e-04 | grad 1.77 | tok/s 9257
step    230 | loss 1.9723 | lr 3.00e-04 | grad 2.22 | tok/s 9246
step    240 | loss 2.3479 | lr 3.00e-04 | grad 4.34 | tok/s 9374
step    250 | loss 1.8377 | lr 3.00e-04 | grad 1.26 | tok/s 9317
step    260 | loss 1.9628 | lr 3.00e-04 | grad 2.47 | tok/s 9578
step    270 | loss 1.8812 | lr 3.00e-04 | grad 1.48 | tok/s 9345
step    280 | loss 1.8254 | lr 3.00e-04 | grad 1.45 | tok/s 8784
step    290 | loss 1.7199 | lr 3.00e-04 | grad 1.70 | tok/s 9084
step    300 | loss 2.0176 | lr 3.00e-04 | grad 1.50 | tok/s 9164
step    310 | loss 1.7089 | lr 3.00e-04 | grad 1.48 | tok/s 9112
step    320 | loss 1.9179 | lr 3.00e-04 | grad 2.12 | tok/s 9231
step    330 | loss 1.7574 | lr 3.00e-04 | grad 1.34 | tok/s 9320
step    340 | loss 2.0678 | lr 3.00e-04 | grad 1.62 | tok/s 9162
step    350 | loss 1.8072 | lr 3.00e-04 | grad 1.55 | tok/s 9563
step    360 | loss 1.6135 | lr 3.00e-04 | grad 1.51 | tok/s 9133
step    370 | loss 1.5434 | lr 3.00e-04 | grad 1.32 | tok/s 9636
step    380 | loss 1.2900 | lr 3.00e-04 | grad 1.41 | tok/s 9718
step    390 | loss 1.1779 | lr 3.00e-04 | grad 1.34 | tok/s 9708
step    400 | loss 1.7984 | lr 3.00e-04 | grad 1.44 | tok/s 9196
step    410 | loss 1.7687 | lr 3.00e-04 | grad 1.78 | tok/s 9286
step    420 | loss 1.6873 | lr 3.00e-04 | grad 2.08 | tok/s 9694
step    430 | loss 1.6262 | lr 3.00e-04 | grad 1.45 | tok/s 9537
step    440 | loss 1.7250 | lr 3.00e-04 | grad 1.72 | tok/s 9236
step    450 | loss 1.6521 | lr 3.00e-04 | grad 1.21 | tok/s 9341
step    460 | loss 1.6215 | lr 3.00e-04 | grad 1.66 | tok/s 9488
step    470 | loss 1.5944 | lr 3.00e-04 | grad 2.48 | tok/s 9412
step    480 | loss 1.6301 | lr 3.00e-04 | grad 2.16 | tok/s 9616
step    490 | loss 1.7224 | lr 3.00e-04 | grad 1.84 | tok/s 9237
step    500 | loss 1.8311 | lr 3.00e-04 | grad 1.38 | tok/s 9386
step    510 | loss 1.7034 | lr 3.00e-04 | grad 1.15 | tok/s 8882
step    520 | loss 1.5552 | lr 3.00e-04 | grad 1.71 | tok/s 9393
step    530 | loss 1.7367 | lr 3.00e-04 | grad 1.64 | tok/s 9243
step    540 | loss 1.6283 | lr 3.00e-04 | grad 1.26 | tok/s 9045
step    550 | loss 1.3675 | lr 3.00e-04 | grad 2.16 | tok/s 9424
step    560 | loss 1.4649 | lr 3.00e-04 | grad 1.33 | tok/s 9731
step    570 | loss 1.3678 | lr 3.00e-04 | grad 1.42 | tok/s 9732
step    580 | loss 1.3288 | lr 3.00e-04 | grad 1.10 | tok/s 9729
step    590 | loss 1.3614 | lr 3.00e-04 | grad 1.02 | tok/s 9726
step    600 | loss 1.3036 | lr 3.00e-04 | grad 1.29 | tok/s 9725
step    610 | loss 1.3301 | lr 3.00e-04 | grad 1.17 | tok/s 9722
step    620 | loss 1.3183 | lr 3.00e-04 | grad 1.34 | tok/s 9685
step    630 | loss 1.6413 | lr 3.00e-04 | grad 3.09 | tok/s 9133
step    640 | loss 1.7476 | lr 3.00e-04 | grad 1.34 | tok/s 9271
step    650 | loss 1.5706 | lr 3.00e-04 | grad 1.38 | tok/s 9266
step    660 | loss 1.6202 | lr 3.00e-04 | grad 1.32 | tok/s 9617
step    670 | loss 1.6385 | lr 3.00e-04 | grad 4.09 | tok/s 9292
step    680 | loss 1.6522 | lr 3.00e-04 | grad 1.53 | tok/s 9146
step    690 | loss 1.5829 | lr 3.00e-04 | grad 1.48 | tok/s 9079
step    700 | loss 1.4988 | lr 3.00e-04 | grad 1.12 | tok/s 9275

Training complete! Final step: 704
