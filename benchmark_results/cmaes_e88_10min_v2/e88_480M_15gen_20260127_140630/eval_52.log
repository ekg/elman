Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_52/levelE88_100m_20260127_150848
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,036,416 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3819 | lr 3.00e-04 | grad 32.75 | tok/s 9017
step     20 | loss 3.6233 | lr 3.00e-04 | grad 40.00 | tok/s 19482
step     30 | loss 3.2647 | lr 3.00e-04 | grad 9.19 | tok/s 20655
step     40 | loss 5.3933 | lr 3.00e-04 | grad 39.50 | tok/s 20938
step     50 | loss 4.0909 | lr 3.00e-04 | grad 8.69 | tok/s 21110
step     60 | loss 3.4144 | lr 3.00e-04 | grad 10.00 | tok/s 20932
step     70 | loss 2.9361 | lr 3.00e-04 | grad 10.06 | tok/s 20918
step     80 | loss 2.6143 | lr 3.00e-04 | grad 5.19 | tok/s 20832
step     90 | loss 2.4886 | lr 3.00e-04 | grad 8.12 | tok/s 20739
step    100 | loss 2.3348 | lr 3.00e-04 | grad 5.34 | tok/s 20773
step    110 | loss 2.5305 | lr 3.00e-04 | grad 17.50 | tok/s 20530
step    120 | loss 2.5578 | lr 3.00e-04 | grad 5.12 | tok/s 18780
step    130 | loss 2.0938 | lr 3.00e-04 | grad 4.44 | tok/s 20140
step    140 | loss 2.3642 | lr 3.00e-04 | grad 9.38 | tok/s 20143
step    150 | loss 1.5143 | lr 3.00e-04 | grad 5.12 | tok/s 20554
step    160 | loss 2.2292 | lr 3.00e-04 | grad 3.66 | tok/s 19696
step    170 | loss 2.3217 | lr 3.00e-04 | grad 3.34 | tok/s 19667
step    180 | loss 1.8055 | lr 3.00e-04 | grad 3.64 | tok/s 19538
step    190 | loss 1.8422 | lr 3.00e-04 | grad 4.47 | tok/s 19882
step    200 | loss 1.6002 | lr 3.00e-04 | grad 2.83 | tok/s 20547
step    210 | loss 1.9602 | lr 3.00e-04 | grad 2.78 | tok/s 19431
step    220 | loss 2.3046 | lr 3.00e-04 | grad 14.19 | tok/s 19650
step    230 | loss 2.0025 | lr 3.00e-04 | grad 4.06 | tok/s 16844
step    240 | loss 2.1986 | lr 3.00e-04 | grad 2.86 | tok/s 19732
step    250 | loss 1.7967 | lr 3.00e-04 | grad 2.66 | tok/s 20128
step    260 | loss 1.9118 | lr 3.00e-04 | grad 4.19 | tok/s 20220
step    270 | loss 1.7528 | lr 3.00e-04 | grad 2.59 | tok/s 19714
step    280 | loss 1.8060 | lr 3.00e-04 | grad 7.16 | tok/s 18949
step    290 | loss 1.7479 | lr 3.00e-04 | grad 9.38 | tok/s 19349
step    300 | loss 1.8803 | lr 3.00e-04 | grad 2.05 | tok/s 19476
step    310 | loss 1.6826 | lr 3.00e-04 | grad 3.41 | tok/s 19168
step    320 | loss 1.8505 | lr 3.00e-04 | grad 3.11 | tok/s 19652
step    330 | loss 1.8964 | lr 3.00e-04 | grad 11.12 | tok/s 19721
step    340 | loss 1.9528 | lr 3.00e-04 | grad 3.20 | tok/s 19834
step    350 | loss 1.6539 | lr 3.00e-04 | grad 2.44 | tok/s 20010
step    360 | loss 1.5115 | lr 3.00e-04 | grad 2.08 | tok/s 19612
step    370 | loss 1.4937 | lr 3.00e-04 | grad 2.41 | tok/s 20504
step    380 | loss 1.1939 | lr 3.00e-04 | grad 2.09 | tok/s 20482
step    390 | loss 1.1220 | lr 3.00e-04 | grad 2.48 | tok/s 20423
step    400 | loss 1.8929 | lr 3.00e-04 | grad 3.25 | tok/s 19543
step    410 | loss 1.7600 | lr 3.00e-04 | grad 2.69 | tok/s 19705
step    420 | loss 1.6047 | lr 3.00e-04 | grad 8.62 | tok/s 20466
step    430 | loss 1.5832 | lr 3.00e-04 | grad 2.58 | tok/s 19906
step    440 | loss 1.7657 | lr 3.00e-04 | grad 2.28 | tok/s 19754
step    450 | loss 1.5814 | lr 3.00e-04 | grad 5.19 | tok/s 19625
step    460 | loss 1.6587 | lr 3.00e-04 | grad 3.67 | tok/s 19771
step    470 | loss 1.5862 | lr 3.00e-04 | grad 2.75 | tok/s 20200
step    480 | loss 1.5906 | lr 3.00e-04 | grad 2.50 | tok/s 19960
step    490 | loss 1.6789 | lr 3.00e-04 | grad 1.96 | tok/s 19318
step    500 | loss 1.8793 | lr 3.00e-04 | grad 4.91 | tok/s 19670
step    510 | loss 1.6840 | lr 3.00e-04 | grad 4.72 | tok/s 18282
step    520 | loss 1.4998 | lr 3.00e-04 | grad 2.22 | tok/s 19203
step    530 | loss 1.7947 | lr 3.00e-04 | grad 2.98 | tok/s 20194
step    540 | loss 1.5218 | lr 3.00e-04 | grad 2.20 | tok/s 19129
step    550 | loss 1.4269 | lr 3.00e-04 | grad 2.53 | tok/s 20121
step    560 | loss 1.4215 | lr 3.00e-04 | grad 2.70 | tok/s 20493
step    570 | loss 1.3503 | lr 3.00e-04 | grad 2.22 | tok/s 20553
step    580 | loss 1.2961 | lr 3.00e-04 | grad 2.88 | tok/s 20562
step    590 | loss 1.3487 | lr 3.00e-04 | grad 1.94 | tok/s 20599
step    600 | loss 1.2784 | lr 3.00e-04 | grad 1.97 | tok/s 20665
step    610 | loss 1.3061 | lr 3.00e-04 | grad 2.64 | tok/s 20576
step    620 | loss 1.5316 | lr 3.00e-04 | grad 8.50 | tok/s 20258
step    630 | loss 1.6402 | lr 3.00e-04 | grad 2.91 | tok/s 19577
step    640 | loss 1.6998 | lr 3.00e-04 | grad 2.27 | tok/s 19525
step    650 | loss 1.5677 | lr 3.00e-04 | grad 2.83 | tok/s 19962
step    660 | loss 1.6925 | lr 3.00e-04 | grad 3.89 | tok/s 20393
step    670 | loss 1.6275 | lr 3.00e-04 | grad 3.84 | tok/s 19342
step    680 | loss 1.5934 | lr 3.00e-04 | grad 3.95 | tok/s 19386
step    690 | loss 1.6465 | lr 3.00e-04 | grad 2.34 | tok/s 19201
step    700 | loss 1.4921 | lr 3.00e-04 | grad 4.22 | tok/s 19495
step    710 | loss 1.6612 | lr 3.00e-04 | grad 2.34 | tok/s 19716
step    720 | loss 1.2575 | lr 3.00e-04 | grad 2.45 | tok/s 20055
step    730 | loss 1.6159 | lr 3.00e-04 | grad 2.34 | tok/s 19637
step    740 | loss 1.8112 | lr 3.00e-04 | grad 4.84 | tok/s 20321
step    750 | loss 1.4786 | lr 3.00e-04 | grad 5.06 | tok/s 18805
step    760 | loss 1.5954 | lr 3.00e-04 | grad 2.44 | tok/s 20188
step    770 | loss 1.5769 | lr 3.00e-04 | grad 1.97 | tok/s 19805
step    780 | loss 1.4912 | lr 3.00e-04 | grad 2.67 | tok/s 19372
step    790 | loss 1.7505 | lr 3.00e-04 | grad 3.50 | tok/s 20173
step    800 | loss 1.1344 | lr 3.00e-04 | grad 2.59 | tok/s 20074
step    810 | loss 1.5121 | lr 3.00e-04 | grad 3.55 | tok/s 19114
step    820 | loss 1.4302 | lr 3.00e-04 | grad 2.19 | tok/s 19664
step    830 | loss 1.4305 | lr 3.00e-04 | grad 2.09 | tok/s 17653
step    840 | loss 1.7668 | lr 3.00e-04 | grad 3.80 | tok/s 19196
step    850 | loss 1.5248 | lr 3.00e-04 | grad 2.95 | tok/s 19757
step    860 | loss 1.6097 | lr 3.00e-04 | grad 2.69 | tok/s 20351
step    870 | loss 1.4473 | lr 3.00e-04 | grad 2.75 | tok/s 20192
step    880 | loss 1.6262 | lr 3.00e-04 | grad 2.61 | tok/s 19779
step    890 | loss 1.5082 | lr 3.00e-04 | grad 2.88 | tok/s 19776
step    900 | loss 1.5902 | lr 3.00e-04 | grad 2.77 | tok/s 19607
step    910 | loss 1.5287 | lr 3.00e-04 | grad 2.73 | tok/s 19799
step    920 | loss 1.5462 | lr 3.00e-04 | grad 2.17 | tok/s 19700
step    930 | loss 1.4146 | lr 3.00e-04 | grad 2.48 | tok/s 19748
step    940 | loss 1.3931 | lr 3.00e-04 | grad 2.17 | tok/s 19242
step    950 | loss 1.4804 | lr 3.00e-04 | grad 2.09 | tok/s 19091
step    960 | loss 1.4679 | lr 3.00e-04 | grad 2.84 | tok/s 19855
step    970 | loss 1.7037 | lr 3.00e-04 | grad 5.97 | tok/s 19785
step    980 | loss 1.8284 | lr 3.00e-04 | grad 4.41 | tok/s 20360
step    990 | loss 1.5347 | lr 3.00e-04 | grad 2.58 | tok/s 19810
step   1000 | loss 1.6288 | lr 3.00e-04 | grad 2.78 | tok/s 19830
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6288.pt
step   1010 | loss 1.1745 | lr 3.00e-04 | grad 4.38 | tok/s 7930
step   1020 | loss 1.3880 | lr 3.00e-04 | grad 2.58 | tok/s 20388
step   1030 | loss 1.8988 | lr 3.00e-04 | grad 7.00 | tok/s 19724
step   1040 | loss 2.0063 | lr 3.00e-04 | grad 3.31 | tok/s 20566
step   1050 | loss 1.4776 | lr 3.00e-04 | grad 2.19 | tok/s 19849
step   1060 | loss 1.1626 | lr 3.00e-04 | grad 2.12 | tok/s 19775
step   1070 | loss 1.4052 | lr 3.00e-04 | grad 1.89 | tok/s 20615
step   1080 | loss 1.2565 | lr 3.00e-04 | grad 2.12 | tok/s 20746
step   1090 | loss 1.2640 | lr 3.00e-04 | grad 2.27 | tok/s 20713
step   1100 | loss 1.1958 | lr 3.00e-04 | grad 1.91 | tok/s 20691
step   1110 | loss 1.3388 | lr 3.00e-04 | grad 2.11 | tok/s 20424
step   1120 | loss 1.5709 | lr 3.00e-04 | grad 3.36 | tok/s 15268
step   1130 | loss 1.8946 | lr 3.00e-04 | grad 3.30 | tok/s 20189
step   1140 | loss 1.5333 | lr 3.00e-04 | grad 2.33 | tok/s 20282
step   1150 | loss 1.6154 | lr 3.00e-04 | grad 2.70 | tok/s 18961
step   1160 | loss 1.7413 | lr 3.00e-04 | grad 2.75 | tok/s 18713
step   1170 | loss 1.5124 | lr 3.00e-04 | grad 3.20 | tok/s 19868
step   1180 | loss 1.5246 | lr 3.00e-04 | grad 3.75 | tok/s 20371
step   1190 | loss 1.3572 | lr 3.00e-04 | grad 3.16 | tok/s 19417
step   1200 | loss 1.1965 | lr 3.00e-04 | grad 2.52 | tok/s 20095
step   1210 | loss 1.4380 | lr 3.00e-04 | grad 2.42 | tok/s 19546
step   1220 | loss 1.5295 | lr 3.00e-04 | grad 2.83 | tok/s 19278
step   1230 | loss 1.2363 | lr 3.00e-04 | grad 2.28 | tok/s 19733
step   1240 | loss 1.4322 | lr 3.00e-04 | grad 3.11 | tok/s 19164
step   1250 | loss 1.3821 | lr 3.00e-04 | grad 2.86 | tok/s 20455
step   1260 | loss 1.5183 | lr 3.00e-04 | grad 3.80 | tok/s 19950
step   1270 | loss 1.3327 | lr 3.00e-04 | grad 2.48 | tok/s 20135
step   1280 | loss 1.5147 | lr 3.00e-04 | grad 1.91 | tok/s 18730
step   1290 | loss 1.4445 | lr 3.00e-04 | grad 1.88 | tok/s 16765
step   1300 | loss 1.6543 | lr 3.00e-04 | grad 3.00 | tok/s 18846
step   1310 | loss 1.5074 | lr 3.00e-04 | grad 3.17 | tok/s 20288
step   1320 | loss 1.5917 | lr 3.00e-04 | grad 2.48 | tok/s 20030
step   1330 | loss 1.4429 | lr 3.00e-04 | grad 2.70 | tok/s 19519
step   1340 | loss 1.5745 | lr 3.00e-04 | grad 3.11 | tok/s 20048
step   1350 | loss 1.5370 | lr 3.00e-04 | grad 4.72 | tok/s 19340
step   1360 | loss 1.3044 | lr 3.00e-04 | grad 3.02 | tok/s 19697
step   1370 | loss 1.8055 | lr 3.00e-04 | grad 2.94 | tok/s 20071
step   1380 | loss 1.4180 | lr 3.00e-04 | grad 2.59 | tok/s 19445
step   1390 | loss 1.6254 | lr 3.00e-04 | grad 2.48 | tok/s 19350
step   1400 | loss 1.3013 | lr 3.00e-04 | grad 2.48 | tok/s 19694
step   1410 | loss 1.2919 | lr 3.00e-04 | grad 4.53 | tok/s 19872
step   1420 | loss 1.5419 | lr 3.00e-04 | grad 5.09 | tok/s 19870
step   1430 | loss 1.5364 | lr 3.00e-04 | grad 2.75 | tok/s 18830
step   1440 | loss 1.4503 | lr 3.00e-04 | grad 2.92 | tok/s 14537
step   1450 | loss 1.5388 | lr 3.00e-04 | grad 2.97 | tok/s 16724

Training complete! Final step: 1452
