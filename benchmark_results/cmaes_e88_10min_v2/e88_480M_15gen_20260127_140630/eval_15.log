Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_15/levelE88_100m_20260127_141655
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 466,458,644 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.6024 | lr 3.00e-04 | grad 24.88 | tok/s 7957
step     20 | loss 2.8797 | lr 3.00e-04 | grad 6.78 | tok/s 12571
step     30 | loss 3.2782 | lr 3.00e-04 | grad 13.06 | tok/s 13233
step     40 | loss 4.4961 | lr 3.00e-04 | grad 105.00 | tok/s 13448
step     50 | loss 5.3744 | lr 3.00e-04 | grad 51.25 | tok/s 13578
step     60 | loss 4.6627 | lr 3.00e-04 | grad 41.50 | tok/s 13517
step     70 | loss 3.8214 | lr 3.00e-04 | grad 24.38 | tok/s 13235
step     80 | loss 3.4571 | lr 3.00e-04 | grad 20.75 | tok/s 13483
step     90 | loss 2.9534 | lr 3.00e-04 | grad 16.38 | tok/s 13447
step    100 | loss 2.7309 | lr 3.00e-04 | grad 9.25 | tok/s 13429
step    110 | loss 2.5323 | lr 3.00e-04 | grad 4.16 | tok/s 13315
step    120 | loss 2.8321 | lr 3.00e-04 | grad 2.52 | tok/s 12666
step    130 | loss 2.1715 | lr 3.00e-04 | grad 7.28 | tok/s 12949
step    140 | loss 2.4643 | lr 3.00e-04 | grad 12.31 | tok/s 12682
step    150 | loss 1.5966 | lr 3.00e-04 | grad 5.25 | tok/s 13287
step    160 | loss 2.3662 | lr 3.00e-04 | grad 2.66 | tok/s 12885
step    170 | loss 2.3577 | lr 3.00e-04 | grad 2.09 | tok/s 12666
step    180 | loss 2.1597 | lr 3.00e-04 | grad 3.84 | tok/s 12972
step    190 | loss 2.0009 | lr 3.00e-04 | grad 2.33 | tok/s 12726
step    200 | loss 1.7411 | lr 3.00e-04 | grad 2.05 | tok/s 13329
step    210 | loss 1.9700 | lr 3.00e-04 | grad 4.75 | tok/s 12628
step    220 | loss 2.3179 | lr 3.00e-04 | grad 4.12 | tok/s 12772
step    230 | loss 2.0566 | lr 3.00e-04 | grad 3.16 | tok/s 12748
step    240 | loss 2.4011 | lr 3.00e-04 | grad 6.41 | tok/s 12902
step    250 | loss 1.8356 | lr 3.00e-04 | grad 1.70 | tok/s 12842
step    260 | loss 1.9709 | lr 3.00e-04 | grad 3.61 | tok/s 13210
step    270 | loss 1.8858 | lr 3.00e-04 | grad 1.91 | tok/s 12893
step    280 | loss 1.8357 | lr 3.00e-04 | grad 2.03 | tok/s 12094
step    290 | loss 1.7286 | lr 3.00e-04 | grad 2.33 | tok/s 12498
step    300 | loss 2.0371 | lr 3.00e-04 | grad 2.19 | tok/s 12632
step    310 | loss 1.7158 | lr 3.00e-04 | grad 2.02 | tok/s 12578
step    320 | loss 1.9356 | lr 3.00e-04 | grad 3.33 | tok/s 12694
step    330 | loss 1.7735 | lr 3.00e-04 | grad 1.84 | tok/s 12857
step    340 | loss 2.1235 | lr 3.00e-04 | grad 2.14 | tok/s 12782
step    350 | loss 1.8324 | lr 3.00e-04 | grad 1.97 | tok/s 13164
step    360 | loss 1.6313 | lr 3.00e-04 | grad 2.17 | tok/s 12601
step    370 | loss 1.5402 | lr 3.00e-04 | grad 1.67 | tok/s 13320
step    380 | loss 1.2781 | lr 3.00e-04 | grad 1.55 | tok/s 13409
step    390 | loss 1.1753 | lr 3.00e-04 | grad 1.45 | tok/s 13430
step    400 | loss 1.8206 | lr 3.00e-04 | grad 1.80 | tok/s 12715
step    410 | loss 1.8099 | lr 3.00e-04 | grad 2.25 | tok/s 12820
step    420 | loss 1.6796 | lr 3.00e-04 | grad 3.22 | tok/s 13130
step    430 | loss 1.6573 | lr 3.00e-04 | grad 1.87 | tok/s 13175
step    440 | loss 1.7540 | lr 3.00e-04 | grad 2.25 | tok/s 12788
step    450 | loss 1.6728 | lr 3.00e-04 | grad 1.54 | tok/s 12921
step    460 | loss 1.6473 | lr 3.00e-04 | grad 2.11 | tok/s 13118
step    470 | loss 1.6156 | lr 3.00e-04 | grad 3.88 | tok/s 13009
step    480 | loss 1.6623 | lr 3.00e-04 | grad 2.73 | tok/s 13300
step    490 | loss 1.7480 | lr 3.00e-04 | grad 2.33 | tok/s 12774
step    500 | loss 1.8588 | lr 3.00e-04 | grad 1.83 | tok/s 12982
step    510 | loss 1.7284 | lr 3.00e-04 | grad 1.52 | tok/s 12389
step    520 | loss 1.5700 | lr 3.00e-04 | grad 2.09 | tok/s 12990
step    530 | loss 1.7603 | lr 3.00e-04 | grad 2.22 | tok/s 12770
step    540 | loss 1.6431 | lr 3.00e-04 | grad 1.66 | tok/s 12519
step    550 | loss 1.4028 | lr 3.00e-04 | grad 2.69 | tok/s 13081
step    560 | loss 1.4800 | lr 3.00e-04 | grad 1.65 | tok/s 13454
step    570 | loss 1.3762 | lr 3.00e-04 | grad 1.66 | tok/s 13449
step    580 | loss 1.3344 | lr 3.00e-04 | grad 1.30 | tok/s 13322
step    590 | loss 1.3714 | lr 3.00e-04 | grad 1.30 | tok/s 13452
step    600 | loss 1.3090 | lr 3.00e-04 | grad 1.62 | tok/s 13453
step    610 | loss 1.3386 | lr 3.00e-04 | grad 1.51 | tok/s 13453
step    620 | loss 1.3281 | lr 3.00e-04 | grad 1.66 | tok/s 13411
step    630 | loss 1.7076 | lr 3.00e-04 | grad 4.19 | tok/s 12679
step    640 | loss 1.7829 | lr 3.00e-04 | grad 1.80 | tok/s 12842
step    650 | loss 1.5914 | lr 3.00e-04 | grad 1.77 | tok/s 12820
step    660 | loss 1.6397 | lr 3.00e-04 | grad 1.79 | tok/s 13343
step    670 | loss 1.6738 | lr 3.00e-04 | grad 5.25 | tok/s 12872
step    680 | loss 1.6828 | lr 3.00e-04 | grad 1.98 | tok/s 12663
step    690 | loss 1.6238 | lr 3.00e-04 | grad 1.94 | tok/s 12578
step    700 | loss 1.5240 | lr 3.00e-04 | grad 1.43 | tok/s 12776
step    710 | loss 1.6970 | lr 3.00e-04 | grad 2.64 | tok/s 12648
step    720 | loss 1.3466 | lr 3.00e-04 | grad 1.52 | tok/s 13125
step    730 | loss 1.5179 | lr 3.00e-04 | grad 1.52 | tok/s 12924
step    740 | loss 1.8646 | lr 3.00e-04 | grad 3.48 | tok/s 13276
step    750 | loss 1.5887 | lr 3.00e-04 | grad 1.48 | tok/s 13443
step    760 | loss 1.5845 | lr 3.00e-04 | grad 2.88 | tok/s 13140
step    770 | loss 1.6331 | lr 3.00e-04 | grad 1.88 | tok/s 12926
step    780 | loss 1.5270 | lr 3.00e-04 | grad 1.84 | tok/s 12997
step    790 | loss 1.7168 | lr 3.00e-04 | grad 4.81 | tok/s 13247
step    800 | loss 1.3672 | lr 3.00e-04 | grad 1.20 | tok/s 13073
step    810 | loss 1.3560 | lr 3.00e-04 | grad 2.89 | tok/s 12653
step    820 | loss 1.4606 | lr 3.00e-04 | grad 2.05 | tok/s 12895
step    830 | loss 1.5407 | lr 3.00e-04 | grad 1.31 | tok/s 12720
step    840 | loss 1.6648 | lr 3.00e-04 | grad 1.54 | tok/s 12665
step    850 | loss 1.6091 | lr 3.00e-04 | grad 1.58 | tok/s 12946
step    860 | loss 1.6446 | lr 3.00e-04 | grad 2.47 | tok/s 13166
step    870 | loss 1.4628 | lr 3.00e-04 | grad 1.88 | tok/s 13236
step    880 | loss 1.6283 | lr 3.00e-04 | grad 1.68 | tok/s 12965
step    890 | loss 1.5267 | lr 3.00e-04 | grad 1.35 | tok/s 12902
step    900 | loss 1.5824 | lr 3.00e-04 | grad 1.91 | tok/s 12862
step    910 | loss 1.5948 | lr 3.00e-04 | grad 5.94 | tok/s 12710
step    920 | loss 1.5244 | lr 3.00e-04 | grad 1.63 | tok/s 12888
step    930 | loss 1.4261 | lr 3.00e-04 | grad 1.88 | tok/s 12845
step    940 | loss 1.3998 | lr 3.00e-04 | grad 1.84 | tok/s 12742
step    950 | loss 1.5384 | lr 3.00e-04 | grad 2.19 | tok/s 12538
step    960 | loss 1.4858 | lr 3.00e-04 | grad 1.37 | tok/s 12853
step    970 | loss 1.5090 | lr 3.00e-04 | grad 1.52 | tok/s 12865

Training complete! Final step: 972
