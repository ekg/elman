Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_75/levelE88_100m_20260127_154024
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,168,708 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1569 | lr 3.00e-04 | grad 59.50 | tok/s 7989
step     20 | loss 3.0907 | lr 3.00e-04 | grad 33.50 | tok/s 16323
step     30 | loss 3.0718 | lr 3.00e-04 | grad 7.16 | tok/s 17747
step     40 | loss 5.2322 | lr 3.00e-04 | grad 81.00 | tok/s 17975
step     50 | loss 4.7961 | lr 3.00e-04 | grad 22.62 | tok/s 18137
step     60 | loss 3.8089 | lr 3.00e-04 | grad 16.75 | tok/s 18104
step     70 | loss 2.9741 | lr 3.00e-04 | grad 10.81 | tok/s 18033
step     80 | loss 2.7168 | lr 3.00e-04 | grad 7.69 | tok/s 17965
step     90 | loss 2.5098 | lr 3.00e-04 | grad 6.28 | tok/s 17929
step    100 | loss 2.3797 | lr 3.00e-04 | grad 5.88 | tok/s 17895
step    110 | loss 2.5414 | lr 3.00e-04 | grad 15.75 | tok/s 17678
step    120 | loss 2.5993 | lr 3.00e-04 | grad 4.62 | tok/s 16778
step    130 | loss 2.1442 | lr 3.00e-04 | grad 4.03 | tok/s 17354
step    140 | loss 2.4626 | lr 3.00e-04 | grad 10.69 | tok/s 17420
step    150 | loss 1.5275 | lr 3.00e-04 | grad 4.66 | tok/s 17735
step    160 | loss 2.2924 | lr 3.00e-04 | grad 2.92 | tok/s 16952
step    170 | loss 2.3402 | lr 3.00e-04 | grad 3.00 | tok/s 16406
step    180 | loss 1.8458 | lr 3.00e-04 | grad 3.62 | tok/s 17063
step    190 | loss 1.8978 | lr 3.00e-04 | grad 3.06 | tok/s 17174
step    200 | loss 1.6608 | lr 3.00e-04 | grad 2.42 | tok/s 17707
step    210 | loss 1.9943 | lr 3.00e-04 | grad 2.64 | tok/s 16776
step    220 | loss 2.3297 | lr 3.00e-04 | grad 15.81 | tok/s 16987
step    230 | loss 1.9943 | lr 3.00e-04 | grad 4.31 | tok/s 16773
step    240 | loss 2.2311 | lr 3.00e-04 | grad 2.41 | tok/s 17131
step    250 | loss 1.8168 | lr 3.00e-04 | grad 2.89 | tok/s 17144
step    260 | loss 1.9467 | lr 3.00e-04 | grad 3.42 | tok/s 17506
step    270 | loss 1.7946 | lr 3.00e-04 | grad 2.14 | tok/s 16942
step    280 | loss 1.8031 | lr 3.00e-04 | grad 2.02 | tok/s 16209
step    290 | loss 1.6855 | lr 3.00e-04 | grad 2.55 | tok/s 16506
step    300 | loss 2.0361 | lr 3.00e-04 | grad 2.50 | tok/s 16823
step    310 | loss 1.6964 | lr 3.00e-04 | grad 3.06 | tok/s 15952
step    320 | loss 1.8841 | lr 3.00e-04 | grad 3.00 | tok/s 16861
step    330 | loss 1.7504 | lr 3.00e-04 | grad 2.23 | tok/s 17008
step    340 | loss 2.1046 | lr 3.00e-04 | grad 2.94 | tok/s 17036
step    350 | loss 1.7204 | lr 3.00e-04 | grad 2.17 | tok/s 17501
step    360 | loss 1.5863 | lr 3.00e-04 | grad 1.77 | tok/s 16694
step    370 | loss 1.4847 | lr 3.00e-04 | grad 1.94 | tok/s 17566
step    380 | loss 1.2328 | lr 3.00e-04 | grad 2.22 | tok/s 17720
step    390 | loss 1.1288 | lr 3.00e-04 | grad 1.90 | tok/s 17705
step    400 | loss 1.8599 | lr 3.00e-04 | grad 2.42 | tok/s 16766
step    410 | loss 1.7851 | lr 3.00e-04 | grad 2.52 | tok/s 16941
step    420 | loss 1.6276 | lr 3.00e-04 | grad 10.06 | tok/s 17665
step    430 | loss 1.6237 | lr 3.00e-04 | grad 2.56 | tok/s 17241
step    440 | loss 1.7457 | lr 3.00e-04 | grad 2.83 | tok/s 16984
step    450 | loss 1.6418 | lr 3.00e-04 | grad 3.17 | tok/s 16945
step    460 | loss 1.6220 | lr 3.00e-04 | grad 1.91 | tok/s 17155
step    470 | loss 1.6436 | lr 3.00e-04 | grad 3.34 | tok/s 17323
step    480 | loss 1.5930 | lr 3.00e-04 | grad 3.00 | tok/s 17308
step    490 | loss 1.7203 | lr 3.00e-04 | grad 2.09 | tok/s 17041
step    500 | loss 1.8770 | lr 3.00e-04 | grad 3.36 | tok/s 17038
step    510 | loss 1.6848 | lr 3.00e-04 | grad 2.33 | tok/s 16222
step    520 | loss 1.5432 | lr 3.00e-04 | grad 2.05 | tok/s 17097
step    530 | loss 1.7602 | lr 3.00e-04 | grad 2.16 | tok/s 17059
step    540 | loss 1.6037 | lr 3.00e-04 | grad 2.22 | tok/s 16429
step    550 | loss 1.4141 | lr 3.00e-04 | grad 2.03 | tok/s 17313
step    560 | loss 1.4377 | lr 3.00e-04 | grad 2.25 | tok/s 17737
step    570 | loss 1.3573 | lr 3.00e-04 | grad 1.83 | tok/s 17703
step    580 | loss 1.3083 | lr 3.00e-04 | grad 1.72 | tok/s 17740
step    590 | loss 1.3664 | lr 3.00e-04 | grad 2.20 | tok/s 17736
step    600 | loss 1.2945 | lr 3.00e-04 | grad 1.87 | tok/s 17740
step    610 | loss 1.3190 | lr 3.00e-04 | grad 1.66 | tok/s 17720
step    620 | loss 1.4105 | lr 3.00e-04 | grad 4.75 | tok/s 17435
step    630 | loss 1.7277 | lr 3.00e-04 | grad 3.61 | tok/s 16804
step    640 | loss 1.7279 | lr 3.00e-04 | grad 2.45 | tok/s 16881
step    650 | loss 1.5760 | lr 3.00e-04 | grad 2.98 | tok/s 16925
step    660 | loss 1.6863 | lr 3.00e-04 | grad 3.67 | tok/s 17491
step    670 | loss 1.6087 | lr 3.00e-04 | grad 2.78 | tok/s 16744
step    680 | loss 1.6496 | lr 3.00e-04 | grad 1.56 | tok/s 16677
step    690 | loss 1.6596 | lr 3.00e-04 | grad 3.31 | tok/s 16763
step    700 | loss 1.4831 | lr 3.00e-04 | grad 1.69 | tok/s 16833
step    710 | loss 1.7033 | lr 3.00e-04 | grad 2.69 | tok/s 16717
step    720 | loss 1.3021 | lr 3.00e-04 | grad 1.85 | tok/s 17309
step    730 | loss 1.5918 | lr 3.00e-04 | grad 4.44 | tok/s 16931
step    740 | loss 1.8144 | lr 3.00e-04 | grad 4.81 | tok/s 16488
step    750 | loss 1.5043 | lr 3.00e-04 | grad 1.89 | tok/s 17702
step    760 | loss 1.6382 | lr 3.00e-04 | grad 2.59 | tok/s 17274
step    770 | loss 1.6102 | lr 3.00e-04 | grad 2.14 | tok/s 17023
step    780 | loss 1.4978 | lr 3.00e-04 | grad 1.84 | tok/s 17156
step    790 | loss 1.7193 | lr 3.00e-04 | grad 3.12 | tok/s 17492
step    800 | loss 1.2213 | lr 3.00e-04 | grad 1.67 | tok/s 17263
step    810 | loss 1.4466 | lr 3.00e-04 | grad 2.38 | tok/s 16609
step    820 | loss 1.5101 | lr 3.00e-04 | grad 4.69 | tok/s 17031
step    830 | loss 1.4529 | lr 3.00e-04 | grad 1.75 | tok/s 16768
step    840 | loss 1.6929 | lr 3.00e-04 | grad 2.09 | tok/s 16523
step    850 | loss 1.5849 | lr 3.00e-04 | grad 2.42 | tok/s 17046
step    860 | loss 1.6409 | lr 3.00e-04 | grad 3.05 | tok/s 17471
step    870 | loss 1.4422 | lr 3.00e-04 | grad 2.38 | tok/s 17389
step    880 | loss 1.6216 | lr 3.00e-04 | grad 1.79 | tok/s 17053
step    890 | loss 1.5341 | lr 3.00e-04 | grad 2.42 | tok/s 17038
step    900 | loss 1.6149 | lr 3.00e-04 | grad 2.95 | tok/s 16866
step    910 | loss 1.5405 | lr 3.00e-04 | grad 3.77 | tok/s 17016
step    920 | loss 1.5172 | lr 3.00e-04 | grad 2.03 | tok/s 16910
step    930 | loss 1.4385 | lr 3.00e-04 | grad 2.38 | tok/s 17009
step    940 | loss 1.4050 | lr 3.00e-04 | grad 3.89 | tok/s 16662
step    950 | loss 1.4888 | lr 3.00e-04 | grad 2.08 | tok/s 16741
step    960 | loss 1.4730 | lr 3.00e-04 | grad 1.96 | tok/s 16955
step    970 | loss 1.6103 | lr 3.00e-04 | grad 5.72 | tok/s 16986
step    980 | loss 1.8787 | lr 3.00e-04 | grad 2.94 | tok/s 17644
step    990 | loss 1.6039 | lr 3.00e-04 | grad 1.66 | tok/s 16990
step   1000 | loss 1.6341 | lr 3.00e-04 | grad 1.84 | tok/s 16447
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6341.pt
step   1010 | loss 1.3437 | lr 3.00e-04 | grad 2.12 | tok/s 10016
step   1020 | loss 1.2255 | lr 3.00e-04 | grad 1.51 | tok/s 17926
step   1030 | loss 1.6349 | lr 3.00e-04 | grad 2.14 | tok/s 16945
step   1040 | loss 2.2164 | lr 3.00e-04 | grad 3.59 | tok/s 17521
step   1050 | loss 1.5213 | lr 3.00e-04 | grad 2.03 | tok/s 17266
step   1060 | loss 1.1459 | lr 3.00e-04 | grad 1.47 | tok/s 17568
step   1070 | loss 1.5245 | lr 3.00e-04 | grad 2.00 | tok/s 17214
step   1080 | loss 1.2899 | lr 3.00e-04 | grad 1.77 | tok/s 17808
step   1090 | loss 1.2627 | lr 3.00e-04 | grad 1.73 | tok/s 17804
step   1100 | loss 1.2161 | lr 3.00e-04 | grad 1.83 | tok/s 17805
step   1110 | loss 1.2063 | lr 3.00e-04 | grad 1.73 | tok/s 17788
step   1120 | loss 1.5351 | lr 3.00e-04 | grad 3.91 | tok/s 17308
step   1130 | loss 1.6965 | lr 3.00e-04 | grad 3.27 | tok/s 17489
step   1140 | loss 1.7423 | lr 3.00e-04 | grad 1.71 | tok/s 17690
step   1150 | loss 1.6891 | lr 3.00e-04 | grad 2.92 | tok/s 16817
step   1160 | loss 1.8192 | lr 3.00e-04 | grad 6.91 | tok/s 17056
step   1170 | loss 1.4847 | lr 3.00e-04 | grad 1.91 | tok/s 16802
step   1180 | loss 1.3897 | lr 3.00e-04 | grad 2.41 | tok/s 17416
step   1190 | loss 1.6021 | lr 3.00e-04 | grad 4.59 | tok/s 17794
step   1200 | loss 1.1611 | lr 3.00e-04 | grad 3.19 | tok/s 17804
step   1210 | loss 1.5014 | lr 3.00e-04 | grad 2.56 | tok/s 16577
step   1220 | loss 1.3989 | lr 3.00e-04 | grad 1.76 | tok/s 17198
step   1230 | loss 1.3590 | lr 3.00e-04 | grad 1.42 | tok/s 17618
step   1240 | loss 1.3431 | lr 3.00e-04 | grad 1.70 | tok/s 17296
step   1250 | loss 1.5631 | lr 3.00e-04 | grad 5.81 | tok/s 16071
step   1260 | loss 1.4479 | lr 3.00e-04 | grad 2.31 | tok/s 17527
step   1270 | loss 1.4203 | lr 3.00e-04 | grad 1.57 | tok/s 17162

Training complete! Final step: 1271
