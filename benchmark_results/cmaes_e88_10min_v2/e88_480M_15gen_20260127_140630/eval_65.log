Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_65/levelE88_100m_20260127_152958
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,168,708 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1525 | lr 3.00e-04 | grad 86.00 | tok/s 8300
step     20 | loss 3.1000 | lr 3.00e-04 | grad 32.25 | tok/s 16792
step     30 | loss 2.9748 | lr 3.00e-04 | grad 6.25 | tok/s 17751
step     40 | loss 5.3097 | lr 3.00e-04 | grad 83.00 | tok/s 17977
step     50 | loss 4.8101 | lr 3.00e-04 | grad 21.00 | tok/s 18098
step     60 | loss 3.8412 | lr 3.00e-04 | grad 16.50 | tok/s 17987
step     70 | loss 2.9447 | lr 3.00e-04 | grad 12.38 | tok/s 17773
step     80 | loss 2.6809 | lr 3.00e-04 | grad 4.69 | tok/s 17667
step     90 | loss 2.4979 | lr 3.00e-04 | grad 6.12 | tok/s 17609
step    100 | loss 2.3820 | lr 3.00e-04 | grad 6.19 | tok/s 17499
step    110 | loss 2.5484 | lr 3.00e-04 | grad 16.12 | tok/s 17115
step    120 | loss 2.5548 | lr 3.00e-04 | grad 4.41 | tok/s 16196
step    130 | loss 2.1114 | lr 3.00e-04 | grad 4.12 | tok/s 16902
step    140 | loss 2.4325 | lr 3.00e-04 | grad 10.31 | tok/s 17130
step    150 | loss 1.6188 | lr 3.00e-04 | grad 4.47 | tok/s 17366
step    160 | loss 2.2800 | lr 3.00e-04 | grad 2.95 | tok/s 16693
step    170 | loss 2.2154 | lr 3.00e-04 | grad 4.25 | tok/s 14284
step    180 | loss 1.8018 | lr 3.00e-04 | grad 2.62 | tok/s 16733
step    190 | loss 1.8586 | lr 3.00e-04 | grad 3.09 | tok/s 16914
step    200 | loss 1.6207 | lr 3.00e-04 | grad 2.73 | tok/s 17410
step    210 | loss 2.0960 | lr 3.00e-04 | grad 4.50 | tok/s 16419
step    220 | loss 2.2174 | lr 3.00e-04 | grad 3.64 | tok/s 16636
step    230 | loss 1.9721 | lr 3.00e-04 | grad 3.25 | tok/s 16721
step    240 | loss 2.2139 | lr 3.00e-04 | grad 2.34 | tok/s 16806
step    250 | loss 1.8303 | lr 3.00e-04 | grad 2.25 | tok/s 17107
step    260 | loss 1.9363 | lr 3.00e-04 | grad 3.91 | tok/s 17167
step    270 | loss 1.7696 | lr 3.00e-04 | grad 2.17 | tok/s 16713
step    280 | loss 1.8232 | lr 3.00e-04 | grad 5.97 | tok/s 16058
step    290 | loss 1.7590 | lr 3.00e-04 | grad 8.06 | tok/s 16427
step    300 | loss 1.8977 | lr 3.00e-04 | grad 2.05 | tok/s 16571
step    310 | loss 1.6993 | lr 3.00e-04 | grad 2.95 | tok/s 15373
step    320 | loss 1.8590 | lr 3.00e-04 | grad 2.69 | tok/s 16682
step    330 | loss 1.9147 | lr 3.00e-04 | grad 9.88 | tok/s 16695
step    340 | loss 1.9608 | lr 3.00e-04 | grad 2.69 | tok/s 16758
step    350 | loss 1.6901 | lr 3.00e-04 | grad 2.05 | tok/s 17024
step    360 | loss 1.5279 | lr 3.00e-04 | grad 1.74 | tok/s 16735
step    370 | loss 1.5031 | lr 3.00e-04 | grad 2.17 | tok/s 17404
step    380 | loss 1.2096 | lr 3.00e-04 | grad 1.84 | tok/s 17523
step    390 | loss 1.1302 | lr 3.00e-04 | grad 2.23 | tok/s 17415
step    400 | loss 1.9088 | lr 3.00e-04 | grad 2.64 | tok/s 16695
step    410 | loss 1.7586 | lr 3.00e-04 | grad 2.34 | tok/s 16722
step    420 | loss 1.6271 | lr 3.00e-04 | grad 6.56 | tok/s 17438
step    430 | loss 1.6007 | lr 3.00e-04 | grad 2.22 | tok/s 16922
step    440 | loss 1.7757 | lr 3.00e-04 | grad 2.16 | tok/s 16852
step    450 | loss 1.5908 | lr 3.00e-04 | grad 5.00 | tok/s 16727
step    460 | loss 1.6706 | lr 3.00e-04 | grad 3.00 | tok/s 16855
step    470 | loss 1.5983 | lr 3.00e-04 | grad 2.31 | tok/s 17345
step    480 | loss 1.5981 | lr 3.00e-04 | grad 2.30 | tok/s 16958
step    490 | loss 1.6942 | lr 3.00e-04 | grad 1.70 | tok/s 17009
step    500 | loss 1.8918 | lr 3.00e-04 | grad 4.59 | tok/s 16673
step    510 | loss 1.6909 | lr 3.00e-04 | grad 4.12 | tok/s 15894
step    520 | loss 1.5061 | lr 3.00e-04 | grad 2.06 | tok/s 16563
step    530 | loss 1.8148 | lr 3.00e-04 | grad 2.88 | tok/s 17168
step    540 | loss 1.5457 | lr 3.00e-04 | grad 1.91 | tok/s 16267
step    550 | loss 1.4269 | lr 3.00e-04 | grad 2.00 | tok/s 17025
step    560 | loss 1.4276 | lr 3.00e-04 | grad 1.89 | tok/s 17345
step    570 | loss 1.3513 | lr 3.00e-04 | grad 1.66 | tok/s 17455
step    580 | loss 1.3033 | lr 3.00e-04 | grad 2.14 | tok/s 17476
step    590 | loss 1.3567 | lr 3.00e-04 | grad 1.74 | tok/s 17479
step    600 | loss 1.2902 | lr 3.00e-04 | grad 1.63 | tok/s 17518
step    610 | loss 1.3112 | lr 3.00e-04 | grad 1.95 | tok/s 17482
step    620 | loss 1.5379 | lr 3.00e-04 | grad 6.22 | tok/s 17206
step    630 | loss 1.6341 | lr 3.00e-04 | grad 2.42 | tok/s 16515
step    640 | loss 1.7074 | lr 3.00e-04 | grad 1.84 | tok/s 16399
step    650 | loss 1.5765 | lr 3.00e-04 | grad 2.31 | tok/s 16916
step    660 | loss 1.7038 | lr 3.00e-04 | grad 3.12 | tok/s 17290
step    670 | loss 1.6365 | lr 3.00e-04 | grad 3.42 | tok/s 16394
step    680 | loss 1.6027 | lr 3.00e-04 | grad 2.86 | tok/s 16567
step    690 | loss 1.6427 | lr 3.00e-04 | grad 1.82 | tok/s 16452
step    700 | loss 1.5056 | lr 3.00e-04 | grad 3.69 | tok/s 16561
step    710 | loss 1.6566 | lr 3.00e-04 | grad 2.09 | tok/s 16680
step    720 | loss 1.2686 | lr 3.00e-04 | grad 1.90 | tok/s 17080
step    730 | loss 1.6192 | lr 3.00e-04 | grad 1.96 | tok/s 16745
step    740 | loss 1.8481 | lr 3.00e-04 | grad 4.53 | tok/s 16067
step    750 | loss 1.4966 | lr 3.00e-04 | grad 3.58 | tok/s 17383
step    760 | loss 1.5970 | lr 3.00e-04 | grad 2.08 | tok/s 17165
step    770 | loss 1.5848 | lr 3.00e-04 | grad 1.73 | tok/s 16802
step    780 | loss 1.4981 | lr 3.00e-04 | grad 1.88 | tok/s 16943
step    790 | loss 1.7744 | lr 3.00e-04 | grad 2.98 | tok/s 17273
step    800 | loss 1.1417 | lr 3.00e-04 | grad 2.19 | tok/s 16976
step    810 | loss 1.5168 | lr 3.00e-04 | grad 3.03 | tok/s 16143
step    820 | loss 1.4363 | lr 3.00e-04 | grad 1.80 | tok/s 16710
step    830 | loss 1.4371 | lr 3.00e-04 | grad 1.76 | tok/s 16751
step    840 | loss 1.7618 | lr 3.00e-04 | grad 3.48 | tok/s 16242
step    850 | loss 1.5453 | lr 3.00e-04 | grad 2.31 | tok/s 16776
step    860 | loss 1.6178 | lr 3.00e-04 | grad 2.33 | tok/s 17218
step    870 | loss 1.4654 | lr 3.00e-04 | grad 2.27 | tok/s 16480
step    880 | loss 1.6285 | lr 3.00e-04 | grad 2.16 | tok/s 16807
step    890 | loss 1.5125 | lr 3.00e-04 | grad 2.23 | tok/s 16666
step    900 | loss 1.6062 | lr 3.00e-04 | grad 2.47 | tok/s 16714
step    910 | loss 1.5346 | lr 3.00e-04 | grad 2.44 | tok/s 16768
step    920 | loss 1.5453 | lr 3.00e-04 | grad 1.90 | tok/s 16667
step    930 | loss 1.4232 | lr 3.00e-04 | grad 2.08 | tok/s 16719
step    940 | loss 1.4027 | lr 3.00e-04 | grad 2.12 | tok/s 16269
step    950 | loss 1.4837 | lr 3.00e-04 | grad 1.82 | tok/s 16487
step    960 | loss 1.4724 | lr 3.00e-04 | grad 2.22 | tok/s 16900
step    970 | loss 1.7124 | lr 3.00e-04 | grad 5.41 | tok/s 16852
step    980 | loss 1.8237 | lr 3.00e-04 | grad 3.83 | tok/s 17299
step    990 | loss 1.5421 | lr 3.00e-04 | grad 2.06 | tok/s 16772
step   1000 | loss 1.5678 | lr 3.00e-04 | grad 4.53 | tok/s 14588
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5678.pt
step   1010 | loss 1.1658 | lr 3.00e-04 | grad 3.56 | tok/s 8171
step   1020 | loss 1.3916 | lr 3.00e-04 | grad 2.12 | tok/s 17420
step   1030 | loss 1.9121 | lr 3.00e-04 | grad 5.97 | tok/s 16762
step   1040 | loss 2.0025 | lr 3.00e-04 | grad 2.53 | tok/s 17528
step   1050 | loss 1.4830 | lr 3.00e-04 | grad 1.89 | tok/s 16962
step   1060 | loss 1.1565 | lr 3.00e-04 | grad 1.80 | tok/s 17204
step   1070 | loss 1.4061 | lr 3.00e-04 | grad 1.45 | tok/s 17517
step   1080 | loss 1.2562 | lr 3.00e-04 | grad 1.55 | tok/s 17646
step   1090 | loss 1.2634 | lr 3.00e-04 | grad 1.79 | tok/s 17634
step   1100 | loss 1.2010 | lr 3.00e-04 | grad 1.53 | tok/s 17548
step   1110 | loss 1.3397 | lr 3.00e-04 | grad 1.82 | tok/s 17418
step   1120 | loss 1.6117 | lr 3.00e-04 | grad 5.19 | tok/s 17195
step   1130 | loss 1.9060 | lr 3.00e-04 | grad 2.89 | tok/s 16989
step   1140 | loss 1.5310 | lr 3.00e-04 | grad 1.90 | tok/s 17211
step   1150 | loss 1.6068 | lr 3.00e-04 | grad 2.83 | tok/s 16883
step   1160 | loss 1.7603 | lr 3.00e-04 | grad 2.00 | tok/s 16487
step   1170 | loss 1.5173 | lr 3.00e-04 | grad 2.69 | tok/s 16972
step   1180 | loss 1.5419 | lr 3.00e-04 | grad 3.23 | tok/s 17415
step   1190 | loss 1.3709 | lr 3.00e-04 | grad 2.38 | tok/s 17598
step   1200 | loss 1.1967 | lr 3.00e-04 | grad 2.02 | tok/s 17198
step   1210 | loss 1.4442 | lr 3.00e-04 | grad 1.91 | tok/s 16762
step   1220 | loss 1.5341 | lr 3.00e-04 | grad 2.09 | tok/s 16919
step   1230 | loss 1.2438 | lr 3.00e-04 | grad 1.99 | tok/s 17463
step   1240 | loss 1.4397 | lr 3.00e-04 | grad 2.55 | tok/s 17021

Training complete! Final step: 1249
