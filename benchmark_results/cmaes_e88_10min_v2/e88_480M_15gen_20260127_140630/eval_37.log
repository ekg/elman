Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_37/levelE88_100m_20260127_144751
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 499,073,000 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.2854 | lr 3.00e-04 | grad 8.44 | tok/s 4712
step     20 | loss 2.5680 | lr 3.00e-04 | grad 3.53 | tok/s 10049
step     30 | loss 2.5000 | lr 3.00e-04 | grad 1.99 | tok/s 10086
step     40 | loss 2.2892 | lr 3.00e-04 | grad 2.22 | tok/s 8300
step     50 | loss 2.9209 | lr 3.00e-04 | grad 8.31 | tok/s 9385
step     60 | loss 2.0350 | lr 3.00e-04 | grad 2.50 | tok/s 9677
step     70 | loss 1.9072 | lr 3.00e-04 | grad 3.09 | tok/s 10206
step     80 | loss 4.8691 | lr 3.00e-04 | grad 40.75 | tok/s 9294
step     90 | loss 4.7241 | lr 3.00e-04 | grad 6.47 | tok/s 10475
step    100 | loss 3.9503 | lr 3.00e-04 | grad 5.75 | tok/s 10449
step    110 | loss 3.4622 | lr 3.00e-04 | grad 9.62 | tok/s 10433
step    120 | loss 3.1352 | lr 3.00e-04 | grad 8.62 | tok/s 10411
step    130 | loss 2.8236 | lr 3.00e-04 | grad 9.56 | tok/s 10389
step    140 | loss 2.5221 | lr 3.00e-04 | grad 6.38 | tok/s 10382
step    150 | loss 2.5989 | lr 3.00e-04 | grad 6.38 | tok/s 10368
step    160 | loss 2.1799 | lr 3.00e-04 | grad 5.84 | tok/s 10326
step    170 | loss 2.2882 | lr 3.00e-04 | grad 7.06 | tok/s 10360
step    180 | loss 2.0493 | lr 3.00e-04 | grad 2.98 | tok/s 10274
step    190 | loss 2.2449 | lr 3.00e-04 | grad 5.19 | tok/s 10357
step    200 | loss 1.9313 | lr 3.00e-04 | grad 3.17 | tok/s 10351
step    210 | loss 1.9492 | lr 3.00e-04 | grad 3.50 | tok/s 10356
step    220 | loss 2.0940 | lr 3.00e-04 | grad 2.09 | tok/s 10237
step    230 | loss 2.0129 | lr 3.00e-04 | grad 2.20 | tok/s 10105
step    240 | loss 2.2240 | lr 3.00e-04 | grad 2.95 | tok/s 8343
step    250 | loss 2.0619 | lr 3.00e-04 | grad 1.66 | tok/s 9845
step    260 | loss 1.5640 | lr 3.00e-04 | grad 1.89 | tok/s 10172
step    270 | loss 2.0499 | lr 3.00e-04 | grad 1.75 | tok/s 10025
step    280 | loss 2.2257 | lr 3.00e-04 | grad 3.17 | tok/s 9785
step    290 | loss 1.4585 | lr 3.00e-04 | grad 4.47 | tok/s 10330
step    300 | loss 0.5653 | lr 3.00e-04 | grad 1.73 | tok/s 10339
step    310 | loss 2.3736 | lr 3.00e-04 | grad 2.88 | tok/s 10187
step    320 | loss 1.9460 | lr 3.00e-04 | grad 3.64 | tok/s 9963
step    330 | loss 1.8980 | lr 3.00e-04 | grad 1.91 | tok/s 9628
step    340 | loss 2.2119 | lr 3.00e-04 | grad 1.74 | tok/s 9765
step    350 | loss 1.8556 | lr 3.00e-04 | grad 2.58 | tok/s 10020
step    360 | loss 1.1961 | lr 3.00e-04 | grad 4.78 | tok/s 10258
step    370 | loss 1.7768 | lr 3.00e-04 | grad 1.73 | tok/s 9251
step    380 | loss 1.7445 | lr 3.00e-04 | grad 1.70 | tok/s 9865
step    390 | loss 1.5216 | lr 3.00e-04 | grad 1.35 | tok/s 10352
step    400 | loss 1.4798 | lr 3.00e-04 | grad 1.66 | tok/s 10185
step    410 | loss 1.2729 | lr 3.00e-04 | grad 1.34 | tok/s 9982
step    420 | loss 1.7800 | lr 3.00e-04 | grad 2.94 | tok/s 8135
step    430 | loss 2.0830 | lr 3.00e-04 | grad 1.54 | tok/s 10167
step    440 | loss 2.1461 | lr 3.00e-04 | grad 3.88 | tok/s 9323
step    450 | loss 1.8084 | lr 3.00e-04 | grad 1.40 | tok/s 9892
step    460 | loss 1.6301 | lr 3.00e-04 | grad 2.53 | tok/s 9817
step    470 | loss 1.8427 | lr 3.00e-04 | grad 2.08 | tok/s 9962
step    480 | loss 2.2401 | lr 3.00e-04 | grad 3.31 | tok/s 10086
step    490 | loss 1.6958 | lr 3.00e-04 | grad 1.62 | tok/s 9521
step    500 | loss 1.6820 | lr 3.00e-04 | grad 1.96 | tok/s 10130
step    510 | loss 1.6478 | lr 3.00e-04 | grad 1.52 | tok/s 10293
step    520 | loss 1.6427 | lr 3.00e-04 | grad 1.61 | tok/s 10068
step    530 | loss 1.8777 | lr 3.00e-04 | grad 1.49 | tok/s 9997
step    540 | loss 1.6934 | lr 3.00e-04 | grad 1.61 | tok/s 9929
step    550 | loss 1.5615 | lr 3.00e-04 | grad 1.80 | tok/s 9540
step    560 | loss 1.6978 | lr 3.00e-04 | grad 2.70 | tok/s 9410
step    570 | loss 1.6000 | lr 3.00e-04 | grad 2.64 | tok/s 8576
step    580 | loss 1.5263 | lr 3.00e-04 | grad 1.48 | tok/s 9560
step    590 | loss 1.8306 | lr 3.00e-04 | grad 1.84 | tok/s 9910
step    600 | loss 1.7739 | lr 3.00e-04 | grad 1.38 | tok/s 9590
step    610 | loss 1.5732 | lr 3.00e-04 | grad 1.59 | tok/s 9755
step    620 | loss 1.5635 | lr 3.00e-04 | grad 1.55 | tok/s 9750
step    630 | loss 1.6079 | lr 3.00e-04 | grad 1.91 | tok/s 9612
step    640 | loss 1.8413 | lr 3.00e-04 | grad 4.31 | tok/s 9706
step    650 | loss 1.5930 | lr 3.00e-04 | grad 2.14 | tok/s 9943
step    660 | loss 1.6679 | lr 3.00e-04 | grad 2.12 | tok/s 9996
step    670 | loss 1.8800 | lr 3.00e-04 | grad 2.28 | tok/s 9977
step    680 | loss 1.6815 | lr 3.00e-04 | grad 2.09 | tok/s 9699
step    690 | loss 1.7388 | lr 3.00e-04 | grad 1.96 | tok/s 10221
step    700 | loss 1.3845 | lr 3.00e-04 | grad 1.77 | tok/s 10360
step    710 | loss 1.5921 | lr 3.00e-04 | grad 2.59 | tok/s 9447
step    720 | loss 1.4631 | lr 3.00e-04 | grad 1.93 | tok/s 9693
step    730 | loss 1.2773 | lr 3.00e-04 | grad 2.11 | tok/s 10282
step    740 | loss 1.4171 | lr 3.00e-04 | grad 1.48 | tok/s 10184
step    750 | loss 1.1896 | lr 3.00e-04 | grad 1.34 | tok/s 10341
step    760 | loss 1.0806 | lr 3.00e-04 | grad 1.28 | tok/s 10329
step    770 | loss 1.0403 | lr 3.00e-04 | grad 1.52 | tok/s 10336
step    780 | loss 0.9490 | lr 3.00e-04 | grad 1.45 | tok/s 10343
step    790 | loss 1.1565 | lr 3.00e-04 | grad 2.06 | tok/s 9899
step    800 | loss 1.8483 | lr 3.00e-04 | grad 3.84 | tok/s 10094
step    810 | loss 1.6263 | lr 3.00e-04 | grad 3.81 | tok/s 9766
step    820 | loss 1.6314 | lr 3.00e-04 | grad 5.84 | tok/s 9671
step    830 | loss 1.4186 | lr 3.00e-04 | grad 1.77 | tok/s 10205
step    840 | loss 1.4194 | lr 3.00e-04 | grad 3.41 | tok/s 10317
step    850 | loss 1.5161 | lr 3.00e-04 | grad 2.91 | tok/s 10178
step    860 | loss 1.4454 | lr 3.00e-04 | grad 1.96 | tok/s 10129
step    870 | loss 1.4748 | lr 3.00e-04 | grad 1.66 | tok/s 9922
step    880 | loss 1.6376 | lr 3.00e-04 | grad 2.02 | tok/s 9835
step    890 | loss 1.5900 | lr 3.00e-04 | grad 1.49 | tok/s 9838
step    900 | loss 1.5214 | lr 3.00e-04 | grad 1.40 | tok/s 9853
step    910 | loss 1.4013 | lr 3.00e-04 | grad 1.68 | tok/s 10007
step    920 | loss 1.5254 | lr 3.00e-04 | grad 2.72 | tok/s 10183
step    930 | loss 1.5073 | lr 3.00e-04 | grad 1.31 | tok/s 9612
step    940 | loss 1.3426 | lr 3.00e-04 | grad 1.49 | tok/s 10352
step    950 | loss 1.4537 | lr 3.00e-04 | grad 1.79 | tok/s 10265
step    960 | loss 1.2957 | lr 3.00e-04 | grad 1.57 | tok/s 10184
step    970 | loss 1.7609 | lr 3.00e-04 | grad 2.61 | tok/s 9727
step    980 | loss 1.5263 | lr 3.00e-04 | grad 1.71 | tok/s 9861
step    990 | loss 1.4440 | lr 3.00e-04 | grad 1.47 | tok/s 10163
step   1000 | loss 1.8167 | lr 3.00e-04 | grad 6.00 | tok/s 9786
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8167.pt
step   1010 | loss 1.6332 | lr 3.00e-04 | grad 2.50 | tok/s 4501
step   1020 | loss 1.6059 | lr 3.00e-04 | grad 1.67 | tok/s 9508
step   1030 | loss 1.3732 | lr 3.00e-04 | grad 1.42 | tok/s 10073
step   1040 | loss 1.4723 | lr 3.00e-04 | grad 1.61 | tok/s 10009
step   1050 | loss 1.5896 | lr 3.00e-04 | grad 1.88 | tok/s 9701
step   1060 | loss 1.6316 | lr 3.00e-04 | grad 1.61 | tok/s 10132
step   1070 | loss 1.6351 | lr 3.00e-04 | grad 1.80 | tok/s 9923
step   1080 | loss 1.3661 | lr 3.00e-04 | grad 1.86 | tok/s 9321
step   1090 | loss 0.9778 | lr 3.00e-04 | grad 3.78 | tok/s 10284
step   1100 | loss 1.5032 | lr 3.00e-04 | grad 1.74 | tok/s 9924
step   1110 | loss 1.3707 | lr 3.00e-04 | grad 1.31 | tok/s 10346
step   1120 | loss 1.3076 | lr 3.00e-04 | grad 1.59 | tok/s 10361
step   1130 | loss 1.2504 | lr 3.00e-04 | grad 1.41 | tok/s 10364
step   1140 | loss 1.2590 | lr 3.00e-04 | grad 1.51 | tok/s 10355
step   1150 | loss 1.2555 | lr 3.00e-04 | grad 1.30 | tok/s 10344
step   1160 | loss 1.1884 | lr 3.00e-04 | grad 1.23 | tok/s 10113
step   1170 | loss 1.2581 | lr 3.00e-04 | grad 1.44 | tok/s 10333
step   1180 | loss 1.2746 | lr 3.00e-04 | grad 1.84 | tok/s 10348
step   1190 | loss 1.1826 | lr 3.00e-04 | grad 1.37 | tok/s 10370
step   1200 | loss 1.2036 | lr 3.00e-04 | grad 1.33 | tok/s 9446
step   1210 | loss 1.2360 | lr 3.00e-04 | grad 1.44 | tok/s 10344
step   1220 | loss 1.2469 | lr 3.00e-04 | grad 1.34 | tok/s 10341
step   1230 | loss 1.2257 | lr 3.00e-04 | grad 1.23 | tok/s 10353
step   1240 | loss 1.3222 | lr 3.00e-04 | grad 6.47 | tok/s 10116
step   1250 | loss 1.6607 | lr 3.00e-04 | grad 1.70 | tok/s 9840
step   1260 | loss 1.2763 | lr 3.00e-04 | grad 1.68 | tok/s 9674
step   1270 | loss 1.6827 | lr 3.00e-04 | grad 1.63 | tok/s 9583
step   1280 | loss 1.5232 | lr 3.00e-04 | grad 1.34 | tok/s 10150
step   1290 | loss 1.4813 | lr 3.00e-04 | grad 1.89 | tok/s 9969
step   1300 | loss 1.4691 | lr 3.00e-04 | grad 2.12 | tok/s 9809
step   1310 | loss 1.4215 | lr 3.00e-04 | grad 2.36 | tok/s 10285
step   1320 | loss 1.5686 | lr 3.00e-04 | grad 2.39 | tok/s 10163
step   1330 | loss 1.3540 | lr 3.00e-04 | grad 1.56 | tok/s 10174
step   1340 | loss 1.5893 | lr 3.00e-04 | grad 1.53 | tok/s 8550
step   1350 | loss 1.6875 | lr 3.00e-04 | grad 2.11 | tok/s 9602
step   1360 | loss 1.4101 | lr 3.00e-04 | grad 1.30 | tok/s 10019
step   1370 | loss 1.4798 | lr 3.00e-04 | grad 3.33 | tok/s 9852
step   1380 | loss 1.5143 | lr 3.00e-04 | grad 1.91 | tok/s 9456
step   1390 | loss 1.3899 | lr 3.00e-04 | grad 1.38 | tok/s 9832
step   1400 | loss 1.3386 | lr 3.00e-04 | grad 1.44 | tok/s 9905
step   1410 | loss 1.4893 | lr 3.00e-04 | grad 2.78 | tok/s 9708
step   1420 | loss 1.5501 | lr 3.00e-04 | grad 1.74 | tok/s 9710
step   1430 | loss 1.2979 | lr 3.00e-04 | grad 1.54 | tok/s 9840
step   1440 | loss 1.1003 | lr 3.00e-04 | grad 1.32 | tok/s 10338
step   1450 | loss 1.2311 | lr 3.00e-04 | grad 5.00 | tok/s 10203
step   1460 | loss 1.5990 | lr 3.00e-04 | grad 3.97 | tok/s 9644
step   1470 | loss 1.3928 | lr 3.00e-04 | grad 1.28 | tok/s 10239

Training complete! Final step: 1470
