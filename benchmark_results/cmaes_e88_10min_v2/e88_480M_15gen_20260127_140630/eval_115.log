Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_115/levelE88_100m_20260127_163236
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 478,745,916 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.5921 | lr 3.00e-04 | grad 23.88 | tok/s 5391
step     20 | loss 3.1854 | lr 3.00e-04 | grad 48.75 | tok/s 7491
step     30 | loss 4.9724 | lr 3.00e-04 | grad 19.25 | tok/s 7850
step     40 | loss 4.4715 | lr 3.00e-04 | grad 10.25 | tok/s 7790
step     50 | loss 3.4715 | lr 3.00e-04 | grad 9.81 | tok/s 7763
step     60 | loss 3.0286 | lr 3.00e-04 | grad 2.72 | tok/s 7588
step     70 | loss 2.5737 | lr 3.00e-04 | grad 2.66 | tok/s 7524
step     80 | loss 2.7774 | lr 3.00e-04 | grad 2.23 | tok/s 7474
step     90 | loss 2.7834 | lr 3.00e-04 | grad 9.50 | tok/s 7421
step    100 | loss 2.3287 | lr 3.00e-04 | grad 1.57 | tok/s 7412
step    110 | loss 2.3603 | lr 3.00e-04 | grad 2.45 | tok/s 7360
step    120 | loss 2.5898 | lr 3.00e-04 | grad 3.20 | tok/s 7271
step    130 | loss 2.3009 | lr 3.00e-04 | grad 1.26 | tok/s 7361
step    140 | loss 2.0898 | lr 3.00e-04 | grad 1.12 | tok/s 7058
step    150 | loss 2.0361 | lr 3.00e-04 | grad 1.61 | tok/s 7180
step    160 | loss 2.0128 | lr 3.00e-04 | grad 2.28 | tok/s 6789
step    170 | loss 2.1184 | lr 3.00e-04 | grad 1.55 | tok/s 7340
step    180 | loss 2.0881 | lr 3.00e-04 | grad 1.09 | tok/s 7336
step    190 | loss 1.8258 | lr 3.00e-04 | grad 1.31 | tok/s 7606
step    200 | loss 1.5901 | lr 3.00e-04 | grad 1.40 | tok/s 7547
step    210 | loss 2.0783 | lr 3.00e-04 | grad 1.50 | tok/s 7463
step    220 | loss 1.8881 | lr 3.00e-04 | grad 1.21 | tok/s 7484
step    230 | loss 1.9306 | lr 3.00e-04 | grad 1.77 | tok/s 7479
step    240 | loss 1.8690 | lr 3.00e-04 | grad 1.67 | tok/s 6978
step    250 | loss 1.9281 | lr 3.00e-04 | grad 1.20 | tok/s 6601
step    260 | loss 1.9985 | lr 3.00e-04 | grad 0.98 | tok/s 7336
step    270 | loss 1.9474 | lr 3.00e-04 | grad 1.25 | tok/s 7392
step    280 | loss 1.6250 | lr 3.00e-04 | grad 0.89 | tok/s 7407
step    290 | loss 1.5460 | lr 3.00e-04 | grad 1.03 | tok/s 7688
step    300 | loss 1.4863 | lr 3.00e-04 | grad 1.13 | tok/s 7675
step    310 | loss 1.4837 | lr 3.00e-04 | grad 1.03 | tok/s 7678
step    320 | loss 1.7513 | lr 3.00e-04 | grad 1.27 | tok/s 6806
step    330 | loss 1.7371 | lr 3.00e-04 | grad 1.43 | tok/s 7432
step    340 | loss 1.8744 | lr 3.00e-04 | grad 1.35 | tok/s 7315
step    350 | loss 1.7052 | lr 3.00e-04 | grad 0.93 | tok/s 7268
step    360 | loss 1.8020 | lr 3.00e-04 | grad 1.73 | tok/s 7368
step    370 | loss 1.6191 | lr 3.00e-04 | grad 0.96 | tok/s 7459
step    380 | loss 1.9638 | lr 3.00e-04 | grad 1.48 | tok/s 7643
step    390 | loss 1.6121 | lr 3.00e-04 | grad 1.98 | tok/s 7388
step    400 | loss 1.9423 | lr 3.00e-04 | grad 1.60 | tok/s 7020
step    410 | loss 1.4994 | lr 3.00e-04 | grad 1.52 | tok/s 7386
step    420 | loss 1.7147 | lr 3.00e-04 | grad 3.16 | tok/s 7249
step    430 | loss 1.7104 | lr 3.00e-04 | grad 1.43 | tok/s 7406
step    440 | loss 1.8128 | lr 3.00e-04 | grad 1.17 | tok/s 7511
step    450 | loss 1.6093 | lr 3.00e-04 | grad 1.05 | tok/s 7299
step    460 | loss 1.6793 | lr 3.00e-04 | grad 1.23 | tok/s 7328
step    470 | loss 1.5755 | lr 3.00e-04 | grad 1.06 | tok/s 7422
step    480 | loss 1.5824 | lr 3.00e-04 | grad 1.07 | tok/s 6719
step    490 | loss 1.8683 | lr 3.00e-04 | grad 2.78 | tok/s 7573
step    500 | loss 1.8111 | lr 3.00e-04 | grad 1.27 | tok/s 7334
step    510 | loss 1.3917 | lr 3.00e-04 | grad 1.33 | tok/s 7636
step    520 | loss 1.6924 | lr 3.00e-04 | grad 1.05 | tok/s 7337
step    530 | loss 1.9836 | lr 3.00e-04 | grad 6.28 | tok/s 7526
step    540 | loss 1.3629 | lr 3.00e-04 | grad 0.92 | tok/s 7609
step    550 | loss 1.3552 | lr 3.00e-04 | grad 0.92 | tok/s 7708

Training complete! Final step: 554
