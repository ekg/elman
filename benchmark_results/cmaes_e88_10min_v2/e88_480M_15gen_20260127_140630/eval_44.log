Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_44/levelE88_100m_20260127_145817
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 501,839,560 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.3990 | lr 3.00e-04 | grad 12.94 | tok/s 5019
step     20 | loss 2.6485 | lr 3.00e-04 | grad 4.97 | tok/s 10329
step     30 | loss 2.5284 | lr 3.00e-04 | grad 2.58 | tok/s 10431
step     40 | loss 2.3731 | lr 3.00e-04 | grad 2.77 | tok/s 10006
step     50 | loss 3.0136 | lr 3.00e-04 | grad 11.81 | tok/s 10143
step     60 | loss 2.0941 | lr 3.00e-04 | grad 3.22 | tok/s 10472
step     70 | loss 1.9465 | lr 3.00e-04 | grad 4.00 | tok/s 10574
step     80 | loss 5.5383 | lr 3.00e-04 | grad 93.00 | tok/s 10649
step     90 | loss 5.4051 | lr 3.00e-04 | grad 10.25 | tok/s 10820
step    100 | loss 4.5472 | lr 3.00e-04 | grad 11.00 | tok/s 10800
step    110 | loss 4.1446 | lr 3.00e-04 | grad 16.38 | tok/s 10802
step    120 | loss 3.6218 | lr 3.00e-04 | grad 17.12 | tok/s 10789
step    130 | loss 3.2389 | lr 3.00e-04 | grad 19.38 | tok/s 10778
step    140 | loss 2.6992 | lr 3.00e-04 | grad 10.69 | tok/s 10819
step    150 | loss 2.9776 | lr 3.00e-04 | grad 15.06 | tok/s 10740
step    160 | loss 2.4021 | lr 3.00e-04 | grad 11.69 | tok/s 10767
step    170 | loss 2.4501 | lr 3.00e-04 | grad 10.69 | tok/s 10755
step    180 | loss 2.2534 | lr 3.00e-04 | grad 3.50 | tok/s 10768
step    190 | loss 2.4332 | lr 3.00e-04 | grad 4.06 | tok/s 10764
step    200 | loss 2.1235 | lr 3.00e-04 | grad 6.81 | tok/s 10786
step    210 | loss 2.1148 | lr 3.00e-04 | grad 5.44 | tok/s 10742
step    220 | loss 2.2235 | lr 3.00e-04 | grad 2.27 | tok/s 10635
step    230 | loss 2.0736 | lr 3.00e-04 | grad 2.38 | tok/s 10499
step    240 | loss 2.2748 | lr 3.00e-04 | grad 3.42 | tok/s 9977
step    250 | loss 2.1014 | lr 3.00e-04 | grad 1.76 | tok/s 10269
step    260 | loss 1.5936 | lr 3.00e-04 | grad 2.06 | tok/s 10598
step    270 | loss 2.1024 | lr 3.00e-04 | grad 1.87 | tok/s 10461
step    280 | loss 2.2753 | lr 3.00e-04 | grad 4.34 | tok/s 10236
step    290 | loss 1.5078 | lr 3.00e-04 | grad 2.91 | tok/s 10772
step    300 | loss 0.5910 | lr 3.00e-04 | grad 1.89 | tok/s 10774
step    310 | loss 2.4298 | lr 3.00e-04 | grad 3.03 | tok/s 10613
step    320 | loss 1.9878 | lr 3.00e-04 | grad 4.06 | tok/s 10301
step    330 | loss 1.9412 | lr 3.00e-04 | grad 2.05 | tok/s 10006
step    340 | loss 2.2589 | lr 3.00e-04 | grad 1.91 | tok/s 10121
step    350 | loss 1.9213 | lr 3.00e-04 | grad 4.09 | tok/s 10432
step    360 | loss 1.2667 | lr 3.00e-04 | grad 5.31 | tok/s 10530
step    370 | loss 1.8200 | lr 3.00e-04 | grad 1.89 | tok/s 9608
step    380 | loss 1.7807 | lr 3.00e-04 | grad 1.73 | tok/s 9311
step    390 | loss 1.5494 | lr 3.00e-04 | grad 1.40 | tok/s 10668
step    400 | loss 1.5134 | lr 3.00e-04 | grad 1.81 | tok/s 10644
step    410 | loss 1.3086 | lr 3.00e-04 | grad 1.46 | tok/s 10394
step    420 | loss 1.8226 | lr 3.00e-04 | grad 3.20 | tok/s 9920
step    430 | loss 2.1633 | lr 3.00e-04 | grad 2.09 | tok/s 10535
step    440 | loss 2.1442 | lr 3.00e-04 | grad 3.05 | tok/s 9952
step    450 | loss 1.9541 | lr 3.00e-04 | grad 2.02 | tok/s 10332
step    460 | loss 1.7188 | lr 3.00e-04 | grad 2.27 | tok/s 10137
step    470 | loss 1.8307 | lr 3.00e-04 | grad 1.69 | tok/s 10439
step    480 | loss 2.2435 | lr 3.00e-04 | grad 4.91 | tok/s 10496
step    490 | loss 1.7887 | lr 3.00e-04 | grad 1.74 | tok/s 9910
step    500 | loss 1.6862 | lr 3.00e-04 | grad 2.45 | tok/s 10577
step    510 | loss 1.7147 | lr 3.00e-04 | grad 1.67 | tok/s 10727
step    520 | loss 1.6768 | lr 3.00e-04 | grad 1.51 | tok/s 10219
step    530 | loss 1.9143 | lr 3.00e-04 | grad 1.86 | tok/s 10263
step    540 | loss 1.7413 | lr 3.00e-04 | grad 1.59 | tok/s 10312
step    550 | loss 1.5762 | lr 3.00e-04 | grad 2.20 | tok/s 10071
step    560 | loss 1.7183 | lr 3.00e-04 | grad 1.90 | tok/s 9796
step    570 | loss 1.6627 | lr 3.00e-04 | grad 2.80 | tok/s 10082
step    580 | loss 1.5475 | lr 3.00e-04 | grad 1.56 | tok/s 9867
step    590 | loss 1.8599 | lr 3.00e-04 | grad 2.30 | tok/s 10140
step    600 | loss 1.8121 | lr 3.00e-04 | grad 1.69 | tok/s 9139
step    610 | loss 1.6230 | lr 3.00e-04 | grad 1.71 | tok/s 10458
step    620 | loss 1.5456 | lr 3.00e-04 | grad 1.77 | tok/s 9861
step    630 | loss 1.6625 | lr 3.00e-04 | grad 3.25 | tok/s 9866
step    640 | loss 1.8038 | lr 3.00e-04 | grad 1.83 | tok/s 10205
step    650 | loss 1.6643 | lr 3.00e-04 | grad 1.90 | tok/s 10235
step    660 | loss 1.7000 | lr 3.00e-04 | grad 1.57 | tok/s 10234
step    670 | loss 1.9234 | lr 3.00e-04 | grad 2.36 | tok/s 10366
step    680 | loss 1.7217 | lr 3.00e-04 | grad 1.82 | tok/s 9884
step    690 | loss 1.8276 | lr 3.00e-04 | grad 2.52 | tok/s 10553
step    700 | loss 1.4535 | lr 3.00e-04 | grad 2.34 | tok/s 8841
step    710 | loss 1.5794 | lr 3.00e-04 | grad 1.73 | tok/s 9626
step    720 | loss 1.4673 | lr 3.00e-04 | grad 2.86 | tok/s 9083
step    730 | loss 1.3103 | lr 3.00e-04 | grad 2.09 | tok/s 9609
step    740 | loss 1.5030 | lr 3.00e-04 | grad 1.78 | tok/s 10611
step    750 | loss 1.2185 | lr 3.00e-04 | grad 1.89 | tok/s 10753
step    760 | loss 1.1262 | lr 3.00e-04 | grad 1.70 | tok/s 10786
step    770 | loss 1.0675 | lr 3.00e-04 | grad 1.52 | tok/s 10741
step    780 | loss 1.0001 | lr 3.00e-04 | grad 1.55 | tok/s 10771
step    790 | loss 1.1259 | lr 3.00e-04 | grad 2.56 | tok/s 10415
step    800 | loss 1.8033 | lr 3.00e-04 | grad 4.34 | tok/s 10429
step    810 | loss 1.6885 | lr 3.00e-04 | grad 1.58 | tok/s 10328
step    820 | loss 1.6942 | lr 3.00e-04 | grad 2.94 | tok/s 9922
step    830 | loss 1.4816 | lr 3.00e-04 | grad 1.83 | tok/s 10648
step    840 | loss 1.3879 | lr 3.00e-04 | grad 1.73 | tok/s 10774
step    850 | loss 1.6054 | lr 3.00e-04 | grad 1.57 | tok/s 10684
step    860 | loss 1.4810 | lr 3.00e-04 | grad 2.77 | tok/s 10575
step    870 | loss 1.4947 | lr 3.00e-04 | grad 2.08 | tok/s 10204
step    880 | loss 1.6673 | lr 3.00e-04 | grad 1.98 | tok/s 10191
step    890 | loss 1.6680 | lr 3.00e-04 | grad 2.30 | tok/s 10368
step    900 | loss 1.5576 | lr 3.00e-04 | grad 1.98 | tok/s 10417
step    910 | loss 1.4132 | lr 3.00e-04 | grad 3.05 | tok/s 10158
step    920 | loss 1.5244 | lr 3.00e-04 | grad 2.73 | tok/s 10462
step    930 | loss 1.5849 | lr 3.00e-04 | grad 2.80 | tok/s 10079
step    940 | loss 1.3881 | lr 3.00e-04 | grad 1.41 | tok/s 10632
step    950 | loss 1.4925 | lr 3.00e-04 | grad 2.16 | tok/s 8402
step    960 | loss 1.3657 | lr 3.00e-04 | grad 1.72 | tok/s 10217
step    970 | loss 1.7914 | lr 3.00e-04 | grad 2.81 | tok/s 10160
step    980 | loss 1.5682 | lr 3.00e-04 | grad 1.84 | tok/s 10292
step    990 | loss 1.4688 | lr 3.00e-04 | grad 1.55 | tok/s 10599
step   1000 | loss 1.8338 | lr 3.00e-04 | grad 4.94 | tok/s 10209
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8338.pt
step   1010 | loss 1.6590 | lr 3.00e-04 | grad 2.66 | tok/s 4663
step   1020 | loss 1.6415 | lr 3.00e-04 | grad 1.76 | tok/s 9904
step   1030 | loss 1.4002 | lr 3.00e-04 | grad 1.59 | tok/s 10467
step   1040 | loss 1.5077 | lr 3.00e-04 | grad 1.71 | tok/s 10448
step   1050 | loss 1.6196 | lr 3.00e-04 | grad 2.03 | tok/s 10113
step   1060 | loss 1.6744 | lr 3.00e-04 | grad 1.64 | tok/s 10539
step   1070 | loss 1.6708 | lr 3.00e-04 | grad 1.88 | tok/s 10318
step   1080 | loss 1.3887 | lr 3.00e-04 | grad 1.88 | tok/s 9697
step   1090 | loss 1.0067 | lr 3.00e-04 | grad 4.00 | tok/s 10720
step   1100 | loss 1.5288 | lr 3.00e-04 | grad 1.89 | tok/s 10319
step   1110 | loss 1.3933 | lr 3.00e-04 | grad 1.34 | tok/s 10789
step   1120 | loss 1.3317 | lr 3.00e-04 | grad 1.71 | tok/s 10796
step   1130 | loss 1.2737 | lr 3.00e-04 | grad 1.52 | tok/s 10232
step   1140 | loss 1.2797 | lr 3.00e-04 | grad 1.56 | tok/s 10820
step   1150 | loss 1.2774 | lr 3.00e-04 | grad 1.34 | tok/s 10838
step   1160 | loss 1.2079 | lr 3.00e-04 | grad 1.34 | tok/s 10843
step   1170 | loss 1.2761 | lr 3.00e-04 | grad 1.54 | tok/s 10836
step   1180 | loss 1.2975 | lr 3.00e-04 | grad 1.93 | tok/s 10813
step   1190 | loss 1.2056 | lr 3.00e-04 | grad 1.43 | tok/s 10799
step   1200 | loss 1.2293 | lr 3.00e-04 | grad 1.38 | tok/s 10803
step   1210 | loss 1.2534 | lr 3.00e-04 | grad 1.52 | tok/s 10810
step   1220 | loss 1.2686 | lr 3.00e-04 | grad 1.45 | tok/s 10808
step   1230 | loss 1.2465 | lr 3.00e-04 | grad 1.34 | tok/s 10789
step   1240 | loss 1.3404 | lr 3.00e-04 | grad 6.38 | tok/s 10545
step   1250 | loss 1.6917 | lr 3.00e-04 | grad 1.83 | tok/s 10279
step   1260 | loss 1.2926 | lr 3.00e-04 | grad 1.79 | tok/s 10101
step   1270 | loss 1.7144 | lr 3.00e-04 | grad 1.79 | tok/s 9992
step   1280 | loss 1.5490 | lr 3.00e-04 | grad 1.52 | tok/s 10597
step   1290 | loss 1.5141 | lr 3.00e-04 | grad 2.00 | tok/s 10426
step   1300 | loss 1.4927 | lr 3.00e-04 | grad 2.33 | tok/s 10233
step   1310 | loss 1.4410 | lr 3.00e-04 | grad 2.56 | tok/s 10344
step   1320 | loss 1.6107 | lr 3.00e-04 | grad 2.58 | tok/s 10595
step   1330 | loss 1.3925 | lr 3.00e-04 | grad 1.91 | tok/s 10614
step   1340 | loss 1.6379 | lr 3.00e-04 | grad 1.66 | tok/s 9831
step   1350 | loss 1.7227 | lr 3.00e-04 | grad 2.28 | tok/s 10002
step   1360 | loss 1.4441 | lr 3.00e-04 | grad 1.42 | tok/s 10473
step   1370 | loss 1.5147 | lr 3.00e-04 | grad 2.86 | tok/s 10295
step   1380 | loss 1.5459 | lr 3.00e-04 | grad 2.05 | tok/s 9865
step   1390 | loss 1.4126 | lr 3.00e-04 | grad 1.49 | tok/s 10232
step   1400 | loss 1.3776 | lr 3.00e-04 | grad 1.52 | tok/s 10331
step   1410 | loss 1.5297 | lr 3.00e-04 | grad 2.94 | tok/s 10155
step   1420 | loss 1.5783 | lr 3.00e-04 | grad 1.81 | tok/s 10151
step   1430 | loss 1.3269 | lr 3.00e-04 | grad 1.71 | tok/s 10259
step   1440 | loss 1.1246 | lr 3.00e-04 | grad 1.47 | tok/s 10791
step   1450 | loss 1.2575 | lr 3.00e-04 | grad 5.19 | tok/s 10644
step   1460 | loss 1.6446 | lr 3.00e-04 | grad 4.28 | tok/s 10066
step   1470 | loss 1.4181 | lr 3.00e-04 | grad 1.38 | tok/s 10688
step   1480 | loss 1.8137 | lr 3.00e-04 | grad 2.92 | tok/s 10601
step   1490 | loss 1.5285 | lr 3.00e-04 | grad 3.25 | tok/s 10712
step   1500 | loss 1.2744 | lr 3.00e-04 | grad 1.45 | tok/s 10744
step   1510 | loss 1.5234 | lr 3.00e-04 | grad 1.67 | tok/s 10544
step   1520 | loss 1.4414 | lr 3.00e-04 | grad 1.86 | tok/s 10327
step   1530 | loss 1.3902 | lr 3.00e-04 | grad 1.56 | tok/s 10412

Training complete! Final step: 1530
