Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_100/levelE88_100m_20260127_161135
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,459,200 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 3.9948 | lr 3.00e-04 | grad 10.25 | tok/s 8341
step     20 | loss 2.8488 | lr 3.00e-04 | grad 3.34 | tok/s 14982
step     30 | loss 2.9441 | lr 3.00e-04 | grad 6.84 | tok/s 15745
step     40 | loss 4.4363 | lr 3.00e-04 | grad 28.88 | tok/s 16021
step     50 | loss 4.2417 | lr 3.00e-04 | grad 11.06 | tok/s 16129
step     60 | loss 3.3083 | lr 3.00e-04 | grad 8.81 | tok/s 16069
step     70 | loss 2.7455 | lr 3.00e-04 | grad 5.31 | tok/s 16026
step     80 | loss 2.4968 | lr 3.00e-04 | grad 4.78 | tok/s 15995
step     90 | loss 2.3386 | lr 3.00e-04 | grad 4.44 | tok/s 15971
step    100 | loss 2.1565 | lr 3.00e-04 | grad 2.89 | tok/s 15964
step    110 | loss 2.2201 | lr 3.00e-04 | grad 3.05 | tok/s 15808
step    120 | loss 2.6889 | lr 3.00e-04 | grad 2.00 | tok/s 15061
step    130 | loss 2.1179 | lr 3.00e-04 | grad 4.66 | tok/s 15439
step    140 | loss 2.3834 | lr 3.00e-04 | grad 7.06 | tok/s 15466
step    150 | loss 1.6115 | lr 3.00e-04 | grad 5.34 | tok/s 15842
step    160 | loss 2.3403 | lr 3.00e-04 | grad 2.14 | tok/s 15335
step    170 | loss 2.3002 | lr 3.00e-04 | grad 1.81 | tok/s 15091
step    180 | loss 1.8095 | lr 3.00e-04 | grad 2.88 | tok/s 15434
step    190 | loss 1.9171 | lr 3.00e-04 | grad 2.25 | tok/s 15131
step    200 | loss 1.6570 | lr 3.00e-04 | grad 1.78 | tok/s 15865
step    210 | loss 1.8964 | lr 3.00e-04 | grad 5.00 | tok/s 15052
step    220 | loss 2.2036 | lr 3.00e-04 | grad 3.34 | tok/s 15211
step    230 | loss 1.9649 | lr 3.00e-04 | grad 2.44 | tok/s 15192
step    240 | loss 2.2584 | lr 3.00e-04 | grad 4.84 | tok/s 15366
step    250 | loss 1.7716 | lr 3.00e-04 | grad 1.61 | tok/s 15241
step    260 | loss 1.8972 | lr 3.00e-04 | grad 3.06 | tok/s 15694
step    270 | loss 1.8230 | lr 3.00e-04 | grad 1.92 | tok/s 15330
step    280 | loss 1.7744 | lr 3.00e-04 | grad 1.79 | tok/s 14413
step    290 | loss 1.6744 | lr 3.00e-04 | grad 2.11 | tok/s 14911
step    300 | loss 1.9645 | lr 3.00e-04 | grad 1.95 | tok/s 15029
step    310 | loss 1.6657 | lr 3.00e-04 | grad 1.70 | tok/s 14934
step    320 | loss 1.8758 | lr 3.00e-04 | grad 3.77 | tok/s 13995
step    330 | loss 1.7166 | lr 3.00e-04 | grad 1.80 | tok/s 15354
step    340 | loss 2.0194 | lr 3.00e-04 | grad 1.95 | tok/s 15207
step    350 | loss 1.7298 | lr 3.00e-04 | grad 1.89 | tok/s 15671
step    360 | loss 1.5876 | lr 3.00e-04 | grad 1.98 | tok/s 14988
step    370 | loss 1.4872 | lr 3.00e-04 | grad 1.64 | tok/s 15790
step    380 | loss 1.2188 | lr 3.00e-04 | grad 1.62 | tok/s 15939
step    390 | loss 1.1230 | lr 3.00e-04 | grad 1.52 | tok/s 15943
step    400 | loss 1.7437 | lr 3.00e-04 | grad 1.76 | tok/s 15093
step    410 | loss 1.7514 | lr 3.00e-04 | grad 2.25 | tok/s 15162
step    420 | loss 1.6230 | lr 3.00e-04 | grad 3.31 | tok/s 15867
step    430 | loss 1.6099 | lr 3.00e-04 | grad 1.85 | tok/s 15601
step    440 | loss 1.6952 | lr 3.00e-04 | grad 2.17 | tok/s 15092
step    450 | loss 1.6268 | lr 3.00e-04 | grad 1.42 | tok/s 15167
step    460 | loss 1.6013 | lr 3.00e-04 | grad 1.98 | tok/s 15508
step    470 | loss 1.5632 | lr 3.00e-04 | grad 3.05 | tok/s 15326
step    480 | loss 1.5706 | lr 3.00e-04 | grad 2.66 | tok/s 15683
step    490 | loss 1.7008 | lr 3.00e-04 | grad 2.27 | tok/s 15113
step    500 | loss 1.7925 | lr 3.00e-04 | grad 1.70 | tok/s 15350
step    510 | loss 1.6688 | lr 3.00e-04 | grad 1.50 | tok/s 14656
step    520 | loss 1.5327 | lr 3.00e-04 | grad 1.95 | tok/s 15364
step    530 | loss 1.7153 | lr 3.00e-04 | grad 1.96 | tok/s 15139
step    540 | loss 1.5954 | lr 3.00e-04 | grad 1.56 | tok/s 14801
step    550 | loss 1.3714 | lr 3.00e-04 | grad 2.61 | tok/s 15440
step    560 | loss 1.4423 | lr 3.00e-04 | grad 1.65 | tok/s 15930
step    570 | loss 1.3452 | lr 3.00e-04 | grad 1.66 | tok/s 13878
step    580 | loss 1.2952 | lr 3.00e-04 | grad 1.34 | tok/s 15935
step    590 | loss 1.3483 | lr 3.00e-04 | grad 1.84 | tok/s 15957
step    600 | loss 1.2772 | lr 3.00e-04 | grad 1.49 | tok/s 15898
step    610 | loss 1.2986 | lr 3.00e-04 | grad 1.27 | tok/s 15910
step    620 | loss 1.3726 | lr 3.00e-04 | grad 3.77 | tok/s 15697
step    630 | loss 1.6781 | lr 3.00e-04 | grad 2.83 | tok/s 15079
step    640 | loss 1.6889 | lr 3.00e-04 | grad 1.98 | tok/s 15161
step    650 | loss 1.5497 | lr 3.00e-04 | grad 2.39 | tok/s 15256
step    660 | loss 1.6555 | lr 3.00e-04 | grad 2.92 | tok/s 15695
step    670 | loss 1.5640 | lr 3.00e-04 | grad 2.39 | tok/s 15042
step    680 | loss 1.6136 | lr 3.00e-04 | grad 1.29 | tok/s 14938
step    690 | loss 1.6089 | lr 3.00e-04 | grad 2.64 | tok/s 15065
step    700 | loss 1.4470 | lr 3.00e-04 | grad 1.38 | tok/s 14895
step    710 | loss 1.6494 | lr 3.00e-04 | grad 2.12 | tok/s 14858
step    720 | loss 1.2793 | lr 3.00e-04 | grad 1.46 | tok/s 15312
step    730 | loss 1.5450 | lr 3.00e-04 | grad 3.61 | tok/s 14596
step    740 | loss 1.7706 | lr 3.00e-04 | grad 3.72 | tok/s 13919
step    750 | loss 1.4710 | lr 3.00e-04 | grad 1.39 | tok/s 15902
step    760 | loss 1.5869 | lr 3.00e-04 | grad 2.12 | tok/s 14629
step    770 | loss 1.5664 | lr 3.00e-04 | grad 1.79 | tok/s 13607
step    780 | loss 1.4692 | lr 3.00e-04 | grad 1.49 | tok/s 14332
step    790 | loss 1.6807 | lr 3.00e-04 | grad 2.64 | tok/s 12594
step    800 | loss 1.1942 | lr 3.00e-04 | grad 1.91 | tok/s 13867
step    810 | loss 1.4106 | lr 3.00e-04 | grad 2.02 | tok/s 14401
step    820 | loss 1.4763 | lr 3.00e-04 | grad 3.70 | tok/s 15316
step    830 | loss 1.4221 | lr 3.00e-04 | grad 1.40 | tok/s 10558
step    840 | loss 1.6422 | lr 3.00e-04 | grad 1.71 | tok/s 13672
step    850 | loss 1.5487 | lr 3.00e-04 | grad 2.02 | tok/s 15321
step    860 | loss 1.6031 | lr 3.00e-04 | grad 2.42 | tok/s 12465
step    870 | loss 1.4256 | lr 3.00e-04 | grad 2.00 | tok/s 15618
step    880 | loss 1.5834 | lr 3.00e-04 | grad 1.43 | tok/s 15302
step    890 | loss 1.5005 | lr 3.00e-04 | grad 1.98 | tok/s 15329
step    900 | loss 1.5634 | lr 3.00e-04 | grad 1.91 | tok/s 15144
step    910 | loss 1.5111 | lr 3.00e-04 | grad 3.00 | tok/s 15265
step    920 | loss 1.4868 | lr 3.00e-04 | grad 1.75 | tok/s 15110
step    930 | loss 1.4142 | lr 3.00e-04 | grad 1.98 | tok/s 14160
step    940 | loss 1.3697 | lr 3.00e-04 | grad 3.06 | tok/s 14913
step    950 | loss 1.4642 | lr 3.00e-04 | grad 1.71 | tok/s 14987
step    960 | loss 1.4421 | lr 3.00e-04 | grad 1.59 | tok/s 15191
step    970 | loss 1.5711 | lr 3.00e-04 | grad 4.59 | tok/s 15211
step    980 | loss 1.7846 | lr 3.00e-04 | grad 2.45 | tok/s 15805
step    990 | loss 1.5603 | lr 3.00e-04 | grad 1.40 | tok/s 15229
step   1000 | loss 1.5793 | lr 3.00e-04 | grad 1.50 | tok/s 15105
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5793.pt
step   1010 | loss 1.2061 | lr 3.00e-04 | grad 2.98 | tok/s 6790
step   1020 | loss 1.3742 | lr 3.00e-04 | grad 1.75 | tok/s 15723
step   1030 | loss 1.8550 | lr 3.00e-04 | grad 4.56 | tok/s 14572
step   1040 | loss 1.9322 | lr 3.00e-04 | grad 2.12 | tok/s 11631
step   1050 | loss 1.4616 | lr 3.00e-04 | grad 1.58 | tok/s 15365
step   1060 | loss 1.1371 | lr 3.00e-04 | grad 1.48 | tok/s 15610
step   1070 | loss 1.3742 | lr 3.00e-04 | grad 1.40 | tok/s 13782
step   1080 | loss 1.2376 | lr 3.00e-04 | grad 1.42 | tok/s 16032
step   1090 | loss 1.2397 | lr 3.00e-04 | grad 1.46 | tok/s 16013
step   1100 | loss 1.1773 | lr 3.00e-04 | grad 1.16 | tok/s 15999
step   1110 | loss 1.3364 | lr 3.00e-04 | grad 1.41 | tok/s 15842

Training complete! Final step: 1113
