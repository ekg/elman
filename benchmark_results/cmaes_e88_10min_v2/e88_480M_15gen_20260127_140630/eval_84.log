Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_84/levelE88_100m_20260127_155051
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 474,417,786 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0691 | lr 3.00e-04 | grad 9.56 | tok/s 8182
step     20 | loss 2.8611 | lr 3.00e-04 | grad 3.41 | tok/s 13564
step     30 | loss 2.9626 | lr 3.00e-04 | grad 6.00 | tok/s 14329
step     40 | loss 4.3255 | lr 3.00e-04 | grad 29.75 | tok/s 14536
step     50 | loss 4.3432 | lr 3.00e-04 | grad 11.94 | tok/s 14677
step     60 | loss 3.3201 | lr 3.00e-04 | grad 8.19 | tok/s 14620
step     70 | loss 2.7889 | lr 3.00e-04 | grad 5.12 | tok/s 14596
step     80 | loss 2.5150 | lr 3.00e-04 | grad 3.92 | tok/s 14593
step     90 | loss 2.3613 | lr 3.00e-04 | grad 3.56 | tok/s 14565
step    100 | loss 2.1554 | lr 3.00e-04 | grad 2.88 | tok/s 14548
step    110 | loss 2.2154 | lr 3.00e-04 | grad 2.67 | tok/s 14435
step    120 | loss 2.7144 | lr 3.00e-04 | grad 2.02 | tok/s 13734
step    130 | loss 2.1272 | lr 3.00e-04 | grad 4.53 | tok/s 14046
step    140 | loss 2.3684 | lr 3.00e-04 | grad 6.25 | tok/s 14065
step    150 | loss 1.3740 | lr 3.00e-04 | grad 4.91 | tok/s 14438
step    160 | loss 2.3362 | lr 3.00e-04 | grad 2.05 | tok/s 13934
step    170 | loss 2.2945 | lr 3.00e-04 | grad 1.59 | tok/s 13729
step    180 | loss 1.8233 | lr 3.00e-04 | grad 2.73 | tok/s 14070
step    190 | loss 1.9200 | lr 3.00e-04 | grad 2.03 | tok/s 13825
step    200 | loss 1.6653 | lr 3.00e-04 | grad 1.68 | tok/s 14458
step    210 | loss 1.8927 | lr 3.00e-04 | grad 4.69 | tok/s 13703
step    220 | loss 2.2024 | lr 3.00e-04 | grad 3.14 | tok/s 13860
step    230 | loss 1.9593 | lr 3.00e-04 | grad 2.52 | tok/s 13859
step    240 | loss 2.2565 | lr 3.00e-04 | grad 4.78 | tok/s 14034
step    250 | loss 1.7742 | lr 3.00e-04 | grad 1.52 | tok/s 13964
step    260 | loss 1.8958 | lr 3.00e-04 | grad 2.92 | tok/s 14358
step    270 | loss 1.8240 | lr 3.00e-04 | grad 1.80 | tok/s 14019
step    280 | loss 1.7697 | lr 3.00e-04 | grad 1.70 | tok/s 13167
step    290 | loss 1.6695 | lr 3.00e-04 | grad 1.98 | tok/s 13610
step    300 | loss 1.9704 | lr 3.00e-04 | grad 1.89 | tok/s 13701
step    310 | loss 1.6657 | lr 3.00e-04 | grad 1.67 | tok/s 13636
step    320 | loss 1.8791 | lr 3.00e-04 | grad 2.84 | tok/s 13801
step    330 | loss 1.7159 | lr 3.00e-04 | grad 1.66 | tok/s 13957
step    340 | loss 2.0231 | lr 3.00e-04 | grad 2.12 | tok/s 13907
step    350 | loss 1.7208 | lr 3.00e-04 | grad 1.81 | tok/s 14299
step    360 | loss 1.5771 | lr 3.00e-04 | grad 1.76 | tok/s 13687
step    370 | loss 1.4932 | lr 3.00e-04 | grad 1.53 | tok/s 14429
step    380 | loss 1.2332 | lr 3.00e-04 | grad 1.61 | tok/s 14563
step    390 | loss 1.1311 | lr 3.00e-04 | grad 1.39 | tok/s 14568
step    400 | loss 1.7437 | lr 3.00e-04 | grad 1.66 | tok/s 13798
step    410 | loss 1.7481 | lr 3.00e-04 | grad 2.12 | tok/s 13921
step    420 | loss 1.6215 | lr 3.00e-04 | grad 3.14 | tok/s 14515
step    430 | loss 1.6154 | lr 3.00e-04 | grad 1.74 | tok/s 14285
step    440 | loss 1.6967 | lr 3.00e-04 | grad 2.06 | tok/s 13827
step    450 | loss 1.6278 | lr 3.00e-04 | grad 1.38 | tok/s 13981
step    460 | loss 1.5907 | lr 3.00e-04 | grad 1.91 | tok/s 14200
step    470 | loss 1.5655 | lr 3.00e-04 | grad 3.00 | tok/s 14116
step    480 | loss 1.5811 | lr 3.00e-04 | grad 2.56 | tok/s 14400
step    490 | loss 1.6986 | lr 3.00e-04 | grad 2.22 | tok/s 13832
step    500 | loss 1.8008 | lr 3.00e-04 | grad 1.61 | tok/s 14052
step    510 | loss 1.6719 | lr 3.00e-04 | grad 1.40 | tok/s 13423
step    520 | loss 1.5333 | lr 3.00e-04 | grad 1.87 | tok/s 14066
step    530 | loss 1.7151 | lr 3.00e-04 | grad 1.87 | tok/s 13835
step    540 | loss 1.5923 | lr 3.00e-04 | grad 1.48 | tok/s 13527
step    550 | loss 1.3683 | lr 3.00e-04 | grad 2.52 | tok/s 13836
step    560 | loss 1.4436 | lr 3.00e-04 | grad 1.58 | tok/s 14555
step    570 | loss 1.3453 | lr 3.00e-04 | grad 1.62 | tok/s 14555
step    580 | loss 1.3044 | lr 3.00e-04 | grad 1.25 | tok/s 14552
step    590 | loss 1.3358 | lr 3.00e-04 | grad 1.23 | tok/s 14549
step    600 | loss 1.2748 | lr 3.00e-04 | grad 1.55 | tok/s 14513
step    610 | loss 1.3070 | lr 3.00e-04 | grad 1.39 | tok/s 14518
step    620 | loss 1.2981 | lr 3.00e-04 | grad 1.54 | tok/s 14420
step    630 | loss 1.6697 | lr 3.00e-04 | grad 4.06 | tok/s 13608
step    640 | loss 1.7377 | lr 3.00e-04 | grad 1.55 | tok/s 13863
step    650 | loss 1.5495 | lr 3.00e-04 | grad 1.60 | tok/s 13867
step    660 | loss 1.5976 | lr 3.00e-04 | grad 1.62 | tok/s 14402
step    670 | loss 1.6263 | lr 3.00e-04 | grad 4.59 | tok/s 13925
step    680 | loss 1.6366 | lr 3.00e-04 | grad 1.86 | tok/s 13725
step    690 | loss 1.5738 | lr 3.00e-04 | grad 1.72 | tok/s 13594
step    700 | loss 1.4853 | lr 3.00e-04 | grad 1.29 | tok/s 13902
step    710 | loss 1.6439 | lr 3.00e-04 | grad 2.73 | tok/s 13689
step    720 | loss 1.3063 | lr 3.00e-04 | grad 1.42 | tok/s 14221
step    730 | loss 1.4761 | lr 3.00e-04 | grad 1.36 | tok/s 13993
step    740 | loss 1.7781 | lr 3.00e-04 | grad 3.23 | tok/s 14353
step    750 | loss 1.5383 | lr 3.00e-04 | grad 1.33 | tok/s 14534
step    760 | loss 1.5353 | lr 3.00e-04 | grad 2.88 | tok/s 14230
step    770 | loss 1.5835 | lr 3.00e-04 | grad 1.70 | tok/s 13986
step    780 | loss 1.4881 | lr 3.00e-04 | grad 1.73 | tok/s 14090
step    790 | loss 1.6372 | lr 3.00e-04 | grad 4.19 | tok/s 14421
step    800 | loss 1.3276 | lr 3.00e-04 | grad 1.16 | tok/s 14170
step    810 | loss 1.3179 | lr 3.00e-04 | grad 2.55 | tok/s 13699
step    820 | loss 1.4205 | lr 3.00e-04 | grad 1.78 | tok/s 14005
step    830 | loss 1.4962 | lr 3.00e-04 | grad 1.23 | tok/s 13808
step    840 | loss 1.6194 | lr 3.00e-04 | grad 1.48 | tok/s 13730
step    850 | loss 1.5518 | lr 3.00e-04 | grad 1.42 | tok/s 14030
step    860 | loss 1.5936 | lr 3.00e-04 | grad 2.09 | tok/s 14271
step    870 | loss 1.4117 | lr 3.00e-04 | grad 1.66 | tok/s 14399
step    880 | loss 1.5892 | lr 3.00e-04 | grad 1.55 | tok/s 14102
step    890 | loss 1.4897 | lr 3.00e-04 | grad 1.25 | tok/s 14037
step    900 | loss 1.5372 | lr 3.00e-04 | grad 1.52 | tok/s 13715
step    910 | loss 1.5244 | lr 3.00e-04 | grad 5.78 | tok/s 13852
step    920 | loss 1.4831 | lr 3.00e-04 | grad 1.53 | tok/s 14017
step    930 | loss 1.3931 | lr 3.00e-04 | grad 1.81 | tok/s 14185
step    940 | loss 1.3601 | lr 3.00e-04 | grad 1.66 | tok/s 13870
step    950 | loss 1.4992 | lr 3.00e-04 | grad 2.09 | tok/s 13651
step    960 | loss 1.4445 | lr 3.00e-04 | grad 1.25 | tok/s 14050
step    970 | loss 1.4778 | lr 3.00e-04 | grad 1.46 | tok/s 14060
step    980 | loss 1.8811 | lr 3.00e-04 | grad 3.03 | tok/s 14612
step    990 | loss 1.5756 | lr 3.00e-04 | grad 1.52 | tok/s 14004
step   1000 | loss 1.5696 | lr 3.00e-04 | grad 1.60 | tok/s 14045
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5696.pt
step   1010 | loss 1.3669 | lr 3.00e-04 | grad 1.71 | tok/s 8584
step   1020 | loss 1.2047 | lr 3.00e-04 | grad 1.27 | tok/s 14780
step   1030 | loss 1.5995 | lr 3.00e-04 | grad 1.57 | tok/s 13959
step   1040 | loss 2.1524 | lr 3.00e-04 | grad 2.33 | tok/s 14426

Training complete! Final step: 1047
