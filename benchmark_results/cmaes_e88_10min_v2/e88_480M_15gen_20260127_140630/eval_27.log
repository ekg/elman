Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_27/levelE88_100m_20260127_143730
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 472,164,200 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1339 | lr 3.00e-04 | grad 4.44 | tok/s 4189
step     20 | loss 3.4024 | lr 3.00e-04 | grad 28.75 | tok/s 5228
step     30 | loss 4.5535 | lr 3.00e-04 | grad 19.12 | tok/s 5404
step     40 | loss 3.6675 | lr 3.00e-04 | grad 10.88 | tok/s 5393
step     50 | loss 3.2424 | lr 3.00e-04 | grad 5.28 | tok/s 5390
step     60 | loss 3.0067 | lr 3.00e-04 | grad 2.22 | tok/s 5258
step     70 | loss 2.6359 | lr 3.00e-04 | grad 2.98 | tok/s 5172
step     80 | loss 2.5560 | lr 3.00e-04 | grad 1.88 | tok/s 5315
step     90 | loss 2.5944 | lr 3.00e-04 | grad 8.88 | tok/s 5059
step    100 | loss 2.2864 | lr 3.00e-04 | grad 2.91 | tok/s 5155
step    110 | loss 2.3831 | lr 3.00e-04 | grad 1.34 | tok/s 5194
step    120 | loss 2.4515 | lr 3.00e-04 | grad 1.50 | tok/s 5197
step    130 | loss 2.4028 | lr 3.00e-04 | grad 1.45 | tok/s 5313
step    140 | loss 2.1752 | lr 3.00e-04 | grad 3.19 | tok/s 5183
step    150 | loss 2.1832 | lr 3.00e-04 | grad 1.20 | tok/s 5103
step    160 | loss 2.0766 | lr 3.00e-04 | grad 2.84 | tok/s 5085
step    170 | loss 2.1783 | lr 3.00e-04 | grad 1.43 | tok/s 5256
step    180 | loss 2.0544 | lr 3.00e-04 | grad 0.99 | tok/s 5076
step    190 | loss 1.8225 | lr 3.00e-04 | grad 1.09 | tok/s 5411
step    200 | loss 1.6559 | lr 3.00e-04 | grad 2.34 | tok/s 5324
step    210 | loss 2.0374 | lr 3.00e-04 | grad 1.43 | tok/s 5223
step    220 | loss 1.9941 | lr 3.00e-04 | grad 1.46 | tok/s 5242
step    230 | loss 1.8400 | lr 3.00e-04 | grad 1.34 | tok/s 5163
step    240 | loss 1.8602 | lr 3.00e-04 | grad 1.42 | tok/s 5258
step    250 | loss 1.8692 | lr 3.00e-04 | grad 1.09 | tok/s 5186
step    260 | loss 2.0138 | lr 3.00e-04 | grad 0.94 | tok/s 5161
step    270 | loss 1.9366 | lr 3.00e-04 | grad 1.13 | tok/s 5190
step    280 | loss 1.6243 | lr 3.00e-04 | grad 0.90 | tok/s 5212
step    290 | loss 1.5595 | lr 3.00e-04 | grad 1.04 | tok/s 5416
step    300 | loss 1.5000 | lr 3.00e-04 | grad 1.07 | tok/s 5417
step    310 | loss 1.4947 | lr 3.00e-04 | grad 0.99 | tok/s 5412
step    320 | loss 1.7509 | lr 3.00e-04 | grad 1.19 | tok/s 5105
step    330 | loss 1.7599 | lr 3.00e-04 | grad 1.36 | tok/s 5234
step    340 | loss 1.8502 | lr 3.00e-04 | grad 1.19 | tok/s 5152
step    350 | loss 1.6968 | lr 3.00e-04 | grad 0.88 | tok/s 5110
step    360 | loss 1.7953 | lr 3.00e-04 | grad 1.72 | tok/s 5192
step    370 | loss 1.6203 | lr 3.00e-04 | grad 0.89 | tok/s 5240
step    380 | loss 1.9230 | lr 3.00e-04 | grad 1.38 | tok/s 5378
step    390 | loss 1.6129 | lr 3.00e-04 | grad 2.02 | tok/s 5208

Training complete! Final step: 393
