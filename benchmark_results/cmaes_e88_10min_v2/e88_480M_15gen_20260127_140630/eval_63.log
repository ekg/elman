Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v2/e88_480M_15gen_20260127_140630/eval_63/levelE88_100m_20260127_151924
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,388,628 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3379 | lr 3.00e-04 | grad 69.50 | tok/s 8648
step     20 | loss 3.4396 | lr 3.00e-04 | grad 20.50 | tok/s 18566
step     30 | loss 3.1256 | lr 3.00e-04 | grad 7.38 | tok/s 19604
step     40 | loss 5.3428 | lr 3.00e-04 | grad 45.50 | tok/s 19795
step     50 | loss 4.2892 | lr 3.00e-04 | grad 10.44 | tok/s 19939
step     60 | loss 3.5841 | lr 3.00e-04 | grad 11.44 | tok/s 19796
step     70 | loss 2.8574 | lr 3.00e-04 | grad 9.06 | tok/s 19742
step     80 | loss 2.5881 | lr 3.00e-04 | grad 4.69 | tok/s 19685
step     90 | loss 2.5243 | lr 3.00e-04 | grad 8.94 | tok/s 19585
step    100 | loss 2.3069 | lr 3.00e-04 | grad 5.31 | tok/s 19511
step    110 | loss 2.5425 | lr 3.00e-04 | grad 16.75 | tok/s 19286
step    120 | loss 2.5285 | lr 3.00e-04 | grad 5.06 | tok/s 18319
step    130 | loss 2.0873 | lr 3.00e-04 | grad 4.09 | tok/s 18852
step    140 | loss 2.3588 | lr 3.00e-04 | grad 8.50 | tok/s 18918
step    150 | loss 1.6480 | lr 3.00e-04 | grad 5.03 | tok/s 19275
step    160 | loss 2.2479 | lr 3.00e-04 | grad 3.36 | tok/s 18461
step    170 | loss 2.3349 | lr 3.00e-04 | grad 3.30 | tok/s 18492
step    180 | loss 1.8363 | lr 3.00e-04 | grad 3.59 | tok/s 18539
step    190 | loss 1.8512 | lr 3.00e-04 | grad 3.89 | tok/s 18672
step    200 | loss 1.6082 | lr 3.00e-04 | grad 2.78 | tok/s 19202
step    210 | loss 1.9579 | lr 3.00e-04 | grad 3.94 | tok/s 18193
step    220 | loss 2.3326 | lr 3.00e-04 | grad 19.38 | tok/s 18530
step    230 | loss 1.9614 | lr 3.00e-04 | grad 4.72 | tok/s 18223
step    240 | loss 2.2063 | lr 3.00e-04 | grad 2.69 | tok/s 18655
step    250 | loss 1.7900 | lr 3.00e-04 | grad 3.81 | tok/s 18664
step    260 | loss 1.9126 | lr 3.00e-04 | grad 3.56 | tok/s 19059
step    270 | loss 1.7684 | lr 3.00e-04 | grad 2.44 | tok/s 18446
step    280 | loss 1.7715 | lr 3.00e-04 | grad 2.19 | tok/s 17650
step    290 | loss 1.6613 | lr 3.00e-04 | grad 2.86 | tok/s 17936
step    300 | loss 2.0140 | lr 3.00e-04 | grad 2.77 | tok/s 18319
step    310 | loss 1.6739 | lr 3.00e-04 | grad 3.34 | tok/s 18003
step    320 | loss 1.8582 | lr 3.00e-04 | grad 3.17 | tok/s 18312
step    330 | loss 1.7310 | lr 3.00e-04 | grad 2.48 | tok/s 18453
step    340 | loss 2.0868 | lr 3.00e-04 | grad 3.31 | tok/s 18601
step    350 | loss 1.6806 | lr 3.00e-04 | grad 2.66 | tok/s 19079
step    360 | loss 1.5629 | lr 3.00e-04 | grad 1.91 | tok/s 18133
step    370 | loss 1.4643 | lr 3.00e-04 | grad 2.06 | tok/s 19068
step    380 | loss 1.2022 | lr 3.00e-04 | grad 2.30 | tok/s 19256
step    390 | loss 1.1007 | lr 3.00e-04 | grad 2.16 | tok/s 19250
step    400 | loss 1.8372 | lr 3.00e-04 | grad 2.94 | tok/s 18274
step    410 | loss 1.7645 | lr 3.00e-04 | grad 2.64 | tok/s 18428
step    420 | loss 1.5825 | lr 3.00e-04 | grad 10.56 | tok/s 19185
step    430 | loss 1.6091 | lr 3.00e-04 | grad 3.05 | tok/s 18715
step    440 | loss 1.7299 | lr 3.00e-04 | grad 3.69 | tok/s 18398
step    450 | loss 1.6244 | lr 3.00e-04 | grad 3.52 | tok/s 18442
step    460 | loss 1.6019 | lr 3.00e-04 | grad 2.06 | tok/s 18648
step    470 | loss 1.6203 | lr 3.00e-04 | grad 3.47 | tok/s 18844
step    480 | loss 1.5788 | lr 3.00e-04 | grad 3.22 | tok/s 17726
step    490 | loss 1.6995 | lr 3.00e-04 | grad 2.31 | tok/s 18524
step    500 | loss 1.8591 | lr 3.00e-04 | grad 3.47 | tok/s 18528
step    510 | loss 1.6611 | lr 3.00e-04 | grad 2.73 | tok/s 17644
step    520 | loss 1.5312 | lr 3.00e-04 | grad 2.50 | tok/s 18578
step    530 | loss 1.7451 | lr 3.00e-04 | grad 2.39 | tok/s 18532
step    540 | loss 1.5828 | lr 3.00e-04 | grad 2.55 | tok/s 15013
step    550 | loss 1.4982 | lr 3.00e-04 | grad 2.36 | tok/s 18883
step    560 | loss 1.4179 | lr 3.00e-04 | grad 2.47 | tok/s 19349
step    570 | loss 1.3428 | lr 3.00e-04 | grad 2.06 | tok/s 19421
step    580 | loss 1.2897 | lr 3.00e-04 | grad 2.64 | tok/s 19400
step    590 | loss 1.3419 | lr 3.00e-04 | grad 1.86 | tok/s 19327
step    600 | loss 1.2735 | lr 3.00e-04 | grad 1.95 | tok/s 19314
step    610 | loss 1.2989 | lr 3.00e-04 | grad 2.39 | tok/s 19289
step    620 | loss 1.5324 | lr 3.00e-04 | grad 7.53 | tok/s 18983
step    630 | loss 1.5978 | lr 3.00e-04 | grad 2.72 | tok/s 18332
step    640 | loss 1.6959 | lr 3.00e-04 | grad 2.09 | tok/s 18281
step    650 | loss 1.5635 | lr 3.00e-04 | grad 2.58 | tok/s 18660
step    660 | loss 1.6960 | lr 3.00e-04 | grad 3.83 | tok/s 19105
step    670 | loss 1.6321 | lr 3.00e-04 | grad 3.73 | tok/s 18048
step    680 | loss 1.5917 | lr 3.00e-04 | grad 3.47 | tok/s 18258
step    690 | loss 1.6380 | lr 3.00e-04 | grad 2.28 | tok/s 18197
step    700 | loss 1.4908 | lr 3.00e-04 | grad 4.22 | tok/s 18307
step    710 | loss 1.6477 | lr 3.00e-04 | grad 2.34 | tok/s 18435
step    720 | loss 1.2571 | lr 3.00e-04 | grad 2.19 | tok/s 18816
step    730 | loss 1.6086 | lr 3.00e-04 | grad 2.19 | tok/s 18421
step    740 | loss 1.8061 | lr 3.00e-04 | grad 4.25 | tok/s 19095
step    750 | loss 1.4767 | lr 3.00e-04 | grad 4.03 | tok/s 19141
step    760 | loss 1.5930 | lr 3.00e-04 | grad 2.25 | tok/s 18977
step    770 | loss 1.5718 | lr 3.00e-04 | grad 1.92 | tok/s 18571
step    780 | loss 1.4862 | lr 3.00e-04 | grad 2.42 | tok/s 18677
step    790 | loss 1.7549 | lr 3.00e-04 | grad 3.34 | tok/s 19007
step    800 | loss 1.1326 | lr 3.00e-04 | grad 2.45 | tok/s 18736
step    810 | loss 1.4978 | lr 3.00e-04 | grad 3.36 | tok/s 17892
step    820 | loss 1.4297 | lr 3.00e-04 | grad 2.05 | tok/s 18440
step    830 | loss 1.4218 | lr 3.00e-04 | grad 2.17 | tok/s 18503
step    840 | loss 1.7519 | lr 3.00e-04 | grad 3.55 | tok/s 18047
step    850 | loss 1.5141 | lr 3.00e-04 | grad 2.80 | tok/s 18559
step    860 | loss 1.6029 | lr 3.00e-04 | grad 2.62 | tok/s 19097
step    870 | loss 1.4342 | lr 3.00e-04 | grad 2.50 | tok/s 18939
step    880 | loss 1.6114 | lr 3.00e-04 | grad 2.59 | tok/s 18583
step    890 | loss 1.5052 | lr 3.00e-04 | grad 2.77 | tok/s 18521
step    900 | loss 1.5844 | lr 3.00e-04 | grad 2.66 | tok/s 17743
step    910 | loss 1.5223 | lr 3.00e-04 | grad 2.72 | tok/s 18531
step    920 | loss 1.5374 | lr 3.00e-04 | grad 2.06 | tok/s 18431
step    930 | loss 1.4096 | lr 3.00e-04 | grad 2.42 | tok/s 18474
step    940 | loss 1.3864 | lr 3.00e-04 | grad 2.17 | tok/s 17995
step    950 | loss 1.4734 | lr 3.00e-04 | grad 2.03 | tok/s 18189
step    960 | loss 1.4612 | lr 3.00e-04 | grad 2.70 | tok/s 18673
step    970 | loss 1.6921 | lr 3.00e-04 | grad 5.81 | tok/s 18615
step    980 | loss 1.7766 | lr 3.00e-04 | grad 4.28 | tok/s 19108
step    990 | loss 1.5273 | lr 3.00e-04 | grad 2.53 | tok/s 18547
step   1000 | loss 1.6232 | lr 3.00e-04 | grad 2.55 | tok/s 18561
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6232.pt
step   1010 | loss 1.1930 | lr 3.00e-04 | grad 1.18 | tok/s 10164
step   1020 | loss 1.3270 | lr 3.00e-04 | grad 2.03 | tok/s 19341
step   1030 | loss 1.6565 | lr 3.00e-04 | grad 1.84 | tok/s 18387
step   1040 | loss 2.2417 | lr 3.00e-04 | grad 3.17 | tok/s 19350
step   1050 | loss 1.5351 | lr 3.00e-04 | grad 12.00 | tok/s 18715
step   1060 | loss 1.0919 | lr 3.00e-04 | grad 4.28 | tok/s 19127
step   1070 | loss 1.4579 | lr 3.00e-04 | grad 2.33 | tok/s 18976
step   1080 | loss 1.2668 | lr 3.00e-04 | grad 2.45 | tok/s 19409
step   1090 | loss 1.2410 | lr 3.00e-04 | grad 1.91 | tok/s 19418
step   1100 | loss 1.2049 | lr 3.00e-04 | grad 2.41 | tok/s 19444
step   1110 | loss 1.2830 | lr 3.00e-04 | grad 2.64 | tok/s 19294
step   1120 | loss 1.5278 | lr 3.00e-04 | grad 3.66 | tok/s 19075
step   1130 | loss 1.8590 | lr 3.00e-04 | grad 4.41 | tok/s 19110
step   1140 | loss 1.4529 | lr 3.00e-04 | grad 2.70 | tok/s 18966
step   1150 | loss 1.7007 | lr 3.00e-04 | grad 2.30 | tok/s 18664
step   1160 | loss 1.7741 | lr 3.00e-04 | grad 2.12 | tok/s 18423
step   1170 | loss 1.4782 | lr 3.00e-04 | grad 2.89 | tok/s 18532
step   1180 | loss 1.4601 | lr 3.00e-04 | grad 3.30 | tok/s 19103
step   1190 | loss 1.4522 | lr 3.00e-04 | grad 1.88 | tok/s 19371
step   1200 | loss 1.1391 | lr 3.00e-04 | grad 2.20 | tok/s 19097
step   1210 | loss 1.4944 | lr 3.00e-04 | grad 2.23 | tok/s 17703
step   1220 | loss 1.4144 | lr 3.00e-04 | grad 2.78 | tok/s 18777
step   1230 | loss 1.3180 | lr 3.00e-04 | grad 2.38 | tok/s 19186
step   1240 | loss 1.3349 | lr 3.00e-04 | grad 3.50 | tok/s 18866
step   1250 | loss 1.4902 | lr 3.00e-04 | grad 5.69 | tok/s 19175
step   1260 | loss 1.4612 | lr 3.00e-04 | grad 1.99 | tok/s 18982
step   1270 | loss 1.3620 | lr 3.00e-04 | grad 1.73 | tok/s 18898
step   1280 | loss 1.4916 | lr 3.00e-04 | grad 2.17 | tok/s 18443
step   1290 | loss 1.3886 | lr 3.00e-04 | grad 2.31 | tok/s 18148
step   1300 | loss 1.6201 | lr 3.00e-04 | grad 2.92 | tok/s 18555
step   1310 | loss 1.5234 | lr 3.00e-04 | grad 2.23 | tok/s 18878
step   1320 | loss 1.5889 | lr 3.00e-04 | grad 3.91 | tok/s 19038
step   1330 | loss 1.4139 | lr 3.00e-04 | grad 2.86 | tok/s 18358
step   1340 | loss 1.5510 | lr 3.00e-04 | grad 1.85 | tok/s 18795
step   1350 | loss 1.5616 | lr 3.00e-04 | grad 3.17 | tok/s 18580
step   1360 | loss 1.3219 | lr 3.00e-04 | grad 1.57 | tok/s 18285
step   1370 | loss 1.7645 | lr 3.00e-04 | grad 2.67 | tok/s 18957
step   1380 | loss 1.4529 | lr 3.00e-04 | grad 2.73 | tok/s 18113

Training complete! Final step: 1385
