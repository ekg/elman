Using device: cuda
Output directory: benchmark_results/cmaes_500m_matched/mamba2/levelmamba2_100m_20260125_030505
Model: Level mamba2, 322,600,544 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3238 | lr 3.00e-04 | grad 4.25 | tok/s 5080
step     20 | loss 2.2887 | lr 3.00e-04 | grad 1.98 | tok/s 32211
step     30 | loss 2.5220 | lr 3.00e-04 | grad 2.64 | tok/s 32103
step     40 | loss 2.4326 | lr 3.00e-04 | grad 9.69 | tok/s 33009
step     50 | loss 3.8140 | lr 3.00e-04 | grad 3.33 | tok/s 34114
step     60 | loss 2.5991 | lr 3.00e-04 | grad 3.64 | tok/s 34047
step     70 | loss 2.2595 | lr 3.00e-04 | grad 2.41 | tok/s 33783
step     80 | loss 2.1614 | lr 3.00e-04 | grad 2.16 | tok/s 33607
step     90 | loss 1.8710 | lr 3.00e-04 | grad 3.00 | tok/s 33510
step    100 | loss 1.8341 | lr 3.00e-04 | grad 2.34 | tok/s 33452
step    110 | loss 1.6822 | lr 3.00e-04 | grad 2.33 | tok/s 33125
step    120 | loss 2.2171 | lr 3.00e-04 | grad 2.14 | tok/s 32099
step    130 | loss 1.9288 | lr 3.00e-04 | grad 1.90 | tok/s 31274
step    140 | loss 1.7910 | lr 3.00e-04 | grad 3.89 | tok/s 31286
step    150 | loss 1.7800 | lr 3.00e-04 | grad 2.52 | tok/s 32263
step    160 | loss 1.9035 | lr 3.00e-04 | grad 2.89 | tok/s 32281
step    170 | loss 2.0086 | lr 3.00e-04 | grad 4.25 | tok/s 30464
step    180 | loss 1.7623 | lr 3.00e-04 | grad 2.52 | tok/s 31384
step    190 | loss 1.6328 | lr 3.00e-04 | grad 2.50 | tok/s 30086
step    200 | loss 1.5463 | lr 3.00e-04 | grad 1.84 | tok/s 32034
step    210 | loss 1.4130 | lr 3.00e-04 | grad 2.25 | tok/s 30990
step    220 | loss 1.9822 | lr 3.00e-04 | grad 3.80 | tok/s 29845
step    230 | loss 1.7945 | lr 3.00e-04 | grad 1.38 | tok/s 29815
step    240 | loss 1.7378 | lr 3.00e-04 | grad 2.17 | tok/s 29892
step    250 | loss 1.9004 | lr 3.00e-04 | grad 1.48 | tok/s 29863
step    260 | loss 1.6017 | lr 3.00e-04 | grad 1.28 | tok/s 30733
step    270 | loss 1.7466 | lr 3.00e-04 | grad 1.36 | tok/s 30697
step    280 | loss 1.5846 | lr 3.00e-04 | grad 1.80 | tok/s 29734
step    290 | loss 1.5295 | lr 3.00e-04 | grad 2.75 | tok/s 28620
step    300 | loss 1.6229 | lr 3.00e-04 | grad 1.91 | tok/s 28910
step    310 | loss 1.6491 | lr 3.00e-04 | grad 1.55 | tok/s 29504
step    320 | loss 1.5065 | lr 3.00e-04 | grad 2.94 | tok/s 28246
step    330 | loss 1.6602 | lr 3.00e-04 | grad 1.86 | tok/s 29462
step    340 | loss 1.7439 | lr 3.00e-04 | grad 2.34 | tok/s 29962
step    350 | loss 1.7072 | lr 3.00e-04 | grad 2.41 | tok/s 29333
step    360 | loss 1.5271 | lr 3.00e-04 | grad 1.64 | tok/s 29941
step    370 | loss 1.3636 | lr 3.00e-04 | grad 1.22 | tok/s 29379
step    380 | loss 1.3942 | lr 3.00e-04 | grad 1.33 | tok/s 30644
step    390 | loss 1.1040 | lr 3.00e-04 | grad 1.16 | tok/s 30801
step    400 | loss 1.0355 | lr 3.00e-04 | grad 1.55 | tok/s 30368
step    410 | loss 1.7448 | lr 3.00e-04 | grad 1.50 | tok/s 29247
step    420 | loss 1.5914 | lr 3.00e-04 | grad 1.73 | tok/s 29219
step    430 | loss 1.5276 | lr 3.00e-04 | grad 1.82 | tok/s 30563
step    440 | loss 1.4520 | lr 3.00e-04 | grad 1.88 | tok/s 29563
step    450 | loss 1.6067 | lr 3.00e-04 | grad 1.49 | tok/s 29154
step    460 | loss 1.4484 | lr 3.00e-04 | grad 3.78 | tok/s 28881
step    470 | loss 1.5064 | lr 3.00e-04 | grad 2.03 | tok/s 28837
step    480 | loss 1.4804 | lr 3.00e-04 | grad 1.72 | tok/s 30220
step    490 | loss 1.4627 | lr 3.00e-04 | grad 1.61 | tok/s 29210
step    500 | loss 1.4995 | lr 3.00e-04 | grad 1.10 | tok/s 28994
step    510 | loss 1.7067 | lr 3.00e-04 | grad 7.47 | tok/s 28543
step    520 | loss 1.5306 | lr 3.00e-04 | grad 1.70 | tok/s 27361
step    530 | loss 1.4102 | lr 3.00e-04 | grad 1.60 | tok/s 28949
step    540 | loss 1.5992 | lr 3.00e-04 | grad 1.45 | tok/s 28842
step    550 | loss 1.4875 | lr 3.00e-04 | grad 1.44 | tok/s 28182
step    560 | loss 1.2656 | lr 3.00e-04 | grad 1.48 | tok/s 29600
step    570 | loss 1.3419 | lr 3.00e-04 | grad 1.52 | tok/s 30261
step    580 | loss 1.2591 | lr 3.00e-04 | grad 1.28 | tok/s 30307
step    590 | loss 1.2114 | lr 3.00e-04 | grad 1.20 | tok/s 30255
step    600 | loss 1.2759 | lr 3.00e-04 | grad 1.53 | tok/s 30220
step    610 | loss 1.1947 | lr 3.00e-04 | grad 1.28 | tok/s 30226
step    620 | loss 1.2289 | lr 3.00e-04 | grad 1.12 | tok/s 30189
step    630 | loss 1.2664 | lr 3.00e-04 | grad 3.36 | tok/s 29770
step    640 | loss 1.5189 | lr 3.00e-04 | grad 2.30 | tok/s 28449
step    650 | loss 1.5528 | lr 3.00e-04 | grad 1.55 | tok/s 28257
step    660 | loss 1.4406 | lr 3.00e-04 | grad 1.75 | tok/s 28555
step    670 | loss 1.4899 | lr 3.00e-04 | grad 1.65 | tok/s 29430
step    680 | loss 1.5125 | lr 3.00e-04 | grad 1.88 | tok/s 28447
step    690 | loss 1.5245 | lr 3.00e-04 | grad 1.31 | tok/s 28223
step    700 | loss 1.4695 | lr 3.00e-04 | grad 1.56 | tok/s 27968
step    710 | loss 1.3820 | lr 3.00e-04 | grad 1.07 | tok/s 28724
step    720 | loss 1.5287 | lr 3.00e-04 | grad 2.25 | tok/s 28137
step    730 | loss 1.2341 | lr 3.00e-04 | grad 1.27 | tok/s 29404
step    740 | loss 1.3167 | lr 3.00e-04 | grad 1.21 | tok/s 28531
step    750 | loss 1.6807 | lr 3.00e-04 | grad 3.33 | tok/s 29689
step    760 | loss 1.4639 | lr 3.00e-04 | grad 1.30 | tok/s 29619
step    770 | loss 1.4050 | lr 3.00e-04 | grad 2.19 | tok/s 29035
step    780 | loss 1.4310 | lr 3.00e-04 | grad 1.43 | tok/s 28178
step    790 | loss 1.3984 | lr 3.00e-04 | grad 1.61 | tok/s 28906
step    800 | loss 1.5110 | lr 3.00e-04 | grad 2.89 | tok/s 29738
step    810 | loss 1.3066 | lr 3.00e-04 | grad 1.75 | tok/s 28877
step    820 | loss 1.1901 | lr 3.00e-04 | grad 2.94 | tok/s 28212
step    830 | loss 1.3146 | lr 3.00e-04 | grad 1.89 | tok/s 28535
step    840 | loss 1.4266 | lr 3.00e-04 | grad 1.20 | tok/s 27955
step    850 | loss 1.4690 | lr 3.00e-04 | grad 1.22 | tok/s 28046
step    860 | loss 1.4856 | lr 3.00e-04 | grad 1.59 | tok/s 28352
step    870 | loss 1.4359 | lr 3.00e-04 | grad 2.31 | tok/s 28576
step    880 | loss 1.3233 | lr 3.00e-04 | grad 1.59 | tok/s 29877
step    890 | loss 1.5039 | lr 3.00e-04 | grad 1.81 | tok/s 28469
step    900 | loss 1.4210 | lr 3.00e-04 | grad 1.22 | tok/s 28424
step    910 | loss 1.3984 | lr 3.00e-04 | grad 1.53 | tok/s 28728
step    920 | loss 1.4174 | lr 3.00e-04 | grad 1.20 | tok/s 28306
step    930 | loss 1.4286 | lr 3.00e-04 | grad 1.20 | tok/s 28444
step    940 | loss 1.3186 | lr 3.00e-04 | grad 1.45 | tok/s 29277
step    950 | loss 1.2581 | lr 3.00e-04 | grad 1.12 | tok/s 28028
step    960 | loss 1.3917 | lr 3.00e-04 | grad 1.31 | tok/s 27666
step    970 | loss 1.3554 | lr 3.00e-04 | grad 1.24 | tok/s 27985
step    980 | loss 1.3675 | lr 3.00e-04 | grad 1.38 | tok/s 28725
step    990 | loss 1.7790 | lr 3.00e-04 | grad 2.33 | tok/s 29863
step   1000 | loss 1.4983 | lr 3.00e-04 | grad 1.14 | tok/s 28570
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4983.pt
step   1010 | loss 1.4171 | lr 3.00e-04 | grad 1.42 | tok/s 15945
step   1020 | loss 1.1755 | lr 3.00e-04 | grad 1.44 | tok/s 29291
step   1030 | loss 1.1184 | lr 3.00e-04 | grad 2.58 | tok/s 29859
step   1040 | loss 1.3081 | lr 3.00e-04 | grad 1.44 | tok/s 29681
step   1050 | loss 1.6398 | lr 3.00e-04 | grad 4.78 | tok/s 28417
step   1060 | loss 1.8175 | lr 3.00e-04 | grad 1.09 | tok/s 29708
step   1070 | loss 1.3829 | lr 3.00e-04 | grad 1.84 | tok/s 28764
step   1080 | loss 0.9988 | lr 3.00e-04 | grad 1.99 | tok/s 29521
step   1090 | loss 1.3211 | lr 3.00e-04 | grad 1.78 | tok/s 29394
step   1100 | loss 1.1822 | lr 3.00e-04 | grad 1.29 | tok/s 30187
step   1110 | loss 1.1684 | lr 3.00e-04 | grad 0.92 | tok/s 30178
step   1120 | loss 1.1359 | lr 3.00e-04 | grad 1.29 | tok/s 30173
step   1130 | loss 1.1953 | lr 3.00e-04 | grad 1.24 | tok/s 29309
step   1140 | loss 1.4064 | lr 3.00e-04 | grad 1.79 | tok/s 29593
step   1150 | loss 1.6996 | lr 3.00e-04 | grad 2.86 | tok/s 29090
step   1160 | loss 1.3575 | lr 3.00e-04 | grad 1.80 | tok/s 29072
step   1170 | loss 1.5067 | lr 3.00e-04 | grad 1.45 | tok/s 28411
step   1180 | loss 1.6763 | lr 3.00e-04 | grad 1.32 | tok/s 28580
step   1190 | loss 1.2771 | lr 3.00e-04 | grad 0.93 | tok/s 27721
step   1200 | loss 1.3559 | lr 3.00e-04 | grad 3.05 | tok/s 29444
step   1210 | loss 1.4204 | lr 3.00e-04 | grad 1.32 | tok/s 30127
step   1220 | loss 1.0698 | lr 3.00e-04 | grad 1.52 | tok/s 29745
step   1230 | loss 1.3715 | lr 3.00e-04 | grad 1.24 | tok/s 28033
step   1240 | loss 1.2571 | lr 3.00e-04 | grad 1.42 | tok/s 28758
step   1250 | loss 1.2170 | lr 3.00e-04 | grad 1.42 | tok/s 29412
step   1260 | loss 1.2543 | lr 3.00e-04 | grad 1.38 | tok/s 29161
step   1270 | loss 1.3456 | lr 3.00e-04 | grad 1.98 | tok/s 29436
step   1280 | loss 1.3161 | lr 3.00e-04 | grad 1.34 | tok/s 29171
step   1290 | loss 1.2790 | lr 3.00e-04 | grad 1.00 | tok/s 28974
step   1300 | loss 1.3221 | lr 3.00e-04 | grad 1.23 | tok/s 28530
step   1310 | loss 1.2721 | lr 3.00e-04 | grad 1.37 | tok/s 28306
step   1320 | loss 1.5518 | lr 3.00e-04 | grad 2.64 | tok/s 28421
step   1330 | loss 1.4367 | lr 3.00e-04 | grad 1.62 | tok/s 29125
step   1340 | loss 1.4310 | lr 3.00e-04 | grad 1.49 | tok/s 29468
step   1350 | loss 1.3148 | lr 3.00e-04 | grad 1.49 | tok/s 28478
step   1360 | loss 1.4016 | lr 3.00e-04 | grad 1.39 | tok/s 28110
step   1370 | loss 1.3882 | lr 3.00e-04 | grad 4.06 | tok/s 28824
step   1380 | loss 1.3248 | lr 3.00e-04 | grad 1.40 | tok/s 28288
step   1390 | loss 1.5893 | lr 3.00e-04 | grad 2.42 | tok/s 29536
step   1400 | loss 1.3172 | lr 3.00e-04 | grad 2.45 | tok/s 27804
step   1410 | loss 1.2060 | lr 3.00e-04 | grad 1.23 | tok/s 29011
step   1420 | loss 1.4205 | lr 3.00e-04 | grad 1.16 | tok/s 28640
step   1430 | loss 1.3341 | lr 3.00e-04 | grad 2.95 | tok/s 27636
step   1440 | loss 1.1347 | lr 3.00e-04 | grad 5.66 | tok/s 29730
step   1450 | loss 1.5349 | lr 3.00e-04 | grad 1.40 | tok/s 28437
step   1460 | loss 1.3645 | lr 3.00e-04 | grad 1.49 | tok/s 29510
step   1470 | loss 1.3395 | lr 3.00e-04 | grad 2.02 | tok/s 29383
step   1480 | loss 1.4718 | lr 3.00e-04 | grad 4.00 | tok/s 28101
step   1490 | loss 1.2603 | lr 3.00e-04 | grad 1.28 | tok/s 27428
step   1500 | loss 1.2577 | lr 3.00e-04 | grad 1.32 | tok/s 29429
step   1510 | loss 1.7187 | lr 3.00e-04 | grad 5.53 | tok/s 28553
step   1520 | loss 1.3593 | lr 3.00e-04 | grad 1.41 | tok/s 28820
step   1530 | loss 1.1752 | lr 3.00e-04 | grad 1.15 | tok/s 28344
step   1540 | loss 1.4044 | lr 3.00e-04 | grad 1.23 | tok/s 28783
step   1550 | loss 1.2938 | lr 3.00e-04 | grad 1.20 | tok/s 28798
step   1560 | loss 1.4271 | lr 3.00e-04 | grad 1.14 | tok/s 28977
step   1570 | loss 1.3519 | lr 3.00e-04 | grad 2.12 | tok/s 28501
step   1580 | loss 1.0542 | lr 3.00e-04 | grad 0.92 | tok/s 29686
step   1590 | loss 1.2616 | lr 3.00e-04 | grad 1.17 | tok/s 29008
step   1600 | loss 1.1627 | lr 3.00e-04 | grad 1.51 | tok/s 29158
step   1610 | loss 1.3662 | lr 3.00e-04 | grad 3.34 | tok/s 27897
step   1620 | loss 1.1818 | lr 3.00e-04 | grad 4.19 | tok/s 29817
step   1630 | loss 1.7672 | lr 3.00e-04 | grad 2.70 | tok/s 29067
step   1640 | loss 1.9173 | lr 3.00e-04 | grad 1.41 | tok/s 29994
step   1650 | loss 1.5532 | lr 3.00e-04 | grad 1.70 | tok/s 29985
step   1660 | loss 1.4100 | lr 3.00e-04 | grad 1.61 | tok/s 30021
step   1670 | loss 1.3142 | lr 3.00e-04 | grad 1.46 | tok/s 29988
step   1680 | loss 1.2776 | lr 3.00e-04 | grad 1.41 | tok/s 30032
step   1690 | loss 1.4220 | lr 3.00e-04 | grad 1.76 | tok/s 28776
step   1700 | loss 1.3131 | lr 3.00e-04 | grad 1.20 | tok/s 28409
step   1710 | loss 1.3694 | lr 3.00e-04 | grad 1.99 | tok/s 27399
step   1720 | loss 1.3007 | lr 3.00e-04 | grad 1.35 | tok/s 28620
step   1730 | loss 1.1200 | lr 3.00e-04 | grad 2.22 | tok/s 29516
step   1740 | loss 1.3979 | lr 3.00e-04 | grad 1.78 | tok/s 28162
step   1750 | loss 1.3177 | lr 3.00e-04 | grad 1.63 | tok/s 28888
step   1760 | loss 1.2981 | lr 3.00e-04 | grad 1.35 | tok/s 28468
step   1770 | loss 1.2374 | lr 3.00e-04 | grad 1.27 | tok/s 28894
step   1780 | loss 1.2563 | lr 3.00e-04 | grad 1.53 | tok/s 28447
step   1790 | loss 1.4993 | lr 3.00e-04 | grad 1.43 | tok/s 28439
step   1800 | loss 1.6567 | lr 3.00e-04 | grad 1.24 | tok/s 27482
step   1810 | loss 1.2939 | lr 3.00e-04 | grad 1.27 | tok/s 28325
step   1820 | loss 1.1456 | lr 3.00e-04 | grad 1.31 | tok/s 28670
step   1830 | loss 1.3532 | lr 3.00e-04 | grad 1.04 | tok/s 28065
step   1840 | loss 1.3477 | lr 3.00e-04 | grad 1.85 | tok/s 28629
step   1850 | loss 1.3898 | lr 3.00e-04 | grad 1.89 | tok/s 28025
step   1860 | loss 1.3472 | lr 3.00e-04 | grad 1.82 | tok/s 28553
step   1870 | loss 1.2468 | lr 3.00e-04 | grad 3.52 | tok/s 27632
step   1880 | loss 1.3460 | lr 3.00e-04 | grad 2.64 | tok/s 28332
step   1890 | loss 1.4148 | lr 3.00e-04 | grad 1.19 | tok/s 28097
step   1900 | loss 1.2692 | lr 3.00e-04 | grad 1.33 | tok/s 29295
step   1910 | loss 1.2363 | lr 3.00e-04 | grad 1.68 | tok/s 29996
step   1920 | loss 1.1566 | lr 3.00e-04 | grad 1.05 | tok/s 29963
step   1930 | loss 1.1435 | lr 3.00e-04 | grad 1.25 | tok/s 29980
step   1940 | loss 1.1119 | lr 3.00e-04 | grad 1.06 | tok/s 29991
step   1950 | loss 1.0738 | lr 3.00e-04 | grad 1.26 | tok/s 29950
step   1960 | loss 1.3427 | lr 3.00e-04 | grad 4.53 | tok/s 28354
step   1970 | loss 1.4088 | lr 3.00e-04 | grad 1.36 | tok/s 28121
step   1980 | loss 1.2926 | lr 3.00e-04 | grad 1.72 | tok/s 28768
step   1990 | loss 1.4140 | lr 3.00e-04 | grad 2.50 | tok/s 28081
step   2000 | loss 1.3068 | lr 3.00e-04 | grad 1.40 | tok/s 27454
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3068.pt
step   2010 | loss 1.4504 | lr 3.00e-04 | grad 1.38 | tok/s 17220
step   2020 | loss 1.3023 | lr 3.00e-04 | grad 1.41 | tok/s 29475
step   2030 | loss 1.0069 | lr 3.00e-04 | grad 1.32 | tok/s 30324
step   2040 | loss 1.3057 | lr 3.00e-04 | grad 1.38 | tok/s 27108
step   2050 | loss 1.3224 | lr 3.00e-04 | grad 1.55 | tok/s 28887
step   2060 | loss 1.6046 | lr 3.00e-04 | grad 1.34 | tok/s 27470
step   2070 | loss 1.3001 | lr 3.00e-04 | grad 1.30 | tok/s 29118
step   2080 | loss 1.3034 | lr 3.00e-04 | grad 1.70 | tok/s 28389
step   2090 | loss 1.5112 | lr 3.00e-04 | grad 2.12 | tok/s 28837
step   2100 | loss 1.0401 | lr 3.00e-04 | grad 1.26 | tok/s 28913
step   2110 | loss 1.1065 | lr 3.00e-04 | grad 1.08 | tok/s 28525
step   2120 | loss 1.4939 | lr 3.00e-04 | grad 3.27 | tok/s 29209
step   2130 | loss 1.4861 | lr 3.00e-04 | grad 1.27 | tok/s 29592
step   2140 | loss 1.2963 | lr 3.00e-04 | grad 1.62 | tok/s 28277
step   2150 | loss 2.0089 | lr 3.00e-04 | grad 1.44 | tok/s 29125

Training complete! Final step: 2155
