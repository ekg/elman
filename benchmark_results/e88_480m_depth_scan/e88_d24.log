Using device: cuda
Output directory: benchmark_results/e88_480m_depth_scan/e88_d24/levelE88_100m_20260125_162909
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,164,928 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2776 | lr 3.00e-04 | grad 11.12 | tok/s 7430
step     20 | loss 2.7879 | lr 3.00e-04 | grad 5.53 | tok/s 11709
step     30 | loss 2.9049 | lr 3.00e-04 | grad 3.16 | tok/s 11638
step     40 | loss 2.8689 | lr 3.00e-04 | grad 47.50 | tok/s 11976
step     50 | loss 5.2101 | lr 3.00e-04 | grad 22.88 | tok/s 12366
step     60 | loss 4.2201 | lr 3.00e-04 | grad 19.12 | tok/s 12281
step     70 | loss 3.4288 | lr 3.00e-04 | grad 5.09 | tok/s 12211
step     80 | loss 3.0421 | lr 3.00e-04 | grad 6.75 | tok/s 12125
step     90 | loss 2.5003 | lr 3.00e-04 | grad 5.16 | tok/s 12061
step    100 | loss 2.3693 | lr 3.00e-04 | grad 3.44 | tok/s 12018
step    110 | loss 2.1972 | lr 3.00e-04 | grad 1.83 | tok/s 11889
step    120 | loss 2.5470 | lr 3.00e-04 | grad 1.71 | tok/s 11457
step    130 | loss 2.2120 | lr 3.00e-04 | grad 1.77 | tok/s 11180
step    140 | loss 2.0852 | lr 3.00e-04 | grad 2.94 | tok/s 11213
step    150 | loss 1.8491 | lr 3.00e-04 | grad 2.42 | tok/s 11587
step    160 | loss 2.0000 | lr 3.00e-04 | grad 2.52 | tok/s 11640
step    170 | loss 2.2617 | lr 3.00e-04 | grad 4.97 | tok/s 10988
step    180 | loss 2.0492 | lr 3.00e-04 | grad 3.50 | tok/s 11384
step    190 | loss 1.8809 | lr 3.00e-04 | grad 2.59 | tok/s 10899
step    200 | loss 1.7743 | lr 3.00e-04 | grad 1.56 | tok/s 11676
step    210 | loss 1.6386 | lr 3.00e-04 | grad 2.09 | tok/s 11328
step    220 | loss 2.1985 | lr 3.00e-04 | grad 3.30 | tok/s 10931
step    230 | loss 2.0481 | lr 3.00e-04 | grad 1.33 | tok/s 10943
step    240 | loss 1.9229 | lr 3.00e-04 | grad 3.77 | tok/s 10997
step    250 | loss 2.1167 | lr 3.00e-04 | grad 1.51 | tok/s 11041
step    260 | loss 1.7989 | lr 3.00e-04 | grad 1.25 | tok/s 11415
step    270 | loss 1.9359 | lr 3.00e-04 | grad 1.32 | tok/s 11423
step    280 | loss 1.7456 | lr 3.00e-04 | grad 1.69 | tok/s 11083
step    290 | loss 1.7091 | lr 3.00e-04 | grad 2.53 | tok/s 10643
step    300 | loss 1.7874 | lr 3.00e-04 | grad 1.98 | tok/s 10821
step    310 | loss 1.8019 | lr 3.00e-04 | grad 1.44 | tok/s 11061
step    320 | loss 1.6425 | lr 3.00e-04 | grad 2.62 | tok/s 10597
step    330 | loss 1.8044 | lr 3.00e-04 | grad 1.57 | tok/s 11105
step    340 | loss 1.9108 | lr 3.00e-04 | grad 2.50 | tok/s 11333
step    350 | loss 1.8548 | lr 3.00e-04 | grad 2.22 | tok/s 11129
step    360 | loss 1.6570 | lr 3.00e-04 | grad 1.56 | tok/s 11407
step    370 | loss 1.4809 | lr 3.00e-04 | grad 1.27 | tok/s 11195
step    380 | loss 1.5129 | lr 3.00e-04 | grad 1.32 | tok/s 11707
step    390 | loss 1.2209 | lr 3.00e-04 | grad 1.21 | tok/s 11806
step    400 | loss 1.1334 | lr 3.00e-04 | grad 1.48 | tok/s 11630
step    410 | loss 1.8404 | lr 3.00e-04 | grad 1.45 | tok/s 11236
step    420 | loss 1.6956 | lr 3.00e-04 | grad 1.74 | tok/s 11223
step    430 | loss 1.6516 | lr 3.00e-04 | grad 2.17 | tok/s 11790
step    440 | loss 1.5605 | lr 3.00e-04 | grad 1.90 | tok/s 11421
step    450 | loss 1.7344 | lr 3.00e-04 | grad 1.51 | tok/s 11024
step    460 | loss 1.5497 | lr 3.00e-04 | grad 4.19 | tok/s 11162
step    470 | loss 1.6147 | lr 3.00e-04 | grad 2.08 | tok/s 11171
step    480 | loss 1.5737 | lr 3.00e-04 | grad 1.62 | tok/s 11725
step    490 | loss 1.5536 | lr 3.00e-04 | grad 1.55 | tok/s 11321
step    500 | loss 1.6179 | lr 3.00e-04 | grad 1.20 | tok/s 11235
step    510 | loss 1.8201 | lr 3.00e-04 | grad 7.44 | tok/s 11055
step    520 | loss 1.6331 | lr 3.00e-04 | grad 1.66 | tok/s 10579
step    530 | loss 1.5000 | lr 3.00e-04 | grad 1.58 | tok/s 11242
step    540 | loss 1.7151 | lr 3.00e-04 | grad 1.54 | tok/s 11218
step    550 | loss 1.5885 | lr 3.00e-04 | grad 1.54 | tok/s 10967
step    560 | loss 1.3630 | lr 3.00e-04 | grad 1.55 | tok/s 11451
step    570 | loss 1.4232 | lr 3.00e-04 | grad 1.55 | tok/s 11818
step    580 | loss 1.3394 | lr 3.00e-04 | grad 1.23 | tok/s 11823
step    590 | loss 1.2961 | lr 3.00e-04 | grad 1.08 | tok/s 11811
step    600 | loss 1.3566 | lr 3.00e-04 | grad 1.61 | tok/s 11812
step    610 | loss 1.2779 | lr 3.00e-04 | grad 1.23 | tok/s 11816
step    620 | loss 1.3067 | lr 3.00e-04 | grad 1.11 | tok/s 11815
step    630 | loss 1.3590 | lr 3.00e-04 | grad 3.20 | tok/s 11650
step    640 | loss 1.6425 | lr 3.00e-04 | grad 2.58 | tok/s 11112
step    650 | loss 1.6436 | lr 3.00e-04 | grad 2.17 | tok/s 11049
step    660 | loss 1.5309 | lr 3.00e-04 | grad 1.86 | tok/s 11148
step    670 | loss 1.5612 | lr 3.00e-04 | grad 1.55 | tok/s 11540
step    680 | loss 1.6146 | lr 3.00e-04 | grad 2.09 | tok/s 11133
step    690 | loss 1.6326 | lr 3.00e-04 | grad 1.36 | tok/s 11044
step    700 | loss 1.5745 | lr 3.00e-04 | grad 1.70 | tok/s 10945
step    710 | loss 1.4773 | lr 3.00e-04 | grad 1.19 | tok/s 11235
step    720 | loss 1.6446 | lr 3.00e-04 | grad 2.19 | tok/s 10853
step    730 | loss 1.3075 | lr 3.00e-04 | grad 1.28 | tok/s 11525
step    740 | loss 1.4282 | lr 3.00e-04 | grad 1.23 | tok/s 11183
step    750 | loss 1.7795 | lr 3.00e-04 | grad 2.89 | tok/s 11647
step    760 | loss 1.5417 | lr 3.00e-04 | grad 1.24 | tok/s 11633
step    770 | loss 1.5074 | lr 3.00e-04 | grad 1.98 | tok/s 11363
step    780 | loss 1.5274 | lr 3.00e-04 | grad 1.49 | tok/s 11056
step    790 | loss 1.4752 | lr 3.00e-04 | grad 1.62 | tok/s 11345
step    800 | loss 1.6027 | lr 3.00e-04 | grad 3.17 | tok/s 11671
step    810 | loss 1.3816 | lr 3.00e-04 | grad 1.62 | tok/s 11297
step    820 | loss 1.2514 | lr 3.00e-04 | grad 3.00 | tok/s 10971
step    830 | loss 1.3798 | lr 3.00e-04 | grad 2.00 | tok/s 11193
step    840 | loss 1.5096 | lr 3.00e-04 | grad 1.20 | tok/s 10936
step    850 | loss 1.5600 | lr 3.00e-04 | grad 1.27 | tok/s 10975

Training complete! Final step: 858
