Using device: cuda
Output directory: benchmark_results/e88_480m_depth_scan/e88_d32/levelE88_100m_20260125_162909
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,349,568 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.6825 | lr 3.00e-04 | grad 11.62 | tok/s 5937
step     20 | loss 2.8279 | lr 3.00e-04 | grad 4.91 | tok/s 8172
step     30 | loss 3.0264 | lr 3.00e-04 | grad 2.75 | tok/s 8100
step     40 | loss 2.9026 | lr 3.00e-04 | grad 59.50 | tok/s 8339
step     50 | loss 5.1658 | lr 3.00e-04 | grad 32.50 | tok/s 8593
step     60 | loss 4.5580 | lr 3.00e-04 | grad 31.75 | tok/s 8515
step     70 | loss 4.0451 | lr 3.00e-04 | grad 8.88 | tok/s 8431
step     80 | loss 3.7531 | lr 3.00e-04 | grad 14.38 | tok/s 8369
step     90 | loss 2.9656 | lr 3.00e-04 | grad 10.06 | tok/s 8262
step    100 | loss 2.6800 | lr 3.00e-04 | grad 3.56 | tok/s 8278
step    110 | loss 2.4732 | lr 3.00e-04 | grad 1.78 | tok/s 8216
step    120 | loss 2.6888 | lr 3.00e-04 | grad 1.47 | tok/s 7955
step    130 | loss 2.2624 | lr 3.00e-04 | grad 1.66 | tok/s 7772
step    140 | loss 2.1525 | lr 3.00e-04 | grad 2.50 | tok/s 7804
step    150 | loss 1.9820 | lr 3.00e-04 | grad 2.78 | tok/s 8064
step    160 | loss 2.0638 | lr 3.00e-04 | grad 2.03 | tok/s 8104
step    170 | loss 2.3123 | lr 3.00e-04 | grad 4.66 | tok/s 7678
step    180 | loss 2.1676 | lr 3.00e-04 | grad 3.53 | tok/s 7961
step    190 | loss 1.9736 | lr 3.00e-04 | grad 2.22 | tok/s 7624
step    200 | loss 1.8558 | lr 3.00e-04 | grad 1.51 | tok/s 8170
step    210 | loss 1.7261 | lr 3.00e-04 | grad 1.78 | tok/s 7933
step    220 | loss 2.2474 | lr 3.00e-04 | grad 3.00 | tok/s 7658
step    230 | loss 2.1027 | lr 3.00e-04 | grad 1.15 | tok/s 7549
step    240 | loss 1.9747 | lr 3.00e-04 | grad 2.67 | tok/s 7717
step    250 | loss 2.1719 | lr 3.00e-04 | grad 1.32 | tok/s 7741
step    260 | loss 1.8540 | lr 3.00e-04 | grad 1.16 | tok/s 8006
step    270 | loss 1.9819 | lr 3.00e-04 | grad 1.10 | tok/s 7998
step    280 | loss 1.7875 | lr 3.00e-04 | grad 1.45 | tok/s 7763
step    290 | loss 1.7581 | lr 3.00e-04 | grad 2.19 | tok/s 7459
step    300 | loss 1.8206 | lr 3.00e-04 | grad 1.74 | tok/s 7582
step    310 | loss 1.8375 | lr 3.00e-04 | grad 1.23 | tok/s 7760
step    320 | loss 1.6786 | lr 3.00e-04 | grad 2.19 | tok/s 7430
step    330 | loss 1.8461 | lr 3.00e-04 | grad 1.34 | tok/s 7778
step    340 | loss 1.9352 | lr 3.00e-04 | grad 2.64 | tok/s 7941
step    350 | loss 1.8881 | lr 3.00e-04 | grad 2.02 | tok/s 7796
step    360 | loss 1.7035 | lr 3.00e-04 | grad 1.32 | tok/s 7995
step    370 | loss 1.5284 | lr 3.00e-04 | grad 1.12 | tok/s 7839
step    380 | loss 1.5524 | lr 3.00e-04 | grad 1.23 | tok/s 8202
step    390 | loss 1.2682 | lr 3.00e-04 | grad 1.14 | tok/s 8277
step    400 | loss 1.1752 | lr 3.00e-04 | grad 1.36 | tok/s 8145
step    410 | loss 1.8738 | lr 3.00e-04 | grad 1.34 | tok/s 7867
step    420 | loss 1.7257 | lr 3.00e-04 | grad 1.55 | tok/s 7857
step    430 | loss 1.7001 | lr 3.00e-04 | grad 2.20 | tok/s 8251
step    440 | loss 1.5939 | lr 3.00e-04 | grad 1.71 | tok/s 7988
step    450 | loss 1.7520 | lr 3.00e-04 | grad 1.35 | tok/s 7873
step    460 | loss 1.5690 | lr 3.00e-04 | grad 3.67 | tok/s 7802
step    470 | loss 1.6392 | lr 3.00e-04 | grad 1.72 | tok/s 7813
step    480 | loss 1.6018 | lr 3.00e-04 | grad 1.47 | tok/s 8190
step    490 | loss 1.5823 | lr 3.00e-04 | grad 1.43 | tok/s 7922
step    500 | loss 1.6381 | lr 3.00e-04 | grad 1.09 | tok/s 7866
step    510 | loss 1.8410 | lr 3.00e-04 | grad 6.38 | tok/s 7740
step    520 | loss 1.6503 | lr 3.00e-04 | grad 1.49 | tok/s 7397
step    530 | loss 1.5180 | lr 3.00e-04 | grad 1.43 | tok/s 7865
step    540 | loss 1.7297 | lr 3.00e-04 | grad 1.40 | tok/s 7850
step    550 | loss 1.6140 | lr 3.00e-04 | grad 1.41 | tok/s 7564
step    560 | loss 1.3740 | lr 3.00e-04 | grad 1.45 | tok/s 8009
step    570 | loss 1.4434 | lr 3.00e-04 | grad 1.37 | tok/s 8265
step    580 | loss 1.3550 | lr 3.00e-04 | grad 1.15 | tok/s 8264
step    590 | loss 1.3102 | lr 3.00e-04 | grad 1.01 | tok/s 8263
step    600 | loss 1.3764 | lr 3.00e-04 | grad 1.40 | tok/s 8259

Training complete! Final step: 602
