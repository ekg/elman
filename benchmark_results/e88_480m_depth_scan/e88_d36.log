Using device: cuda
Output directory: benchmark_results/e88_480m_depth_scan/e88_d36/levelE88_100m_20260125_162909
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,976,224 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9626 | lr 3.00e-04 | grad 16.88 | tok/s 5343
step     20 | loss 2.8729 | lr 3.00e-04 | grad 4.47 | tok/s 7007
step     30 | loss 3.1272 | lr 3.00e-04 | grad 3.03 | tok/s 6932
step     40 | loss 2.9211 | lr 3.00e-04 | grad 67.00 | tok/s 7139
step     50 | loss 5.0977 | lr 3.00e-04 | grad 40.75 | tok/s 7351
step     60 | loss 4.7159 | lr 3.00e-04 | grad 43.00 | tok/s 7282
step     70 | loss 4.2766 | lr 3.00e-04 | grad 12.00 | tok/s 7212
step     80 | loss 4.0033 | lr 3.00e-04 | grad 20.75 | tok/s 7134
step     90 | loss 3.3586 | lr 3.00e-04 | grad 15.69 | tok/s 7116
step    100 | loss 2.9006 | lr 3.00e-04 | grad 4.75 | tok/s 7081
step    110 | loss 2.6312 | lr 3.00e-04 | grad 2.19 | tok/s 7032
step    120 | loss 2.7925 | lr 3.00e-04 | grad 1.39 | tok/s 6800
step    130 | loss 2.3001 | lr 3.00e-04 | grad 1.80 | tok/s 6641
step    140 | loss 2.2008 | lr 3.00e-04 | grad 2.30 | tok/s 6653
step    150 | loss 2.1336 | lr 3.00e-04 | grad 3.17 | tok/s 6877
step    160 | loss 2.1342 | lr 3.00e-04 | grad 2.30 | tok/s 6840
step    170 | loss 2.3529 | lr 3.00e-04 | grad 5.91 | tok/s 6532
step    180 | loss 2.2855 | lr 3.00e-04 | grad 5.22 | tok/s 6766
step    190 | loss 2.0748 | lr 3.00e-04 | grad 2.36 | tok/s 6469
step    200 | loss 1.8819 | lr 3.00e-04 | grad 1.45 | tok/s 6925
step    210 | loss 1.7768 | lr 3.00e-04 | grad 1.75 | tok/s 6729
step    220 | loss 2.2693 | lr 3.00e-04 | grad 3.09 | tok/s 6492
step    230 | loss 2.2041 | lr 3.00e-04 | grad 1.12 | tok/s 6500
step    240 | loss 2.0012 | lr 3.00e-04 | grad 3.28 | tok/s 6529
step    250 | loss 2.2184 | lr 3.00e-04 | grad 1.16 | tok/s 6567
step    260 | loss 1.8828 | lr 3.00e-04 | grad 1.05 | tok/s 6780
step    270 | loss 2.0104 | lr 3.00e-04 | grad 1.08 | tok/s 6785
step    280 | loss 1.8039 | lr 3.00e-04 | grad 1.33 | tok/s 6579
step    290 | loss 1.7706 | lr 3.00e-04 | grad 2.20 | tok/s 6332
step    300 | loss 1.8443 | lr 3.00e-04 | grad 1.70 | tok/s 6432
step    310 | loss 1.8554 | lr 3.00e-04 | grad 1.24 | tok/s 6582
step    320 | loss 1.6903 | lr 3.00e-04 | grad 2.06 | tok/s 6256
step    330 | loss 1.8718 | lr 3.00e-04 | grad 1.32 | tok/s 6597
step    340 | loss 1.9609 | lr 3.00e-04 | grad 6.62 | tok/s 6732
step    350 | loss 1.9101 | lr 3.00e-04 | grad 1.99 | tok/s 6614
step    360 | loss 1.7294 | lr 3.00e-04 | grad 1.32 | tok/s 6769
step    370 | loss 1.5368 | lr 3.00e-04 | grad 1.09 | tok/s 6637
step    380 | loss 1.5705 | lr 3.00e-04 | grad 1.25 | tok/s 6942
step    390 | loss 1.2880 | lr 3.00e-04 | grad 1.11 | tok/s 7004
step    400 | loss 1.1932 | lr 3.00e-04 | grad 1.39 | tok/s 6834
step    410 | loss 1.8878 | lr 3.00e-04 | grad 1.25 | tok/s 6650
step    420 | loss 1.7455 | lr 3.00e-04 | grad 1.51 | tok/s 6649
step    430 | loss 1.7247 | lr 3.00e-04 | grad 1.76 | tok/s 6982
step    440 | loss 1.6105 | lr 3.00e-04 | grad 1.70 | tok/s 6761
step    450 | loss 1.7693 | lr 3.00e-04 | grad 1.29 | tok/s 6672
step    460 | loss 1.5845 | lr 3.00e-04 | grad 3.81 | tok/s 6604
step    470 | loss 1.6546 | lr 3.00e-04 | grad 1.71 | tok/s 6618
step    480 | loss 1.6173 | lr 3.00e-04 | grad 1.47 | tok/s 6938
step    490 | loss 1.6103 | lr 3.00e-04 | grad 1.38 | tok/s 6704
step    500 | loss 1.6555 | lr 3.00e-04 | grad 1.08 | tok/s 6660
step    510 | loss 1.8451 | lr 3.00e-04 | grad 6.22 | tok/s 6546

Training complete! Final step: 512
