Using device: cuda
Output directory: benchmark_results/500m_full_20260121_151947/e88_h96n32/levelE88_h96n32_100m_20260121_151954
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h96n32, 507,820,416 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8651 | lr 9.00e-07 | grad 94.00 | tok/s 5955
step     20 | loss 5.8387 | lr 1.90e-06 | grad 85.50 | tok/s 9204
step     30 | loss 5.8145 | lr 2.90e-06 | grad 22.62 | tok/s 8972
step     40 | loss 5.8350 | lr 3.90e-06 | grad 25.12 | tok/s 9454
step     50 | loss 5.9507 | lr 4.90e-06 | grad 24.62 | tok/s 9746
step     60 | loss 5.9244 | lr 5.90e-06 | grad 22.00 | tok/s 9642
step     70 | loss 5.8627 | lr 6.90e-06 | grad 26.75 | tok/s 9580
step     80 | loss 5.8243 | lr 7.90e-06 | grad 23.00 | tok/s 9522
step     90 | loss 5.7440 | lr 8.90e-06 | grad 21.00 | tok/s 9473
step    100 | loss 5.6608 | lr 9.90e-06 | grad 22.12 | tok/s 9459
step    110 | loss 5.5312 | lr 1.09e-05 | grad 104.50 | tok/s 9400
step    120 | loss 5.2541 | lr 1.19e-05 | grad 100.00 | tok/s 9095
step    130 | loss 4.9469 | lr 1.29e-05 | grad 19.88 | tok/s 8906
step    140 | loss 4.6122 | lr 1.39e-05 | grad 83.50 | tok/s 8946
step    150 | loss 4.1339 | lr 1.49e-05 | grad 37.25 | tok/s 9113
step    160 | loss 3.9829 | lr 1.59e-05 | grad 18.12 | tok/s 9262
step    170 | loss 4.1070 | lr 1.69e-05 | grad 52.00 | tok/s 8762
step    180 | loss 4.1082 | lr 1.79e-05 | grad 24.00 | tok/s 9087
step    190 | loss 3.8131 | lr 1.89e-05 | grad 18.75 | tok/s 8659
step    200 | loss 3.5466 | lr 1.99e-05 | grad 12.94 | tok/s 9344
step    210 | loss 3.3154 | lr 2.09e-05 | grad 16.25 | tok/s 9068
step    220 | loss 3.4207 | lr 2.19e-05 | grad 15.19 | tok/s 8750
step    230 | loss 3.7231 | lr 2.29e-05 | grad 10.56 | tok/s 8773
step    240 | loss 3.2304 | lr 2.39e-05 | grad 18.75 | tok/s 8815
step    250 | loss 3.4976 | lr 2.49e-05 | grad 10.50 | tok/s 8835
step    260 | loss 3.0872 | lr 2.59e-05 | grad 7.75 | tok/s 9145
step    270 | loss 3.1510 | lr 2.69e-05 | grad 7.19 | tok/s 9163
step    280 | loss 2.7712 | lr 2.79e-05 | grad 5.97 | tok/s 8899
step    290 | loss 2.7581 | lr 2.89e-05 | grad 8.62 | tok/s 8544
step    300 | loss 2.8233 | lr 2.99e-05 | grad 6.94 | tok/s 8691
step    310 | loss 2.7938 | lr 3.09e-05 | grad 4.84 | tok/s 8892
step    320 | loss 2.4986 | lr 3.19e-05 | grad 5.06 | tok/s 8518
step    330 | loss 2.7656 | lr 3.29e-05 | grad 3.92 | tok/s 8920
step    340 | loss 2.8014 | lr 3.39e-05 | grad 33.50 | tok/s 9103
step    350 | loss 2.8062 | lr 3.49e-05 | grad 8.69 | tok/s 8930
step    360 | loss 2.8201 | lr 3.59e-05 | grad 3.70 | tok/s 9155
step    370 | loss 2.4901 | lr 3.69e-05 | grad 3.83 | tok/s 8971
step    380 | loss 2.5238 | lr 3.79e-05 | grad 4.25 | tok/s 9390
step    390 | loss 2.2702 | lr 3.89e-05 | grad 3.72 | tok/s 9464
step    400 | loss 2.0873 | lr 3.99e-05 | grad 3.19 | tok/s 9324
step    410 | loss 2.6569 | lr 4.09e-05 | grad 4.34 | tok/s 8989
step    420 | loss 2.4904 | lr 4.19e-05 | grad 4.72 | tok/s 8989
step    430 | loss 2.6260 | lr 4.29e-05 | grad 8.38 | tok/s 9446
step    440 | loss 2.3664 | lr 4.39e-05 | grad 4.16 | tok/s 9140
step    450 | loss 2.4368 | lr 4.49e-05 | grad 3.19 | tok/s 9002
step    460 | loss 2.1680 | lr 4.59e-05 | grad 6.91 | tok/s 8925
step    470 | loss 2.3161 | lr 4.69e-05 | grad 4.50 | tok/s 8936
step    480 | loss 2.3681 | lr 4.79e-05 | grad 4.28 | tok/s 9358
step    490 | loss 2.2849 | lr 4.89e-05 | grad 3.52 | tok/s 9051
step    500 | loss 2.2825 | lr 4.99e-05 | grad 3.38 | tok/s 8983
step    510 | loss 2.4458 | lr 5.09e-05 | grad 10.19 | tok/s 8846
step    520 | loss 2.1609 | lr 5.19e-05 | grad 2.64 | tok/s 8460
step    530 | loss 2.0446 | lr 5.29e-05 | grad 2.86 | tok/s 8985
step    540 | loss 2.2419 | lr 5.39e-05 | grad 3.47 | tok/s 8971
step    550 | loss 2.1860 | lr 5.49e-05 | grad 3.33 | tok/s 8774
step    560 | loss 1.9040 | lr 5.59e-05 | grad 3.75 | tok/s 9187
step    570 | loss 2.0142 | lr 5.69e-05 | grad 2.80 | tok/s 9457
step    580 | loss 1.8271 | lr 5.79e-05 | grad 2.78 | tok/s 9462
step    590 | loss 1.7201 | lr 5.89e-05 | grad 2.62 | tok/s 9429
step    600 | loss 1.7948 | lr 5.99e-05 | grad 3.36 | tok/s 9443
step    610 | loss 1.6911 | lr 6.09e-05 | grad 2.73 | tok/s 9446
step    620 | loss 1.6551 | lr 6.19e-05 | grad 2.59 | tok/s 9446
step    630 | loss 1.7830 | lr 6.29e-05 | grad 6.31 | tok/s 9299
step    640 | loss 2.1182 | lr 6.39e-05 | grad 5.34 | tok/s 8699
step    650 | loss 2.1461 | lr 6.49e-05 | grad 4.03 | tok/s 8835
step    660 | loss 2.0026 | lr 6.59e-05 | grad 4.09 | tok/s 8922
step    670 | loss 2.0617 | lr 6.69e-05 | grad 3.56 | tok/s 9240
step    680 | loss 2.1088 | lr 6.79e-05 | grad 4.12 | tok/s 8906

Training complete! Final step: 686
