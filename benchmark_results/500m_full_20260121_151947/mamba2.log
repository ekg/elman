Using device: cuda
Output directory: benchmark_results/500m_full_20260121_151947/mamba2/levelmamba2_500m_20260121_151954
Created Mamba2 model: dim=1600, depth=32, expand=2, params=508,362,560
Model: Level mamba2, 508,362,560 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8371 | lr 9.00e-07 | grad 32.50 | tok/s 4225
step     20 | loss 5.8173 | lr 1.90e-06 | grad 26.88 | tok/s 17827
step     30 | loss 5.7504 | lr 2.90e-06 | grad 12.50 | tok/s 17712
step     40 | loss 5.6424 | lr 3.90e-06 | grad 15.38 | tok/s 18250
step     50 | loss 5.7940 | lr 4.90e-06 | grad 15.38 | tok/s 18790
step     60 | loss 5.6741 | lr 5.90e-06 | grad 8.19 | tok/s 18617
step     70 | loss 5.4840 | lr 6.90e-06 | grad 21.12 | tok/s 18461
step     80 | loss 5.3326 | lr 7.90e-06 | grad 9.81 | tok/s 18309
step     90 | loss 5.0260 | lr 8.90e-06 | grad 7.53 | tok/s 18172
step    100 | loss 4.7503 | lr 9.90e-06 | grad 8.31 | tok/s 18026
step    110 | loss 4.3648 | lr 1.09e-05 | grad 11.56 | tok/s 17789
step    120 | loss 4.3424 | lr 1.19e-05 | grad 7.81 | tok/s 17153
step    130 | loss 3.4822 | lr 1.29e-05 | grad 6.41 | tok/s 16653
step    140 | loss 3.0413 | lr 1.39e-05 | grad 7.47 | tok/s 16630
step    150 | loss 3.3307 | lr 1.49e-05 | grad 12.19 | tok/s 17097
step    160 | loss 3.0499 | lr 1.59e-05 | grad 6.28 | tok/s 17125
step    170 | loss 3.0376 | lr 1.69e-05 | grad 8.19 | tok/s 16146
step    180 | loss 3.0833 | lr 1.79e-05 | grad 10.31 | tok/s 16672
step    190 | loss 2.8674 | lr 1.89e-05 | grad 4.34 | tok/s 15941
step    200 | loss 2.4143 | lr 1.99e-05 | grad 3.66 | tok/s 17020
step    210 | loss 2.2967 | lr 2.09e-05 | grad 4.50 | tok/s 16474
step    220 | loss 2.6739 | lr 2.19e-05 | grad 6.25 | tok/s 15864
step    230 | loss 2.9378 | lr 2.29e-05 | grad 3.50 | tok/s 15888
step    240 | loss 2.4019 | lr 2.39e-05 | grad 5.00 | tok/s 15922
step    250 | loss 2.6285 | lr 2.49e-05 | grad 4.44 | tok/s 15960
step    260 | loss 2.1897 | lr 2.59e-05 | grad 3.47 | tok/s 16464
step    270 | loss 2.3673 | lr 2.69e-05 | grad 3.55 | tok/s 16450
step    280 | loss 2.0099 | lr 2.79e-05 | grad 4.19 | tok/s 15959
step    290 | loss 2.0120 | lr 2.89e-05 | grad 7.22 | tok/s 15316
step    300 | loss 2.1065 | lr 2.99e-05 | grad 5.44 | tok/s 15560
step    310 | loss 2.0762 | lr 3.09e-05 | grad 3.39 | tok/s 15896
step    320 | loss 1.8520 | lr 3.19e-05 | grad 5.31 | tok/s 15290
step    330 | loss 2.1144 | lr 3.29e-05 | grad 3.09 | tok/s 15983
step    340 | loss 2.1774 | lr 3.39e-05 | grad 6.59 | tok/s 16289
step    350 | loss 2.1977 | lr 3.49e-05 | grad 5.53 | tok/s 15961
step    360 | loss 2.0828 | lr 3.59e-05 | grad 4.28 | tok/s 16339
step    370 | loss 1.7974 | lr 3.69e-05 | grad 3.33 | tok/s 16028
step    380 | loss 1.8346 | lr 3.79e-05 | grad 3.80 | tok/s 16750
step    390 | loss 1.4260 | lr 3.89e-05 | grad 3.05 | tok/s 16872
step    400 | loss 1.3031 | lr 3.99e-05 | grad 3.17 | tok/s 16598
step    410 | loss 2.2312 | lr 4.09e-05 | grad 3.83 | tok/s 16042
step    420 | loss 2.0547 | lr 4.19e-05 | grad 4.53 | tok/s 16036
step    430 | loss 1.9879 | lr 4.29e-05 | grad 5.78 | tok/s 16832
step    440 | loss 1.8449 | lr 4.39e-05 | grad 4.28 | tok/s 16278
step    450 | loss 1.9867 | lr 4.49e-05 | grad 2.77 | tok/s 16080
step    460 | loss 1.7404 | lr 4.59e-05 | grad 7.41 | tok/s 15911
step    470 | loss 1.8380 | lr 4.69e-05 | grad 3.70 | tok/s 15937
step    480 | loss 1.8041 | lr 4.79e-05 | grad 4.34 | tok/s 16707
step    490 | loss 1.8277 | lr 4.89e-05 | grad 3.34 | tok/s 16144
step    500 | loss 1.8861 | lr 4.99e-05 | grad 3.14 | tok/s 16060
step    510 | loss 2.1045 | lr 5.09e-05 | grad 10.88 | tok/s 15808
step    520 | loss 1.8085 | lr 5.19e-05 | grad 3.17 | tok/s 15107
step    530 | loss 1.6762 | lr 5.29e-05 | grad 3.09 | tok/s 16022
step    540 | loss 1.9084 | lr 5.39e-05 | grad 2.94 | tok/s 15991
step    550 | loss 1.8118 | lr 5.49e-05 | grad 2.86 | tok/s 15641
step    560 | loss 1.5504 | lr 5.59e-05 | grad 3.42 | tok/s 16376
step    570 | loss 1.5916 | lr 5.69e-05 | grad 2.55 | tok/s 16855
step    580 | loss 1.4199 | lr 5.79e-05 | grad 2.20 | tok/s 16853
step    590 | loss 1.3680 | lr 5.89e-05 | grad 1.95 | tok/s 16834
step    600 | loss 1.4661 | lr 5.99e-05 | grad 2.50 | tok/s 16837
step    610 | loss 1.3674 | lr 6.09e-05 | grad 1.99 | tok/s 16856
step    620 | loss 1.3917 | lr 6.19e-05 | grad 1.67 | tok/s 16859
step    630 | loss 1.4968 | lr 6.29e-05 | grad 10.31 | tok/s 16613
step    640 | loss 1.9130 | lr 6.39e-05 | grad 4.09 | tok/s 15867
step    650 | loss 1.8831 | lr 6.49e-05 | grad 3.28 | tok/s 15756
step    660 | loss 1.7306 | lr 6.59e-05 | grad 3.52 | tok/s 15897
step    670 | loss 1.7924 | lr 6.69e-05 | grad 2.62 | tok/s 16447
step    680 | loss 1.8968 | lr 6.79e-05 | grad 3.56 | tok/s 15876
step    690 | loss 1.8511 | lr 6.89e-05 | grad 2.81 | tok/s 15774
step    700 | loss 1.8435 | lr 6.99e-05 | grad 2.66 | tok/s 15622
step    710 | loss 1.6800 | lr 7.09e-05 | grad 2.55 | tok/s 16061
step    720 | loss 1.8963 | lr 7.19e-05 | grad 4.03 | tok/s 15712
step    730 | loss 1.4769 | lr 7.29e-05 | grad 2.08 | tok/s 16432
step    740 | loss 1.6318 | lr 7.39e-05 | grad 1.98 | tok/s 15932
step    750 | loss 2.2013 | lr 7.49e-05 | grad 5.12 | tok/s 16575
step    760 | loss 1.8679 | lr 7.59e-05 | grad 2.06 | tok/s 16585
step    770 | loss 1.7471 | lr 7.69e-05 | grad 3.02 | tok/s 16224
step    780 | loss 1.7972 | lr 7.79e-05 | grad 2.31 | tok/s 15767
step    790 | loss 1.7305 | lr 7.89e-05 | grad 2.66 | tok/s 16197
step    800 | loss 1.9791 | lr 7.99e-05 | grad 5.50 | tok/s 16649
step    810 | loss 1.7277 | lr 8.09e-05 | grad 2.27 | tok/s 16125
step    820 | loss 1.4831 | lr 8.19e-05 | grad 4.72 | tok/s 15729
step    830 | loss 1.6889 | lr 8.29e-05 | grad 2.73 | tok/s 16000
step    840 | loss 1.7503 | lr 8.39e-05 | grad 1.66 | tok/s 15633
step    850 | loss 1.8603 | lr 8.49e-05 | grad 1.89 | tok/s 15677
step    860 | loss 1.8607 | lr 8.59e-05 | grad 1.99 | tok/s 15846
step    870 | loss 1.7927 | lr 8.69e-05 | grad 4.28 | tok/s 16011
step    880 | loss 1.6887 | lr 8.79e-05 | grad 2.80 | tok/s 16775
step    890 | loss 1.8268 | lr 8.89e-05 | grad 2.59 | tok/s 15967
step    900 | loss 1.6629 | lr 8.99e-05 | grad 1.78 | tok/s 15939
step    910 | loss 1.6203 | lr 9.09e-05 | grad 2.05 | tok/s 16143
step    920 | loss 1.7464 | lr 9.19e-05 | grad 1.66 | tok/s 15925
step    930 | loss 1.7271 | lr 9.29e-05 | grad 1.73 | tok/s 16001
step    940 | loss 1.5987 | lr 9.39e-05 | grad 1.85 | tok/s 16497
step    950 | loss 1.5498 | lr 9.49e-05 | grad 1.55 | tok/s 15762
step    960 | loss 1.6465 | lr 9.59e-05 | grad 1.45 | tok/s 15582
step    970 | loss 1.5615 | lr 9.69e-05 | grad 1.42 | tok/s 15748
step    980 | loss 1.5543 | lr 9.79e-05 | grad 1.52 | tok/s 16176
step    990 | loss 2.1539 | lr 9.89e-05 | grad 2.80 | tok/s 16814
step   1000 | loss 1.9093 | lr 9.99e-05 | grad 1.84 | tok/s 16104
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9093.pt
step   1010 | loss 1.8744 | lr 1.02e-06 | grad 2.28 | tok/s 9581
step   1020 | loss 1.5292 | lr 1.09e-06 | grad 1.86 | tok/s 16557
step   1030 | loss 1.5354 | lr 1.21e-06 | grad 3.23 | tok/s 16891
step   1040 | loss 1.9838 | lr 1.37e-06 | grad 1.78 | tok/s 16858
step   1050 | loss 2.0565 | lr 1.59e-06 | grad 6.28 | tok/s 16100
step   1060 | loss 2.7068 | lr 1.85e-06 | grad 1.55 | tok/s 16878
step   1070 | loss 1.8911 | lr 2.16e-06 | grad 8.75 | tok/s 16268
step   1080 | loss 1.3973 | lr 2.52e-06 | grad 2.64 | tok/s 16651
step   1090 | loss 1.6616 | lr 2.92e-06 | grad 1.95 | tok/s 16643
step   1100 | loss 1.5921 | lr 3.37e-06 | grad 1.73 | tok/s 17099
step   1110 | loss 1.5782 | lr 3.87e-06 | grad 1.57 | tok/s 17055
step   1120 | loss 1.5603 | lr 4.42e-06 | grad 1.54 | tok/s 17097
step   1130 | loss 1.5701 | lr 5.01e-06 | grad 1.66 | tok/s 16608
step   1140 | loss 2.0470 | lr 5.65e-06 | grad 4.03 | tok/s 16760
step   1150 | loss 2.2706 | lr 6.32e-06 | grad 3.28 | tok/s 16466
step   1160 | loss 1.7847 | lr 7.05e-06 | grad 2.08 | tok/s 16433
step   1170 | loss 2.3868 | lr 7.81e-06 | grad 3.30 | tok/s 16059
step   1180 | loss 2.0282 | lr 8.62e-06 | grad 1.86 | tok/s 16185
step   1190 | loss 1.7873 | lr 9.47e-06 | grad 1.60 | tok/s 15668
step   1200 | loss 1.7473 | lr 1.04e-05 | grad 3.08 | tok/s 16678
step   1210 | loss 2.2873 | lr 1.13e-05 | grad 1.73 | tok/s 17064

Training complete! Final step: 1210
