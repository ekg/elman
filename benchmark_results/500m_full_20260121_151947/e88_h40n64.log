Using device: cuda
Output directory: benchmark_results/500m_full_20260121_151947/e88_h40n64/levelE88_h40n64_100m_20260121_151953
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h40n64, 500,695,744 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.7915 | lr 9.00e-07 | grad 57.75 | tok/s 5106
step     20 | loss 5.7568 | lr 1.90e-06 | grad 48.75 | tok/s 7037
step     30 | loss 5.6460 | lr 2.90e-06 | grad 20.62 | tok/s 7023
step     40 | loss 5.7292 | lr 3.90e-06 | grad 20.12 | tok/s 7336
step     50 | loss 5.9787 | lr 4.90e-06 | grad 20.62 | tok/s 7593
step     60 | loss 5.9012 | lr 5.90e-06 | grad 15.88 | tok/s 7546
step     70 | loss 5.7785 | lr 6.90e-06 | grad 24.00 | tok/s 7509
step     80 | loss 5.6657 | lr 7.90e-06 | grad 17.12 | tok/s 7471
step     90 | loss 5.5010 | lr 8.90e-06 | grad 14.75 | tok/s 7463
step    100 | loss 5.2997 | lr 9.90e-06 | grad 15.62 | tok/s 7447
step    110 | loss 5.1018 | lr 1.09e-05 | grad 61.75 | tok/s 7410
step    120 | loss 4.8336 | lr 1.19e-05 | grad 60.00 | tok/s 7080
step    130 | loss 4.4002 | lr 1.29e-05 | grad 12.38 | tok/s 6979
step    140 | loss 4.0190 | lr 1.39e-05 | grad 40.00 | tok/s 7002
step    150 | loss 3.9430 | lr 1.49e-05 | grad 23.88 | tok/s 7210
step    160 | loss 3.5860 | lr 1.59e-05 | grad 9.88 | tok/s 7302
step    170 | loss 3.6720 | lr 1.69e-05 | grad 16.12 | tok/s 6925
step    180 | loss 3.7276 | lr 1.79e-05 | grad 14.50 | tok/s 7179
step    190 | loss 3.4708 | lr 1.89e-05 | grad 7.78 | tok/s 6880
step    200 | loss 3.1271 | lr 1.99e-05 | grad 6.03 | tok/s 7340
step    210 | loss 2.9544 | lr 2.09e-05 | grad 7.16 | tok/s 7128
step    220 | loss 3.2102 | lr 2.19e-05 | grad 8.44 | tok/s 6869
step    230 | loss 3.4568 | lr 2.29e-05 | grad 4.19 | tok/s 6901
step    240 | loss 2.9440 | lr 2.39e-05 | grad 8.62 | tok/s 6916
step    250 | loss 3.1804 | lr 2.49e-05 | grad 7.00 | tok/s 6938
step    260 | loss 2.8029 | lr 2.59e-05 | grad 4.12 | tok/s 7206
step    270 | loss 2.8844 | lr 2.69e-05 | grad 4.78 | tok/s 7204
step    280 | loss 2.5393 | lr 2.79e-05 | grad 4.19 | tok/s 6985
step    290 | loss 2.5261 | lr 2.89e-05 | grad 6.34 | tok/s 6723
step    300 | loss 2.5756 | lr 2.99e-05 | grad 5.72 | tok/s 6816
step    310 | loss 2.5562 | lr 3.09e-05 | grad 4.19 | tok/s 6976
step    320 | loss 2.3189 | lr 3.19e-05 | grad 4.91 | tok/s 6673
step    330 | loss 2.5737 | lr 3.29e-05 | grad 3.38 | tok/s 6976
step    340 | loss 2.6271 | lr 3.39e-05 | grad 18.38 | tok/s 7134
step    350 | loss 2.6221 | lr 3.49e-05 | grad 5.38 | tok/s 6993
step    360 | loss 2.6388 | lr 3.59e-05 | grad 4.66 | tok/s 7174
step    370 | loss 2.3306 | lr 3.69e-05 | grad 3.38 | tok/s 7043
step    380 | loss 2.3386 | lr 3.79e-05 | grad 3.86 | tok/s 7354
step    390 | loss 2.0683 | lr 3.89e-05 | grad 3.83 | tok/s 7429
step    400 | loss 1.9209 | lr 3.99e-05 | grad 3.70 | tok/s 7306
step    410 | loss 2.5438 | lr 4.09e-05 | grad 5.50 | tok/s 7056
step    420 | loss 2.3789 | lr 4.19e-05 | grad 4.94 | tok/s 7052
step    430 | loss 2.5224 | lr 4.29e-05 | grad 7.94 | tok/s 7392
step    440 | loss 2.2706 | lr 4.39e-05 | grad 4.59 | tok/s 7175
step    450 | loss 2.3394 | lr 4.49e-05 | grad 3.64 | tok/s 7067
step    460 | loss 2.0673 | lr 4.59e-05 | grad 6.47 | tok/s 6970
step    470 | loss 2.2240 | lr 4.69e-05 | grad 4.12 | tok/s 6982
step    480 | loss 2.2503 | lr 4.79e-05 | grad 4.56 | tok/s 7291
step    490 | loss 2.1973 | lr 4.89e-05 | grad 3.73 | tok/s 7083
step    500 | loss 2.2172 | lr 4.99e-05 | grad 3.73 | tok/s 7038
step    510 | loss 2.3810 | lr 5.09e-05 | grad 11.31 | tok/s 6918
step    520 | loss 2.0951 | lr 5.19e-05 | grad 2.80 | tok/s 6637
step    530 | loss 1.9847 | lr 5.29e-05 | grad 3.62 | tok/s 7041

Training complete! Final step: 539
