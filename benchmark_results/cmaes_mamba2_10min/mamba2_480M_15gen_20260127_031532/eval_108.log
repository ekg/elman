Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_108/levelmamba2_100m_20260127_053016
Model: Level mamba2, 451,796,120 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5505 | lr 3.00e-04 | grad 24.62 | tok/s 4691
step     20 | loss 3.0161 | lr 3.00e-04 | grad 3.62 | tok/s 24236
step     30 | loss 3.9284 | lr 3.00e-04 | grad 6.75 | tok/s 24880
step     40 | loss 3.4745 | lr 3.00e-04 | grad 9.50 | tok/s 25226
step     50 | loss 2.5952 | lr 3.00e-04 | grad 4.09 | tok/s 25092
step     60 | loss 2.2375 | lr 3.00e-04 | grad 2.47 | tok/s 25010
step     70 | loss 2.0245 | lr 3.00e-04 | grad 2.78 | tok/s 24922
step     80 | loss 1.9266 | lr 3.00e-04 | grad 3.36 | tok/s 24869
step     90 | loss 1.7908 | lr 3.00e-04 | grad 1.76 | tok/s 24834
step    100 | loss 1.9649 | lr 3.00e-04 | grad 6.03 | tok/s 24675
step    110 | loss 2.6919 | lr 3.00e-04 | grad 3.78 | tok/s 23622
step    120 | loss 1.8725 | lr 3.00e-04 | grad 3.31 | tok/s 24002
step    130 | loss 2.3284 | lr 3.00e-04 | grad 8.50 | tok/s 23837
step    140 | loss 1.3734 | lr 3.00e-04 | grad 2.89 | tok/s 24673
step    150 | loss 2.3163 | lr 3.00e-04 | grad 3.16 | tok/s 24146
step    160 | loss 2.1355 | lr 3.00e-04 | grad 2.42 | tok/s 23478
step    170 | loss 1.6733 | lr 3.00e-04 | grad 7.34 | tok/s 24118
step    180 | loss 1.8015 | lr 3.00e-04 | grad 2.69 | tok/s 23173
step    190 | loss 1.5265 | lr 3.00e-04 | grad 3.36 | tok/s 24610
step    200 | loss 1.6542 | lr 3.00e-04 | grad 3.30 | tok/s 23414
step    210 | loss 2.0568 | lr 3.00e-04 | grad 4.75 | tok/s 23818
step    220 | loss 1.7848 | lr 3.00e-04 | grad 2.17 | tok/s 23464
step    230 | loss 2.0648 | lr 3.00e-04 | grad 2.34 | tok/s 23873
step    240 | loss 1.7050 | lr 3.00e-04 | grad 1.95 | tok/s 23805
step    250 | loss 1.7112 | lr 3.00e-04 | grad 2.41 | tok/s 24334
step    260 | loss 1.7320 | lr 3.00e-04 | grad 2.41 | tok/s 23840
step    270 | loss 1.6115 | lr 3.00e-04 | grad 1.93 | tok/s 22559
step    280 | loss 1.5399 | lr 3.00e-04 | grad 2.02 | tok/s 23356
step    290 | loss 1.8406 | lr 3.00e-04 | grad 1.68 | tok/s 23346
step    300 | loss 1.5595 | lr 3.00e-04 | grad 1.53 | tok/s 23203
step    310 | loss 1.7389 | lr 3.00e-04 | grad 4.09 | tok/s 23333
step    320 | loss 1.6194 | lr 3.00e-04 | grad 2.66 | tok/s 23900
step    330 | loss 1.8704 | lr 3.00e-04 | grad 4.97 | tok/s 23663
step    340 | loss 1.6380 | lr 3.00e-04 | grad 4.16 | tok/s 24656
step    350 | loss 1.4965 | lr 3.00e-04 | grad 1.99 | tok/s 23059
step    360 | loss 1.4181 | lr 3.00e-04 | grad 1.49 | tok/s 24559
step    370 | loss 1.1958 | lr 3.00e-04 | grad 1.47 | tok/s 24780
step    380 | loss 1.0684 | lr 3.00e-04 | grad 1.59 | tok/s 24794
step    390 | loss 1.6032 | lr 3.00e-04 | grad 2.48 | tok/s 23715
step    400 | loss 1.6291 | lr 3.00e-04 | grad 7.22 | tok/s 23654
step    410 | loss 1.5529 | lr 3.00e-04 | grad 2.02 | tok/s 24604
step    420 | loss 1.5184 | lr 3.00e-04 | grad 1.66 | tok/s 24469
step    430 | loss 1.5759 | lr 3.00e-04 | grad 2.39 | tok/s 23722
step    440 | loss 1.5603 | lr 3.00e-04 | grad 1.74 | tok/s 23736
step    450 | loss 1.5084 | lr 3.00e-04 | grad 2.03 | tok/s 24180
step    460 | loss 1.4143 | lr 3.00e-04 | grad 1.44 | tok/s 23901
step    470 | loss 1.5694 | lr 3.00e-04 | grad 1.97 | tok/s 24645
step    480 | loss 1.6164 | lr 3.00e-04 | grad 1.81 | tok/s 23490
step    490 | loss 1.7135 | lr 3.00e-04 | grad 2.83 | tok/s 24001
step    500 | loss 1.6005 | lr 3.00e-04 | grad 1.43 | tok/s 22921
step    510 | loss 1.4629 | lr 3.00e-04 | grad 4.34 | tok/s 24012
step    520 | loss 1.6046 | lr 3.00e-04 | grad 2.50 | tok/s 23471
step    530 | loss 1.5521 | lr 3.00e-04 | grad 1.40 | tok/s 23555
step    540 | loss 1.2292 | lr 3.00e-04 | grad 1.63 | tok/s 23763
step    550 | loss 1.4290 | lr 3.00e-04 | grad 1.57 | tok/s 24780
step    560 | loss 1.2890 | lr 3.00e-04 | grad 1.48 | tok/s 24775
step    570 | loss 1.2569 | lr 3.00e-04 | grad 1.81 | tok/s 24766
step    580 | loss 1.2899 | lr 3.00e-04 | grad 1.52 | tok/s 24785
step    590 | loss 1.2105 | lr 3.00e-04 | grad 1.38 | tok/s 24769
step    600 | loss 1.2631 | lr 3.00e-04 | grad 1.66 | tok/s 24803
step    610 | loss 1.2198 | lr 3.00e-04 | grad 0.99 | tok/s 24805
step    620 | loss 1.6082 | lr 3.00e-04 | grad 3.69 | tok/s 23442
step    630 | loss 1.5861 | lr 3.00e-04 | grad 1.95 | tok/s 23582
step    640 | loss 1.4628 | lr 3.00e-04 | grad 1.67 | tok/s 24036
step    650 | loss 1.5207 | lr 3.00e-04 | grad 1.83 | tok/s 24180
step    660 | loss 1.4872 | lr 3.00e-04 | grad 1.89 | tok/s 23856
step    670 | loss 1.5971 | lr 3.00e-04 | grad 1.41 | tok/s 23248
step    680 | loss 1.4865 | lr 3.00e-04 | grad 1.68 | tok/s 23354
step    690 | loss 1.4426 | lr 3.00e-04 | grad 1.84 | tok/s 23553
step    700 | loss 1.5010 | lr 3.00e-04 | grad 2.89 | tok/s 23390
step    710 | loss 1.2867 | lr 3.00e-04 | grad 1.62 | tok/s 24176
step    720 | loss 1.3463 | lr 3.00e-04 | grad 1.80 | tok/s 24128
step    730 | loss 1.6537 | lr 3.00e-04 | grad 6.56 | tok/s 24252
step    740 | loss 1.5443 | lr 3.00e-04 | grad 1.57 | tok/s 24791
step    750 | loss 1.4032 | lr 3.00e-04 | grad 1.40 | tok/s 24304
step    760 | loss 1.4891 | lr 3.00e-04 | grad 1.42 | tok/s 23848
step    770 | loss 1.4358 | lr 3.00e-04 | grad 2.09 | tok/s 23915
step    780 | loss 1.5152 | lr 3.00e-04 | grad 3.61 | tok/s 24509
step    790 | loss 1.3994 | lr 3.00e-04 | grad 1.50 | tok/s 24094
step    800 | loss 1.1713 | lr 3.00e-04 | grad 3.23 | tok/s 23447
step    810 | loss 1.3620 | lr 3.00e-04 | grad 2.20 | tok/s 24147
step    820 | loss 1.4414 | lr 3.00e-04 | grad 1.43 | tok/s 23121
step    830 | loss 1.5215 | lr 3.00e-04 | grad 1.85 | tok/s 23594
step    840 | loss 1.4388 | lr 3.00e-04 | grad 2.14 | tok/s 23822
step    850 | loss 1.4945 | lr 3.00e-04 | grad 2.61 | tok/s 24063
step    860 | loss 1.3255 | lr 3.00e-04 | grad 1.83 | tok/s 24620
step    870 | loss 1.5063 | lr 3.00e-04 | grad 2.11 | tok/s 23733
step    880 | loss 1.4424 | lr 3.00e-04 | grad 1.54 | tok/s 23918
step    890 | loss 1.4598 | lr 3.00e-04 | grad 2.19 | tok/s 23863
step    900 | loss 1.3912 | lr 3.00e-04 | grad 1.82 | tok/s 23342
step    910 | loss 1.4369 | lr 3.00e-04 | grad 1.89 | tok/s 23830
step    920 | loss 1.3169 | lr 3.00e-04 | grad 1.39 | tok/s 23963
step    930 | loss 1.3053 | lr 3.00e-04 | grad 2.08 | tok/s 23547
step    940 | loss 1.4089 | lr 3.00e-04 | grad 1.38 | tok/s 23000
step    950 | loss 1.4047 | lr 3.00e-04 | grad 1.71 | tok/s 23773
step    960 | loss 1.3983 | lr 3.00e-04 | grad 1.47 | tok/s 23745
step    970 | loss 1.8397 | lr 3.00e-04 | grad 3.00 | tok/s 24684
step    980 | loss 1.5289 | lr 3.00e-04 | grad 1.47 | tok/s 23684
step    990 | loss 1.4914 | lr 3.00e-04 | grad 1.42 | tok/s 23952
step   1000 | loss 1.1520 | lr 3.00e-04 | grad 1.62 | tok/s 24253
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1520.pt
step   1010 | loss 1.2190 | lr 3.00e-04 | grad 1.65 | tok/s 12204
step   1020 | loss 1.4102 | lr 3.00e-04 | grad 1.88 | tok/s 23867
step   1030 | loss 1.9875 | lr 3.00e-04 | grad 4.16 | tok/s 24444
step   1040 | loss 1.4701 | lr 3.00e-04 | grad 1.42 | tok/s 24529
step   1050 | loss 1.1646 | lr 3.00e-04 | grad 1.38 | tok/s 24352
step   1060 | loss 1.2992 | lr 3.00e-04 | grad 1.53 | tok/s 24243
step   1070 | loss 1.2187 | lr 3.00e-04 | grad 1.44 | tok/s 24966
step   1080 | loss 1.1991 | lr 3.00e-04 | grad 1.52 | tok/s 24901
step   1090 | loss 1.1742 | lr 3.00e-04 | grad 1.44 | tok/s 24907
step   1100 | loss 1.1212 | lr 3.00e-04 | grad 1.31 | tok/s 24884
step   1110 | loss 1.3330 | lr 3.00e-04 | grad 1.66 | tok/s 24228
step   1120 | loss 1.5706 | lr 3.00e-04 | grad 1.45 | tok/s 24487
step   1130 | loss 1.6923 | lr 3.00e-04 | grad 1.57 | tok/s 24783
step   1140 | loss 1.5696 | lr 3.00e-04 | grad 2.95 | tok/s 24208
step   1150 | loss 1.5901 | lr 3.00e-04 | grad 3.64 | tok/s 23453
step   1160 | loss 1.4721 | lr 3.00e-04 | grad 1.88 | tok/s 23386
step   1170 | loss 1.3062 | lr 3.00e-04 | grad 1.36 | tok/s 24566
step   1180 | loss 1.5256 | lr 3.00e-04 | grad 2.38 | tok/s 24627
step   1190 | loss 1.1104 | lr 3.00e-04 | grad 1.29 | tok/s 24854
step   1200 | loss 1.2723 | lr 3.00e-04 | grad 1.53 | tok/s 23467
step   1210 | loss 1.3122 | lr 3.00e-04 | grad 1.66 | tok/s 24200
step   1220 | loss 1.3459 | lr 3.00e-04 | grad 1.35 | tok/s 24339
step   1230 | loss 1.2041 | lr 3.00e-04 | grad 2.05 | tok/s 24617
step   1240 | loss 1.4220 | lr 3.00e-04 | grad 2.17 | tok/s 24045
step   1250 | loss 1.2914 | lr 3.00e-04 | grad 1.50 | tok/s 24652
step   1260 | loss 1.3201 | lr 3.00e-04 | grad 2.22 | tok/s 23939
step   1270 | loss 1.3246 | lr 3.00e-04 | grad 2.08 | tok/s 23950
step   1280 | loss 1.2999 | lr 3.00e-04 | grad 1.84 | tok/s 23464
step   1290 | loss 1.5133 | lr 3.00e-04 | grad 6.84 | tok/s 23392
step   1300 | loss 1.4421 | lr 3.00e-04 | grad 1.90 | tok/s 24300
step   1310 | loss 1.4251 | lr 3.00e-04 | grad 3.16 | tok/s 24248
step   1320 | loss 1.3685 | lr 3.00e-04 | grad 1.57 | tok/s 24187
step   1330 | loss 1.4612 | lr 3.00e-04 | grad 1.41 | tok/s 23486
step   1340 | loss 1.3630 | lr 3.00e-04 | grad 1.47 | tok/s 24356
step   1350 | loss 1.4131 | lr 3.00e-04 | grad 1.51 | tok/s 22911
step   1360 | loss 1.4647 | lr 3.00e-04 | grad 3.34 | tok/s 24394
step   1370 | loss 1.4174 | lr 3.00e-04 | grad 1.70 | tok/s 23654
step   1380 | loss 1.2668 | lr 3.00e-04 | grad 2.16 | tok/s 24099
step   1390 | loss 1.4714 | lr 3.00e-04 | grad 1.34 | tok/s 23585
step   1400 | loss 1.3021 | lr 3.00e-04 | grad 1.55 | tok/s 23207
step   1410 | loss 1.0147 | lr 3.00e-04 | grad 1.59 | tok/s 24591
step   1420 | loss 1.6386 | lr 3.00e-04 | grad 1.23 | tok/s 23824
step   1430 | loss 1.3978 | lr 3.00e-04 | grad 2.12 | tok/s 24004
step   1440 | loss 1.3551 | lr 3.00e-04 | grad 1.44 | tok/s 24280
step   1450 | loss 1.4594 | lr 3.00e-04 | grad 2.27 | tok/s 23600
step   1460 | loss 1.3382 | lr 3.00e-04 | grad 1.62 | tok/s 23302
step   1470 | loss 1.3162 | lr 3.00e-04 | grad 2.39 | tok/s 24071
step   1480 | loss 1.5903 | lr 3.00e-04 | grad 6.31 | tok/s 23915
step   1490 | loss 1.5170 | lr 3.00e-04 | grad 1.41 | tok/s 24248
step   1500 | loss 1.2207 | lr 3.00e-04 | grad 1.49 | tok/s 23827
step   1510 | loss 1.4102 | lr 3.00e-04 | grad 1.93 | tok/s 23682
step   1520 | loss 1.3287 | lr 3.00e-04 | grad 1.66 | tok/s 24311
step   1530 | loss 1.4426 | lr 3.00e-04 | grad 1.42 | tok/s 24323
step   1540 | loss 1.4099 | lr 3.00e-04 | grad 4.41 | tok/s 23864
step   1550 | loss 1.0974 | lr 3.00e-04 | grad 1.09 | tok/s 24648
step   1560 | loss 1.2545 | lr 3.00e-04 | grad 1.28 | tok/s 24094
step   1570 | loss 1.1755 | lr 3.00e-04 | grad 1.69 | tok/s 24208
step   1580 | loss 1.3828 | lr 3.00e-04 | grad 3.33 | tok/s 23335
step   1590 | loss 1.2108 | lr 3.00e-04 | grad 5.78 | tok/s 24695
step   1600 | loss 1.7995 | lr 3.00e-04 | grad 3.09 | tok/s 24070
step   1610 | loss 1.9738 | lr 3.00e-04 | grad 2.42 | tok/s 24816
step   1620 | loss 1.6386 | lr 3.00e-04 | grad 2.28 | tok/s 24787
step   1630 | loss 1.4676 | lr 3.00e-04 | grad 1.83 | tok/s 24804
step   1640 | loss 1.3602 | lr 3.00e-04 | grad 2.17 | tok/s 24769
step   1650 | loss 1.3149 | lr 3.00e-04 | grad 1.61 | tok/s 24776
step   1660 | loss 1.4451 | lr 3.00e-04 | grad 2.09 | tok/s 24050
step   1670 | loss 1.3490 | lr 3.00e-04 | grad 1.82 | tok/s 23837
step   1680 | loss 1.4200 | lr 3.00e-04 | grad 2.33 | tok/s 23315
step   1690 | loss 1.2796 | lr 3.00e-04 | grad 1.37 | tok/s 24171
step   1700 | loss 1.1825 | lr 3.00e-04 | grad 2.45 | tok/s 24252
step   1710 | loss 1.3973 | lr 3.00e-04 | grad 1.37 | tok/s 23515
step   1720 | loss 1.3629 | lr 3.00e-04 | grad 3.38 | tok/s 24256
step   1730 | loss 1.3476 | lr 3.00e-04 | grad 1.89 | tok/s 24373
step   1740 | loss 1.2058 | lr 3.00e-04 | grad 1.59 | tok/s 23884
step   1750 | loss 1.3513 | lr 3.00e-04 | grad 1.60 | tok/s 23671

Training complete! Final step: 1753
