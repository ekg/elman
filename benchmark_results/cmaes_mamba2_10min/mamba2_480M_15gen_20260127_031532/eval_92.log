Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_92/levelmamba2_100m_20260127_050938
Model: Level mamba2, 923,862,828 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 8.5956 | lr 3.00e-04 | grad 21.75 | tok/s 3826
step     20 | loss 3.5903 | lr 3.00e-04 | grad 9.38 | tok/s 12013
step     30 | loss 4.4881 | lr 3.00e-04 | grad 18.50 | tok/s 12185
step     40 | loss 3.5206 | lr 3.00e-04 | grad 9.62 | tok/s 12266
step     50 | loss 2.7076 | lr 3.00e-04 | grad 4.97 | tok/s 12219
step     60 | loss 2.3245 | lr 3.00e-04 | grad 4.09 | tok/s 12189
step     70 | loss 2.1250 | lr 3.00e-04 | grad 5.56 | tok/s 12183
step     80 | loss 2.0543 | lr 3.00e-04 | grad 3.89 | tok/s 12169
step     90 | loss 1.9100 | lr 3.00e-04 | grad 5.78 | tok/s 12166
step    100 | loss 2.1654 | lr 3.00e-04 | grad 7.66 | tok/s 12092
step    110 | loss 2.9788 | lr 3.00e-04 | grad 5.56 | tok/s 11526
step    120 | loss 2.2085 | lr 3.00e-04 | grad 6.91 | tok/s 11789
step    130 | loss 2.4754 | lr 3.00e-04 | grad 8.88 | tok/s 11820
step    140 | loss 1.7366 | lr 3.00e-04 | grad 11.81 | tok/s 12093
step    150 | loss 2.4812 | lr 3.00e-04 | grad 4.78 | tok/s 11700
step    160 | loss 2.2979 | lr 3.00e-04 | grad 4.09 | tok/s 11533
step    170 | loss 2.0819 | lr 3.00e-04 | grad 3.95 | tok/s 11804
step    180 | loss 1.9563 | lr 3.00e-04 | grad 4.84 | tok/s 11590
step    190 | loss 1.7229 | lr 3.00e-04 | grad 4.44 | tok/s 12127
step    200 | loss 1.8882 | lr 3.00e-04 | grad 6.31 | tok/s 11505
step    210 | loss 2.2275 | lr 3.00e-04 | grad 3.97 | tok/s 11637
step    220 | loss 1.9314 | lr 3.00e-04 | grad 4.56 | tok/s 11613
step    230 | loss 2.2641 | lr 3.00e-04 | grad 6.06 | tok/s 11758
step    240 | loss 1.7887 | lr 3.00e-04 | grad 2.98 | tok/s 11689
step    250 | loss 1.9184 | lr 3.00e-04 | grad 3.81 | tok/s 12015
step    260 | loss 1.8367 | lr 3.00e-04 | grad 3.25 | tok/s 11733
step    270 | loss 1.7733 | lr 3.00e-04 | grad 3.20 | tok/s 11029
step    280 | loss 1.6744 | lr 3.00e-04 | grad 3.06 | tok/s 11410
step    290 | loss 1.9658 | lr 3.00e-04 | grad 3.89 | tok/s 11491
step    300 | loss 1.6745 | lr 3.00e-04 | grad 3.48 | tok/s 11434
step    310 | loss 1.9010 | lr 3.00e-04 | grad 4.81 | tok/s 11580
step    320 | loss 1.7205 | lr 3.00e-04 | grad 3.16 | tok/s 11708
step    330 | loss 2.0323 | lr 3.00e-04 | grad 4.00 | tok/s 11647
step    340 | loss 1.8399 | lr 3.00e-04 | grad 4.47 | tok/s 11983
step    350 | loss 1.6203 | lr 3.00e-04 | grad 2.78 | tok/s 11468
step    360 | loss 1.5897 | lr 3.00e-04 | grad 2.86 | tok/s 12080
step    370 | loss 1.3444 | lr 3.00e-04 | grad 3.08 | tok/s 12196
step    380 | loss 1.2427 | lr 3.00e-04 | grad 40.25 | tok/s 12185
step    390 | loss 1.8246 | lr 3.00e-04 | grad 6.62 | tok/s 11535
step    400 | loss 1.7951 | lr 3.00e-04 | grad 3.78 | tok/s 11641
step    410 | loss 1.7841 | lr 3.00e-04 | grad 3.56 | tok/s 12149
step    420 | loss 1.6685 | lr 3.00e-04 | grad 3.17 | tok/s 11939
step    430 | loss 1.7589 | lr 3.00e-04 | grad 4.59 | tok/s 11572
step    440 | loss 1.6610 | lr 3.00e-04 | grad 2.77 | tok/s 11705
step    450 | loss 1.6559 | lr 3.00e-04 | grad 2.69 | tok/s 11888
step    460 | loss 1.6240 | lr 3.00e-04 | grad 9.69 | tok/s 11804
step    470 | loss 1.7299 | lr 3.00e-04 | grad 3.86 | tok/s 12069
step    480 | loss 1.7098 | lr 3.00e-04 | grad 4.53 | tok/s 11587
step    490 | loss 1.8481 | lr 3.00e-04 | grad 2.92 | tok/s 11763
step    500 | loss 1.7545 | lr 3.00e-04 | grad 3.41 | tok/s 11231
step    510 | loss 1.5743 | lr 3.00e-04 | grad 3.28 | tok/s 11752
step    520 | loss 1.7601 | lr 3.00e-04 | grad 2.66 | tok/s 11558
step    530 | loss 1.6688 | lr 3.00e-04 | grad 3.00 | tok/s 11337
step    540 | loss 1.3812 | lr 3.00e-04 | grad 5.53 | tok/s 11857
step    550 | loss 1.5072 | lr 3.00e-04 | grad 2.20 | tok/s 12181
step    560 | loss 1.4022 | lr 3.00e-04 | grad 4.56 | tok/s 12167
step    570 | loss 1.3667 | lr 3.00e-04 | grad 5.91 | tok/s 12155
step    580 | loss 1.4009 | lr 3.00e-04 | grad 2.84 | tok/s 12179
step    590 | loss 1.3410 | lr 3.00e-04 | grad 3.08 | tok/s 12173
step    600 | loss 1.3550 | lr 3.00e-04 | grad 4.38 | tok/s 12168
step    610 | loss 1.3433 | lr 3.00e-04 | grad 4.03 | tok/s 12110
step    620 | loss 1.6417 | lr 3.00e-04 | grad 3.78 | tok/s 11452
step    630 | loss 1.7671 | lr 3.00e-04 | grad 2.64 | tok/s 11599
step    640 | loss 1.5893 | lr 3.00e-04 | grad 20.50 | tok/s 11584
step    650 | loss 1.6493 | lr 3.00e-04 | grad 2.83 | tok/s 12031
step    660 | loss 1.6671 | lr 3.00e-04 | grad 6.50 | tok/s 11629
step    670 | loss 1.6941 | lr 3.00e-04 | grad 2.91 | tok/s 11449
step    680 | loss 1.6182 | lr 3.00e-04 | grad 2.50 | tok/s 11364
step    690 | loss 1.5196 | lr 3.00e-04 | grad 1.83 | tok/s 11628
step    700 | loss 1.6846 | lr 3.00e-04 | grad 5.19 | tok/s 11450
step    710 | loss 1.3715 | lr 3.00e-04 | grad 3.12 | tok/s 11885
step    720 | loss 1.4769 | lr 3.00e-04 | grad 2.70 | tok/s 11686
step    730 | loss 1.8538 | lr 3.00e-04 | grad 5.62 | tok/s 12020
step    740 | loss 1.6188 | lr 3.00e-04 | grad 2.28 | tok/s 12150
step    750 | loss 1.5613 | lr 3.00e-04 | grad 4.34 | tok/s 11895
step    760 | loss 1.6129 | lr 3.00e-04 | grad 2.69 | tok/s 11709
step    770 | loss 1.5252 | lr 3.00e-04 | grad 2.55 | tok/s 11768
step    780 | loss 1.7391 | lr 3.00e-04 | grad 5.34 | tok/s 12022
step    790 | loss 1.3701 | lr 3.00e-04 | grad 2.19 | tok/s 11825
step    800 | loss 1.3651 | lr 3.00e-04 | grad 3.73 | tok/s 11439
step    810 | loss 1.4742 | lr 3.00e-04 | grad 2.33 | tok/s 11663
step    820 | loss 1.5489 | lr 3.00e-04 | grad 2.03 | tok/s 11517
step    830 | loss 1.6542 | lr 3.00e-04 | grad 2.92 | tok/s 11452
step    840 | loss 1.5915 | lr 3.00e-04 | grad 2.20 | tok/s 11688
step    850 | loss 1.6506 | lr 3.00e-04 | grad 8.56 | tok/s 11889
step    860 | loss 1.5165 | lr 3.00e-04 | grad 2.42 | tok/s 11966

Training complete! Final step: 864
