Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_110/levelmamba2_100m_20260127_053016
Model: Level mamba2, 451,796,120 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5505 | lr 3.00e-04 | grad 24.62 | tok/s 4678
step     20 | loss 3.0161 | lr 3.00e-04 | grad 3.62 | tok/s 23575
step     30 | loss 3.9284 | lr 3.00e-04 | grad 6.75 | tok/s 24230
step     40 | loss 3.4745 | lr 3.00e-04 | grad 9.50 | tok/s 24549
step     50 | loss 2.5952 | lr 3.00e-04 | grad 4.09 | tok/s 24403
step     60 | loss 2.2375 | lr 3.00e-04 | grad 2.47 | tok/s 24371
step     70 | loss 2.0246 | lr 3.00e-04 | grad 2.77 | tok/s 24302
step     80 | loss 1.9268 | lr 3.00e-04 | grad 3.38 | tok/s 24226
step     90 | loss 1.7908 | lr 3.00e-04 | grad 1.76 | tok/s 24209
step    100 | loss 1.9645 | lr 3.00e-04 | grad 6.03 | tok/s 24081
step    110 | loss 2.6923 | lr 3.00e-04 | grad 3.80 | tok/s 23088
step    120 | loss 1.8727 | lr 3.00e-04 | grad 3.33 | tok/s 23469
step    130 | loss 2.3282 | lr 3.00e-04 | grad 8.50 | tok/s 23303
step    140 | loss 1.3697 | lr 3.00e-04 | grad 2.98 | tok/s 24154
step    150 | loss 2.3275 | lr 3.00e-04 | grad 3.16 | tok/s 23609
step    160 | loss 2.1351 | lr 3.00e-04 | grad 2.44 | tok/s 22964
step    170 | loss 1.6745 | lr 3.00e-04 | grad 2.59 | tok/s 23586
step    180 | loss 1.7970 | lr 3.00e-04 | grad 2.67 | tok/s 22702
step    190 | loss 1.5240 | lr 3.00e-04 | grad 3.31 | tok/s 24062
step    200 | loss 1.6531 | lr 3.00e-04 | grad 3.50 | tok/s 22888
step    210 | loss 2.0505 | lr 3.00e-04 | grad 4.72 | tok/s 23317
step    220 | loss 1.7871 | lr 3.00e-04 | grad 2.19 | tok/s 22967
step    230 | loss 2.0623 | lr 3.00e-04 | grad 2.34 | tok/s 23367
step    240 | loss 1.7043 | lr 3.00e-04 | grad 1.93 | tok/s 23306
step    250 | loss 1.7112 | lr 3.00e-04 | grad 2.38 | tok/s 23838
step    260 | loss 1.7294 | lr 3.00e-04 | grad 2.38 | tok/s 23361
step    270 | loss 1.6109 | lr 3.00e-04 | grad 1.95 | tok/s 22036
step    280 | loss 1.5388 | lr 3.00e-04 | grad 2.00 | tok/s 22848
step    290 | loss 1.8397 | lr 3.00e-04 | grad 1.68 | tok/s 22827
step    300 | loss 1.5598 | lr 3.00e-04 | grad 1.52 | tok/s 22703
step    310 | loss 1.7376 | lr 3.00e-04 | grad 4.09 | tok/s 22797
step    320 | loss 1.6190 | lr 3.00e-04 | grad 2.64 | tok/s 23368
step    330 | loss 1.8702 | lr 3.00e-04 | grad 5.00 | tok/s 23137
step    340 | loss 1.6373 | lr 3.00e-04 | grad 4.19 | tok/s 24077
step    350 | loss 1.4961 | lr 3.00e-04 | grad 2.16 | tok/s 22511
step    360 | loss 1.4224 | lr 3.00e-04 | grad 1.48 | tok/s 24011
step    370 | loss 1.1993 | lr 3.00e-04 | grad 1.54 | tok/s 24261
step    380 | loss 1.0703 | lr 3.00e-04 | grad 1.71 | tok/s 24263
step    390 | loss 1.6035 | lr 3.00e-04 | grad 2.48 | tok/s 23209
step    400 | loss 1.6273 | lr 3.00e-04 | grad 7.09 | tok/s 23188
step    410 | loss 1.5546 | lr 3.00e-04 | grad 1.98 | tok/s 24073
step    420 | loss 1.5178 | lr 3.00e-04 | grad 1.66 | tok/s 23978
step    430 | loss 1.5758 | lr 3.00e-04 | grad 2.42 | tok/s 23240
step    440 | loss 1.5605 | lr 3.00e-04 | grad 1.73 | tok/s 23260
step    450 | loss 1.5088 | lr 3.00e-04 | grad 2.05 | tok/s 23698
step    460 | loss 1.4150 | lr 3.00e-04 | grad 1.44 | tok/s 23418
step    470 | loss 1.5660 | lr 3.00e-04 | grad 1.92 | tok/s 24186
step    480 | loss 1.6175 | lr 3.00e-04 | grad 1.83 | tok/s 22999
step    490 | loss 1.7139 | lr 3.00e-04 | grad 2.80 | tok/s 23508
step    500 | loss 1.6008 | lr 3.00e-04 | grad 1.42 | tok/s 22416
step    510 | loss 1.4636 | lr 3.00e-04 | grad 4.16 | tok/s 23470
step    520 | loss 1.6056 | lr 3.00e-04 | grad 2.58 | tok/s 23010
step    530 | loss 1.5554 | lr 3.00e-04 | grad 1.41 | tok/s 23030
step    540 | loss 1.2313 | lr 3.00e-04 | grad 1.63 | tok/s 23236
step    550 | loss 1.4303 | lr 3.00e-04 | grad 1.55 | tok/s 24223
step    560 | loss 1.2888 | lr 3.00e-04 | grad 1.45 | tok/s 24286
step    570 | loss 1.2589 | lr 3.00e-04 | grad 1.72 | tok/s 24259
step    580 | loss 1.2897 | lr 3.00e-04 | grad 1.51 | tok/s 24265
step    590 | loss 1.2112 | lr 3.00e-04 | grad 1.38 | tok/s 24265
step    600 | loss 1.2636 | lr 3.00e-04 | grad 1.70 | tok/s 24251
step    610 | loss 1.2203 | lr 3.00e-04 | grad 0.99 | tok/s 24314
step    620 | loss 1.6067 | lr 3.00e-04 | grad 3.52 | tok/s 22998
step    630 | loss 1.5852 | lr 3.00e-04 | grad 1.93 | tok/s 23140
step    640 | loss 1.4645 | lr 3.00e-04 | grad 1.70 | tok/s 23547
step    650 | loss 1.5197 | lr 3.00e-04 | grad 1.82 | tok/s 23682
step    660 | loss 1.4897 | lr 3.00e-04 | grad 1.89 | tok/s 23368
step    670 | loss 1.5996 | lr 3.00e-04 | grad 1.41 | tok/s 22752
step    680 | loss 1.4881 | lr 3.00e-04 | grad 1.69 | tok/s 22879
step    690 | loss 1.4441 | lr 3.00e-04 | grad 1.85 | tok/s 23064
step    700 | loss 1.5008 | lr 3.00e-04 | grad 2.84 | tok/s 22911
step    710 | loss 1.2872 | lr 3.00e-04 | grad 1.62 | tok/s 23681
step    720 | loss 1.3462 | lr 3.00e-04 | grad 1.80 | tok/s 23615
step    730 | loss 1.6529 | lr 3.00e-04 | grad 6.66 | tok/s 23727
step    740 | loss 1.5453 | lr 3.00e-04 | grad 1.55 | tok/s 24260
step    750 | loss 1.4040 | lr 3.00e-04 | grad 1.41 | tok/s 23784
step    760 | loss 1.4890 | lr 3.00e-04 | grad 1.41 | tok/s 23416
step    770 | loss 1.4381 | lr 3.00e-04 | grad 2.08 | tok/s 23445
step    780 | loss 1.5151 | lr 3.00e-04 | grad 3.75 | tok/s 24035
step    790 | loss 1.4034 | lr 3.00e-04 | grad 1.72 | tok/s 23633
step    800 | loss 1.1754 | lr 3.00e-04 | grad 3.72 | tok/s 22968
step    810 | loss 1.3696 | lr 3.00e-04 | grad 2.20 | tok/s 23632
step    820 | loss 1.4411 | lr 3.00e-04 | grad 1.44 | tok/s 22684
step    830 | loss 1.5249 | lr 3.00e-04 | grad 1.89 | tok/s 23142
step    840 | loss 1.4415 | lr 3.00e-04 | grad 2.14 | tok/s 23344
step    850 | loss 1.4950 | lr 3.00e-04 | grad 2.61 | tok/s 23574
step    860 | loss 1.3257 | lr 3.00e-04 | grad 1.81 | tok/s 24155
step    870 | loss 1.5085 | lr 3.00e-04 | grad 2.11 | tok/s 23240
step    880 | loss 1.4432 | lr 3.00e-04 | grad 1.52 | tok/s 23468
step    890 | loss 1.4617 | lr 3.00e-04 | grad 2.17 | tok/s 23413
step    900 | loss 1.3922 | lr 3.00e-04 | grad 1.80 | tok/s 22864
step    910 | loss 1.4373 | lr 3.00e-04 | grad 1.91 | tok/s 23384
step    920 | loss 1.3172 | lr 3.00e-04 | grad 1.41 | tok/s 23582
step    930 | loss 1.3072 | lr 3.00e-04 | grad 2.08 | tok/s 23078
step    940 | loss 1.4095 | lr 3.00e-04 | grad 1.38 | tok/s 22542
step    950 | loss 1.4047 | lr 3.00e-04 | grad 1.73 | tok/s 23296
step    960 | loss 1.3996 | lr 3.00e-04 | grad 1.46 | tok/s 23289
step    970 | loss 1.8499 | lr 3.00e-04 | grad 3.16 | tok/s 24219
step    980 | loss 1.5309 | lr 3.00e-04 | grad 1.47 | tok/s 23260
step    990 | loss 1.4935 | lr 3.00e-04 | grad 1.44 | tok/s 23537
step   1000 | loss 1.1529 | lr 3.00e-04 | grad 1.63 | tok/s 23749
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1529.pt
step   1010 | loss 1.2247 | lr 3.00e-04 | grad 1.65 | tok/s 11996
step   1020 | loss 1.4125 | lr 3.00e-04 | grad 1.91 | tok/s 23340
step   1030 | loss 1.9862 | lr 3.00e-04 | grad 4.09 | tok/s 23859
step   1040 | loss 1.4722 | lr 3.00e-04 | grad 1.41 | tok/s 24015
step   1050 | loss 1.1591 | lr 3.00e-04 | grad 1.53 | tok/s 23842
step   1060 | loss 1.3024 | lr 3.00e-04 | grad 1.52 | tok/s 23720
step   1070 | loss 1.2195 | lr 3.00e-04 | grad 1.51 | tok/s 24394
step   1080 | loss 1.2000 | lr 3.00e-04 | grad 1.52 | tok/s 24459
step   1090 | loss 1.1751 | lr 3.00e-04 | grad 1.43 | tok/s 24373
step   1100 | loss 1.1218 | lr 3.00e-04 | grad 1.30 | tok/s 24405
step   1110 | loss 1.3332 | lr 3.00e-04 | grad 1.63 | tok/s 23765
step   1120 | loss 1.5690 | lr 3.00e-04 | grad 1.45 | tok/s 23926
step   1130 | loss 1.6991 | lr 3.00e-04 | grad 1.55 | tok/s 24225
step   1140 | loss 1.5665 | lr 3.00e-04 | grad 3.25 | tok/s 23740
step   1150 | loss 1.5895 | lr 3.00e-04 | grad 3.67 | tok/s 22963
step   1160 | loss 1.4711 | lr 3.00e-04 | grad 1.86 | tok/s 22934
step   1170 | loss 1.3081 | lr 3.00e-04 | grad 1.34 | tok/s 24058
step   1180 | loss 1.5200 | lr 3.00e-04 | grad 2.38 | tok/s 24154
step   1190 | loss 1.1088 | lr 3.00e-04 | grad 1.28 | tok/s 24387
step   1200 | loss 1.2731 | lr 3.00e-04 | grad 1.53 | tok/s 23032
step   1210 | loss 1.3150 | lr 3.00e-04 | grad 1.65 | tok/s 23658
step   1220 | loss 1.3469 | lr 3.00e-04 | grad 1.34 | tok/s 23892
step   1230 | loss 1.2055 | lr 3.00e-04 | grad 2.06 | tok/s 24160
step   1240 | loss 1.4222 | lr 3.00e-04 | grad 2.17 | tok/s 23597
step   1250 | loss 1.2998 | lr 3.00e-04 | grad 1.50 | tok/s 24167
step   1260 | loss 1.3220 | lr 3.00e-04 | grad 2.19 | tok/s 23427
step   1270 | loss 1.3271 | lr 3.00e-04 | grad 2.06 | tok/s 23523
step   1280 | loss 1.3009 | lr 3.00e-04 | grad 1.84 | tok/s 22982
step   1290 | loss 1.5120 | lr 3.00e-04 | grad 7.09 | tok/s 22886
step   1300 | loss 1.4458 | lr 3.00e-04 | grad 1.88 | tok/s 23804
step   1310 | loss 1.4265 | lr 3.00e-04 | grad 3.12 | tok/s 23795
step   1320 | loss 1.3663 | lr 3.00e-04 | grad 1.57 | tok/s 23738
step   1330 | loss 1.4650 | lr 3.00e-04 | grad 1.47 | tok/s 23008
step   1340 | loss 1.3642 | lr 3.00e-04 | grad 1.47 | tok/s 23964
step   1350 | loss 1.4107 | lr 3.00e-04 | grad 1.52 | tok/s 22452
step   1360 | loss 1.4665 | lr 3.00e-04 | grad 3.39 | tok/s 23916
step   1370 | loss 1.4149 | lr 3.00e-04 | grad 1.66 | tok/s 23203
step   1380 | loss 1.2692 | lr 3.00e-04 | grad 2.12 | tok/s 23656
step   1390 | loss 1.4742 | lr 3.00e-04 | grad 1.32 | tok/s 23145
step   1400 | loss 1.3031 | lr 3.00e-04 | grad 1.54 | tok/s 22760
step   1410 | loss 1.0466 | lr 3.00e-04 | grad 1.61 | tok/s 24124
step   1420 | loss 1.6408 | lr 3.00e-04 | grad 1.22 | tok/s 23350
step   1430 | loss 1.3996 | lr 3.00e-04 | grad 2.09 | tok/s 23562
step   1440 | loss 1.3540 | lr 3.00e-04 | grad 1.43 | tok/s 23808
step   1450 | loss 1.4573 | lr 3.00e-04 | grad 2.22 | tok/s 23104
step   1460 | loss 1.3372 | lr 3.00e-04 | grad 1.62 | tok/s 22859
step   1470 | loss 1.3171 | lr 3.00e-04 | grad 2.39 | tok/s 23650
step   1480 | loss 1.5916 | lr 3.00e-04 | grad 6.44 | tok/s 23458
step   1490 | loss 1.5167 | lr 3.00e-04 | grad 1.40 | tok/s 23783
step   1500 | loss 1.2230 | lr 3.00e-04 | grad 1.48 | tok/s 23412
step   1510 | loss 1.4137 | lr 3.00e-04 | grad 1.91 | tok/s 23292
step   1520 | loss 1.3299 | lr 3.00e-04 | grad 1.68 | tok/s 23855
step   1530 | loss 1.4409 | lr 3.00e-04 | grad 1.41 | tok/s 23901
step   1540 | loss 1.4115 | lr 3.00e-04 | grad 4.38 | tok/s 23499
step   1550 | loss 1.0987 | lr 3.00e-04 | grad 1.09 | tok/s 24262
step   1560 | loss 1.2575 | lr 3.00e-04 | grad 1.30 | tok/s 23696
step   1570 | loss 1.1765 | lr 3.00e-04 | grad 1.65 | tok/s 23822
step   1580 | loss 1.3854 | lr 3.00e-04 | grad 3.34 | tok/s 22934
step   1590 | loss 1.1873 | lr 3.00e-04 | grad 5.72 | tok/s 24233
step   1600 | loss 1.8044 | lr 3.00e-04 | grad 3.11 | tok/s 23572
step   1610 | loss 1.9704 | lr 3.00e-04 | grad 2.30 | tok/s 24318
step   1620 | loss 1.6337 | lr 3.00e-04 | grad 2.39 | tok/s 24325
step   1630 | loss 1.4673 | lr 3.00e-04 | grad 2.02 | tok/s 24330
step   1640 | loss 1.3617 | lr 3.00e-04 | grad 2.06 | tok/s 24267
step   1650 | loss 1.3173 | lr 3.00e-04 | grad 1.62 | tok/s 24377
step   1660 | loss 1.4460 | lr 3.00e-04 | grad 2.08 | tok/s 23499
step   1670 | loss 1.3450 | lr 3.00e-04 | grad 1.78 | tok/s 23332
step   1680 | loss 1.4195 | lr 3.00e-04 | grad 2.33 | tok/s 22836
step   1690 | loss 1.2806 | lr 3.00e-04 | grad 1.64 | tok/s 23667
step   1700 | loss 1.1885 | lr 3.00e-04 | grad 2.42 | tok/s 23810
step   1710 | loss 1.3965 | lr 3.00e-04 | grad 1.36 | tok/s 23064

Training complete! Final step: 1717
