Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_35/levelmamba2_100m_20260127_035647
Model: Level mamba2, 451,796,120 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5505 | lr 3.00e-04 | grad 24.62 | tok/s 4646
step     20 | loss 3.0161 | lr 3.00e-04 | grad 3.62 | tok/s 23054
step     30 | loss 3.9284 | lr 3.00e-04 | grad 6.75 | tok/s 23638
step     40 | loss 3.4744 | lr 3.00e-04 | grad 9.50 | tok/s 23868
step     50 | loss 2.5958 | lr 3.00e-04 | grad 4.09 | tok/s 23796
step     60 | loss 2.2374 | lr 3.00e-04 | grad 2.45 | tok/s 23656
step     70 | loss 2.0240 | lr 3.00e-04 | grad 2.75 | tok/s 23581
step     80 | loss 1.9198 | lr 3.00e-04 | grad 3.33 | tok/s 23529
step     90 | loss 1.7916 | lr 3.00e-04 | grad 2.16 | tok/s 23457
step    100 | loss 1.9679 | lr 3.00e-04 | grad 6.03 | tok/s 23311
step    110 | loss 2.6945 | lr 3.00e-04 | grad 3.77 | tok/s 22351
step    120 | loss 1.8731 | lr 3.00e-04 | grad 3.33 | tok/s 22689
step    130 | loss 2.3267 | lr 3.00e-04 | grad 8.50 | tok/s 22596
step    140 | loss 1.3716 | lr 3.00e-04 | grad 4.81 | tok/s 23348
step    150 | loss 2.5018 | lr 3.00e-04 | grad 2.95 | tok/s 22821
step    160 | loss 2.1358 | lr 3.00e-04 | grad 2.41 | tok/s 22196
step    170 | loss 1.6966 | lr 3.00e-04 | grad 2.31 | tok/s 22809
step    180 | loss 1.7979 | lr 3.00e-04 | grad 2.62 | tok/s 21941
step    190 | loss 1.5273 | lr 3.00e-04 | grad 5.31 | tok/s 23246
step    200 | loss 1.6741 | lr 3.00e-04 | grad 3.12 | tok/s 22083
step    210 | loss 2.0616 | lr 3.00e-04 | grad 5.56 | tok/s 22486
step    220 | loss 1.7842 | lr 3.00e-04 | grad 2.19 | tok/s 22208
step    230 | loss 2.0778 | lr 3.00e-04 | grad 2.36 | tok/s 22557
step    240 | loss 1.7129 | lr 3.00e-04 | grad 1.92 | tok/s 22489
step    250 | loss 1.7158 | lr 3.00e-04 | grad 2.44 | tok/s 23020
step    260 | loss 1.7377 | lr 3.00e-04 | grad 2.42 | tok/s 22565
step    270 | loss 1.6199 | lr 3.00e-04 | grad 1.93 | tok/s 21352
step    280 | loss 1.5492 | lr 3.00e-04 | grad 2.02 | tok/s 22112
step    290 | loss 1.8469 | lr 3.00e-04 | grad 1.69 | tok/s 22130
step    300 | loss 1.5665 | lr 3.00e-04 | grad 1.49 | tok/s 21990
step    310 | loss 1.7438 | lr 3.00e-04 | grad 4.09 | tok/s 22063
step    320 | loss 1.6237 | lr 3.00e-04 | grad 2.66 | tok/s 22620
step    330 | loss 1.8717 | lr 3.00e-04 | grad 4.97 | tok/s 22398
step    340 | loss 1.6418 | lr 3.00e-04 | grad 4.19 | tok/s 23309
step    350 | loss 1.5005 | lr 3.00e-04 | grad 1.95 | tok/s 21779
step    360 | loss 1.4228 | lr 3.00e-04 | grad 1.48 | tok/s 23257
step    370 | loss 1.1991 | lr 3.00e-04 | grad 1.48 | tok/s 23496
step    380 | loss 1.0761 | lr 3.00e-04 | grad 1.41 | tok/s 23492
step    390 | loss 1.6109 | lr 3.00e-04 | grad 2.45 | tok/s 22478
step    400 | loss 1.6302 | lr 3.00e-04 | grad 7.06 | tok/s 22448
step    410 | loss 1.5600 | lr 3.00e-04 | grad 2.03 | tok/s 23322
step    420 | loss 1.5255 | lr 3.00e-04 | grad 1.68 | tok/s 23216
step    430 | loss 1.5798 | lr 3.00e-04 | grad 2.50 | tok/s 22517
step    440 | loss 1.5641 | lr 3.00e-04 | grad 1.73 | tok/s 22508
step    450 | loss 1.5102 | lr 3.00e-04 | grad 1.96 | tok/s 22924
step    460 | loss 1.4168 | lr 3.00e-04 | grad 1.42 | tok/s 22700
step    470 | loss 1.5736 | lr 3.00e-04 | grad 1.88 | tok/s 23416
step    480 | loss 1.6213 | lr 3.00e-04 | grad 1.86 | tok/s 22287
step    490 | loss 1.7186 | lr 3.00e-04 | grad 2.84 | tok/s 22799
step    500 | loss 1.6026 | lr 3.00e-04 | grad 1.42 | tok/s 21780
step    510 | loss 1.4642 | lr 3.00e-04 | grad 3.97 | tok/s 22805
step    520 | loss 1.6051 | lr 3.00e-04 | grad 2.48 | tok/s 22322
step    530 | loss 1.5573 | lr 3.00e-04 | grad 1.42 | tok/s 22348
step    540 | loss 1.2327 | lr 3.00e-04 | grad 1.60 | tok/s 22551
step    550 | loss 1.4311 | lr 3.00e-04 | grad 1.59 | tok/s 23566
step    560 | loss 1.2908 | lr 3.00e-04 | grad 1.50 | tok/s 23555
step    570 | loss 1.2581 | lr 3.00e-04 | grad 1.82 | tok/s 23531
step    580 | loss 1.2905 | lr 3.00e-04 | grad 1.51 | tok/s 23570
step    590 | loss 1.2109 | lr 3.00e-04 | grad 1.39 | tok/s 23596
step    600 | loss 1.2649 | lr 3.00e-04 | grad 1.73 | tok/s 23600
step    610 | loss 1.2207 | lr 3.00e-04 | grad 0.99 | tok/s 23568
step    620 | loss 1.6088 | lr 3.00e-04 | grad 3.58 | tok/s 22299
step    630 | loss 1.5841 | lr 3.00e-04 | grad 1.89 | tok/s 22451
step    640 | loss 1.4627 | lr 3.00e-04 | grad 1.66 | tok/s 22888
step    650 | loss 1.5241 | lr 3.00e-04 | grad 1.82 | tok/s 23035
step    660 | loss 1.4907 | lr 3.00e-04 | grad 1.91 | tok/s 22745
step    670 | loss 1.5961 | lr 3.00e-04 | grad 1.38 | tok/s 22123
step    680 | loss 1.4926 | lr 3.00e-04 | grad 1.68 | tok/s 22242
step    690 | loss 1.4443 | lr 3.00e-04 | grad 1.81 | tok/s 22457
step    700 | loss 1.5052 | lr 3.00e-04 | grad 2.91 | tok/s 22301
step    710 | loss 1.2869 | lr 3.00e-04 | grad 1.52 | tok/s 23057
step    720 | loss 1.3463 | lr 3.00e-04 | grad 1.79 | tok/s 22987
step    730 | loss 1.6546 | lr 3.00e-04 | grad 6.31 | tok/s 23088
step    740 | loss 1.5493 | lr 3.00e-04 | grad 1.56 | tok/s 23598
step    750 | loss 1.4066 | lr 3.00e-04 | grad 1.41 | tok/s 23147
step    760 | loss 1.4880 | lr 3.00e-04 | grad 1.43 | tok/s 22790
step    770 | loss 1.4389 | lr 3.00e-04 | grad 2.11 | tok/s 22864
step    780 | loss 1.5169 | lr 3.00e-04 | grad 3.47 | tok/s 23416
step    790 | loss 1.4021 | lr 3.00e-04 | grad 1.54 | tok/s 23020
step    800 | loss 1.1741 | lr 3.00e-04 | grad 3.27 | tok/s 22422
step    810 | loss 1.3664 | lr 3.00e-04 | grad 2.23 | tok/s 23062
step    820 | loss 1.4444 | lr 3.00e-04 | grad 1.46 | tok/s 22090
step    830 | loss 1.5213 | lr 3.00e-04 | grad 1.83 | tok/s 22548
step    840 | loss 1.4459 | lr 3.00e-04 | grad 2.16 | tok/s 22757
step    850 | loss 1.4960 | lr 3.00e-04 | grad 2.62 | tok/s 23000
step    860 | loss 1.3330 | lr 3.00e-04 | grad 1.84 | tok/s 23552
step    870 | loss 1.5076 | lr 3.00e-04 | grad 2.09 | tok/s 22674
step    880 | loss 1.4440 | lr 3.00e-04 | grad 1.55 | tok/s 22892
step    890 | loss 1.4651 | lr 3.00e-04 | grad 2.20 | tok/s 22828
step    900 | loss 1.3914 | lr 3.00e-04 | grad 1.82 | tok/s 22356
step    910 | loss 1.4422 | lr 3.00e-04 | grad 1.92 | tok/s 22854
step    920 | loss 1.3199 | lr 3.00e-04 | grad 1.38 | tok/s 22949
step    930 | loss 1.3079 | lr 3.00e-04 | grad 2.09 | tok/s 22571
step    940 | loss 1.4088 | lr 3.00e-04 | grad 1.36 | tok/s 22017
step    950 | loss 1.4048 | lr 3.00e-04 | grad 1.74 | tok/s 22751
step    960 | loss 1.3997 | lr 3.00e-04 | grad 1.53 | tok/s 22786
step    970 | loss 1.8503 | lr 3.00e-04 | grad 2.78 | tok/s 23674
step    980 | loss 1.5309 | lr 3.00e-04 | grad 1.46 | tok/s 22751
step    990 | loss 1.4912 | lr 3.00e-04 | grad 1.41 | tok/s 23024
step   1000 | loss 1.1648 | lr 3.00e-04 | grad 1.62 | tok/s 23238
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1648.pt
step   1010 | loss 1.2187 | lr 3.00e-04 | grad 1.62 | tok/s 12555
step   1020 | loss 1.4108 | lr 3.00e-04 | grad 1.90 | tok/s 22915
step   1030 | loss 1.9980 | lr 3.00e-04 | grad 3.84 | tok/s 23445
step   1040 | loss 1.4715 | lr 3.00e-04 | grad 1.42 | tok/s 23571
step   1050 | loss 1.1634 | lr 3.00e-04 | grad 1.66 | tok/s 23368
step   1060 | loss 1.3017 | lr 3.00e-04 | grad 1.53 | tok/s 23254
step   1070 | loss 1.2196 | lr 3.00e-04 | grad 1.45 | tok/s 23978
step   1080 | loss 1.1989 | lr 3.00e-04 | grad 1.49 | tok/s 23957
step   1090 | loss 1.1739 | lr 3.00e-04 | grad 1.44 | tok/s 23934
step   1100 | loss 1.1219 | lr 3.00e-04 | grad 1.31 | tok/s 23942
step   1110 | loss 1.3352 | lr 3.00e-04 | grad 1.66 | tok/s 23273
step   1120 | loss 1.5687 | lr 3.00e-04 | grad 1.45 | tok/s 23504
step   1130 | loss 1.6936 | lr 3.00e-04 | grad 1.54 | tok/s 23820
step   1140 | loss 1.5734 | lr 3.00e-04 | grad 2.94 | tok/s 23279
step   1150 | loss 1.5852 | lr 3.00e-04 | grad 3.67 | tok/s 22552
step   1160 | loss 1.4740 | lr 3.00e-04 | grad 1.86 | tok/s 22506
step   1170 | loss 1.3104 | lr 3.00e-04 | grad 1.37 | tok/s 23628
step   1180 | loss 1.5281 | lr 3.00e-04 | grad 2.47 | tok/s 23663
step   1190 | loss 1.1128 | lr 3.00e-04 | grad 1.32 | tok/s 23898
step   1200 | loss 1.2722 | lr 3.00e-04 | grad 1.49 | tok/s 22559
step   1210 | loss 1.3136 | lr 3.00e-04 | grad 1.70 | tok/s 23252
step   1220 | loss 1.3497 | lr 3.00e-04 | grad 1.36 | tok/s 23409
step   1230 | loss 1.2083 | lr 3.00e-04 | grad 2.11 | tok/s 23672
step   1240 | loss 1.4215 | lr 3.00e-04 | grad 2.19 | tok/s 23114
step   1250 | loss 1.2971 | lr 3.00e-04 | grad 1.49 | tok/s 23748
step   1260 | loss 1.3202 | lr 3.00e-04 | grad 2.22 | tok/s 23055
step   1270 | loss 1.3219 | lr 3.00e-04 | grad 2.03 | tok/s 23131
step   1280 | loss 1.3009 | lr 3.00e-04 | grad 1.85 | tok/s 22617
step   1290 | loss 1.5142 | lr 3.00e-04 | grad 7.06 | tok/s 22533
step   1300 | loss 1.4433 | lr 3.00e-04 | grad 1.95 | tok/s 23385
step   1310 | loss 1.4250 | lr 3.00e-04 | grad 3.22 | tok/s 23356
step   1320 | loss 1.3716 | lr 3.00e-04 | grad 1.54 | tok/s 23297
step   1330 | loss 1.4639 | lr 3.00e-04 | grad 1.40 | tok/s 22580
step   1340 | loss 1.3632 | lr 3.00e-04 | grad 1.45 | tok/s 23458
step   1350 | loss 1.4112 | lr 3.00e-04 | grad 1.52 | tok/s 22080
step   1360 | loss 1.4666 | lr 3.00e-04 | grad 3.36 | tok/s 23513
step   1370 | loss 1.4144 | lr 3.00e-04 | grad 1.73 | tok/s 22813
step   1380 | loss 1.2719 | lr 3.00e-04 | grad 2.19 | tok/s 23226
step   1390 | loss 1.4726 | lr 3.00e-04 | grad 1.31 | tok/s 22796
step   1400 | loss 1.3030 | lr 3.00e-04 | grad 1.55 | tok/s 22396
step   1410 | loss 1.0499 | lr 3.00e-04 | grad 1.60 | tok/s 23716
step   1420 | loss 1.6430 | lr 3.00e-04 | grad 1.23 | tok/s 22961
step   1430 | loss 1.3961 | lr 3.00e-04 | grad 2.09 | tok/s 23173
step   1440 | loss 1.3534 | lr 3.00e-04 | grad 1.42 | tok/s 23443
step   1450 | loss 1.4560 | lr 3.00e-04 | grad 2.25 | tok/s 22740
step   1460 | loss 1.3369 | lr 3.00e-04 | grad 1.61 | tok/s 22452
step   1470 | loss 1.3154 | lr 3.00e-04 | grad 2.38 | tok/s 23240
step   1480 | loss 1.5879 | lr 3.00e-04 | grad 5.94 | tok/s 23056
step   1490 | loss 1.5198 | lr 3.00e-04 | grad 1.40 | tok/s 23422
step   1500 | loss 1.2301 | lr 3.00e-04 | grad 1.45 | tok/s 23007
step   1510 | loss 1.4105 | lr 3.00e-04 | grad 1.95 | tok/s 22866
step   1520 | loss 1.3303 | lr 3.00e-04 | grad 1.69 | tok/s 23449
step   1530 | loss 1.4470 | lr 3.00e-04 | grad 1.43 | tok/s 23468
step   1540 | loss 1.4122 | lr 3.00e-04 | grad 4.44 | tok/s 23052
step   1550 | loss 1.1199 | lr 3.00e-04 | grad 1.07 | tok/s 23788
step   1560 | loss 1.2565 | lr 3.00e-04 | grad 1.30 | tok/s 23276
step   1570 | loss 1.1802 | lr 3.00e-04 | grad 1.68 | tok/s 23377
step   1580 | loss 1.3850 | lr 3.00e-04 | grad 3.19 | tok/s 22503
step   1590 | loss 1.1898 | lr 3.00e-04 | grad 5.59 | tok/s 23774
step   1600 | loss 1.8009 | lr 3.00e-04 | grad 3.14 | tok/s 23186
step   1610 | loss 1.9639 | lr 3.00e-04 | grad 2.28 | tok/s 23907
step   1620 | loss 1.6330 | lr 3.00e-04 | grad 1.80 | tok/s 23913
step   1630 | loss 1.4678 | lr 3.00e-04 | grad 1.82 | tok/s 23930
step   1640 | loss 1.3617 | lr 3.00e-04 | grad 1.98 | tok/s 23918
step   1650 | loss 1.3192 | lr 3.00e-04 | grad 1.98 | tok/s 23941
step   1660 | loss 1.4441 | lr 3.00e-04 | grad 2.08 | tok/s 23217
step   1670 | loss 1.3450 | lr 3.00e-04 | grad 1.77 | tok/s 23040

Training complete! Final step: 1677
