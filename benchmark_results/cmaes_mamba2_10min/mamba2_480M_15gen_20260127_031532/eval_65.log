Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_65/levelmamba2_100m_20260127_043827
Model: Level mamba2, 958,039,680 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 8.6941 | lr 3.00e-04 | grad 17.00 | tok/s 2381
step     20 | loss 3.1248 | lr 3.00e-04 | grad 5.44 | tok/s 11377
step     30 | loss 2.4723 | lr 3.00e-04 | grad 5.75 | tok/s 10850
step     40 | loss 2.7108 | lr 3.00e-04 | grad 7.69 | tok/s 10838
step     50 | loss 2.1023 | lr 3.00e-04 | grad 8.38 | tok/s 11359
step     60 | loss 1.7586 | lr 3.00e-04 | grad 3.70 | tok/s 11371
step     70 | loss 4.7521 | lr 3.00e-04 | grad 11.38 | tok/s 11337
step     80 | loss 3.9913 | lr 3.00e-04 | grad 18.25 | tok/s 11617
step     90 | loss 3.1426 | lr 3.00e-04 | grad 15.00 | tok/s 11601
step    100 | loss 2.6807 | lr 3.00e-04 | grad 19.88 | tok/s 11554
step    110 | loss 2.4209 | lr 3.00e-04 | grad 5.62 | tok/s 11526
step    120 | loss 2.2896 | lr 3.00e-04 | grad 4.41 | tok/s 11512
step    130 | loss 2.1579 | lr 3.00e-04 | grad 5.12 | tok/s 11483
step    140 | loss 2.0744 | lr 3.00e-04 | grad 6.94 | tok/s 11477
step    150 | loss 1.9123 | lr 3.00e-04 | grad 6.25 | tok/s 11456
step    160 | loss 1.9780 | lr 3.00e-04 | grad 6.78 | tok/s 11461
step    170 | loss 1.8462 | lr 3.00e-04 | grad 5.75 | tok/s 11449
step    180 | loss 1.9170 | lr 3.00e-04 | grad 3.27 | tok/s 11434
step    190 | loss 1.7174 | lr 3.00e-04 | grad 5.25 | tok/s 11419
step    200 | loss 1.7709 | lr 3.00e-04 | grad 4.72 | tok/s 11422
step    210 | loss 2.0214 | lr 3.00e-04 | grad 3.86 | tok/s 11282
step    220 | loss 2.3226 | lr 3.00e-04 | grad 10.31 | tok/s 11301
step    230 | loss 2.1984 | lr 3.00e-04 | grad 4.59 | tok/s 10514
step    240 | loss 2.1127 | lr 3.00e-04 | grad 4.44 | tok/s 10853
step    250 | loss 1.5527 | lr 3.00e-04 | grad 3.48 | tok/s 11424
step    260 | loss 1.9933 | lr 3.00e-04 | grad 5.09 | tok/s 10920
step    270 | loss 2.2432 | lr 3.00e-04 | grad 3.89 | tok/s 11100
step    280 | loss 1.6102 | lr 3.00e-04 | grad 3.64 | tok/s 11256
step    290 | loss 0.6291 | lr 3.00e-04 | grad 2.78 | tok/s 11479
step    300 | loss 2.3510 | lr 3.00e-04 | grad 6.62 | tok/s 11261
step    310 | loss 1.8201 | lr 3.00e-04 | grad 6.06 | tok/s 11153
step    320 | loss 1.9519 | lr 3.00e-04 | grad 3.20 | tok/s 10530
step    330 | loss 2.1934 | lr 3.00e-04 | grad 3.72 | tok/s 10845
step    340 | loss 1.8775 | lr 3.00e-04 | grad 3.91 | tok/s 11031
step    350 | loss 1.3597 | lr 3.00e-04 | grad 3.28 | tok/s 11344
step    360 | loss 1.8387 | lr 3.00e-04 | grad 3.36 | tok/s 10240
step    370 | loss 1.7356 | lr 3.00e-04 | grad 4.97 | tok/s 10933
step    380 | loss 1.5365 | lr 3.00e-04 | grad 4.69 | tok/s 11406
step    390 | loss 1.4561 | lr 3.00e-04 | grad 3.52 | tok/s 11315
step    400 | loss 1.3303 | lr 3.00e-04 | grad 2.08 | tok/s 11420
step    410 | loss 1.6155 | lr 3.00e-04 | grad 3.77 | tok/s 10285
step    420 | loss 2.1260 | lr 3.00e-04 | grad 2.89 | tok/s 11200
step    430 | loss 2.1055 | lr 3.00e-04 | grad 5.06 | tok/s 10718
step    440 | loss 1.8955 | lr 3.00e-04 | grad 2.94 | tok/s 10911
step    450 | loss 1.7715 | lr 3.00e-04 | grad 3.09 | tok/s 10770
step    460 | loss 1.7362 | lr 3.00e-04 | grad 24.88 | tok/s 11090
step    470 | loss 2.1340 | lr 3.00e-04 | grad 5.56 | tok/s 11109
step    480 | loss 1.8589 | lr 3.00e-04 | grad 20.12 | tok/s 10521
step    490 | loss 1.6153 | lr 3.00e-04 | grad 8.44 | tok/s 11188
step    500 | loss 1.6739 | lr 3.00e-04 | grad 3.56 | tok/s 11337
step    510 | loss 1.6735 | lr 3.00e-04 | grad 4.84 | tok/s 11317
step    520 | loss 1.8540 | lr 3.00e-04 | grad 3.05 | tok/s 10905
step    530 | loss 1.7087 | lr 3.00e-04 | grad 5.28 | tok/s 10893
step    540 | loss 1.5078 | lr 3.00e-04 | grad 1.90 | tok/s 10700
step    550 | loss 1.7183 | lr 3.00e-04 | grad 3.52 | tok/s 10369
step    560 | loss 1.6276 | lr 3.00e-04 | grad 2.53 | tok/s 10770
step    570 | loss 1.4978 | lr 3.00e-04 | grad 2.22 | tok/s 10571
step    580 | loss 1.7886 | lr 3.00e-04 | grad 6.78 | tok/s 10983
step    590 | loss 1.7970 | lr 3.00e-04 | grad 4.12 | tok/s 10505
step    600 | loss 1.6089 | lr 3.00e-04 | grad 2.67 | tok/s 11109
step    610 | loss 1.5178 | lr 3.00e-04 | grad 2.44 | tok/s 10614
step    620 | loss 1.6135 | lr 3.00e-04 | grad 3.33 | tok/s 10508
step    630 | loss 1.7457 | lr 3.00e-04 | grad 3.19 | tok/s 10889
step    640 | loss 1.6302 | lr 3.00e-04 | grad 2.12 | tok/s 10955
step    650 | loss 1.7036 | lr 3.00e-04 | grad 3.50 | tok/s 11078
step    660 | loss 1.8041 | lr 3.00e-04 | grad 4.19 | tok/s 10975
step    670 | loss 1.7380 | lr 3.00e-04 | grad 3.41 | tok/s 10818
step    680 | loss 1.7844 | lr 3.00e-04 | grad 2.55 | tok/s 11145
step    690 | loss 1.4671 | lr 3.00e-04 | grad 3.16 | tok/s 11397
step    700 | loss 1.5540 | lr 3.00e-04 | grad 2.73 | tok/s 10786
step    710 | loss 1.4347 | lr 3.00e-04 | grad 13.62 | tok/s 10411
step    720 | loss 1.3479 | lr 3.00e-04 | grad 2.83 | tok/s 11401
step    730 | loss 1.5569 | lr 3.00e-04 | grad 2.27 | tok/s 11240
step    740 | loss 1.2342 | lr 3.00e-04 | grad 2.72 | tok/s 11444
step    750 | loss 1.1340 | lr 3.00e-04 | grad 2.48 | tok/s 11422
step    760 | loss 1.0821 | lr 3.00e-04 | grad 2.64 | tok/s 11448
step    770 | loss 1.0260 | lr 3.00e-04 | grad 1.73 | tok/s 11427
step    780 | loss 1.0660 | lr 3.00e-04 | grad 3.00 | tok/s 11177
step    790 | loss 1.7912 | lr 3.00e-04 | grad 3.38 | tok/s 10931
step    800 | loss 1.7769 | lr 3.00e-04 | grad 2.47 | tok/s 10971
step    810 | loss 1.6265 | lr 3.00e-04 | grad 2.34 | tok/s 10560
step    820 | loss 1.5545 | lr 3.00e-04 | grad 2.62 | tok/s 11321
step    830 | loss 1.3908 | lr 3.00e-04 | grad 3.09 | tok/s 11415
step    840 | loss 1.5623 | lr 3.00e-04 | grad 2.33 | tok/s 11386
step    850 | loss 1.3784 | lr 3.00e-04 | grad 2.23 | tok/s 11315
step    860 | loss 1.5357 | lr 3.00e-04 | grad 2.12 | tok/s 10788
step    870 | loss 1.6251 | lr 3.00e-04 | grad 9.31 | tok/s 10999
step    880 | loss 1.6183 | lr 3.00e-04 | grad 2.45 | tok/s 10944
step    890 | loss 1.5200 | lr 3.00e-04 | grad 3.67 | tok/s 11053
step    900 | loss 1.3759 | lr 3.00e-04 | grad 2.98 | tok/s 10829
step    910 | loss 1.5496 | lr 3.00e-04 | grad 3.61 | tok/s 11227
step    920 | loss 1.5421 | lr 3.00e-04 | grad 2.27 | tok/s 10747
step    930 | loss 1.4112 | lr 3.00e-04 | grad 1.91 | tok/s 11315
step    940 | loss 1.5434 | lr 3.00e-04 | grad 2.95 | tok/s 11345
step    950 | loss 1.4254 | lr 3.00e-04 | grad 4.09 | tok/s 11372
step    960 | loss 1.6173 | lr 3.00e-04 | grad 4.09 | tok/s 10680
step    970 | loss 1.6379 | lr 3.00e-04 | grad 2.91 | tok/s 11134
step    980 | loss 1.4204 | lr 3.00e-04 | grad 2.33 | tok/s 11012
step    990 | loss 1.7185 | lr 3.00e-04 | grad 5.09 | tok/s 10741
step   1000 | loss 1.7385 | lr 3.00e-04 | grad 3.48 | tok/s 11061
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7385.pt
step   1010 | loss 1.4758 | lr 3.00e-04 | grad 3.47 | tok/s 2774
step   1020 | loss 1.4138 | lr 3.00e-04 | grad 1.82 | tok/s 11545
step   1030 | loss 1.4407 | lr 3.00e-04 | grad 2.12 | tok/s 10588
step   1040 | loss 1.7205 | lr 3.00e-04 | grad 2.89 | tok/s 11539
step   1050 | loss 1.6749 | lr 3.00e-04 | grad 3.34 | tok/s 11451
step   1060 | loss 1.4926 | lr 3.00e-04 | grad 2.53 | tok/s 10893
step   1070 | loss 1.2991 | lr 3.00e-04 | grad 2.20 | tok/s 10622
step   1080 | loss 1.1024 | lr 3.00e-04 | grad 1.94 | tok/s 11317
step   1090 | loss 1.5177 | lr 3.00e-04 | grad 3.11 | tok/s 11325
step   1100 | loss 1.3609 | lr 3.00e-04 | grad 1.98 | tok/s 11509
step   1110 | loss 1.3112 | lr 3.00e-04 | grad 2.64 | tok/s 11523
step   1120 | loss 1.2439 | lr 3.00e-04 | grad 2.14 | tok/s 11530
step   1130 | loss 1.2868 | lr 3.00e-04 | grad 2.47 | tok/s 11457
step   1140 | loss 1.2134 | lr 3.00e-04 | grad 3.09 | tok/s 11501
step   1150 | loss 1.2105 | lr 3.00e-04 | grad 2.19 | tok/s 11488
step   1160 | loss 1.3177 | lr 3.00e-04 | grad 2.17 | tok/s 11449
step   1170 | loss 1.2319 | lr 3.00e-04 | grad 2.78 | tok/s 11430
step   1180 | loss 1.1551 | lr 3.00e-04 | grad 2.33 | tok/s 11410
step   1190 | loss 1.2350 | lr 3.00e-04 | grad 2.09 | tok/s 11412
step   1200 | loss 1.2690 | lr 3.00e-04 | grad 2.70 | tok/s 11412
step   1210 | loss 1.2117 | lr 3.00e-04 | grad 2.81 | tok/s 11403
step   1220 | loss 1.2279 | lr 3.00e-04 | grad 2.08 | tok/s 11411
step   1230 | loss 1.6239 | lr 3.00e-04 | grad 3.41 | tok/s 11002
step   1240 | loss 1.5861 | lr 3.00e-04 | grad 2.59 | tok/s 10669
step   1250 | loss 1.3142 | lr 3.00e-04 | grad 2.03 | tok/s 10987
step   1260 | loss 1.6875 | lr 3.00e-04 | grad 4.31 | tok/s 10555
step   1270 | loss 1.4410 | lr 3.00e-04 | grad 2.56 | tok/s 11127
step   1280 | loss 1.4867 | lr 3.00e-04 | grad 3.39 | tok/s 11193
step   1290 | loss 1.4199 | lr 3.00e-04 | grad 2.61 | tok/s 10850
step   1300 | loss 1.4946 | lr 3.00e-04 | grad 3.38 | tok/s 11166
step   1310 | loss 1.6642 | lr 3.00e-04 | grad 2.06 | tok/s 11334
step   1320 | loss 1.2782 | lr 3.00e-04 | grad 4.16 | tok/s 10765
step   1330 | loss 1.6851 | lr 3.00e-04 | grad 2.64 | tok/s 10514
step   1340 | loss 1.5879 | lr 3.00e-04 | grad 2.05 | tok/s 10926
step   1350 | loss 1.3869 | lr 3.00e-04 | grad 4.47 | tok/s 10762
step   1360 | loss 1.5332 | lr 3.00e-04 | grad 2.19 | tok/s 10962
step   1370 | loss 1.4657 | lr 3.00e-04 | grad 2.97 | tok/s 10470
step   1380 | loss 1.4206 | lr 3.00e-04 | grad 4.59 | tok/s 10624
step   1390 | loss 1.3330 | lr 3.00e-04 | grad 2.50 | tok/s 11039
step   1400 | loss 1.4632 | lr 3.00e-04 | grad 2.77 | tok/s 10629
step   1410 | loss 1.4960 | lr 3.00e-04 | grad 2.28 | tok/s 11129
step   1420 | loss 1.2270 | lr 3.00e-04 | grad 1.75 | tok/s 10850
step   1430 | loss 1.0899 | lr 3.00e-04 | grad 1.76 | tok/s 11370
step   1440 | loss 1.4249 | lr 3.00e-04 | grad 3.31 | tok/s 10986
step   1450 | loss 1.4996 | lr 3.00e-04 | grad 2.88 | tok/s 10793
step   1460 | loss 1.5792 | lr 3.00e-04 | grad 25.88 | tok/s 11149
step   1470 | loss 1.7961 | lr 3.00e-04 | grad 3.75 | tok/s 11379
step   1480 | loss 1.4067 | lr 3.00e-04 | grad 2.25 | tok/s 11331
step   1490 | loss 1.3834 | lr 3.00e-04 | grad 4.41 | tok/s 11231
step   1500 | loss 1.4142 | lr 3.00e-04 | grad 2.67 | tok/s 11347
step   1510 | loss 1.4098 | lr 3.00e-04 | grad 2.45 | tok/s 11053
step   1520 | loss 1.4712 | lr 3.00e-04 | grad 2.22 | tok/s 10685
step   1530 | loss 1.4224 | lr 3.00e-04 | grad 1.84 | tok/s 11194
step   1540 | loss 1.3021 | lr 3.00e-04 | grad 3.17 | tok/s 10939
step   1550 | loss 1.4852 | lr 3.00e-04 | grad 1.95 | tok/s 11066
step   1560 | loss 1.4353 | lr 3.00e-04 | grad 5.00 | tok/s 11117
step   1570 | loss 1.6328 | lr 3.00e-04 | grad 2.34 | tok/s 11108
step   1580 | loss 1.2339 | lr 3.00e-04 | grad 1.67 | tok/s 10983

Training complete! Final step: 1583
