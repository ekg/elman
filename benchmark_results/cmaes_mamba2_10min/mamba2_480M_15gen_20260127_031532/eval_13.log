Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_13/levelmamba2_100m_20260127_032553
Model: Level mamba2, 474,664,976 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.2518 | lr 3.00e-04 | grad 13.12 | tok/s 4521
step     20 | loss 3.6380 | lr 3.00e-04 | grad 5.12 | tok/s 19625
step     30 | loss 3.8427 | lr 3.00e-04 | grad 5.47 | tok/s 20165
step     40 | loss 3.5234 | lr 3.00e-04 | grad 7.91 | tok/s 20386
step     50 | loss 2.6300 | lr 3.00e-04 | grad 3.44 | tok/s 20378
step     60 | loss 2.2783 | lr 3.00e-04 | grad 2.83 | tok/s 20318
step     70 | loss 2.0634 | lr 3.00e-04 | grad 1.89 | tok/s 20251
step     80 | loss 1.9554 | lr 3.00e-04 | grad 2.47 | tok/s 20257
step     90 | loss 1.8496 | lr 3.00e-04 | grad 1.77 | tok/s 20200
step    100 | loss 1.9514 | lr 3.00e-04 | grad 3.33 | tok/s 20079
step    110 | loss 2.7041 | lr 3.00e-04 | grad 3.11 | tok/s 19231
step    120 | loss 1.8709 | lr 3.00e-04 | grad 2.45 | tok/s 19573
step    130 | loss 2.3002 | lr 3.00e-04 | grad 7.34 | tok/s 19385
step    140 | loss 1.2863 | lr 3.00e-04 | grad 2.59 | tok/s 20020
step    150 | loss 2.2555 | lr 3.00e-04 | grad 2.55 | tok/s 19682
step    160 | loss 2.1185 | lr 3.00e-04 | grad 2.00 | tok/s 19109
step    170 | loss 1.6733 | lr 3.00e-04 | grad 2.08 | tok/s 19620
step    180 | loss 1.7839 | lr 3.00e-04 | grad 2.16 | tok/s 18866
step    190 | loss 1.5117 | lr 3.00e-04 | grad 2.61 | tok/s 20050
step    200 | loss 1.6419 | lr 3.00e-04 | grad 2.69 | tok/s 19006
step    210 | loss 2.0337 | lr 3.00e-04 | grad 3.88 | tok/s 19327
step    220 | loss 1.7297 | lr 3.00e-04 | grad 1.86 | tok/s 19054
step    230 | loss 2.0459 | lr 3.00e-04 | grad 2.03 | tok/s 19383
step    240 | loss 1.6972 | lr 3.00e-04 | grad 1.70 | tok/s 19300
step    250 | loss 1.6989 | lr 3.00e-04 | grad 2.02 | tok/s 19769
step    260 | loss 1.7211 | lr 3.00e-04 | grad 1.89 | tok/s 19353
step    270 | loss 1.6098 | lr 3.00e-04 | grad 1.70 | tok/s 18272
step    280 | loss 1.5269 | lr 3.00e-04 | grad 1.54 | tok/s 18919
step    290 | loss 1.8187 | lr 3.00e-04 | grad 1.51 | tok/s 18934
step    300 | loss 1.5554 | lr 3.00e-04 | grad 1.37 | tok/s 18805
step    310 | loss 1.7382 | lr 3.00e-04 | grad 3.66 | tok/s 18893
step    320 | loss 1.6146 | lr 3.00e-04 | grad 2.14 | tok/s 19356
step    330 | loss 1.8779 | lr 3.00e-04 | grad 4.28 | tok/s 19115
step    340 | loss 1.6123 | lr 3.00e-04 | grad 4.09 | tok/s 19956
step    350 | loss 1.4869 | lr 3.00e-04 | grad 1.82 | tok/s 18592
step    360 | loss 1.4047 | lr 3.00e-04 | grad 1.30 | tok/s 19869
step    370 | loss 1.1776 | lr 3.00e-04 | grad 1.23 | tok/s 20036
step    380 | loss 1.0493 | lr 3.00e-04 | grad 1.43 | tok/s 20015
step    390 | loss 1.5911 | lr 3.00e-04 | grad 2.19 | tok/s 19154
step    400 | loss 1.6245 | lr 3.00e-04 | grad 6.00 | tok/s 19104
step    410 | loss 1.5347 | lr 3.00e-04 | grad 1.77 | tok/s 19867
step    420 | loss 1.5056 | lr 3.00e-04 | grad 1.40 | tok/s 19755
step    430 | loss 1.5623 | lr 3.00e-04 | grad 2.20 | tok/s 19180
step    440 | loss 1.5540 | lr 3.00e-04 | grad 1.48 | tok/s 19163
step    450 | loss 1.4969 | lr 3.00e-04 | grad 1.81 | tok/s 19499
step    460 | loss 1.3961 | lr 3.00e-04 | grad 1.27 | tok/s 19306
step    470 | loss 1.5187 | lr 3.00e-04 | grad 1.77 | tok/s 19917
step    480 | loss 1.6099 | lr 3.00e-04 | grad 1.58 | tok/s 18909
step    490 | loss 1.6994 | lr 3.00e-04 | grad 2.41 | tok/s 19347
step    500 | loss 1.5943 | lr 3.00e-04 | grad 1.27 | tok/s 18469
step    510 | loss 1.4567 | lr 3.00e-04 | grad 4.25 | tok/s 19383
step    520 | loss 1.5953 | lr 3.00e-04 | grad 2.20 | tok/s 18974
step    530 | loss 1.5385 | lr 3.00e-04 | grad 1.30 | tok/s 18992
step    540 | loss 1.2246 | lr 3.00e-04 | grad 1.37 | tok/s 19072
step    550 | loss 1.4138 | lr 3.00e-04 | grad 1.38 | tok/s 20011
step    560 | loss 1.2710 | lr 3.00e-04 | grad 1.24 | tok/s 19993
step    570 | loss 1.2400 | lr 3.00e-04 | grad 1.47 | tok/s 20016
step    580 | loss 1.2726 | lr 3.00e-04 | grad 1.25 | tok/s 20009
step    590 | loss 1.1903 | lr 3.00e-04 | grad 1.20 | tok/s 19976
step    600 | loss 1.2479 | lr 3.00e-04 | grad 1.47 | tok/s 20002
step    610 | loss 1.2024 | lr 3.00e-04 | grad 0.96 | tok/s 19985
step    620 | loss 1.5777 | lr 3.00e-04 | grad 2.69 | tok/s 18829
step    630 | loss 1.5753 | lr 3.00e-04 | grad 1.73 | tok/s 18974
step    640 | loss 1.4584 | lr 3.00e-04 | grad 1.46 | tok/s 19330
step    650 | loss 1.5094 | lr 3.00e-04 | grad 1.62 | tok/s 19444
step    660 | loss 1.4824 | lr 3.00e-04 | grad 1.70 | tok/s 19251
step    670 | loss 1.5876 | lr 3.00e-04 | grad 1.26 | tok/s 18717
step    680 | loss 1.4858 | lr 3.00e-04 | grad 1.52 | tok/s 18764
step    690 | loss 1.4340 | lr 3.00e-04 | grad 1.56 | tok/s 18951
step    700 | loss 1.4853 | lr 3.00e-04 | grad 2.41 | tok/s 18802
step    710 | loss 1.2637 | lr 3.00e-04 | grad 1.40 | tok/s 19448
step    720 | loss 1.3395 | lr 3.00e-04 | grad 1.56 | tok/s 19386
step    730 | loss 1.6261 | lr 3.00e-04 | grad 4.38 | tok/s 19492
step    740 | loss 1.5225 | lr 3.00e-04 | grad 1.41 | tok/s 19958
step    750 | loss 1.3945 | lr 3.00e-04 | grad 1.26 | tok/s 19551
step    760 | loss 1.4745 | lr 3.00e-04 | grad 1.29 | tok/s 19218
step    770 | loss 1.4248 | lr 3.00e-04 | grad 1.88 | tok/s 19292
step    780 | loss 1.4998 | lr 3.00e-04 | grad 3.25 | tok/s 19768
step    790 | loss 1.3824 | lr 3.00e-04 | grad 1.61 | tok/s 19432
step    800 | loss 1.1656 | lr 3.00e-04 | grad 2.77 | tok/s 18810
step    810 | loss 1.3488 | lr 3.00e-04 | grad 1.94 | tok/s 19434
step    820 | loss 1.4302 | lr 3.00e-04 | grad 1.20 | tok/s 18609
step    830 | loss 1.5074 | lr 3.00e-04 | grad 1.54 | tok/s 19003
step    840 | loss 1.4275 | lr 3.00e-04 | grad 1.83 | tok/s 19198
step    850 | loss 1.4820 | lr 3.00e-04 | grad 2.30 | tok/s 19416
step    860 | loss 1.3024 | lr 3.00e-04 | grad 1.61 | tok/s 19861
step    870 | loss 1.5022 | lr 3.00e-04 | grad 1.78 | tok/s 19134
step    880 | loss 1.4308 | lr 3.00e-04 | grad 1.28 | tok/s 19299
step    890 | loss 1.4522 | lr 3.00e-04 | grad 1.91 | tok/s 19261
step    900 | loss 1.3828 | lr 3.00e-04 | grad 1.53 | tok/s 18812
step    910 | loss 1.4295 | lr 3.00e-04 | grad 1.53 | tok/s 19235
step    920 | loss 1.3028 | lr 3.00e-04 | grad 1.20 | tok/s 19352
step    930 | loss 1.2937 | lr 3.00e-04 | grad 1.80 | tok/s 18975
step    940 | loss 1.3923 | lr 3.00e-04 | grad 1.16 | tok/s 18534
step    950 | loss 1.3884 | lr 3.00e-04 | grad 1.41 | tok/s 19162
step    960 | loss 1.3828 | lr 3.00e-04 | grad 1.41 | tok/s 19138
step    970 | loss 1.7740 | lr 3.00e-04 | grad 2.66 | tok/s 19885
step    980 | loss 1.5134 | lr 3.00e-04 | grad 1.29 | tok/s 19095
step    990 | loss 1.4732 | lr 3.00e-04 | grad 1.27 | tok/s 19332
step   1000 | loss 1.1469 | lr 3.00e-04 | grad 1.33 | tok/s 19457
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1469.pt
step   1010 | loss 1.1986 | lr 3.00e-04 | grad 1.49 | tok/s 11352
step   1020 | loss 1.3973 | lr 3.00e-04 | grad 1.84 | tok/s 19222
step   1030 | loss 1.9757 | lr 3.00e-04 | grad 4.22 | tok/s 19652
step   1040 | loss 1.4615 | lr 3.00e-04 | grad 1.23 | tok/s 19796
step   1050 | loss 1.1424 | lr 3.00e-04 | grad 1.31 | tok/s 19518
step   1060 | loss 1.2932 | lr 3.00e-04 | grad 1.30 | tok/s 19488
step   1070 | loss 1.2032 | lr 3.00e-04 | grad 1.26 | tok/s 20132
step   1080 | loss 1.1815 | lr 3.00e-04 | grad 1.10 | tok/s 20132
step   1090 | loss 1.1573 | lr 3.00e-04 | grad 1.16 | tok/s 20144
step   1100 | loss 1.1048 | lr 3.00e-04 | grad 1.08 | tok/s 20121
step   1110 | loss 1.3197 | lr 3.00e-04 | grad 1.40 | tok/s 19551
step   1120 | loss 1.5663 | lr 3.00e-04 | grad 1.26 | tok/s 19761
step   1130 | loss 1.6853 | lr 3.00e-04 | grad 1.37 | tok/s 20005
step   1140 | loss 1.5368 | lr 3.00e-04 | grad 2.73 | tok/s 19551
step   1150 | loss 1.5748 | lr 3.00e-04 | grad 3.16 | tok/s 18943
step   1160 | loss 1.4572 | lr 3.00e-04 | grad 1.53 | tok/s 18867
step   1170 | loss 1.2905 | lr 3.00e-04 | grad 1.23 | tok/s 19861
step   1180 | loss 1.5122 | lr 3.00e-04 | grad 1.97 | tok/s 19883
step   1190 | loss 1.1010 | lr 3.00e-04 | grad 1.00 | tok/s 20079
step   1200 | loss 1.2679 | lr 3.00e-04 | grad 1.34 | tok/s 18929
step   1210 | loss 1.3024 | lr 3.00e-04 | grad 1.45 | tok/s 19511
step   1220 | loss 1.3304 | lr 3.00e-04 | grad 1.18 | tok/s 19640
step   1230 | loss 1.1929 | lr 3.00e-04 | grad 1.77 | tok/s 19870
step   1240 | loss 1.4097 | lr 3.00e-04 | grad 2.08 | tok/s 19395
step   1250 | loss 1.2885 | lr 3.00e-04 | grad 1.36 | tok/s 19921
step   1260 | loss 1.3019 | lr 3.00e-04 | grad 1.91 | tok/s 19317
step   1270 | loss 1.3042 | lr 3.00e-04 | grad 1.88 | tok/s 19369
step   1280 | loss 1.2854 | lr 3.00e-04 | grad 1.61 | tok/s 18946
step   1290 | loss 1.4968 | lr 3.00e-04 | grad 6.28 | tok/s 18878
step   1300 | loss 1.4316 | lr 3.00e-04 | grad 1.64 | tok/s 19656
step   1310 | loss 1.4128 | lr 3.00e-04 | grad 2.75 | tok/s 19585
step   1320 | loss 1.3639 | lr 3.00e-04 | grad 1.34 | tok/s 19537
step   1330 | loss 1.4569 | lr 3.00e-04 | grad 1.22 | tok/s 18932
step   1340 | loss 1.3430 | lr 3.00e-04 | grad 1.29 | tok/s 19674
step   1350 | loss 1.4035 | lr 3.00e-04 | grad 1.39 | tok/s 18496
step   1360 | loss 1.4479 | lr 3.00e-04 | grad 2.86 | tok/s 19735
step   1370 | loss 1.4047 | lr 3.00e-04 | grad 1.55 | tok/s 19142
step   1380 | loss 1.2557 | lr 3.00e-04 | grad 1.95 | tok/s 19489
step   1390 | loss 1.4624 | lr 3.00e-04 | grad 1.12 | tok/s 19045
step   1400 | loss 1.2875 | lr 3.00e-04 | grad 1.29 | tok/s 18736
step   1410 | loss 1.0499 | lr 3.00e-04 | grad 1.44 | tok/s 19879
step   1420 | loss 1.6322 | lr 3.00e-04 | grad 1.08 | tok/s 19263

Training complete! Final step: 1420
