Using device: cuda
Output directory: benchmark_results/cmaes_mamba2_10min/mamba2_480M_15gen_20260127_031532/eval_58/levelmamba2_100m_20260127_042802
Model: Level mamba2, 471,419,328 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.3765 | lr 3.00e-04 | grad 6.94 | tok/s 4670
step     20 | loss 3.0211 | lr 3.00e-04 | grad 5.81 | tok/s 24194
step     30 | loss 3.8889 | lr 3.00e-04 | grad 22.00 | tok/s 24842
step     40 | loss 3.4874 | lr 3.00e-04 | grad 8.12 | tok/s 25060
step     50 | loss 2.5974 | lr 3.00e-04 | grad 2.78 | tok/s 25014
step     60 | loss 2.2487 | lr 3.00e-04 | grad 6.78 | tok/s 24909
step     70 | loss 1.9943 | lr 3.00e-04 | grad 2.28 | tok/s 24806
step     80 | loss 1.9230 | lr 3.00e-04 | grad 3.77 | tok/s 24749
step     90 | loss 1.7909 | lr 3.00e-04 | grad 1.52 | tok/s 24672
step    100 | loss 1.9747 | lr 3.00e-04 | grad 6.03 | tok/s 24497
step    110 | loss 2.7392 | lr 3.00e-04 | grad 3.78 | tok/s 23477
step    120 | loss 1.8610 | lr 3.00e-04 | grad 3.17 | tok/s 23851
step    130 | loss 2.3141 | lr 3.00e-04 | grad 8.38 | tok/s 23694
step    140 | loss 1.3754 | lr 3.00e-04 | grad 6.16 | tok/s 24513
step    150 | loss 2.3597 | lr 3.00e-04 | grad 3.09 | tok/s 23989
step    160 | loss 2.1306 | lr 3.00e-04 | grad 2.48 | tok/s 23300
step    170 | loss 1.6773 | lr 3.00e-04 | grad 2.36 | tok/s 23949
step    180 | loss 1.8028 | lr 3.00e-04 | grad 2.52 | tok/s 23007
step    190 | loss 1.5210 | lr 3.00e-04 | grad 3.30 | tok/s 24404
step    200 | loss 1.6544 | lr 3.00e-04 | grad 3.06 | tok/s 23171
step    210 | loss 2.0402 | lr 3.00e-04 | grad 4.66 | tok/s 23610
step    220 | loss 1.7462 | lr 3.00e-04 | grad 2.25 | tok/s 23281
step    230 | loss 2.0625 | lr 3.00e-04 | grad 2.34 | tok/s 23698
step    240 | loss 1.7035 | lr 3.00e-04 | grad 1.93 | tok/s 23615
step    250 | loss 1.7088 | lr 3.00e-04 | grad 2.42 | tok/s 24185
step    260 | loss 1.7245 | lr 3.00e-04 | grad 2.42 | tok/s 23707
step    270 | loss 1.6123 | lr 3.00e-04 | grad 1.89 | tok/s 22391
step    280 | loss 1.5381 | lr 3.00e-04 | grad 1.97 | tok/s 23183
step    290 | loss 1.8438 | lr 3.00e-04 | grad 1.77 | tok/s 23195
step    300 | loss 1.5592 | lr 3.00e-04 | grad 1.53 | tok/s 23040
step    310 | loss 1.7411 | lr 3.00e-04 | grad 4.19 | tok/s 23149
step    320 | loss 1.6225 | lr 3.00e-04 | grad 2.55 | tok/s 23749
step    330 | loss 1.8709 | lr 3.00e-04 | grad 4.97 | tok/s 23464
step    340 | loss 1.6445 | lr 3.00e-04 | grad 4.53 | tok/s 24425
step    350 | loss 1.4938 | lr 3.00e-04 | grad 2.09 | tok/s 22833
step    360 | loss 1.4209 | lr 3.00e-04 | grad 1.52 | tok/s 24361
step    370 | loss 1.2007 | lr 3.00e-04 | grad 1.59 | tok/s 24573
step    380 | loss 1.0665 | lr 3.00e-04 | grad 1.54 | tok/s 24585
step    390 | loss 1.6089 | lr 3.00e-04 | grad 2.31 | tok/s 23501
step    400 | loss 1.6257 | lr 3.00e-04 | grad 6.97 | tok/s 23463
step    410 | loss 1.5563 | lr 3.00e-04 | grad 2.05 | tok/s 24397
step    420 | loss 1.5235 | lr 3.00e-04 | grad 1.59 | tok/s 24238
step    430 | loss 1.5722 | lr 3.00e-04 | grad 2.62 | tok/s 23576
step    440 | loss 1.5589 | lr 3.00e-04 | grad 1.69 | tok/s 23532
step    450 | loss 1.5088 | lr 3.00e-04 | grad 2.00 | tok/s 23940
step    460 | loss 1.4091 | lr 3.00e-04 | grad 1.42 | tok/s 23687
step    470 | loss 1.5610 | lr 3.00e-04 | grad 1.94 | tok/s 24426
step    480 | loss 1.6218 | lr 3.00e-04 | grad 1.84 | tok/s 23246
step    490 | loss 1.7068 | lr 3.00e-04 | grad 2.88 | tok/s 23829
step    500 | loss 1.5943 | lr 3.00e-04 | grad 1.39 | tok/s 22732
step    510 | loss 1.4608 | lr 3.00e-04 | grad 3.81 | tok/s 23852
step    520 | loss 1.6074 | lr 3.00e-04 | grad 2.55 | tok/s 23304
step    530 | loss 1.5539 | lr 3.00e-04 | grad 1.44 | tok/s 23373
step    540 | loss 1.2307 | lr 3.00e-04 | grad 1.62 | tok/s 23537
step    550 | loss 1.4306 | lr 3.00e-04 | grad 1.55 | tok/s 24523
step    560 | loss 1.2848 | lr 3.00e-04 | grad 1.52 | tok/s 24559
step    570 | loss 1.2554 | lr 3.00e-04 | grad 1.81 | tok/s 24557
step    580 | loss 1.2875 | lr 3.00e-04 | grad 1.52 | tok/s 24571
step    590 | loss 1.2107 | lr 3.00e-04 | grad 1.37 | tok/s 24556
step    600 | loss 1.2611 | lr 3.00e-04 | grad 1.71 | tok/s 24562
step    610 | loss 1.2201 | lr 3.00e-04 | grad 1.00 | tok/s 24620
step    620 | loss 1.6002 | lr 3.00e-04 | grad 3.38 | tok/s 23255
step    630 | loss 1.5882 | lr 3.00e-04 | grad 2.58 | tok/s 23381
step    640 | loss 1.4755 | lr 3.00e-04 | grad 1.63 | tok/s 23850
step    650 | loss 1.5200 | lr 3.00e-04 | grad 1.85 | tok/s 23984
step    660 | loss 1.4897 | lr 3.00e-04 | grad 2.05 | tok/s 23654
step    670 | loss 1.5952 | lr 3.00e-04 | grad 1.39 | tok/s 23076
step    680 | loss 1.4861 | lr 3.00e-04 | grad 1.66 | tok/s 23138
step    690 | loss 1.4427 | lr 3.00e-04 | grad 1.92 | tok/s 23368
step    700 | loss 1.4973 | lr 3.00e-04 | grad 2.67 | tok/s 23161
step    710 | loss 1.2781 | lr 3.00e-04 | grad 1.63 | tok/s 23983
step    720 | loss 1.3491 | lr 3.00e-04 | grad 1.77 | tok/s 23893
step    730 | loss 1.6374 | lr 3.00e-04 | grad 5.50 | tok/s 24002
step    740 | loss 1.5401 | lr 3.00e-04 | grad 1.67 | tok/s 24541
step    750 | loss 1.4054 | lr 3.00e-04 | grad 1.40 | tok/s 24076
step    760 | loss 1.4886 | lr 3.00e-04 | grad 1.48 | tok/s 23693
step    770 | loss 1.4375 | lr 3.00e-04 | grad 2.22 | tok/s 23731
step    780 | loss 1.5135 | lr 3.00e-04 | grad 3.62 | tok/s 24293
step    790 | loss 1.4001 | lr 3.00e-04 | grad 1.16 | tok/s 23889
step    800 | loss 1.1705 | lr 3.00e-04 | grad 3.22 | tok/s 23257
step    810 | loss 1.3692 | lr 3.00e-04 | grad 2.19 | tok/s 23924
step    820 | loss 1.4518 | lr 3.00e-04 | grad 1.55 | tok/s 22936
step    830 | loss 1.5225 | lr 3.00e-04 | grad 1.87 | tok/s 23393
step    840 | loss 1.4395 | lr 3.00e-04 | grad 2.16 | tok/s 23601
step    850 | loss 1.4883 | lr 3.00e-04 | grad 2.50 | tok/s 23826
step    860 | loss 1.3377 | lr 3.00e-04 | grad 1.79 | tok/s 24393
step    870 | loss 1.5075 | lr 3.00e-04 | grad 2.12 | tok/s 23531
step    880 | loss 1.4443 | lr 3.00e-04 | grad 1.62 | tok/s 23717
step    890 | loss 1.4614 | lr 3.00e-04 | grad 2.17 | tok/s 23684
step    900 | loss 1.3907 | lr 3.00e-04 | grad 1.76 | tok/s 23152
step    910 | loss 1.4299 | lr 3.00e-04 | grad 1.89 | tok/s 23631
step    920 | loss 1.3202 | lr 3.00e-04 | grad 1.45 | tok/s 23791
step    930 | loss 1.3078 | lr 3.00e-04 | grad 2.08 | tok/s 23340
step    940 | loss 1.4053 | lr 3.00e-04 | grad 1.36 | tok/s 22762
step    950 | loss 1.4036 | lr 3.00e-04 | grad 1.62 | tok/s 23545
step    960 | loss 1.3967 | lr 3.00e-04 | grad 1.59 | tok/s 23565
step    970 | loss 1.7694 | lr 3.00e-04 | grad 3.25 | tok/s 24489
step    980 | loss 1.5289 | lr 3.00e-04 | grad 1.49 | tok/s 23502
step    990 | loss 1.4860 | lr 3.00e-04 | grad 1.45 | tok/s 23786
step   1000 | loss 1.1572 | lr 3.00e-04 | grad 1.62 | tok/s 24019
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1572.pt
step   1010 | loss 1.2145 | lr 3.00e-04 | grad 1.62 | tok/s 12576
step   1020 | loss 1.4115 | lr 3.00e-04 | grad 1.91 | tok/s 23609
step   1030 | loss 1.9599 | lr 3.00e-04 | grad 3.61 | tok/s 24145
step   1040 | loss 1.4769 | lr 3.00e-04 | grad 1.39 | tok/s 24300
step   1050 | loss 1.1599 | lr 3.00e-04 | grad 1.43 | tok/s 24123
step   1060 | loss 1.3050 | lr 3.00e-04 | grad 1.44 | tok/s 23980
step   1070 | loss 1.2153 | lr 3.00e-04 | grad 1.52 | tok/s 24712
step   1080 | loss 1.1987 | lr 3.00e-04 | grad 1.42 | tok/s 24714
step   1090 | loss 1.1735 | lr 3.00e-04 | grad 1.44 | tok/s 24702
step   1100 | loss 1.1212 | lr 3.00e-04 | grad 1.30 | tok/s 24681
step   1110 | loss 1.3333 | lr 3.00e-04 | grad 1.60 | tok/s 24052
step   1120 | loss 1.5616 | lr 3.00e-04 | grad 1.43 | tok/s 24269
step   1130 | loss 1.6903 | lr 3.00e-04 | grad 1.52 | tok/s 24556
step   1140 | loss 1.5706 | lr 3.00e-04 | grad 3.97 | tok/s 24004
step   1150 | loss 1.5930 | lr 3.00e-04 | grad 3.84 | tok/s 23267
step   1160 | loss 1.4863 | lr 3.00e-04 | grad 1.80 | tok/s 23186
step   1170 | loss 1.3061 | lr 3.00e-04 | grad 1.32 | tok/s 24360
step   1180 | loss 1.5228 | lr 3.00e-04 | grad 2.19 | tok/s 24388
step   1190 | loss 1.1165 | lr 3.00e-04 | grad 1.23 | tok/s 24630
step   1200 | loss 1.2723 | lr 3.00e-04 | grad 1.52 | tok/s 23282
step   1210 | loss 1.3141 | lr 3.00e-04 | grad 1.70 | tok/s 24009
step   1220 | loss 1.3434 | lr 3.00e-04 | grad 1.37 | tok/s 24144
step   1230 | loss 1.2024 | lr 3.00e-04 | grad 2.06 | tok/s 24412
step   1240 | loss 1.4192 | lr 3.00e-04 | grad 2.30 | tok/s 23802
step   1250 | loss 1.3045 | lr 3.00e-04 | grad 1.54 | tok/s 24454
step   1260 | loss 1.3197 | lr 3.00e-04 | grad 2.22 | tok/s 23728
step   1270 | loss 1.3181 | lr 3.00e-04 | grad 2.03 | tok/s 23793
step   1280 | loss 1.3050 | lr 3.00e-04 | grad 1.82 | tok/s 23259
step   1290 | loss 1.5175 | lr 3.00e-04 | grad 6.88 | tok/s 23174
step   1300 | loss 1.4425 | lr 3.00e-04 | grad 1.81 | tok/s 24138
step   1310 | loss 1.4299 | lr 3.00e-04 | grad 3.23 | tok/s 24057
step   1320 | loss 1.3824 | lr 3.00e-04 | grad 1.48 | tok/s 24026
step   1330 | loss 1.4632 | lr 3.00e-04 | grad 1.35 | tok/s 23292
step   1340 | loss 1.3635 | lr 3.00e-04 | grad 1.42 | tok/s 24171
step   1350 | loss 1.4032 | lr 3.00e-04 | grad 1.50 | tok/s 22769
step   1360 | loss 1.4673 | lr 3.00e-04 | grad 3.55 | tok/s 24247
step   1370 | loss 1.4141 | lr 3.00e-04 | grad 1.72 | tok/s 23531
step   1380 | loss 1.2724 | lr 3.00e-04 | grad 2.19 | tok/s 23975
step   1390 | loss 1.4940 | lr 3.00e-04 | grad 1.33 | tok/s 23461
step   1400 | loss 1.3048 | lr 3.00e-04 | grad 1.45 | tok/s 23080
step   1410 | loss 1.0711 | lr 3.00e-04 | grad 1.66 | tok/s 24442
step   1420 | loss 1.6397 | lr 3.00e-04 | grad 1.21 | tok/s 23671
step   1430 | loss 1.3959 | lr 3.00e-04 | grad 2.05 | tok/s 23863
step   1440 | loss 1.3515 | lr 3.00e-04 | grad 1.45 | tok/s 24138
step   1450 | loss 1.4541 | lr 3.00e-04 | grad 2.27 | tok/s 23420
step   1460 | loss 1.3307 | lr 3.00e-04 | grad 1.57 | tok/s 23156
step   1470 | loss 1.3200 | lr 3.00e-04 | grad 2.33 | tok/s 23960
step   1480 | loss 1.5837 | lr 3.00e-04 | grad 6.47 | tok/s 23756
step   1490 | loss 1.5220 | lr 3.00e-04 | grad 1.41 | tok/s 24098
step   1500 | loss 1.2128 | lr 3.00e-04 | grad 1.48 | tok/s 23675
step   1510 | loss 1.4126 | lr 3.00e-04 | grad 1.91 | tok/s 23548
step   1520 | loss 1.3323 | lr 3.00e-04 | grad 1.66 | tok/s 24153
step   1530 | loss 1.4397 | lr 3.00e-04 | grad 1.42 | tok/s 24182
step   1540 | loss 1.4153 | lr 3.00e-04 | grad 4.62 | tok/s 23734
step   1550 | loss 1.1012 | lr 3.00e-04 | grad 1.11 | tok/s 24510
step   1560 | loss 1.2643 | lr 3.00e-04 | grad 1.28 | tok/s 23975
step   1570 | loss 1.1765 | lr 3.00e-04 | grad 1.61 | tok/s 24073
step   1580 | loss 1.3831 | lr 3.00e-04 | grad 3.31 | tok/s 23230
step   1590 | loss 1.1765 | lr 3.00e-04 | grad 5.75 | tok/s 24542
step   1600 | loss 1.8014 | lr 3.00e-04 | grad 3.17 | tok/s 23878
step   1610 | loss 1.9509 | lr 3.00e-04 | grad 2.77 | tok/s 24656
step   1620 | loss 1.6294 | lr 3.00e-04 | grad 2.02 | tok/s 24635
step   1630 | loss 1.4602 | lr 3.00e-04 | grad 1.66 | tok/s 24618
step   1640 | loss 1.3512 | lr 3.00e-04 | grad 1.93 | tok/s 24642
step   1650 | loss 1.3062 | lr 3.00e-04 | grad 1.77 | tok/s 24602
step   1660 | loss 1.4448 | lr 3.00e-04 | grad 2.11 | tok/s 23882
step   1670 | loss 1.3613 | lr 3.00e-04 | grad 1.91 | tok/s 23639
step   1680 | loss 1.4201 | lr 3.00e-04 | grad 2.30 | tok/s 23146
step   1690 | loss 1.2813 | lr 3.00e-04 | grad 2.17 | tok/s 24006
step   1700 | loss 1.1893 | lr 3.00e-04 | grad 2.47 | tok/s 24081
step   1710 | loss 1.3942 | lr 3.00e-04 | grad 1.36 | tok/s 23330
step   1720 | loss 1.3652 | lr 3.00e-04 | grad 3.69 | tok/s 24078
step   1730 | loss 1.3488 | lr 3.00e-04 | grad 1.75 | tok/s 24219
step   1740 | loss 1.2084 | lr 3.00e-04 | grad 1.59 | tok/s 23743

Training complete! Final step: 1740
