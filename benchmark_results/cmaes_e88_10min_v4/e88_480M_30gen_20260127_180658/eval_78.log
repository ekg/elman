Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_78/levelE88_100m_20260127_194046
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 470,725,024 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.5849 | lr 3.00e-04 | grad 13.31 | tok/s 9193
step     20 | loss 3.2339 | lr 3.00e-04 | grad 3.98 | tok/s 17083
step     30 | loss 2.9489 | lr 3.00e-04 | grad 5.81 | tok/s 18026
step     40 | loss 4.0682 | lr 3.00e-04 | grad 12.50 | tok/s 18290
step     50 | loss 3.8341 | lr 3.00e-04 | grad 14.56 | tok/s 18476
step     60 | loss 3.0533 | lr 3.00e-04 | grad 5.91 | tok/s 18399
step     70 | loss 2.6195 | lr 3.00e-04 | grad 3.97 | tok/s 18418
step     80 | loss 2.4823 | lr 3.00e-04 | grad 3.72 | tok/s 18368
step     90 | loss 2.2958 | lr 3.00e-04 | grad 3.16 | tok/s 18352
step    100 | loss 2.0925 | lr 3.00e-04 | grad 2.42 | tok/s 18305
step    110 | loss 2.1789 | lr 3.00e-04 | grad 3.12 | tok/s 18167
step    120 | loss 2.6615 | lr 3.00e-04 | grad 2.41 | tok/s 17301
step    130 | loss 2.0940 | lr 3.00e-04 | grad 4.97 | tok/s 17673
step    140 | loss 2.3861 | lr 3.00e-04 | grad 7.47 | tok/s 17774
step    150 | loss 1.4431 | lr 3.00e-04 | grad 6.09 | tok/s 18166
step    160 | loss 2.3478 | lr 3.00e-04 | grad 2.42 | tok/s 17578
step    170 | loss 2.3056 | lr 3.00e-04 | grad 1.98 | tok/s 17301
step    180 | loss 1.8602 | lr 3.00e-04 | grad 3.17 | tok/s 17729
step    190 | loss 1.8993 | lr 3.00e-04 | grad 2.64 | tok/s 17356
step    200 | loss 1.6419 | lr 3.00e-04 | grad 2.05 | tok/s 18144
step    210 | loss 1.8792 | lr 3.00e-04 | grad 5.22 | tok/s 17198
step    220 | loss 2.1991 | lr 3.00e-04 | grad 3.78 | tok/s 17412
step    230 | loss 1.9604 | lr 3.00e-04 | grad 2.77 | tok/s 17402
step    240 | loss 2.2377 | lr 3.00e-04 | grad 5.88 | tok/s 17627
step    250 | loss 1.7507 | lr 3.00e-04 | grad 1.76 | tok/s 17511
step    260 | loss 1.8780 | lr 3.00e-04 | grad 3.31 | tok/s 18007
step    270 | loss 1.8084 | lr 3.00e-04 | grad 2.17 | tok/s 17576
step    280 | loss 1.7609 | lr 3.00e-04 | grad 1.84 | tok/s 16515
step    290 | loss 1.6564 | lr 3.00e-04 | grad 2.23 | tok/s 17089
step    300 | loss 1.9584 | lr 3.00e-04 | grad 2.25 | tok/s 17216
step    310 | loss 1.6501 | lr 3.00e-04 | grad 1.78 | tok/s 17138
step    320 | loss 1.8760 | lr 3.00e-04 | grad 4.34 | tok/s 17363
step    330 | loss 1.7032 | lr 3.00e-04 | grad 1.96 | tok/s 17536
step    340 | loss 2.0192 | lr 3.00e-04 | grad 2.30 | tok/s 17446
step    350 | loss 1.7123 | lr 3.00e-04 | grad 2.09 | tok/s 17957
step    360 | loss 1.5715 | lr 3.00e-04 | grad 1.91 | tok/s 17192
step    370 | loss 1.4710 | lr 3.00e-04 | grad 1.80 | tok/s 18095
step    380 | loss 1.2078 | lr 3.00e-04 | grad 1.85 | tok/s 18245
step    390 | loss 1.1198 | lr 3.00e-04 | grad 1.55 | tok/s 18266
step    400 | loss 1.7286 | lr 3.00e-04 | grad 1.75 | tok/s 17304
step    410 | loss 1.7402 | lr 3.00e-04 | grad 2.31 | tok/s 17502
step    420 | loss 1.6058 | lr 3.00e-04 | grad 4.31 | tok/s 18236
step    430 | loss 1.5953 | lr 3.00e-04 | grad 1.91 | tok/s 17904
step    440 | loss 1.6930 | lr 3.00e-04 | grad 2.47 | tok/s 17385
step    450 | loss 1.6197 | lr 3.00e-04 | grad 1.59 | tok/s 17534
step    460 | loss 1.5876 | lr 3.00e-04 | grad 2.17 | tok/s 17788
step    470 | loss 1.5531 | lr 3.00e-04 | grad 3.44 | tok/s 17673
step    480 | loss 1.5711 | lr 3.00e-04 | grad 2.62 | tok/s 18064
step    490 | loss 1.6910 | lr 3.00e-04 | grad 2.39 | tok/s 17343
step    500 | loss 1.7916 | lr 3.00e-04 | grad 1.72 | tok/s 17633
step    510 | loss 1.6701 | lr 3.00e-04 | grad 1.59 | tok/s 16857
step    520 | loss 1.5306 | lr 3.00e-04 | grad 2.09 | tok/s 17659
step    530 | loss 1.6994 | lr 3.00e-04 | grad 1.98 | tok/s 17346
step    540 | loss 1.5830 | lr 3.00e-04 | grad 1.59 | tok/s 17004
step    550 | loss 1.3653 | lr 3.00e-04 | grad 3.17 | tok/s 17740
step    560 | loss 1.4376 | lr 3.00e-04 | grad 1.98 | tok/s 18258
step    570 | loss 1.3441 | lr 3.00e-04 | grad 1.98 | tok/s 18285
step    580 | loss 1.3014 | lr 3.00e-04 | grad 1.48 | tok/s 18279
step    590 | loss 1.3298 | lr 3.00e-04 | grad 1.42 | tok/s 18297
step    600 | loss 1.2694 | lr 3.00e-04 | grad 1.75 | tok/s 18288
step    610 | loss 1.3038 | lr 3.00e-04 | grad 1.70 | tok/s 18286
step    620 | loss 1.2909 | lr 3.00e-04 | grad 1.81 | tok/s 18243
step    630 | loss 1.6557 | lr 3.00e-04 | grad 5.16 | tok/s 17208
step    640 | loss 1.7289 | lr 3.00e-04 | grad 1.99 | tok/s 17425
step    650 | loss 1.5442 | lr 3.00e-04 | grad 1.71 | tok/s 17425
step    660 | loss 1.5857 | lr 3.00e-04 | grad 1.91 | tok/s 18048
step    670 | loss 1.6239 | lr 3.00e-04 | grad 4.81 | tok/s 17501
step    680 | loss 1.6333 | lr 3.00e-04 | grad 2.23 | tok/s 17234
step    690 | loss 1.5889 | lr 3.00e-04 | grad 1.84 | tok/s 17073
step    700 | loss 1.4762 | lr 3.00e-04 | grad 1.38 | tok/s 17479
step    710 | loss 1.6369 | lr 3.00e-04 | grad 3.17 | tok/s 17199
step    720 | loss 1.3083 | lr 3.00e-04 | grad 1.77 | tok/s 17898
step    730 | loss 1.4714 | lr 3.00e-04 | grad 1.45 | tok/s 17552
step    740 | loss 1.7530 | lr 3.00e-04 | grad 3.67 | tok/s 18054
step    750 | loss 1.5169 | lr 3.00e-04 | grad 1.62 | tok/s 18276
step    760 | loss 1.5331 | lr 3.00e-04 | grad 4.06 | tok/s 17854
step    770 | loss 1.5742 | lr 3.00e-04 | grad 1.94 | tok/s 17565
step    780 | loss 1.4870 | lr 3.00e-04 | grad 1.91 | tok/s 17695
step    790 | loss 1.6112 | lr 3.00e-04 | grad 4.38 | tok/s 18058
step    800 | loss 1.3173 | lr 3.00e-04 | grad 1.20 | tok/s 17782
step    810 | loss 1.3230 | lr 3.00e-04 | grad 2.72 | tok/s 17205
step    820 | loss 1.4307 | lr 3.00e-04 | grad 1.88 | tok/s 17524
step    830 | loss 1.4876 | lr 3.00e-04 | grad 1.41 | tok/s 17256
step    840 | loss 1.6231 | lr 3.00e-04 | grad 1.76 | tok/s 17196
step    850 | loss 1.5357 | lr 3.00e-04 | grad 1.52 | tok/s 17586
step    860 | loss 1.5906 | lr 3.00e-04 | grad 2.39 | tok/s 17847
step    870 | loss 1.3943 | lr 3.00e-04 | grad 1.77 | tok/s 17940
step    880 | loss 1.5937 | lr 3.00e-04 | grad 1.78 | tok/s 17626
step    890 | loss 1.4849 | lr 3.00e-04 | grad 1.40 | tok/s 17533
step    900 | loss 1.5331 | lr 3.00e-04 | grad 1.53 | tok/s 17471
step    910 | loss 1.5211 | lr 3.00e-04 | grad 6.66 | tok/s 17298
step    920 | loss 1.4788 | lr 3.00e-04 | grad 1.84 | tok/s 17103
step    930 | loss 1.3901 | lr 3.00e-04 | grad 2.08 | tok/s 17714
step    940 | loss 1.3570 | lr 3.00e-04 | grad 1.84 | tok/s 17315
step    950 | loss 1.4865 | lr 3.00e-04 | grad 2.36 | tok/s 17023
step    960 | loss 1.4410 | lr 3.00e-04 | grad 1.65 | tok/s 17501
step    970 | loss 1.4743 | lr 3.00e-04 | grad 1.80 | tok/s 17470
step    980 | loss 1.8859 | lr 3.00e-04 | grad 3.31 | tok/s 18187
step    990 | loss 1.5732 | lr 3.00e-04 | grad 1.79 | tok/s 17439
step   1000 | loss 1.5711 | lr 3.00e-04 | grad 2.02 | tok/s 17458
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5711.pt
step   1010 | loss 1.3648 | lr 3.00e-04 | grad 1.84 | tok/s 8948
step   1020 | loss 1.2016 | lr 3.00e-04 | grad 1.39 | tok/s 18493
step   1030 | loss 1.5969 | lr 3.00e-04 | grad 1.80 | tok/s 17457
step   1040 | loss 2.1739 | lr 3.00e-04 | grad 3.88 | tok/s 18008
step   1050 | loss 1.4838 | lr 3.00e-04 | grad 1.89 | tok/s 17747
step   1060 | loss 1.1116 | lr 3.00e-04 | grad 1.33 | tok/s 18059
step   1070 | loss 1.4808 | lr 3.00e-04 | grad 1.79 | tok/s 17719
step   1080 | loss 1.2643 | lr 3.00e-04 | grad 1.70 | tok/s 18328
step   1090 | loss 1.2412 | lr 3.00e-04 | grad 1.55 | tok/s 18352
step   1100 | loss 1.1955 | lr 3.00e-04 | grad 1.75 | tok/s 18337
step   1110 | loss 1.1825 | lr 3.00e-04 | grad 1.62 | tok/s 18298
step   1120 | loss 1.4899 | lr 3.00e-04 | grad 3.28 | tok/s 17822
step   1130 | loss 1.6491 | lr 3.00e-04 | grad 3.28 | tok/s 18003
step   1140 | loss 1.6928 | lr 3.00e-04 | grad 1.41 | tok/s 18213
step   1150 | loss 1.6542 | lr 3.00e-04 | grad 3.48 | tok/s 17296
step   1160 | loss 1.7605 | lr 3.00e-04 | grad 6.59 | tok/s 17544
step   1170 | loss 1.4381 | lr 3.00e-04 | grad 1.91 | tok/s 17266
step   1180 | loss 1.3522 | lr 3.00e-04 | grad 2.05 | tok/s 17904
step   1190 | loss 1.5290 | lr 3.00e-04 | grad 3.59 | tok/s 18254
step   1200 | loss 1.1304 | lr 3.00e-04 | grad 3.95 | tok/s 18301
step   1210 | loss 1.4629 | lr 3.00e-04 | grad 2.31 | tok/s 17032
step   1220 | loss 1.3549 | lr 3.00e-04 | grad 1.66 | tok/s 17701
step   1230 | loss 1.3173 | lr 3.00e-04 | grad 1.27 | tok/s 18161
step   1240 | loss 1.3133 | lr 3.00e-04 | grad 1.50 | tok/s 17939
step   1250 | loss 1.4966 | lr 3.00e-04 | grad 5.00 | tok/s 17936
step   1260 | loss 1.4051 | lr 3.00e-04 | grad 2.20 | tok/s 18075
step   1270 | loss 1.3829 | lr 3.00e-04 | grad 1.62 | tok/s 17737
step   1280 | loss 1.4330 | lr 3.00e-04 | grad 1.84 | tok/s 17432
step   1290 | loss 1.3283 | lr 3.00e-04 | grad 1.69 | tok/s 17480
step   1300 | loss 1.6255 | lr 3.00e-04 | grad 3.86 | tok/s 17193
step   1310 | loss 1.4889 | lr 3.00e-04 | grad 1.74 | tok/s 17777

Training complete! Final step: 1311
