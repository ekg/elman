Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_69/levelE88_100m_20260127_193024
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 476,371,792 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9220 | lr 3.00e-04 | grad 15.38 | tok/s 9456
step     20 | loss 3.3009 | lr 3.00e-04 | grad 4.31 | tok/s 18395
step     30 | loss 3.0246 | lr 3.00e-04 | grad 4.88 | tok/s 19409
step     40 | loss 4.2028 | lr 3.00e-04 | grad 12.44 | tok/s 19711
step     50 | loss 3.9781 | lr 3.00e-04 | grad 14.50 | tok/s 19898
step     60 | loss 3.1017 | lr 3.00e-04 | grad 7.75 | tok/s 19842
step     70 | loss 2.7248 | lr 3.00e-04 | grad 3.69 | tok/s 19705
step     80 | loss 2.5268 | lr 3.00e-04 | grad 4.06 | tok/s 19682
step     90 | loss 2.3695 | lr 3.00e-04 | grad 4.00 | tok/s 19667
step    100 | loss 2.1386 | lr 3.00e-04 | grad 2.84 | tok/s 19633
step    110 | loss 2.2082 | lr 3.00e-04 | grad 3.72 | tok/s 19460
step    120 | loss 2.7607 | lr 3.00e-04 | grad 2.38 | tok/s 18540
step    130 | loss 2.1535 | lr 3.00e-04 | grad 5.47 | tok/s 18938
step    140 | loss 2.4505 | lr 3.00e-04 | grad 8.19 | tok/s 18983
step    150 | loss 1.5875 | lr 3.00e-04 | grad 6.91 | tok/s 19477
step    160 | loss 2.4294 | lr 3.00e-04 | grad 2.70 | tok/s 18809
step    170 | loss 2.3592 | lr 3.00e-04 | grad 2.30 | tok/s 18493
step    180 | loss 1.9643 | lr 3.00e-04 | grad 3.27 | tok/s 18931
step    190 | loss 1.9772 | lr 3.00e-04 | grad 2.48 | tok/s 18559
step    200 | loss 1.7192 | lr 3.00e-04 | grad 2.31 | tok/s 19402
step    210 | loss 1.9388 | lr 3.00e-04 | grad 6.03 | tok/s 18423
step    220 | loss 2.2558 | lr 3.00e-04 | grad 4.16 | tok/s 18629
step    230 | loss 2.0385 | lr 3.00e-04 | grad 2.86 | tok/s 18587
step    240 | loss 2.2865 | lr 3.00e-04 | grad 5.69 | tok/s 18821
step    250 | loss 1.7994 | lr 3.00e-04 | grad 2.08 | tok/s 18744
step    260 | loss 1.9317 | lr 3.00e-04 | grad 3.56 | tok/s 19247
step    270 | loss 1.8519 | lr 3.00e-04 | grad 2.56 | tok/s 18828
step    280 | loss 1.7905 | lr 3.00e-04 | grad 2.17 | tok/s 17681
step    290 | loss 1.6907 | lr 3.00e-04 | grad 2.50 | tok/s 18294
step    300 | loss 1.9922 | lr 3.00e-04 | grad 2.64 | tok/s 18419
step    310 | loss 1.6810 | lr 3.00e-04 | grad 1.89 | tok/s 18333
step    320 | loss 1.9028 | lr 3.00e-04 | grad 4.12 | tok/s 18532
step    330 | loss 1.7374 | lr 3.00e-04 | grad 2.03 | tok/s 18744
step    340 | loss 2.0336 | lr 3.00e-04 | grad 2.36 | tok/s 18660
step    350 | loss 1.7168 | lr 3.00e-04 | grad 2.39 | tok/s 19198
step    360 | loss 1.5954 | lr 3.00e-04 | grad 1.99 | tok/s 18360
step    370 | loss 1.5019 | lr 3.00e-04 | grad 2.00 | tok/s 19325
step    380 | loss 1.2344 | lr 3.00e-04 | grad 1.88 | tok/s 19495
step    390 | loss 1.1504 | lr 3.00e-04 | grad 1.88 | tok/s 19494
step    400 | loss 1.7517 | lr 3.00e-04 | grad 1.86 | tok/s 18471
step    410 | loss 1.7781 | lr 3.00e-04 | grad 2.55 | tok/s 18626
step    420 | loss 1.6185 | lr 3.00e-04 | grad 4.00 | tok/s 19424
step    430 | loss 1.6165 | lr 3.00e-04 | grad 2.23 | tok/s 19122
step    440 | loss 1.7138 | lr 3.00e-04 | grad 2.88 | tok/s 18556
step    450 | loss 1.6381 | lr 3.00e-04 | grad 1.94 | tok/s 18760
step    460 | loss 1.6090 | lr 3.00e-04 | grad 2.22 | tok/s 19040
step    470 | loss 1.5760 | lr 3.00e-04 | grad 4.06 | tok/s 18894
step    480 | loss 1.5871 | lr 3.00e-04 | grad 2.89 | tok/s 19318
step    490 | loss 1.7030 | lr 3.00e-04 | grad 2.69 | tok/s 18547
step    500 | loss 1.8094 | lr 3.00e-04 | grad 1.84 | tok/s 18895
step    510 | loss 1.6823 | lr 3.00e-04 | grad 1.74 | tok/s 18072
step    520 | loss 1.5380 | lr 3.00e-04 | grad 2.30 | tok/s 18893
step    530 | loss 1.7123 | lr 3.00e-04 | grad 2.02 | tok/s 18580
step    540 | loss 1.5935 | lr 3.00e-04 | grad 1.68 | tok/s 18198
step    550 | loss 1.3709 | lr 3.00e-04 | grad 3.75 | tok/s 18993
step    560 | loss 1.4505 | lr 3.00e-04 | grad 2.19 | tok/s 19571
step    570 | loss 1.3570 | lr 3.00e-04 | grad 2.08 | tok/s 19571
step    580 | loss 1.3155 | lr 3.00e-04 | grad 1.56 | tok/s 19552
step    590 | loss 1.3442 | lr 3.00e-04 | grad 1.64 | tok/s 19523
step    600 | loss 1.2795 | lr 3.00e-04 | grad 1.98 | tok/s 19552
step    610 | loss 1.3124 | lr 3.00e-04 | grad 1.93 | tok/s 19508
step    620 | loss 1.3038 | lr 3.00e-04 | grad 1.95 | tok/s 19447
step    630 | loss 1.6998 | lr 3.00e-04 | grad 5.50 | tok/s 18417
step    640 | loss 1.7377 | lr 3.00e-04 | grad 1.95 | tok/s 18630
step    650 | loss 1.5585 | lr 3.00e-04 | grad 1.87 | tok/s 18633
step    660 | loss 1.6027 | lr 3.00e-04 | grad 2.03 | tok/s 19346
step    670 | loss 1.6321 | lr 3.00e-04 | grad 5.12 | tok/s 18649
step    680 | loss 1.6478 | lr 3.00e-04 | grad 2.61 | tok/s 18386
step    690 | loss 1.6034 | lr 3.00e-04 | grad 1.99 | tok/s 18261
step    700 | loss 1.4814 | lr 3.00e-04 | grad 1.49 | tok/s 18643
step    710 | loss 1.6470 | lr 3.00e-04 | grad 3.16 | tok/s 18335
step    720 | loss 1.3234 | lr 3.00e-04 | grad 2.06 | tok/s 19060
step    730 | loss 1.4837 | lr 3.00e-04 | grad 1.58 | tok/s 18750
step    740 | loss 1.7665 | lr 3.00e-04 | grad 3.98 | tok/s 19274
step    750 | loss 1.5282 | lr 3.00e-04 | grad 2.03 | tok/s 19484
step    760 | loss 1.5497 | lr 3.00e-04 | grad 4.72 | tok/s 19089
step    770 | loss 1.5906 | lr 3.00e-04 | grad 2.19 | tok/s 18760
step    780 | loss 1.4949 | lr 3.00e-04 | grad 2.16 | tok/s 18893
step    790 | loss 1.6257 | lr 3.00e-04 | grad 4.88 | tok/s 19294
step    800 | loss 1.3270 | lr 3.00e-04 | grad 1.27 | tok/s 18965
step    810 | loss 1.3261 | lr 3.00e-04 | grad 2.80 | tok/s 18328
step    820 | loss 1.4404 | lr 3.00e-04 | grad 2.11 | tok/s 18666
step    830 | loss 1.5000 | lr 3.00e-04 | grad 1.75 | tok/s 18426
step    840 | loss 1.6434 | lr 3.00e-04 | grad 1.93 | tok/s 18384
step    850 | loss 1.5504 | lr 3.00e-04 | grad 1.66 | tok/s 18755
step    860 | loss 1.5972 | lr 3.00e-04 | grad 2.78 | tok/s 19030
step    870 | loss 1.4027 | lr 3.00e-04 | grad 1.95 | tok/s 19176
step    880 | loss 1.6006 | lr 3.00e-04 | grad 2.02 | tok/s 18838
step    890 | loss 1.4975 | lr 3.00e-04 | grad 1.62 | tok/s 18772
step    900 | loss 1.5419 | lr 3.00e-04 | grad 1.69 | tok/s 18661
step    910 | loss 1.5346 | lr 3.00e-04 | grad 7.47 | tok/s 18500
step    920 | loss 1.4986 | lr 3.00e-04 | grad 2.16 | tok/s 18695
step    930 | loss 1.4009 | lr 3.00e-04 | grad 2.45 | tok/s 18955
step    940 | loss 1.3673 | lr 3.00e-04 | grad 1.95 | tok/s 18492
step    950 | loss 1.4957 | lr 3.00e-04 | grad 2.67 | tok/s 18168
step    960 | loss 1.4504 | lr 3.00e-04 | grad 2.08 | tok/s 18669
step    970 | loss 1.4813 | lr 3.00e-04 | grad 2.00 | tok/s 18711
step    980 | loss 1.9076 | lr 3.00e-04 | grad 3.70 | tok/s 19429
step    990 | loss 1.5792 | lr 3.00e-04 | grad 2.11 | tok/s 18633
step   1000 | loss 1.5823 | lr 3.00e-04 | grad 2.45 | tok/s 18665
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5823.pt
step   1010 | loss 1.2727 | lr 3.00e-04 | grad 0.96 | tok/s 7908
step   1020 | loss 1.3235 | lr 3.00e-04 | grad 1.72 | tok/s 19613
step   1030 | loss 1.6408 | lr 3.00e-04 | grad 1.62 | tok/s 18591
step   1040 | loss 2.1999 | lr 3.00e-04 | grad 2.56 | tok/s 19526
step   1050 | loss 1.5240 | lr 3.00e-04 | grad 8.62 | tok/s 18890
step   1060 | loss 1.0685 | lr 3.00e-04 | grad 4.06 | tok/s 19370
step   1070 | loss 1.4444 | lr 3.00e-04 | grad 2.02 | tok/s 19223
step   1080 | loss 1.2593 | lr 3.00e-04 | grad 2.02 | tok/s 19703
step   1090 | loss 1.2346 | lr 3.00e-04 | grad 1.39 | tok/s 19683
step   1100 | loss 1.2022 | lr 3.00e-04 | grad 2.42 | tok/s 19679
step   1110 | loss 1.2750 | lr 3.00e-04 | grad 2.42 | tok/s 19492
step   1120 | loss 1.5092 | lr 3.00e-04 | grad 3.41 | tok/s 19263
step   1130 | loss 1.8441 | lr 3.00e-04 | grad 3.38 | tok/s 19314
step   1140 | loss 1.4431 | lr 3.00e-04 | grad 2.58 | tok/s 19170
step   1150 | loss 1.6999 | lr 3.00e-04 | grad 2.03 | tok/s 18813
step   1160 | loss 1.7607 | lr 3.00e-04 | grad 1.97 | tok/s 18579
step   1170 | loss 1.4492 | lr 3.00e-04 | grad 2.27 | tok/s 18700
step   1180 | loss 1.4436 | lr 3.00e-04 | grad 3.03 | tok/s 19256
step   1190 | loss 1.4511 | lr 3.00e-04 | grad 1.56 | tok/s 19559
step   1200 | loss 1.1459 | lr 3.00e-04 | grad 1.94 | tok/s 19236
step   1210 | loss 1.4790 | lr 3.00e-04 | grad 2.03 | tok/s 18479
step   1220 | loss 1.3969 | lr 3.00e-04 | grad 2.52 | tok/s 18926
step   1230 | loss 1.3028 | lr 3.00e-04 | grad 2.03 | tok/s 19360
step   1240 | loss 1.3263 | lr 3.00e-04 | grad 3.05 | tok/s 19023
step   1250 | loss 1.4524 | lr 3.00e-04 | grad 4.78 | tok/s 19306
step   1260 | loss 1.4446 | lr 3.00e-04 | grad 1.62 | tok/s 19106
step   1270 | loss 1.3508 | lr 3.00e-04 | grad 1.55 | tok/s 19027
step   1280 | loss 1.4668 | lr 3.00e-04 | grad 1.84 | tok/s 18600
step   1290 | loss 1.3780 | lr 3.00e-04 | grad 1.99 | tok/s 18288
step   1300 | loss 1.6094 | lr 3.00e-04 | grad 2.45 | tok/s 18711
step   1310 | loss 1.5105 | lr 3.00e-04 | grad 2.05 | tok/s 19012
step   1320 | loss 1.5683 | lr 3.00e-04 | grad 2.73 | tok/s 19157
step   1330 | loss 1.3882 | lr 3.00e-04 | grad 2.55 | tok/s 18481
step   1340 | loss 1.5529 | lr 3.00e-04 | grad 1.44 | tok/s 18919
step   1350 | loss 1.5413 | lr 3.00e-04 | grad 2.75 | tok/s 18680
step   1360 | loss 1.3021 | lr 3.00e-04 | grad 1.28 | tok/s 18441
step   1370 | loss 1.7784 | lr 3.00e-04 | grad 2.31 | tok/s 19116
step   1380 | loss 1.4304 | lr 3.00e-04 | grad 2.31 | tok/s 18287
step   1390 | loss 1.4413 | lr 3.00e-04 | grad 3.12 | tok/s 18828

Training complete! Final step: 1397
