Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_171/levelE88_100m_20260127_214441
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 474,442,020 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9420 | lr 3.00e-04 | grad 18.50 | tok/s 9615
step     20 | loss 3.5748 | lr 3.00e-04 | grad 7.06 | tok/s 19515
step     30 | loss 3.0883 | lr 3.00e-04 | grad 7.78 | tok/s 20534
step     40 | loss 4.3784 | lr 3.00e-04 | grad 16.38 | tok/s 20897
step     50 | loss 4.2643 | lr 3.00e-04 | grad 16.88 | tok/s 21095
step     60 | loss 3.4071 | lr 3.00e-04 | grad 9.00 | tok/s 20991
step     70 | loss 2.9097 | lr 3.00e-04 | grad 5.75 | tok/s 20931
step     80 | loss 2.7055 | lr 3.00e-04 | grad 5.81 | tok/s 20901
step     90 | loss 2.4971 | lr 3.00e-04 | grad 5.62 | tok/s 20851
step    100 | loss 2.2429 | lr 3.00e-04 | grad 3.42 | tok/s 20809
step    110 | loss 2.2396 | lr 3.00e-04 | grad 6.84 | tok/s 20626
step    120 | loss 2.7021 | lr 3.00e-04 | grad 3.17 | tok/s 19631
step    130 | loss 2.0637 | lr 3.00e-04 | grad 6.91 | tok/s 20079
step    140 | loss 2.3824 | lr 3.00e-04 | grad 10.50 | tok/s 20138
step    150 | loss 1.4278 | lr 3.00e-04 | grad 8.25 | tok/s 20619
step    160 | loss 2.3130 | lr 3.00e-04 | grad 3.59 | tok/s 19881
step    170 | loss 2.3084 | lr 3.00e-04 | grad 3.19 | tok/s 19583
step    180 | loss 1.8208 | lr 3.00e-04 | grad 4.16 | tok/s 20042
step    190 | loss 1.8997 | lr 3.00e-04 | grad 3.61 | tok/s 19671
step    200 | loss 1.6127 | lr 3.00e-04 | grad 2.86 | tok/s 20592
step    210 | loss 1.8714 | lr 3.00e-04 | grad 7.78 | tok/s 19524
step    220 | loss 2.1955 | lr 3.00e-04 | grad 6.12 | tok/s 19728
step    230 | loss 2.0315 | lr 3.00e-04 | grad 3.70 | tok/s 19708
step    240 | loss 2.2729 | lr 3.00e-04 | grad 8.06 | tok/s 19970
step    250 | loss 1.7475 | lr 3.00e-04 | grad 2.47 | tok/s 19834
step    260 | loss 1.8902 | lr 3.00e-04 | grad 4.88 | tok/s 20383
step    270 | loss 1.8140 | lr 3.00e-04 | grad 3.08 | tok/s 19924
step    280 | loss 1.7641 | lr 3.00e-04 | grad 2.56 | tok/s 18710
step    290 | loss 1.6622 | lr 3.00e-04 | grad 3.14 | tok/s 19282
step    300 | loss 1.9897 | lr 3.00e-04 | grad 3.44 | tok/s 19474
step    310 | loss 1.6584 | lr 3.00e-04 | grad 2.44 | tok/s 19364
step    320 | loss 1.8924 | lr 3.00e-04 | grad 5.81 | tok/s 19632
step    330 | loss 1.7212 | lr 3.00e-04 | grad 2.62 | tok/s 19819
step    340 | loss 2.0447 | lr 3.00e-04 | grad 3.48 | tok/s 19671
step    350 | loss 1.7128 | lr 3.00e-04 | grad 2.83 | tok/s 20252
step    360 | loss 1.5834 | lr 3.00e-04 | grad 2.39 | tok/s 19415
step    370 | loss 1.4819 | lr 3.00e-04 | grad 2.56 | tok/s 20442
step    380 | loss 1.2045 | lr 3.00e-04 | grad 2.30 | tok/s 20582
step    390 | loss 1.1248 | lr 3.00e-04 | grad 2.19 | tok/s 20606
step    400 | loss 1.7587 | lr 3.00e-04 | grad 2.39 | tok/s 19531
step    410 | loss 1.7800 | lr 3.00e-04 | grad 3.22 | tok/s 19702
step    420 | loss 1.5886 | lr 3.00e-04 | grad 4.34 | tok/s 20530
step    430 | loss 1.6113 | lr 3.00e-04 | grad 2.62 | tok/s 20202
step    440 | loss 1.7133 | lr 3.00e-04 | grad 3.44 | tok/s 19602
step    450 | loss 1.6469 | lr 3.00e-04 | grad 2.62 | tok/s 19780
step    460 | loss 1.6040 | lr 3.00e-04 | grad 2.67 | tok/s 20087
step    470 | loss 1.5723 | lr 3.00e-04 | grad 4.53 | tok/s 19939
step    480 | loss 1.5922 | lr 3.00e-04 | grad 3.91 | tok/s 20346
step    490 | loss 1.7119 | lr 3.00e-04 | grad 3.19 | tok/s 19593
step    500 | loss 1.8181 | lr 3.00e-04 | grad 2.31 | tok/s 19866
step    510 | loss 1.6858 | lr 3.00e-04 | grad 2.19 | tok/s 18969
step    520 | loss 1.5477 | lr 3.00e-04 | grad 2.89 | tok/s 19879
step    530 | loss 1.7161 | lr 3.00e-04 | grad 2.39 | tok/s 19547
step    540 | loss 1.5957 | lr 3.00e-04 | grad 2.14 | tok/s 19135
step    550 | loss 1.3824 | lr 3.00e-04 | grad 4.47 | tok/s 20017
step    560 | loss 1.4539 | lr 3.00e-04 | grad 2.70 | tok/s 20516
step    570 | loss 1.3548 | lr 3.00e-04 | grad 2.66 | tok/s 20550
step    580 | loss 1.3103 | lr 3.00e-04 | grad 2.00 | tok/s 20548
step    590 | loss 1.3399 | lr 3.00e-04 | grad 2.12 | tok/s 20550
step    600 | loss 1.2807 | lr 3.00e-04 | grad 2.41 | tok/s 20537
step    610 | loss 1.3128 | lr 3.00e-04 | grad 2.28 | tok/s 20525
step    620 | loss 1.3027 | lr 3.00e-04 | grad 2.59 | tok/s 20471
step    630 | loss 1.7326 | lr 3.00e-04 | grad 7.41 | tok/s 19361
step    640 | loss 1.7526 | lr 3.00e-04 | grad 3.62 | tok/s 19625
step    650 | loss 1.5668 | lr 3.00e-04 | grad 2.42 | tok/s 19589
step    660 | loss 1.6058 | lr 3.00e-04 | grad 2.53 | tok/s 20323
step    670 | loss 1.6559 | lr 3.00e-04 | grad 6.66 | tok/s 19648
step    680 | loss 1.6634 | lr 3.00e-04 | grad 3.03 | tok/s 19347
step    690 | loss 1.6119 | lr 3.00e-04 | grad 2.39 | tok/s 19180
step    700 | loss 1.4912 | lr 3.00e-04 | grad 1.80 | tok/s 19613
step    710 | loss 1.6732 | lr 3.00e-04 | grad 3.69 | tok/s 19297
step    720 | loss 1.3270 | lr 3.00e-04 | grad 2.38 | tok/s 20046
step    730 | loss 1.5006 | lr 3.00e-04 | grad 2.03 | tok/s 19728
step    740 | loss 1.7843 | lr 3.00e-04 | grad 4.78 | tok/s 20272
step    750 | loss 1.5198 | lr 3.00e-04 | grad 2.31 | tok/s 20508
step    760 | loss 1.5647 | lr 3.00e-04 | grad 5.78 | tok/s 20047
step    770 | loss 1.6133 | lr 3.00e-04 | grad 2.70 | tok/s 19751
step    780 | loss 1.5039 | lr 3.00e-04 | grad 2.47 | tok/s 19861
step    790 | loss 1.6444 | lr 3.00e-04 | grad 6.09 | tok/s 20271
step    800 | loss 1.3371 | lr 3.00e-04 | grad 1.67 | tok/s 19955
step    810 | loss 1.3333 | lr 3.00e-04 | grad 3.20 | tok/s 19283
step    820 | loss 1.4427 | lr 3.00e-04 | grad 2.61 | tok/s 19695
step    830 | loss 1.5095 | lr 3.00e-04 | grad 2.09 | tok/s 19443
step    840 | loss 1.6580 | lr 3.00e-04 | grad 2.50 | tok/s 19346
step    850 | loss 1.5671 | lr 3.00e-04 | grad 2.02 | tok/s 19741
step    860 | loss 1.6203 | lr 3.00e-04 | grad 4.22 | tok/s 20025
step    870 | loss 1.4204 | lr 3.00e-04 | grad 2.23 | tok/s 20209
step    880 | loss 1.6188 | lr 3.00e-04 | grad 2.73 | tok/s 19826
step    890 | loss 1.5110 | lr 3.00e-04 | grad 2.02 | tok/s 19735
step    900 | loss 1.5604 | lr 3.00e-04 | grad 1.94 | tok/s 19662
step    910 | loss 1.5566 | lr 3.00e-04 | grad 8.31 | tok/s 18367
step    920 | loss 1.5100 | lr 3.00e-04 | grad 3.11 | tok/s 19673
step    930 | loss 1.4031 | lr 3.00e-04 | grad 3.09 | tok/s 19923
step    940 | loss 1.3807 | lr 3.00e-04 | grad 2.28 | tok/s 19483
step    950 | loss 1.5160 | lr 3.00e-04 | grad 3.25 | tok/s 19183
step    960 | loss 1.4640 | lr 3.00e-04 | grad 2.45 | tok/s 19681
step    970 | loss 1.4969 | lr 3.00e-04 | grad 2.48 | tok/s 19651
step    980 | loss 1.9260 | lr 3.00e-04 | grad 4.00 | tok/s 20448
step    990 | loss 1.6035 | lr 3.00e-04 | grad 2.27 | tok/s 19654
step   1000 | loss 1.6131 | lr 3.00e-04 | grad 2.75 | tok/s 19698
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6131.pt
step   1010 | loss 1.3819 | lr 3.00e-04 | grad 3.42 | tok/s 9217
step   1020 | loss 1.2222 | lr 3.00e-04 | grad 2.36 | tok/s 20873
step   1030 | loss 1.6604 | lr 3.00e-04 | grad 2.97 | tok/s 19649
step   1040 | loss 2.2147 | lr 3.00e-04 | grad 4.78 | tok/s 20502
step   1050 | loss 1.5208 | lr 3.00e-04 | grad 2.72 | tok/s 19962
step   1060 | loss 1.0765 | lr 3.00e-04 | grad 2.27 | tok/s 20491
step   1070 | loss 1.5263 | lr 3.00e-04 | grad 2.91 | tok/s 20162
step   1080 | loss 1.2711 | lr 3.00e-04 | grad 1.88 | tok/s 20721
step   1090 | loss 1.2577 | lr 3.00e-04 | grad 2.48 | tok/s 20729
step   1100 | loss 1.2057 | lr 3.00e-04 | grad 2.16 | tok/s 20697
step   1110 | loss 1.2546 | lr 3.00e-04 | grad 3.56 | tok/s 20610
step   1120 | loss 1.5204 | lr 3.00e-04 | grad 4.06 | tok/s 20231
step   1130 | loss 1.8083 | lr 3.00e-04 | grad 9.88 | tok/s 20320
step   1140 | loss 1.5456 | lr 3.00e-04 | grad 2.06 | tok/s 20602
step   1150 | loss 1.7530 | lr 3.00e-04 | grad 3.39 | tok/s 19530
step   1160 | loss 1.7755 | lr 3.00e-04 | grad 2.56 | tok/s 19627
step   1170 | loss 1.4624 | lr 3.00e-04 | grad 1.82 | tok/s 19846
step   1180 | loss 1.4272 | lr 3.00e-04 | grad 3.84 | tok/s 20236
step   1190 | loss 1.5089 | lr 3.00e-04 | grad 3.61 | tok/s 20637
step   1200 | loss 1.1404 | lr 3.00e-04 | grad 2.77 | tok/s 20579
step   1210 | loss 1.5099 | lr 3.00e-04 | grad 2.06 | tok/s 19347
step   1220 | loss 1.3745 | lr 3.00e-04 | grad 2.72 | tok/s 19956
step   1230 | loss 1.3384 | lr 3.00e-04 | grad 2.41 | tok/s 20482
step   1240 | loss 1.3352 | lr 3.00e-04 | grad 2.28 | tok/s 20047
step   1250 | loss 1.5062 | lr 3.00e-04 | grad 3.59 | tok/s 20457
step   1260 | loss 1.4545 | lr 3.00e-04 | grad 2.64 | tok/s 20400
step   1270 | loss 1.3793 | lr 3.00e-04 | grad 2.05 | tok/s 20024
step   1280 | loss 1.4808 | lr 3.00e-04 | grad 2.02 | tok/s 19695
step   1290 | loss 1.3800 | lr 3.00e-04 | grad 2.56 | tok/s 19620
step   1300 | loss 1.6079 | lr 3.00e-04 | grad 5.22 | tok/s 19581
step   1310 | loss 1.5532 | lr 3.00e-04 | grad 2.75 | tok/s 20159
step   1320 | loss 1.5641 | lr 3.00e-04 | grad 2.52 | tok/s 20291
step   1330 | loss 1.4224 | lr 3.00e-04 | grad 2.95 | tok/s 19789
step   1340 | loss 1.6077 | lr 3.00e-04 | grad 2.08 | tok/s 19866
step   1350 | loss 1.5403 | lr 3.00e-04 | grad 8.94 | tok/s 19908
step   1360 | loss 1.3541 | lr 3.00e-04 | grad 2.31 | tok/s 19410
step   1370 | loss 1.7364 | lr 3.00e-04 | grad 2.95 | tok/s 20242
step   1380 | loss 1.4644 | lr 3.00e-04 | grad 4.09 | tok/s 19337
step   1390 | loss 1.4295 | lr 3.00e-04 | grad 5.75 | tok/s 20125
step   1400 | loss 1.4293 | lr 3.00e-04 | grad 2.61 | tok/s 19739
step   1410 | loss 1.4305 | lr 3.00e-04 | grad 5.59 | tok/s 19471
step   1420 | loss 1.3544 | lr 3.00e-04 | grad 6.75 | tok/s 20433
step   1430 | loss 1.5661 | lr 3.00e-04 | grad 2.25 | tok/s 19708
step   1440 | loss 1.4635 | lr 3.00e-04 | grad 2.47 | tok/s 20285
step   1450 | loss 1.5326 | lr 3.00e-04 | grad 2.58 | tok/s 19232
step   1460 | loss 1.5899 | lr 3.00e-04 | grad 2.30 | tok/s 19515
step   1470 | loss 1.3858 | lr 3.00e-04 | grad 4.19 | tok/s 19260

Training complete! Final step: 1476
