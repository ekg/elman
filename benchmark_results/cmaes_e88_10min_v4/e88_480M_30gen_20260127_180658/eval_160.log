Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_160/levelE88_100m_20260127_212403
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,271,484 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.7792 | lr 3.00e-04 | grad 16.12 | tok/s 9514
step     20 | loss 3.4471 | lr 3.00e-04 | grad 7.53 | tok/s 19235
step     30 | loss 3.1878 | lr 3.00e-04 | grad 8.56 | tok/s 20229
step     40 | loss 4.5290 | lr 3.00e-04 | grad 16.38 | tok/s 20624
step     50 | loss 4.2153 | lr 3.00e-04 | grad 13.38 | tok/s 20890
step     60 | loss 3.3533 | lr 3.00e-04 | grad 8.44 | tok/s 20787
step     70 | loss 2.9545 | lr 3.00e-04 | grad 5.97 | tok/s 20738
step     80 | loss 2.6248 | lr 3.00e-04 | grad 5.97 | tok/s 20681
step     90 | loss 2.5314 | lr 3.00e-04 | grad 5.28 | tok/s 20680
step    100 | loss 2.2084 | lr 3.00e-04 | grad 3.84 | tok/s 20681
step    110 | loss 2.2178 | lr 3.00e-04 | grad 4.47 | tok/s 20412
step    120 | loss 2.6988 | lr 3.00e-04 | grad 3.42 | tok/s 19451
step    130 | loss 2.0484 | lr 3.00e-04 | grad 6.97 | tok/s 19876
step    140 | loss 2.3595 | lr 3.00e-04 | grad 9.12 | tok/s 19940
step    150 | loss 1.3713 | lr 3.00e-04 | grad 7.84 | tok/s 20481
step    160 | loss 2.3210 | lr 3.00e-04 | grad 3.28 | tok/s 19715
step    170 | loss 2.2837 | lr 3.00e-04 | grad 2.61 | tok/s 19467
step    180 | loss 1.8230 | lr 3.00e-04 | grad 4.16 | tok/s 19935
step    190 | loss 1.8743 | lr 3.00e-04 | grad 3.11 | tok/s 19522
step    200 | loss 1.6060 | lr 3.00e-04 | grad 2.41 | tok/s 20452
step    210 | loss 1.8654 | lr 3.00e-04 | grad 7.22 | tok/s 19416
step    220 | loss 2.1797 | lr 3.00e-04 | grad 4.56 | tok/s 19576
step    230 | loss 1.9840 | lr 3.00e-04 | grad 3.45 | tok/s 19546
step    240 | loss 2.2577 | lr 3.00e-04 | grad 7.59 | tok/s 19815
step    250 | loss 1.7439 | lr 3.00e-04 | grad 2.53 | tok/s 19687
step    260 | loss 1.8719 | lr 3.00e-04 | grad 4.25 | tok/s 20243
step    270 | loss 1.8067 | lr 3.00e-04 | grad 3.25 | tok/s 19772
step    280 | loss 1.7581 | lr 3.00e-04 | grad 2.30 | tok/s 18549
step    290 | loss 1.6540 | lr 3.00e-04 | grad 3.02 | tok/s 19223
step    300 | loss 1.9651 | lr 3.00e-04 | grad 3.27 | tok/s 19311
step    310 | loss 1.6538 | lr 3.00e-04 | grad 2.27 | tok/s 19281
step    320 | loss 1.8758 | lr 3.00e-04 | grad 5.22 | tok/s 19471
step    330 | loss 1.7156 | lr 3.00e-04 | grad 2.70 | tok/s 19703
step    340 | loss 2.0355 | lr 3.00e-04 | grad 3.05 | tok/s 19636
step    350 | loss 1.6863 | lr 3.00e-04 | grad 2.70 | tok/s 20129
step    360 | loss 1.5810 | lr 3.00e-04 | grad 2.39 | tok/s 19273
step    370 | loss 1.4767 | lr 3.00e-04 | grad 2.48 | tok/s 20310
step    380 | loss 1.1994 | lr 3.00e-04 | grad 2.12 | tok/s 20516
step    390 | loss 1.1130 | lr 3.00e-04 | grad 1.86 | tok/s 20513
step    400 | loss 1.7542 | lr 3.00e-04 | grad 2.27 | tok/s 19413
step    410 | loss 1.7762 | lr 3.00e-04 | grad 3.12 | tok/s 19580
step    420 | loss 1.5749 | lr 3.00e-04 | grad 5.03 | tok/s 20413
step    430 | loss 1.6006 | lr 3.00e-04 | grad 2.56 | tok/s 20061
step    440 | loss 1.7114 | lr 3.00e-04 | grad 3.22 | tok/s 19461
step    450 | loss 1.6351 | lr 3.00e-04 | grad 2.44 | tok/s 19677
step    460 | loss 1.5948 | lr 3.00e-04 | grad 2.56 | tok/s 19976
step    470 | loss 1.5608 | lr 3.00e-04 | grad 4.91 | tok/s 19827
step    480 | loss 1.5603 | lr 3.00e-04 | grad 3.73 | tok/s 20249
step    490 | loss 1.7068 | lr 3.00e-04 | grad 3.16 | tok/s 19466
step    500 | loss 1.8097 | lr 3.00e-04 | grad 2.23 | tok/s 19010
step    510 | loss 1.6785 | lr 3.00e-04 | grad 2.22 | tok/s 18884
step    520 | loss 1.5281 | lr 3.00e-04 | grad 3.02 | tok/s 19782
step    530 | loss 1.7144 | lr 3.00e-04 | grad 2.42 | tok/s 19482
step    540 | loss 1.5883 | lr 3.00e-04 | grad 2.16 | tok/s 19023
step    550 | loss 1.3716 | lr 3.00e-04 | grad 4.16 | tok/s 19896
step    560 | loss 1.4447 | lr 3.00e-04 | grad 2.53 | tok/s 20501
step    570 | loss 1.3515 | lr 3.00e-04 | grad 2.58 | tok/s 20458
step    580 | loss 1.3051 | lr 3.00e-04 | grad 1.87 | tok/s 20491
step    590 | loss 1.3377 | lr 3.00e-04 | grad 2.08 | tok/s 20525
step    600 | loss 1.2742 | lr 3.00e-04 | grad 2.27 | tok/s 20491
step    610 | loss 1.3109 | lr 3.00e-04 | grad 2.44 | tok/s 20500
step    620 | loss 1.2980 | lr 3.00e-04 | grad 2.31 | tok/s 20379
step    630 | loss 1.7393 | lr 3.00e-04 | grad 7.31 | tok/s 19259
step    640 | loss 1.7506 | lr 3.00e-04 | grad 2.59 | tok/s 19537
step    650 | loss 1.5635 | lr 3.00e-04 | grad 2.31 | tok/s 19488
step    660 | loss 1.6050 | lr 3.00e-04 | grad 2.38 | tok/s 20255
step    670 | loss 1.6501 | lr 3.00e-04 | grad 6.50 | tok/s 19591
step    680 | loss 1.6604 | lr 3.00e-04 | grad 3.02 | tok/s 19269
step    690 | loss 1.6124 | lr 3.00e-04 | grad 2.44 | tok/s 19154
step    700 | loss 1.4870 | lr 3.00e-04 | grad 1.78 | tok/s 19549
step    710 | loss 1.6675 | lr 3.00e-04 | grad 3.50 | tok/s 19241
step    720 | loss 1.3174 | lr 3.00e-04 | grad 2.47 | tok/s 19976
step    730 | loss 1.5013 | lr 3.00e-04 | grad 1.99 | tok/s 19651
step    740 | loss 1.7860 | lr 3.00e-04 | grad 4.62 | tok/s 20198
step    750 | loss 1.5277 | lr 3.00e-04 | grad 2.31 | tok/s 20419
step    760 | loss 1.5601 | lr 3.00e-04 | grad 5.47 | tok/s 20015
step    770 | loss 1.6053 | lr 3.00e-04 | grad 2.62 | tok/s 19624
step    780 | loss 1.4982 | lr 3.00e-04 | grad 2.39 | tok/s 19777
step    790 | loss 1.6361 | lr 3.00e-04 | grad 5.97 | tok/s 20241
step    800 | loss 1.3332 | lr 3.00e-04 | grad 1.66 | tok/s 19900
step    810 | loss 1.3378 | lr 3.00e-04 | grad 3.44 | tok/s 19245
step    820 | loss 1.4346 | lr 3.00e-04 | grad 2.42 | tok/s 19569
step    830 | loss 1.5127 | lr 3.00e-04 | grad 1.72 | tok/s 19288
step    840 | loss 1.6611 | lr 3.00e-04 | grad 2.48 | tok/s 19274
step    850 | loss 1.5669 | lr 3.00e-04 | grad 1.98 | tok/s 19694
step    860 | loss 1.6206 | lr 3.00e-04 | grad 3.16 | tok/s 19997
step    870 | loss 1.4005 | lr 3.00e-04 | grad 2.28 | tok/s 20122
step    880 | loss 1.6131 | lr 3.00e-04 | grad 2.58 | tok/s 19678
step    890 | loss 1.5100 | lr 3.00e-04 | grad 1.97 | tok/s 19635
step    900 | loss 1.5616 | lr 3.00e-04 | grad 1.90 | tok/s 19553
step    910 | loss 1.5539 | lr 3.00e-04 | grad 8.12 | tok/s 19356
step    920 | loss 1.5136 | lr 3.00e-04 | grad 2.73 | tok/s 19528
step    930 | loss 1.3976 | lr 3.00e-04 | grad 2.56 | tok/s 19846
step    940 | loss 1.3768 | lr 3.00e-04 | grad 2.25 | tok/s 19392
step    950 | loss 1.5125 | lr 3.00e-04 | grad 3.03 | tok/s 19057
step    960 | loss 1.4634 | lr 3.00e-04 | grad 2.34 | tok/s 19576
step    970 | loss 1.4927 | lr 3.00e-04 | grad 2.28 | tok/s 19578
step    980 | loss 1.9214 | lr 3.00e-04 | grad 4.03 | tok/s 20369
step    990 | loss 1.6077 | lr 3.00e-04 | grad 2.22 | tok/s 19542
step   1000 | loss 1.6091 | lr 3.00e-04 | grad 2.69 | tok/s 19573
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6091.pt
step   1010 | loss 1.3767 | lr 3.00e-04 | grad 2.36 | tok/s 9669
step   1020 | loss 1.2099 | lr 3.00e-04 | grad 2.02 | tok/s 20731
step   1030 | loss 1.6288 | lr 3.00e-04 | grad 2.62 | tok/s 19524
step   1040 | loss 2.2167 | lr 3.00e-04 | grad 6.22 | tok/s 20227
step   1050 | loss 1.5003 | lr 3.00e-04 | grad 2.47 | tok/s 19983
step   1060 | loss 1.1319 | lr 3.00e-04 | grad 1.88 | tok/s 20335
step   1070 | loss 1.5104 | lr 3.00e-04 | grad 2.36 | tok/s 19952
step   1080 | loss 1.2831 | lr 3.00e-04 | grad 2.31 | tok/s 20667
step   1090 | loss 1.2566 | lr 3.00e-04 | grad 2.14 | tok/s 20594
step   1100 | loss 1.2080 | lr 3.00e-04 | grad 2.52 | tok/s 20586
step   1110 | loss 1.1967 | lr 3.00e-04 | grad 2.03 | tok/s 20570
step   1120 | loss 1.5117 | lr 3.00e-04 | grad 4.31 | tok/s 19993
step   1130 | loss 1.6729 | lr 3.00e-04 | grad 4.38 | tok/s 20197
step   1140 | loss 1.7250 | lr 3.00e-04 | grad 1.74 | tok/s 20433
step   1150 | loss 1.6717 | lr 3.00e-04 | grad 2.72 | tok/s 19451
step   1160 | loss 1.7997 | lr 3.00e-04 | grad 9.44 | tok/s 19683
step   1170 | loss 1.4707 | lr 3.00e-04 | grad 2.47 | tok/s 19415
step   1180 | loss 1.3685 | lr 3.00e-04 | grad 2.89 | tok/s 20138
step   1190 | loss 1.5621 | lr 3.00e-04 | grad 4.59 | tok/s 20500
step   1200 | loss 1.1478 | lr 3.00e-04 | grad 6.62 | tok/s 20525
step   1210 | loss 1.4941 | lr 3.00e-04 | grad 2.89 | tok/s 19095
step   1220 | loss 1.3752 | lr 3.00e-04 | grad 2.16 | tok/s 19858
step   1230 | loss 1.3362 | lr 3.00e-04 | grad 1.82 | tok/s 20324
step   1240 | loss 1.3289 | lr 3.00e-04 | grad 1.87 | tok/s 20008
step   1250 | loss 1.5193 | lr 3.00e-04 | grad 6.25 | tok/s 20065
step   1260 | loss 1.4335 | lr 3.00e-04 | grad 3.44 | tok/s 20241
step   1270 | loss 1.4084 | lr 3.00e-04 | grad 2.16 | tok/s 19873
step   1280 | loss 1.4674 | lr 3.00e-04 | grad 2.48 | tok/s 19555
step   1290 | loss 1.3429 | lr 3.00e-04 | grad 2.02 | tok/s 19615
step   1300 | loss 1.6812 | lr 3.00e-04 | grad 4.84 | tok/s 19278
step   1310 | loss 1.5204 | lr 3.00e-04 | grad 2.23 | tok/s 20011
step   1320 | loss 1.5511 | lr 3.00e-04 | grad 3.86 | tok/s 20102
step   1330 | loss 1.4223 | lr 3.00e-04 | grad 3.17 | tok/s 19846
step   1340 | loss 1.6392 | lr 3.00e-04 | grad 2.48 | tok/s 19515
step   1350 | loss 1.4926 | lr 3.00e-04 | grad 5.16 | tok/s 19770
step   1360 | loss 1.4202 | lr 3.00e-04 | grad 2.03 | tok/s 19261
step   1370 | loss 1.6913 | lr 3.00e-04 | grad 3.56 | tok/s 20159
step   1380 | loss 1.4385 | lr 3.00e-04 | grad 3.06 | tok/s 19217
step   1390 | loss 1.3436 | lr 3.00e-04 | grad 3.31 | tok/s 20212
step   1400 | loss 1.5242 | lr 3.00e-04 | grad 1.83 | tok/s 19539
step   1410 | loss 1.4723 | lr 3.00e-04 | grad 7.09 | tok/s 19054
step   1420 | loss 1.2726 | lr 3.00e-04 | grad 8.81 | tok/s 20313
step   1430 | loss 1.6119 | lr 3.00e-04 | grad 2.80 | tok/s 19515
step   1440 | loss 1.4575 | lr 3.00e-04 | grad 2.41 | tok/s 20094
step   1450 | loss 1.5086 | lr 3.00e-04 | grad 2.38 | tok/s 19987
step   1460 | loss 1.6120 | lr 3.00e-04 | grad 4.50 | tok/s 19502

Training complete! Final step: 1468
