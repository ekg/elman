Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_60/levelE88_100m_20260127_192004
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,893,404 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.4443 | lr 3.00e-04 | grad 20.12 | tok/s 9585
step     20 | loss 3.5861 | lr 3.00e-04 | grad 8.50 | tok/s 19241
step     30 | loss 3.2484 | lr 3.00e-04 | grad 8.56 | tok/s 20225
step     40 | loss 4.5750 | lr 3.00e-04 | grad 24.62 | tok/s 20529
step     50 | loss 4.2306 | lr 3.00e-04 | grad 13.69 | tok/s 20732
step     60 | loss 3.2541 | lr 3.00e-04 | grad 7.50 | tok/s 20675
step     70 | loss 2.9096 | lr 3.00e-04 | grad 5.66 | tok/s 20594
step     80 | loss 2.6150 | lr 3.00e-04 | grad 5.62 | tok/s 20555
step     90 | loss 2.5889 | lr 3.00e-04 | grad 6.03 | tok/s 20528
step    100 | loss 2.2629 | lr 3.00e-04 | grad 3.86 | tok/s 20484
step    110 | loss 2.2388 | lr 3.00e-04 | grad 5.59 | tok/s 20318
step    120 | loss 2.7462 | lr 3.00e-04 | grad 3.05 | tok/s 19341
step    130 | loss 2.0683 | lr 3.00e-04 | grad 6.66 | tok/s 19782
step    140 | loss 2.3592 | lr 3.00e-04 | grad 8.38 | tok/s 19830
step    150 | loss 1.3447 | lr 3.00e-04 | grad 7.44 | tok/s 20303
step    160 | loss 2.3215 | lr 3.00e-04 | grad 3.06 | tok/s 19604
step    170 | loss 2.3006 | lr 3.00e-04 | grad 2.53 | tok/s 19275
step    180 | loss 1.8037 | lr 3.00e-04 | grad 4.03 | tok/s 19769
step    190 | loss 1.8890 | lr 3.00e-04 | grad 3.70 | tok/s 19394
step    200 | loss 1.6208 | lr 3.00e-04 | grad 2.52 | tok/s 20275
step    210 | loss 1.8752 | lr 3.00e-04 | grad 7.09 | tok/s 19271
step    220 | loss 2.1797 | lr 3.00e-04 | grad 5.38 | tok/s 19432
step    230 | loss 1.9906 | lr 3.00e-04 | grad 3.41 | tok/s 19412
step    240 | loss 2.2584 | lr 3.00e-04 | grad 7.97 | tok/s 19665
step    250 | loss 1.7465 | lr 3.00e-04 | grad 2.27 | tok/s 19547
step    260 | loss 1.8815 | lr 3.00e-04 | grad 4.44 | tok/s 20081
step    270 | loss 1.8160 | lr 3.00e-04 | grad 2.77 | tok/s 19623
step    280 | loss 1.7642 | lr 3.00e-04 | grad 2.31 | tok/s 18456
step    290 | loss 1.6603 | lr 3.00e-04 | grad 2.94 | tok/s 19073
step    300 | loss 1.9760 | lr 3.00e-04 | grad 3.12 | tok/s 19217
step    310 | loss 1.6616 | lr 3.00e-04 | grad 2.31 | tok/s 19129
step    320 | loss 1.8726 | lr 3.00e-04 | grad 4.78 | tok/s 19346
step    330 | loss 1.7151 | lr 3.00e-04 | grad 2.48 | tok/s 19554
step    340 | loss 2.0409 | lr 3.00e-04 | grad 2.83 | tok/s 19465
step    350 | loss 1.6941 | lr 3.00e-04 | grad 2.58 | tok/s 20012
step    360 | loss 1.5796 | lr 3.00e-04 | grad 2.30 | tok/s 19154
step    370 | loss 1.4788 | lr 3.00e-04 | grad 2.48 | tok/s 20163
step    380 | loss 1.2022 | lr 3.00e-04 | grad 2.11 | tok/s 20340
step    390 | loss 1.1188 | lr 3.00e-04 | grad 1.88 | tok/s 20317
step    400 | loss 1.7534 | lr 3.00e-04 | grad 2.19 | tok/s 19275
step    410 | loss 1.7758 | lr 3.00e-04 | grad 2.97 | tok/s 19475
step    420 | loss 1.5904 | lr 3.00e-04 | grad 4.28 | tok/s 20274
step    430 | loss 1.5997 | lr 3.00e-04 | grad 2.39 | tok/s 19943
step    440 | loss 1.7122 | lr 3.00e-04 | grad 3.17 | tok/s 19340
step    450 | loss 1.6368 | lr 3.00e-04 | grad 2.28 | tok/s 19558
step    460 | loss 1.6041 | lr 3.00e-04 | grad 2.41 | tok/s 19861
step    470 | loss 1.5634 | lr 3.00e-04 | grad 4.31 | tok/s 19694
step    480 | loss 1.5759 | lr 3.00e-04 | grad 3.55 | tok/s 20102
step    490 | loss 1.7136 | lr 3.00e-04 | grad 3.00 | tok/s 19322
step    500 | loss 1.8197 | lr 3.00e-04 | grad 2.14 | tok/s 19643
step    510 | loss 1.6871 | lr 3.00e-04 | grad 2.02 | tok/s 18752
step    520 | loss 1.5402 | lr 3.00e-04 | grad 2.80 | tok/s 19638
step    530 | loss 1.7211 | lr 3.00e-04 | grad 2.28 | tok/s 19266
step    540 | loss 1.5915 | lr 3.00e-04 | grad 2.05 | tok/s 18881
step    550 | loss 1.3714 | lr 3.00e-04 | grad 4.12 | tok/s 19758
step    560 | loss 1.4468 | lr 3.00e-04 | grad 2.53 | tok/s 20298
step    570 | loss 1.3545 | lr 3.00e-04 | grad 2.42 | tok/s 20286
step    580 | loss 1.3060 | lr 3.00e-04 | grad 1.88 | tok/s 20300
step    590 | loss 1.3381 | lr 3.00e-04 | grad 1.76 | tok/s 20283
step    600 | loss 1.2787 | lr 3.00e-04 | grad 2.06 | tok/s 20292
step    610 | loss 1.3125 | lr 3.00e-04 | grad 2.11 | tok/s 20253
step    620 | loss 1.2998 | lr 3.00e-04 | grad 2.23 | tok/s 20192
step    630 | loss 1.7433 | lr 3.00e-04 | grad 6.41 | tok/s 19114
step    640 | loss 1.7548 | lr 3.00e-04 | grad 2.83 | tok/s 19334
step    650 | loss 1.5609 | lr 3.00e-04 | grad 2.17 | tok/s 19338
step    660 | loss 1.6077 | lr 3.00e-04 | grad 2.34 | tok/s 20061
step    670 | loss 1.6559 | lr 3.00e-04 | grad 6.19 | tok/s 19399
step    680 | loss 1.6627 | lr 3.00e-04 | grad 2.84 | tok/s 19076
step    690 | loss 1.6118 | lr 3.00e-04 | grad 2.30 | tok/s 18930
step    700 | loss 1.4885 | lr 3.00e-04 | grad 1.74 | tok/s 19352
step    710 | loss 1.6740 | lr 3.00e-04 | grad 3.25 | tok/s 19033
step    720 | loss 1.3217 | lr 3.00e-04 | grad 2.25 | tok/s 19786
step    730 | loss 1.5010 | lr 3.00e-04 | grad 1.80 | tok/s 19494
step    740 | loss 1.7885 | lr 3.00e-04 | grad 4.34 | tok/s 19991
step    750 | loss 1.5302 | lr 3.00e-04 | grad 2.14 | tok/s 20214
step    760 | loss 1.5596 | lr 3.00e-04 | grad 5.22 | tok/s 19803
step    770 | loss 1.6138 | lr 3.00e-04 | grad 2.50 | tok/s 19460
step    780 | loss 1.5025 | lr 3.00e-04 | grad 2.45 | tok/s 19595
step    790 | loss 1.6348 | lr 3.00e-04 | grad 5.50 | tok/s 20052
step    800 | loss 1.3362 | lr 3.00e-04 | grad 1.53 | tok/s 19688
step    810 | loss 1.3406 | lr 3.00e-04 | grad 3.42 | tok/s 19060
step    820 | loss 1.4376 | lr 3.00e-04 | grad 2.38 | tok/s 19396
step    830 | loss 1.5163 | lr 3.00e-04 | grad 1.79 | tok/s 19132
step    840 | loss 1.6651 | lr 3.00e-04 | grad 2.39 | tok/s 19046
step    850 | loss 1.5687 | lr 3.00e-04 | grad 1.91 | tok/s 19468
step    860 | loss 1.6093 | lr 3.00e-04 | grad 2.88 | tok/s 19795
step    870 | loss 1.3906 | lr 3.00e-04 | grad 2.19 | tok/s 19946
step    880 | loss 1.6158 | lr 3.00e-04 | grad 2.39 | tok/s 19534
step    890 | loss 1.5124 | lr 3.00e-04 | grad 1.87 | tok/s 18828
step    900 | loss 1.5610 | lr 3.00e-04 | grad 1.81 | tok/s 19390
step    910 | loss 1.5562 | lr 3.00e-04 | grad 8.25 | tok/s 19176
step    920 | loss 1.5148 | lr 3.00e-04 | grad 2.52 | tok/s 19394
step    930 | loss 1.3984 | lr 3.00e-04 | grad 2.77 | tok/s 19626
step    940 | loss 1.3781 | lr 3.00e-04 | grad 2.23 | tok/s 19208
step    950 | loss 1.5121 | lr 3.00e-04 | grad 2.81 | tok/s 18878
step    960 | loss 1.4632 | lr 3.00e-04 | grad 2.27 | tok/s 19418
step    970 | loss 1.4948 | lr 3.00e-04 | grad 2.28 | tok/s 19419
step    980 | loss 1.9137 | lr 3.00e-04 | grad 3.92 | tok/s 20180
step    990 | loss 1.6036 | lr 3.00e-04 | grad 2.20 | tok/s 19367
step   1000 | loss 1.6136 | lr 3.00e-04 | grad 2.64 | tok/s 19434
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6136.pt
step   1010 | loss 1.2794 | lr 3.00e-04 | grad 1.02 | tok/s 7830
step   1020 | loss 1.3141 | lr 3.00e-04 | grad 1.91 | tok/s 20367
step   1030 | loss 1.6589 | lr 3.00e-04 | grad 1.85 | tok/s 19308
step   1040 | loss 2.2515 | lr 3.00e-04 | grad 2.89 | tok/s 20324
step   1050 | loss 1.5373 | lr 3.00e-04 | grad 11.12 | tok/s 19681
step   1060 | loss 1.0795 | lr 3.00e-04 | grad 4.34 | tok/s 20180
step   1070 | loss 1.4636 | lr 3.00e-04 | grad 2.36 | tok/s 19949
step   1080 | loss 1.2695 | lr 3.00e-04 | grad 2.08 | tok/s 20427
step   1090 | loss 1.2427 | lr 3.00e-04 | grad 1.75 | tok/s 20410
step   1100 | loss 1.2065 | lr 3.00e-04 | grad 2.55 | tok/s 20422
step   1110 | loss 1.2889 | lr 3.00e-04 | grad 2.70 | tok/s 20223
step   1120 | loss 1.5371 | lr 3.00e-04 | grad 3.75 | tok/s 20009
step   1130 | loss 1.8701 | lr 3.00e-04 | grad 3.72 | tok/s 20052
step   1140 | loss 1.4535 | lr 3.00e-04 | grad 2.62 | tok/s 19890
step   1150 | loss 1.7295 | lr 3.00e-04 | grad 2.27 | tok/s 19585
step   1160 | loss 1.7764 | lr 3.00e-04 | grad 2.20 | tok/s 19322
step   1170 | loss 1.4667 | lr 3.00e-04 | grad 2.67 | tok/s 19448
step   1180 | loss 1.4625 | lr 3.00e-04 | grad 3.56 | tok/s 20069
step   1190 | loss 1.4662 | lr 3.00e-04 | grad 1.84 | tok/s 20359
step   1200 | loss 1.1548 | lr 3.00e-04 | grad 2.08 | tok/s 20032
step   1210 | loss 1.4972 | lr 3.00e-04 | grad 2.20 | tok/s 19207
step   1220 | loss 1.4137 | lr 3.00e-04 | grad 2.89 | tok/s 19668
step   1230 | loss 1.3112 | lr 3.00e-04 | grad 2.31 | tok/s 20088
step   1240 | loss 1.3457 | lr 3.00e-04 | grad 3.39 | tok/s 19793
step   1250 | loss 1.4771 | lr 3.00e-04 | grad 5.09 | tok/s 20108
step   1260 | loss 1.4640 | lr 3.00e-04 | grad 1.93 | tok/s 19938
step   1270 | loss 1.3682 | lr 3.00e-04 | grad 1.59 | tok/s 19846
step   1280 | loss 1.4944 | lr 3.00e-04 | grad 2.33 | tok/s 19371
step   1290 | loss 1.3863 | lr 3.00e-04 | grad 2.16 | tok/s 19029
step   1300 | loss 1.6827 | lr 3.00e-04 | grad 2.53 | tok/s 19480
step   1310 | loss 1.5237 | lr 3.00e-04 | grad 2.44 | tok/s 19750
step   1320 | loss 1.5947 | lr 3.00e-04 | grad 3.20 | tok/s 19922
step   1330 | loss 1.4173 | lr 3.00e-04 | grad 3.00 | tok/s 19219
step   1340 | loss 1.5712 | lr 3.00e-04 | grad 1.65 | tok/s 19685
step   1350 | loss 1.5699 | lr 3.00e-04 | grad 3.12 | tok/s 19457
step   1360 | loss 1.3216 | lr 3.00e-04 | grad 1.59 | tok/s 19183
step   1370 | loss 1.7790 | lr 3.00e-04 | grad 2.70 | tok/s 19888
step   1380 | loss 1.4447 | lr 3.00e-04 | grad 2.59 | tok/s 18982
step   1390 | loss 1.4786 | lr 3.00e-04 | grad 3.69 | tok/s 19585
step   1400 | loss 1.3822 | lr 3.00e-04 | grad 1.95 | tok/s 19428
step   1410 | loss 1.3890 | lr 3.00e-04 | grad 3.08 | tok/s 19340
step   1420 | loss 1.4043 | lr 3.00e-04 | grad 2.84 | tok/s 19971
step   1430 | loss 1.5574 | lr 3.00e-04 | grad 1.74 | tok/s 19319
step   1440 | loss 1.4607 | lr 3.00e-04 | grad 2.53 | tok/s 20013
step   1450 | loss 1.5102 | lr 3.00e-04 | grad 1.65 | tok/s 19480

Training complete! Final step: 1452
