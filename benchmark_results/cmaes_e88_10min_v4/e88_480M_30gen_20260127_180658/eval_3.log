Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_3/levelE88_100m_20260127_180705
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,217,656 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2539 | lr 3.00e-04 | grad 12.50 | tok/s 4241
step     20 | loss 4.0951 | lr 3.00e-04 | grad 20.75 | tok/s 6744
step     30 | loss 4.2507 | lr 3.00e-04 | grad 18.38 | tok/s 6787
step     40 | loss 2.9785 | lr 3.00e-04 | grad 5.38 | tok/s 6774
step     50 | loss 2.6456 | lr 3.00e-04 | grad 5.09 | tok/s 6753
step     60 | loss 2.9089 | lr 3.00e-04 | grad 3.58 | tok/s 6587
step     70 | loss 2.6235 | lr 3.00e-04 | grad 14.81 | tok/s 6446
step     80 | loss 2.4949 | lr 3.00e-04 | grad 5.03 | tok/s 6225
step     90 | loss 2.6492 | lr 3.00e-04 | grad 17.38 | tok/s 6187
step    100 | loss 2.2028 | lr 3.00e-04 | grad 3.36 | tok/s 6660
step    110 | loss 2.3383 | lr 3.00e-04 | grad 2.81 | tok/s 6305
step    120 | loss 2.4702 | lr 3.00e-04 | grad 7.06 | tok/s 6468
step    130 | loss 2.2067 | lr 3.00e-04 | grad 1.99 | tok/s 5387
step    140 | loss 2.1411 | lr 3.00e-04 | grad 2.23 | tok/s 5385
step    150 | loss 2.1902 | lr 3.00e-04 | grad 2.45 | tok/s 5145
step    160 | loss 2.0245 | lr 3.00e-04 | grad 1.56 | tok/s 5496
step    170 | loss 2.1718 | lr 3.00e-04 | grad 2.52 | tok/s 9643
step    180 | loss 2.0467 | lr 3.00e-04 | grad 3.03 | tok/s 15076
step    190 | loss 1.7497 | lr 3.00e-04 | grad 1.94 | tok/s 14618
step    200 | loss 1.6848 | lr 3.00e-04 | grad 1.90 | tok/s 15081
step    210 | loss 1.4873 | lr 3.00e-04 | grad 1.77 | tok/s 15458
step    220 | loss 1.2395 | lr 3.00e-04 | grad 1.48 | tok/s 15441
step    230 | loss 1.3682 | lr 3.00e-04 | grad 2.02 | tok/s 14912
step    240 | loss 1.9979 | lr 3.00e-04 | grad 1.77 | tok/s 14766
step    250 | loss 1.8082 | lr 3.00e-04 | grad 2.19 | tok/s 15041
step    260 | loss 1.7291 | lr 3.00e-04 | grad 2.22 | tok/s 15257
step    270 | loss 1.6439 | lr 3.00e-04 | grad 1.66 | tok/s 14675
step    280 | loss 1.8861 | lr 3.00e-04 | grad 2.14 | tok/s 14769
step    290 | loss 1.6225 | lr 3.00e-04 | grad 2.48 | tok/s 14774
step    300 | loss 1.6547 | lr 3.00e-04 | grad 1.73 | tok/s 14692
step    310 | loss 1.5524 | lr 3.00e-04 | grad 2.08 | tok/s 15260
step    320 | loss 1.8118 | lr 3.00e-04 | grad 2.81 | tok/s 14668
step    330 | loss 1.6579 | lr 3.00e-04 | grad 2.16 | tok/s 14696
step    340 | loss 1.8659 | lr 3.00e-04 | grad 1.88 | tok/s 14522
step    350 | loss 1.6516 | lr 3.00e-04 | grad 1.81 | tok/s 14298
step    360 | loss 1.6638 | lr 3.00e-04 | grad 2.38 | tok/s 14464
step    370 | loss 1.7910 | lr 3.00e-04 | grad 2.09 | tok/s 14881
step    380 | loss 1.3158 | lr 3.00e-04 | grad 1.34 | tok/s 14401
step    390 | loss 1.5888 | lr 3.00e-04 | grad 1.55 | tok/s 14910
step    400 | loss 1.3951 | lr 3.00e-04 | grad 2.31 | tok/s 15300
step    410 | loss 1.3699 | lr 3.00e-04 | grad 1.57 | tok/s 15275
step    420 | loss 1.2942 | lr 3.00e-04 | grad 1.59 | tok/s 15305
step    430 | loss 1.3331 | lr 3.00e-04 | grad 1.61 | tok/s 15279
step    440 | loss 1.3034 | lr 3.00e-04 | grad 1.67 | tok/s 15293
step    450 | loss 1.3162 | lr 3.00e-04 | grad 1.44 | tok/s 15285
step    460 | loss 1.6569 | lr 3.00e-04 | grad 2.06 | tok/s 14816
step    470 | loss 1.6755 | lr 3.00e-04 | grad 3.34 | tok/s 14286
step    480 | loss 1.6356 | lr 3.00e-04 | grad 2.09 | tok/s 14656
step    490 | loss 1.5826 | lr 3.00e-04 | grad 2.05 | tok/s 14844
step    500 | loss 1.6701 | lr 3.00e-04 | grad 2.30 | tok/s 13018
step    510 | loss 1.7251 | lr 3.00e-04 | grad 2.11 | tok/s 14078
step    520 | loss 1.5770 | lr 3.00e-04 | grad 2.25 | tok/s 14288
step    530 | loss 1.5422 | lr 3.00e-04 | grad 1.46 | tok/s 14248
step    540 | loss 1.6077 | lr 3.00e-04 | grad 1.74 | tok/s 14665
step    550 | loss 1.5193 | lr 3.00e-04 | grad 1.98 | tok/s 14475
step    560 | loss 1.3291 | lr 3.00e-04 | grad 5.66 | tok/s 15232
step    570 | loss 1.5876 | lr 3.00e-04 | grad 1.43 | tok/s 14654
step    580 | loss 1.8332 | lr 3.00e-04 | grad 2.69 | tok/s 15211
step    590 | loss 1.4869 | lr 3.00e-04 | grad 1.75 | tok/s 15214
step    600 | loss 1.5022 | lr 3.00e-04 | grad 1.53 | tok/s 14758
step    610 | loss 1.5074 | lr 3.00e-04 | grad 2.12 | tok/s 14921
step    620 | loss 1.5664 | lr 3.00e-04 | grad 2.52 | tok/s 14797
step    630 | loss 1.7208 | lr 3.00e-04 | grad 1.53 | tok/s 14749
step    640 | loss 1.0847 | lr 3.00e-04 | grad 1.39 | tok/s 14767
step    650 | loss 1.4606 | lr 3.00e-04 | grad 1.84 | tok/s 14732
step    660 | loss 1.5804 | lr 3.00e-04 | grad 2.19 | tok/s 14071
step    670 | loss 1.5299 | lr 3.00e-04 | grad 4.31 | tok/s 14874
step    680 | loss 1.5403 | lr 3.00e-04 | grad 3.48 | tok/s 14659
step    690 | loss 1.5805 | lr 3.00e-04 | grad 1.74 | tok/s 14669
step    700 | loss 1.4940 | lr 3.00e-04 | grad 2.02 | tok/s 15331
step    710 | loss 1.5271 | lr 3.00e-04 | grad 1.86 | tok/s 14750
step    720 | loss 1.5381 | lr 3.00e-04 | grad 1.64 | tok/s 14578
step    730 | loss 1.5111 | lr 3.00e-04 | grad 1.62 | tok/s 14588
step    740 | loss 1.5486 | lr 3.00e-04 | grad 2.47 | tok/s 14446
step    750 | loss 1.5282 | lr 3.00e-04 | grad 1.52 | tok/s 14881
step    760 | loss 1.4386 | lr 3.00e-04 | grad 1.50 | tok/s 14828
step    770 | loss 1.3673 | lr 3.00e-04 | grad 1.08 | tok/s 14705
step    780 | loss 1.4763 | lr 3.00e-04 | grad 1.59 | tok/s 13973
step    790 | loss 1.4468 | lr 3.00e-04 | grad 1.20 | tok/s 14396
step    800 | loss 1.4491 | lr 3.00e-04 | grad 1.53 | tok/s 14730
step    810 | loss 1.8072 | lr 3.00e-04 | grad 3.22 | tok/s 15152
step    820 | loss 1.6613 | lr 3.00e-04 | grad 2.91 | tok/s 15002
step    830 | loss 1.5769 | lr 3.00e-04 | grad 1.64 | tok/s 14509
step    840 | loss 1.2682 | lr 3.00e-04 | grad 1.30 | tok/s 15076
step    850 | loss 1.2558 | lr 3.00e-04 | grad 1.27 | tok/s 14953
step    860 | loss 1.3104 | lr 3.00e-04 | grad 1.41 | tok/s 15146
step    870 | loss 1.6298 | lr 3.00e-04 | grad 1.34 | tok/s 14426

Training complete! Final step: 872
