Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_80/levelE88_100m_20260127_194046
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 472,126,888 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2079 | lr 3.00e-04 | grad 15.06 | tok/s 9303
step     20 | loss 3.1178 | lr 3.00e-04 | grad 7.91 | tok/s 17737
step     30 | loss 3.1824 | lr 3.00e-04 | grad 8.94 | tok/s 18728
step     40 | loss 4.6734 | lr 3.00e-04 | grad 36.75 | tok/s 18979
step     50 | loss 4.5304 | lr 3.00e-04 | grad 16.62 | tok/s 19138
step     60 | loss 3.3818 | lr 3.00e-04 | grad 9.81 | tok/s 18995
step     70 | loss 2.8752 | lr 3.00e-04 | grad 8.19 | tok/s 18938
step     80 | loss 2.5720 | lr 3.00e-04 | grad 6.06 | tok/s 18927
step     90 | loss 2.5085 | lr 3.00e-04 | grad 4.88 | tok/s 18830
step    100 | loss 2.2498 | lr 3.00e-04 | grad 5.38 | tok/s 18810
step    110 | loss 2.2560 | lr 3.00e-04 | grad 4.50 | tok/s 18650
step    120 | loss 2.6887 | lr 3.00e-04 | grad 2.92 | tok/s 17742
step    130 | loss 2.0932 | lr 3.00e-04 | grad 6.62 | tok/s 18179
step    140 | loss 2.3495 | lr 3.00e-04 | grad 7.91 | tok/s 18217
step    150 | loss 1.3460 | lr 3.00e-04 | grad 6.41 | tok/s 18645
step    160 | loss 2.2925 | lr 3.00e-04 | grad 2.94 | tok/s 18055
step    170 | loss 2.3056 | lr 3.00e-04 | grad 2.45 | tok/s 17764
step    180 | loss 1.7663 | lr 3.00e-04 | grad 3.91 | tok/s 18217
step    190 | loss 1.9036 | lr 3.00e-04 | grad 3.05 | tok/s 17869
step    200 | loss 1.6252 | lr 3.00e-04 | grad 2.31 | tok/s 18685
step    210 | loss 1.8913 | lr 3.00e-04 | grad 7.47 | tok/s 17733
step    220 | loss 2.2003 | lr 3.00e-04 | grad 5.44 | tok/s 17922
step    230 | loss 2.0154 | lr 3.00e-04 | grad 3.39 | tok/s 17903
step    240 | loss 2.2837 | lr 3.00e-04 | grad 7.28 | tok/s 18140
step    250 | loss 1.7612 | lr 3.00e-04 | grad 2.02 | tok/s 18022
step    260 | loss 1.8905 | lr 3.00e-04 | grad 3.88 | tok/s 18512
step    270 | loss 1.8191 | lr 3.00e-04 | grad 2.59 | tok/s 18107
step    280 | loss 1.7766 | lr 3.00e-04 | grad 2.22 | tok/s 16993
step    290 | loss 1.6717 | lr 3.00e-04 | grad 2.70 | tok/s 17572
step    300 | loss 1.9785 | lr 3.00e-04 | grad 2.56 | tok/s 17716
step    310 | loss 1.6667 | lr 3.00e-04 | grad 2.14 | tok/s 17638
step    320 | loss 1.8784 | lr 3.00e-04 | grad 4.31 | tok/s 17246
step    330 | loss 1.7244 | lr 3.00e-04 | grad 2.31 | tok/s 18034
step    340 | loss 2.0514 | lr 3.00e-04 | grad 2.53 | tok/s 17954
step    350 | loss 1.7167 | lr 3.00e-04 | grad 2.33 | tok/s 18483
step    360 | loss 1.5834 | lr 3.00e-04 | grad 2.23 | tok/s 17664
step    370 | loss 1.4700 | lr 3.00e-04 | grad 2.12 | tok/s 18628
step    380 | loss 1.2017 | lr 3.00e-04 | grad 1.84 | tok/s 18792
step    390 | loss 1.1089 | lr 3.00e-04 | grad 1.89 | tok/s 18787
step    400 | loss 1.7568 | lr 3.00e-04 | grad 2.12 | tok/s 17791
step    410 | loss 1.7734 | lr 3.00e-04 | grad 2.83 | tok/s 17950
step    420 | loss 1.6070 | lr 3.00e-04 | grad 3.97 | tok/s 18741
step    430 | loss 1.6133 | lr 3.00e-04 | grad 2.33 | tok/s 18420
step    440 | loss 1.7138 | lr 3.00e-04 | grad 2.62 | tok/s 17825
step    450 | loss 1.6429 | lr 3.00e-04 | grad 1.71 | tok/s 18048
step    460 | loss 1.6070 | lr 3.00e-04 | grad 2.44 | tok/s 18299
step    470 | loss 1.5694 | lr 3.00e-04 | grad 3.75 | tok/s 18180
step    480 | loss 1.5653 | lr 3.00e-04 | grad 3.25 | tok/s 18579
step    490 | loss 1.7194 | lr 3.00e-04 | grad 2.91 | tok/s 17832
step    500 | loss 1.8227 | lr 3.00e-04 | grad 2.05 | tok/s 18114
step    510 | loss 1.6833 | lr 3.00e-04 | grad 1.94 | tok/s 17309
step    520 | loss 1.5399 | lr 3.00e-04 | grad 2.47 | tok/s 18146
step    530 | loss 1.7204 | lr 3.00e-04 | grad 2.27 | tok/s 17838
step    540 | loss 1.5953 | lr 3.00e-04 | grad 1.94 | tok/s 17451
step    550 | loss 1.3883 | lr 3.00e-04 | grad 3.20 | tok/s 18170
step    560 | loss 1.4458 | lr 3.00e-04 | grad 2.09 | tok/s 18775
step    570 | loss 1.3487 | lr 3.00e-04 | grad 2.17 | tok/s 18785
step    580 | loss 1.3047 | lr 3.00e-04 | grad 1.73 | tok/s 18795
step    590 | loss 1.3388 | lr 3.00e-04 | grad 1.66 | tok/s 18787
step    600 | loss 1.2722 | lr 3.00e-04 | grad 1.96 | tok/s 18789
step    610 | loss 1.3101 | lr 3.00e-04 | grad 1.82 | tok/s 18794
step    620 | loss 1.3009 | lr 3.00e-04 | grad 2.06 | tok/s 18709
step    630 | loss 1.6991 | lr 3.00e-04 | grad 5.69 | tok/s 17668
step    640 | loss 1.7594 | lr 3.00e-04 | grad 2.33 | tok/s 17904
step    650 | loss 1.5648 | lr 3.00e-04 | grad 2.00 | tok/s 17870
step    660 | loss 1.6103 | lr 3.00e-04 | grad 2.09 | tok/s 18576
step    670 | loss 1.6478 | lr 3.00e-04 | grad 5.59 | tok/s 17436
step    680 | loss 1.6553 | lr 3.00e-04 | grad 2.45 | tok/s 17667
step    690 | loss 1.6025 | lr 3.00e-04 | grad 2.17 | tok/s 17534
step    700 | loss 1.4955 | lr 3.00e-04 | grad 1.61 | tok/s 17916
step    710 | loss 1.6694 | lr 3.00e-04 | grad 3.05 | tok/s 17631
step    720 | loss 1.3173 | lr 3.00e-04 | grad 1.98 | tok/s 18329
step    730 | loss 1.4899 | lr 3.00e-04 | grad 1.73 | tok/s 18019
step    740 | loss 1.7988 | lr 3.00e-04 | grad 4.31 | tok/s 18528
step    750 | loss 1.5382 | lr 3.00e-04 | grad 1.90 | tok/s 18750
step    760 | loss 1.5545 | lr 3.00e-04 | grad 3.91 | tok/s 18342
step    770 | loss 1.6093 | lr 3.00e-04 | grad 2.22 | tok/s 18034
step    780 | loss 1.5008 | lr 3.00e-04 | grad 2.16 | tok/s 18157
step    790 | loss 1.6422 | lr 3.00e-04 | grad 5.50 | tok/s 18568
step    800 | loss 1.3387 | lr 3.00e-04 | grad 1.50 | tok/s 18225
step    810 | loss 1.3328 | lr 3.00e-04 | grad 3.02 | tok/s 17617
step    820 | loss 1.4328 | lr 3.00e-04 | grad 2.39 | tok/s 17967
step    830 | loss 1.5134 | lr 3.00e-04 | grad 1.68 | tok/s 17757
step    840 | loss 1.6498 | lr 3.00e-04 | grad 2.08 | tok/s 17654
step    850 | loss 1.5721 | lr 3.00e-04 | grad 1.72 | tok/s 18028
step    860 | loss 1.6150 | lr 3.00e-04 | grad 3.23 | tok/s 18336
step    870 | loss 1.4288 | lr 3.00e-04 | grad 2.12 | tok/s 18466
step    880 | loss 1.6109 | lr 3.00e-04 | grad 2.06 | tok/s 18113
step    890 | loss 1.5083 | lr 3.00e-04 | grad 1.66 | tok/s 18027
step    900 | loss 1.5593 | lr 3.00e-04 | grad 1.83 | tok/s 17948
step    910 | loss 1.5495 | lr 3.00e-04 | grad 7.06 | tok/s 17773
step    920 | loss 1.5084 | lr 3.00e-04 | grad 2.11 | tok/s 17958
step    930 | loss 1.3994 | lr 3.00e-04 | grad 2.28 | tok/s 18191
step    940 | loss 1.3759 | lr 3.00e-04 | grad 2.03 | tok/s 17769
step    950 | loss 1.5188 | lr 3.00e-04 | grad 2.61 | tok/s 17471
step    960 | loss 1.4628 | lr 3.00e-04 | grad 1.64 | tok/s 17958
step    970 | loss 1.4919 | lr 3.00e-04 | grad 1.89 | tok/s 17970
step    980 | loss 1.9402 | lr 3.00e-04 | grad 3.64 | tok/s 18691
step    990 | loss 1.6058 | lr 3.00e-04 | grad 1.90 | tok/s 17918
step   1000 | loss 1.6104 | lr 3.00e-04 | grad 2.11 | tok/s 17971
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6104.pt
step   1010 | loss 1.3789 | lr 3.00e-04 | grad 2.12 | tok/s 10094
step   1020 | loss 1.2008 | lr 3.00e-04 | grad 1.66 | tok/s 18373
step   1030 | loss 1.6202 | lr 3.00e-04 | grad 2.09 | tok/s 17963
step   1040 | loss 2.2196 | lr 3.00e-04 | grad 3.89 | tok/s 18551
step   1050 | loss 1.5024 | lr 3.00e-04 | grad 2.06 | tok/s 18325
step   1060 | loss 1.1460 | lr 3.00e-04 | grad 1.52 | tok/s 18593
step   1070 | loss 1.5064 | lr 3.00e-04 | grad 1.95 | tok/s 18246
step   1080 | loss 1.2742 | lr 3.00e-04 | grad 1.84 | tok/s 18843
step   1090 | loss 1.2496 | lr 3.00e-04 | grad 1.86 | tok/s 18790
step   1100 | loss 1.2021 | lr 3.00e-04 | grad 1.80 | tok/s 18789
step   1110 | loss 1.1916 | lr 3.00e-04 | grad 1.76 | tok/s 18838
step   1120 | loss 1.5137 | lr 3.00e-04 | grad 3.78 | tok/s 18330
step   1130 | loss 1.6770 | lr 3.00e-04 | grad 3.27 | tok/s 18450
step   1140 | loss 1.7203 | lr 3.00e-04 | grad 1.64 | tok/s 18680
step   1150 | loss 1.6566 | lr 3.00e-04 | grad 2.58 | tok/s 17757
step   1160 | loss 1.8132 | lr 3.00e-04 | grad 7.81 | tok/s 18052
step   1170 | loss 1.4670 | lr 3.00e-04 | grad 1.97 | tok/s 17778
step   1180 | loss 1.3698 | lr 3.00e-04 | grad 2.48 | tok/s 18411
step   1190 | loss 1.5698 | lr 3.00e-04 | grad 4.16 | tok/s 18769
step   1200 | loss 1.1450 | lr 3.00e-04 | grad 3.02 | tok/s 18814
step   1210 | loss 1.4855 | lr 3.00e-04 | grad 2.59 | tok/s 17497
step   1220 | loss 1.3840 | lr 3.00e-04 | grad 1.84 | tok/s 18192
step   1230 | loss 1.3387 | lr 3.00e-04 | grad 1.45 | tok/s 18629
step   1240 | loss 1.3271 | lr 3.00e-04 | grad 1.67 | tok/s 18342
step   1250 | loss 1.5370 | lr 3.00e-04 | grad 5.78 | tok/s 18391
step   1260 | loss 1.4280 | lr 3.00e-04 | grad 2.31 | tok/s 18546
step   1270 | loss 1.4027 | lr 3.00e-04 | grad 1.61 | tok/s 18187
step   1280 | loss 1.4648 | lr 3.00e-04 | grad 2.11 | tok/s 17891
step   1290 | loss 1.3416 | lr 3.00e-04 | grad 1.88 | tok/s 17967
step   1300 | loss 1.6412 | lr 3.00e-04 | grad 4.38 | tok/s 17673
step   1310 | loss 1.5102 | lr 3.00e-04 | grad 1.92 | tok/s 18319
step   1320 | loss 1.5442 | lr 3.00e-04 | grad 3.11 | tok/s 18437
step   1330 | loss 1.4171 | lr 3.00e-04 | grad 2.11 | tok/s 18180
step   1340 | loss 1.6229 | lr 3.00e-04 | grad 2.12 | tok/s 17829

Training complete! Final step: 1348
