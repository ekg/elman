Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_35/levelE88_100m_20260127_184855
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 479,753,478 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2131 | lr 3.00e-04 | grad 20.12 | tok/s 8990
step     20 | loss 3.3516 | lr 3.00e-04 | grad 9.38 | tok/s 17562
step     30 | loss 3.2961 | lr 3.00e-04 | grad 10.56 | tok/s 18547
step     40 | loss 4.7560 | lr 3.00e-04 | grad 36.75 | tok/s 18824
step     50 | loss 4.4505 | lr 3.00e-04 | grad 16.38 | tok/s 19012
step     60 | loss 3.4620 | lr 3.00e-04 | grad 10.50 | tok/s 18917
step     70 | loss 2.9763 | lr 3.00e-04 | grad 7.25 | tok/s 18853
step     80 | loss 2.5981 | lr 3.00e-04 | grad 8.12 | tok/s 18809
step     90 | loss 2.5649 | lr 3.00e-04 | grad 5.12 | tok/s 18771
step    100 | loss 2.3058 | lr 3.00e-04 | grad 5.62 | tok/s 18775
step    110 | loss 2.2982 | lr 3.00e-04 | grad 4.69 | tok/s 18570
step    120 | loss 2.7303 | lr 3.00e-04 | grad 3.39 | tok/s 17664
step    130 | loss 2.0882 | lr 3.00e-04 | grad 6.66 | tok/s 18083
step    140 | loss 2.3645 | lr 3.00e-04 | grad 8.25 | tok/s 18129
step    150 | loss 1.3728 | lr 3.00e-04 | grad 7.16 | tok/s 18619
step    160 | loss 2.3149 | lr 3.00e-04 | grad 3.20 | tok/s 17939
step    170 | loss 2.3042 | lr 3.00e-04 | grad 2.58 | tok/s 17676
step    180 | loss 1.7872 | lr 3.00e-04 | grad 4.06 | tok/s 18100
step    190 | loss 1.8954 | lr 3.00e-04 | grad 3.64 | tok/s 17741
step    200 | loss 1.6163 | lr 3.00e-04 | grad 2.33 | tok/s 18558
step    210 | loss 1.8790 | lr 3.00e-04 | grad 6.78 | tok/s 17629
step    220 | loss 2.1846 | lr 3.00e-04 | grad 4.78 | tok/s 17821
step    230 | loss 2.0271 | lr 3.00e-04 | grad 3.39 | tok/s 17800
step    240 | loss 2.2767 | lr 3.00e-04 | grad 7.62 | tok/s 18000
step    250 | loss 1.7533 | lr 3.00e-04 | grad 2.20 | tok/s 17874
step    260 | loss 1.8855 | lr 3.00e-04 | grad 4.44 | tok/s 18384
step    270 | loss 1.8251 | lr 3.00e-04 | grad 2.78 | tok/s 17955
step    280 | loss 1.7711 | lr 3.00e-04 | grad 2.28 | tok/s 16884
step    290 | loss 1.6715 | lr 3.00e-04 | grad 2.84 | tok/s 17442
step    300 | loss 1.9800 | lr 3.00e-04 | grad 2.81 | tok/s 17603
step    310 | loss 1.6659 | lr 3.00e-04 | grad 2.28 | tok/s 17539
step    320 | loss 1.8894 | lr 3.00e-04 | grad 4.56 | tok/s 17724
step    330 | loss 1.7250 | lr 3.00e-04 | grad 2.48 | tok/s 17935
step    340 | loss 2.0557 | lr 3.00e-04 | grad 2.80 | tok/s 17808
step    350 | loss 1.7141 | lr 3.00e-04 | grad 2.42 | tok/s 18311
step    360 | loss 1.5784 | lr 3.00e-04 | grad 2.39 | tok/s 17535
step    370 | loss 1.4762 | lr 3.00e-04 | grad 2.20 | tok/s 18498
step    380 | loss 1.2107 | lr 3.00e-04 | grad 1.98 | tok/s 18628
step    390 | loss 1.1229 | lr 3.00e-04 | grad 1.91 | tok/s 18637
step    400 | loss 1.7591 | lr 3.00e-04 | grad 2.14 | tok/s 17683
step    410 | loss 1.7859 | lr 3.00e-04 | grad 2.97 | tok/s 17855
step    420 | loss 1.5978 | lr 3.00e-04 | grad 4.06 | tok/s 18614
step    430 | loss 1.6065 | lr 3.00e-04 | grad 2.44 | tok/s 18305
step    440 | loss 1.7207 | lr 3.00e-04 | grad 2.88 | tok/s 17739
step    450 | loss 1.6434 | lr 3.00e-04 | grad 1.96 | tok/s 17929
step    460 | loss 1.6072 | lr 3.00e-04 | grad 2.42 | tok/s 18163
step    470 | loss 1.5757 | lr 3.00e-04 | grad 4.12 | tok/s 18062
step    480 | loss 1.5769 | lr 3.00e-04 | grad 3.48 | tok/s 18447
step    490 | loss 1.7224 | lr 3.00e-04 | grad 2.92 | tok/s 17708
step    500 | loss 1.8221 | lr 3.00e-04 | grad 2.09 | tok/s 18006
step    510 | loss 1.6931 | lr 3.00e-04 | grad 2.03 | tok/s 17194
step    520 | loss 1.5467 | lr 3.00e-04 | grad 2.72 | tok/s 18017
step    530 | loss 1.7303 | lr 3.00e-04 | grad 2.34 | tok/s 17714
step    540 | loss 1.5993 | lr 3.00e-04 | grad 1.99 | tok/s 17356
step    550 | loss 1.3827 | lr 3.00e-04 | grad 3.48 | tok/s 17687
step    560 | loss 1.4551 | lr 3.00e-04 | grad 2.22 | tok/s 18651
step    570 | loss 1.3579 | lr 3.00e-04 | grad 2.42 | tok/s 18646
step    580 | loss 1.3090 | lr 3.00e-04 | grad 1.78 | tok/s 18642
step    590 | loss 1.3430 | lr 3.00e-04 | grad 1.73 | tok/s 18625
step    600 | loss 1.2803 | lr 3.00e-04 | grad 2.02 | tok/s 18628
step    610 | loss 1.3148 | lr 3.00e-04 | grad 2.06 | tok/s 18626
step    620 | loss 1.3040 | lr 3.00e-04 | grad 2.34 | tok/s 18587
step    630 | loss 1.7240 | lr 3.00e-04 | grad 6.34 | tok/s 17571
step    640 | loss 1.7621 | lr 3.00e-04 | grad 2.31 | tok/s 17809
step    650 | loss 1.5715 | lr 3.00e-04 | grad 2.12 | tok/s 17794
step    660 | loss 1.6137 | lr 3.00e-04 | grad 2.12 | tok/s 18467
step    670 | loss 1.6588 | lr 3.00e-04 | grad 6.25 | tok/s 17857
step    680 | loss 1.6692 | lr 3.00e-04 | grad 2.69 | tok/s 17579
step    690 | loss 1.6152 | lr 3.00e-04 | grad 2.25 | tok/s 17431
step    700 | loss 1.4952 | lr 3.00e-04 | grad 1.64 | tok/s 17796
step    710 | loss 1.6682 | lr 3.00e-04 | grad 3.06 | tok/s 17530
step    720 | loss 1.3246 | lr 3.00e-04 | grad 2.17 | tok/s 18192
step    730 | loss 1.5016 | lr 3.00e-04 | grad 1.83 | tok/s 17920
step    740 | loss 1.8026 | lr 3.00e-04 | grad 4.59 | tok/s 18392
step    750 | loss 1.5423 | lr 3.00e-04 | grad 2.05 | tok/s 18638
step    760 | loss 1.5666 | lr 3.00e-04 | grad 4.47 | tok/s 18228
step    770 | loss 1.6234 | lr 3.00e-04 | grad 2.39 | tok/s 17925
step    780 | loss 1.5048 | lr 3.00e-04 | grad 2.30 | tok/s 18038
step    790 | loss 1.6541 | lr 3.00e-04 | grad 5.38 | tok/s 18441
step    800 | loss 1.3419 | lr 3.00e-04 | grad 1.48 | tok/s 18139
step    810 | loss 1.3432 | lr 3.00e-04 | grad 3.33 | tok/s 17541
step    820 | loss 1.4430 | lr 3.00e-04 | grad 2.39 | tok/s 17873
step    830 | loss 1.5255 | lr 3.00e-04 | grad 1.90 | tok/s 17642
step    840 | loss 1.6622 | lr 3.00e-04 | grad 2.23 | tok/s 17557
step    850 | loss 1.5835 | lr 3.00e-04 | grad 1.84 | tok/s 17936
step    860 | loss 1.6134 | lr 3.00e-04 | grad 3.30 | tok/s 18212
step    870 | loss 1.4310 | lr 3.00e-04 | grad 2.12 | tok/s 18349
step    880 | loss 1.6233 | lr 3.00e-04 | grad 2.25 | tok/s 17999
step    890 | loss 1.5157 | lr 3.00e-04 | grad 1.73 | tok/s 17923
step    900 | loss 1.5680 | lr 3.00e-04 | grad 1.85 | tok/s 17447
step    910 | loss 1.5651 | lr 3.00e-04 | grad 7.56 | tok/s 17658
step    920 | loss 1.5169 | lr 3.00e-04 | grad 2.25 | tok/s 17871
step    930 | loss 1.4057 | lr 3.00e-04 | grad 2.39 | tok/s 18068
step    940 | loss 1.3825 | lr 3.00e-04 | grad 2.20 | tok/s 17670
step    950 | loss 1.5260 | lr 3.00e-04 | grad 2.84 | tok/s 17389
step    960 | loss 1.4733 | lr 3.00e-04 | grad 1.96 | tok/s 17864
step    970 | loss 1.5005 | lr 3.00e-04 | grad 2.00 | tok/s 17872
step    980 | loss 2.0270 | lr 3.00e-04 | grad 4.03 | tok/s 18547
step    990 | loss 1.6129 | lr 3.00e-04 | grad 2.12 | tok/s 17788
step   1000 | loss 1.6168 | lr 3.00e-04 | grad 2.23 | tok/s 17863
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6168.pt
step   1010 | loss 1.3815 | lr 3.00e-04 | grad 2.16 | tok/s 9446
step   1020 | loss 1.2103 | lr 3.00e-04 | grad 1.71 | tok/s 18832
step   1030 | loss 1.6298 | lr 3.00e-04 | grad 2.28 | tok/s 17802
step   1040 | loss 2.2169 | lr 3.00e-04 | grad 4.88 | tok/s 18370
step   1050 | loss 1.5113 | lr 3.00e-04 | grad 2.25 | tok/s 18135
step   1060 | loss 1.1312 | lr 3.00e-04 | grad 1.55 | tok/s 18448
step   1070 | loss 1.5201 | lr 3.00e-04 | grad 2.19 | tok/s 18133
step   1080 | loss 1.2871 | lr 3.00e-04 | grad 2.02 | tok/s 18793
step   1090 | loss 1.2605 | lr 3.00e-04 | grad 1.91 | tok/s 18773
step   1100 | loss 1.2113 | lr 3.00e-04 | grad 2.12 | tok/s 18735
step   1110 | loss 1.2023 | lr 3.00e-04 | grad 1.89 | tok/s 18731
step   1120 | loss 1.5244 | lr 3.00e-04 | grad 4.06 | tok/s 18201
step   1130 | loss 1.6885 | lr 3.00e-04 | grad 3.30 | tok/s 18373
step   1140 | loss 1.7246 | lr 3.00e-04 | grad 1.70 | tok/s 18589
step   1150 | loss 1.6873 | lr 3.00e-04 | grad 3.20 | tok/s 16859
step   1160 | loss 1.8107 | lr 3.00e-04 | grad 7.81 | tok/s 17926
step   1170 | loss 1.4786 | lr 3.00e-04 | grad 2.05 | tok/s 17670
step   1180 | loss 1.3782 | lr 3.00e-04 | grad 2.62 | tok/s 18279
step   1190 | loss 1.5715 | lr 3.00e-04 | grad 4.66 | tok/s 18642
step   1200 | loss 1.1566 | lr 3.00e-04 | grad 5.19 | tok/s 18704
step   1210 | loss 1.4927 | lr 3.00e-04 | grad 2.73 | tok/s 17412
step   1220 | loss 1.3900 | lr 3.00e-04 | grad 1.87 | tok/s 18097
step   1230 | loss 1.3381 | lr 3.00e-04 | grad 1.54 | tok/s 18524
step   1240 | loss 1.3352 | lr 3.00e-04 | grad 1.76 | tok/s 18245
step   1250 | loss 1.5345 | lr 3.00e-04 | grad 5.84 | tok/s 18277
step   1260 | loss 1.4361 | lr 3.00e-04 | grad 2.73 | tok/s 18454
step   1270 | loss 1.4079 | lr 3.00e-04 | grad 1.80 | tok/s 18101
step   1280 | loss 1.4744 | lr 3.00e-04 | grad 2.17 | tok/s 17787
step   1290 | loss 1.3503 | lr 3.00e-04 | grad 1.99 | tok/s 17826
step   1300 | loss 1.6786 | lr 3.00e-04 | grad 5.03 | tok/s 17558
step   1310 | loss 1.5216 | lr 3.00e-04 | grad 2.03 | tok/s 18185
step   1320 | loss 1.5556 | lr 3.00e-04 | grad 3.28 | tok/s 18315
step   1330 | loss 1.4285 | lr 3.00e-04 | grad 2.30 | tok/s 18067

Training complete! Final step: 1338
