Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_40/levelE88_100m_20260127_184855
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 477,055,904 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0668 | lr 3.00e-04 | grad 10.38 | tok/s 8356
step     20 | loss 2.8254 | lr 3.00e-04 | grad 2.25 | tok/s 14583
step     30 | loss 2.8960 | lr 3.00e-04 | grad 4.97 | tok/s 15352
step     40 | loss 4.2798 | lr 3.00e-04 | grad 22.00 | tok/s 15608
step     50 | loss 4.0191 | lr 3.00e-04 | grad 8.81 | tok/s 15765
step     60 | loss 3.1850 | lr 3.00e-04 | grad 6.16 | tok/s 15706
step     70 | loss 2.7140 | lr 3.00e-04 | grad 4.12 | tok/s 15670
step     80 | loss 2.4258 | lr 3.00e-04 | grad 3.59 | tok/s 15601
step     90 | loss 2.3030 | lr 3.00e-04 | grad 3.23 | tok/s 15569
step    100 | loss 2.0845 | lr 3.00e-04 | grad 2.23 | tok/s 15529
step    110 | loss 2.1480 | lr 3.00e-04 | grad 3.06 | tok/s 15400
step    120 | loss 2.6081 | lr 3.00e-04 | grad 1.95 | tok/s 14663
step    130 | loss 2.0694 | lr 3.00e-04 | grad 4.66 | tok/s 15058
step    140 | loss 2.3451 | lr 3.00e-04 | grad 7.16 | tok/s 15063
step    150 | loss 1.3457 | lr 3.00e-04 | grad 5.09 | tok/s 15433
step    160 | loss 2.2903 | lr 3.00e-04 | grad 2.14 | tok/s 14937
step    170 | loss 2.2636 | lr 3.00e-04 | grad 1.71 | tok/s 14710
step    180 | loss 1.7817 | lr 3.00e-04 | grad 2.84 | tok/s 15036
step    190 | loss 1.8794 | lr 3.00e-04 | grad 2.12 | tok/s 14771
step    200 | loss 1.6101 | lr 3.00e-04 | grad 1.61 | tok/s 15434
step    210 | loss 1.8564 | lr 3.00e-04 | grad 4.50 | tok/s 14649
step    220 | loss 2.1691 | lr 3.00e-04 | grad 3.47 | tok/s 14822
step    230 | loss 1.9416 | lr 3.00e-04 | grad 2.53 | tok/s 14379
step    240 | loss 2.2307 | lr 3.00e-04 | grad 5.22 | tok/s 14998
step    250 | loss 1.7415 | lr 3.00e-04 | grad 1.57 | tok/s 14905
step    260 | loss 1.8684 | lr 3.00e-04 | grad 3.02 | tok/s 15312
step    270 | loss 1.8012 | lr 3.00e-04 | grad 1.87 | tok/s 14952
step    280 | loss 1.7545 | lr 3.00e-04 | grad 1.77 | tok/s 14049
step    290 | loss 1.6521 | lr 3.00e-04 | grad 2.12 | tok/s 14546
step    300 | loss 1.9543 | lr 3.00e-04 | grad 2.08 | tok/s 14635
step    310 | loss 1.6488 | lr 3.00e-04 | grad 1.70 | tok/s 14563
step    320 | loss 1.8551 | lr 3.00e-04 | grad 3.14 | tok/s 14741
step    330 | loss 1.6959 | lr 3.00e-04 | grad 1.77 | tok/s 14906
step    340 | loss 2.0110 | lr 3.00e-04 | grad 2.03 | tok/s 14851
step    350 | loss 1.7025 | lr 3.00e-04 | grad 1.92 | tok/s 15272
step    360 | loss 1.5639 | lr 3.00e-04 | grad 1.81 | tok/s 14623
step    370 | loss 1.4778 | lr 3.00e-04 | grad 1.59 | tok/s 15395
step    380 | loss 1.2103 | lr 3.00e-04 | grad 1.62 | tok/s 15547
step    390 | loss 1.1194 | lr 3.00e-04 | grad 1.52 | tok/s 15534
step    400 | loss 1.7360 | lr 3.00e-04 | grad 1.66 | tok/s 14702
step    410 | loss 1.7451 | lr 3.00e-04 | grad 2.22 | tok/s 14829
step    420 | loss 1.6028 | lr 3.00e-04 | grad 3.25 | tok/s 15485
step    430 | loss 1.6023 | lr 3.00e-04 | grad 1.76 | tok/s 15210
step    440 | loss 1.6867 | lr 3.00e-04 | grad 2.16 | tok/s 14759
step    450 | loss 1.6184 | lr 3.00e-04 | grad 1.43 | tok/s 14903
step    460 | loss 1.5854 | lr 3.00e-04 | grad 1.98 | tok/s 15134
step    470 | loss 1.5491 | lr 3.00e-04 | grad 2.91 | tok/s 15012
step    480 | loss 1.5581 | lr 3.00e-04 | grad 2.61 | tok/s 15338
step    490 | loss 1.6933 | lr 3.00e-04 | grad 2.23 | tok/s 14736
step    500 | loss 1.7994 | lr 3.00e-04 | grad 1.62 | tok/s 14990
step    510 | loss 1.6633 | lr 3.00e-04 | grad 1.44 | tok/s 14292
step    520 | loss 1.5227 | lr 3.00e-04 | grad 1.99 | tok/s 14994
step    530 | loss 1.6979 | lr 3.00e-04 | grad 1.84 | tok/s 14756
step    540 | loss 1.5785 | lr 3.00e-04 | grad 1.52 | tok/s 14453
step    550 | loss 1.3720 | lr 3.00e-04 | grad 2.64 | tok/s 15101
step    560 | loss 1.4362 | lr 3.00e-04 | grad 1.68 | tok/s 15530
step    570 | loss 1.3403 | lr 3.00e-04 | grad 1.75 | tok/s 15537
step    580 | loss 1.2982 | lr 3.00e-04 | grad 1.30 | tok/s 15522
step    590 | loss 1.3300 | lr 3.00e-04 | grad 1.27 | tok/s 15511
step    600 | loss 1.2686 | lr 3.00e-04 | grad 1.54 | tok/s 15519
step    610 | loss 1.3032 | lr 3.00e-04 | grad 1.46 | tok/s 15507
step    620 | loss 1.2926 | lr 3.00e-04 | grad 1.66 | tok/s 15452
step    630 | loss 1.6484 | lr 3.00e-04 | grad 4.25 | tok/s 14600
step    640 | loss 1.7297 | lr 3.00e-04 | grad 1.76 | tok/s 14805
step    650 | loss 1.5434 | lr 3.00e-04 | grad 1.65 | tok/s 14800
step    660 | loss 1.5889 | lr 3.00e-04 | grad 1.72 | tok/s 15351
step    670 | loss 1.6142 | lr 3.00e-04 | grad 4.56 | tok/s 14842
step    680 | loss 1.6290 | lr 3.00e-04 | grad 1.91 | tok/s 14614
step    690 | loss 1.5748 | lr 3.00e-04 | grad 1.78 | tok/s 14507
step    700 | loss 1.4701 | lr 3.00e-04 | grad 1.27 | tok/s 14818
step    710 | loss 1.6304 | lr 3.00e-04 | grad 2.66 | tok/s 14584
step    720 | loss 1.3015 | lr 3.00e-04 | grad 1.51 | tok/s 15162
step    730 | loss 1.4661 | lr 3.00e-04 | grad 1.38 | tok/s 14909
step    740 | loss 1.7738 | lr 3.00e-04 | grad 3.45 | tok/s 15312
step    750 | loss 1.5191 | lr 3.00e-04 | grad 1.39 | tok/s 15482
step    760 | loss 1.5268 | lr 3.00e-04 | grad 3.11 | tok/s 15153
step    770 | loss 1.5795 | lr 3.00e-04 | grad 1.80 | tok/s 14916
step    780 | loss 1.4789 | lr 3.00e-04 | grad 1.73 | tok/s 15014
step    790 | loss 1.6223 | lr 3.00e-04 | grad 4.28 | tok/s 15354
step    800 | loss 1.3175 | lr 3.00e-04 | grad 1.09 | tok/s 15072
step    810 | loss 1.3158 | lr 3.00e-04 | grad 2.58 | tok/s 14591
step    820 | loss 1.4179 | lr 3.00e-04 | grad 1.71 | tok/s 14866
step    830 | loss 1.4872 | lr 3.00e-04 | grad 1.30 | tok/s 14657
step    840 | loss 1.6117 | lr 3.00e-04 | grad 1.52 | tok/s 14592
step    850 | loss 1.5477 | lr 3.00e-04 | grad 1.42 | tok/s 14864
step    860 | loss 1.5872 | lr 3.00e-04 | grad 2.48 | tok/s 14659
step    870 | loss 1.4069 | lr 3.00e-04 | grad 1.69 | tok/s 15259
step    880 | loss 1.5854 | lr 3.00e-04 | grad 1.65 | tok/s 14977
step    890 | loss 1.4864 | lr 3.00e-04 | grad 1.27 | tok/s 14892
step    900 | loss 1.5281 | lr 3.00e-04 | grad 1.52 | tok/s 14855
step    910 | loss 1.5216 | lr 3.00e-04 | grad 5.94 | tok/s 14685
step    920 | loss 1.4749 | lr 3.00e-04 | grad 1.56 | tok/s 14837
step    930 | loss 1.3831 | lr 3.00e-04 | grad 1.77 | tok/s 15018
step    940 | loss 1.3508 | lr 3.00e-04 | grad 1.71 | tok/s 14685
step    950 | loss 1.4961 | lr 3.00e-04 | grad 2.08 | tok/s 14432
step    960 | loss 1.4396 | lr 3.00e-04 | grad 1.31 | tok/s 14834
step    970 | loss 1.4730 | lr 3.00e-04 | grad 1.48 | tok/s 14827
step    980 | loss 1.8693 | lr 3.00e-04 | grad 3.03 | tok/s 15450
step    990 | loss 1.5627 | lr 3.00e-04 | grad 1.59 | tok/s 14797
step   1000 | loss 1.5644 | lr 3.00e-04 | grad 1.70 | tok/s 14854
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5644.pt
step   1010 | loss 1.3564 | lr 3.00e-04 | grad 2.14 | tok/s 8069
step   1020 | loss 1.2160 | lr 3.00e-04 | grad 1.69 | tok/s 15730
step   1030 | loss 1.6344 | lr 3.00e-04 | grad 2.19 | tok/s 14761
step   1040 | loss 2.1153 | lr 3.00e-04 | grad 2.12 | tok/s 15429
step   1050 | loss 1.4919 | lr 3.00e-04 | grad 2.05 | tok/s 15034
step   1060 | loss 1.0400 | lr 3.00e-04 | grad 1.46 | tok/s 15393
step   1070 | loss 1.4887 | lr 3.00e-04 | grad 1.53 | tok/s 15171
step   1080 | loss 1.2495 | lr 3.00e-04 | grad 1.24 | tok/s 15589
step   1090 | loss 1.2363 | lr 3.00e-04 | grad 1.34 | tok/s 15566
step   1100 | loss 1.1886 | lr 3.00e-04 | grad 1.21 | tok/s 15587
step   1110 | loss 1.2331 | lr 3.00e-04 | grad 2.09 | tok/s 15502

Training complete! Final step: 1114
