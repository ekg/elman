Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_204/levelE88_100m_20260127_222602
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 473,222,650 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.5216 | lr 3.00e-04 | grad 14.50 | tok/s 9571
step     20 | loss 3.4434 | lr 3.00e-04 | grad 7.28 | tok/s 18910
step     30 | loss 3.1094 | lr 3.00e-04 | grad 7.81 | tok/s 19949
step     40 | loss 4.4591 | lr 3.00e-04 | grad 24.38 | tok/s 20299
step     50 | loss 4.1934 | lr 3.00e-04 | grad 11.94 | tok/s 20495
step     60 | loss 3.3365 | lr 3.00e-04 | grad 8.62 | tok/s 20447
step     70 | loss 2.8840 | lr 3.00e-04 | grad 5.41 | tok/s 20345
step     80 | loss 2.6077 | lr 3.00e-04 | grad 6.81 | tok/s 20342
step     90 | loss 2.4940 | lr 3.00e-04 | grad 5.03 | tok/s 20339
step    100 | loss 2.2463 | lr 3.00e-04 | grad 4.81 | tok/s 20318
step    110 | loss 2.2243 | lr 3.00e-04 | grad 5.94 | tok/s 20136
step    120 | loss 2.7021 | lr 3.00e-04 | grad 3.17 | tok/s 19148
step    130 | loss 2.0590 | lr 3.00e-04 | grad 6.34 | tok/s 19600
step    140 | loss 2.3592 | lr 3.00e-04 | grad 8.25 | tok/s 19690
step    150 | loss 1.3504 | lr 3.00e-04 | grad 7.09 | tok/s 20167
step    160 | loss 2.3130 | lr 3.00e-04 | grad 2.89 | tok/s 19477
step    170 | loss 2.2995 | lr 3.00e-04 | grad 2.39 | tok/s 19147
step    180 | loss 1.7685 | lr 3.00e-04 | grad 3.86 | tok/s 19611
step    190 | loss 1.8819 | lr 3.00e-04 | grad 2.84 | tok/s 19239
step    200 | loss 1.6177 | lr 3.00e-04 | grad 2.27 | tok/s 20139
step    210 | loss 1.8803 | lr 3.00e-04 | grad 6.22 | tok/s 19093
step    220 | loss 2.1896 | lr 3.00e-04 | grad 3.80 | tok/s 19307
step    230 | loss 1.9740 | lr 3.00e-04 | grad 3.22 | tok/s 19283
step    240 | loss 2.2626 | lr 3.00e-04 | grad 7.25 | tok/s 19531
step    250 | loss 1.7480 | lr 3.00e-04 | grad 2.12 | tok/s 19430
step    260 | loss 1.8731 | lr 3.00e-04 | grad 5.41 | tok/s 19940
step    270 | loss 1.8174 | lr 3.00e-04 | grad 2.58 | tok/s 19498
step    280 | loss 1.7677 | lr 3.00e-04 | grad 2.22 | tok/s 18312
step    290 | loss 1.6674 | lr 3.00e-04 | grad 2.72 | tok/s 18935
step    300 | loss 1.9842 | lr 3.00e-04 | grad 2.91 | tok/s 19086
step    310 | loss 1.6600 | lr 3.00e-04 | grad 2.11 | tok/s 18976
step    320 | loss 1.8792 | lr 3.00e-04 | grad 4.44 | tok/s 19208
step    330 | loss 1.7183 | lr 3.00e-04 | grad 2.38 | tok/s 19373
step    340 | loss 2.0547 | lr 3.00e-04 | grad 3.33 | tok/s 19317
step    350 | loss 1.7166 | lr 3.00e-04 | grad 2.36 | tok/s 19903
step    360 | loss 1.5792 | lr 3.00e-04 | grad 2.23 | tok/s 19036
step    370 | loss 1.4732 | lr 3.00e-04 | grad 2.19 | tok/s 20057
step    380 | loss 1.2033 | lr 3.00e-04 | grad 1.96 | tok/s 20256
step    390 | loss 1.1183 | lr 3.00e-04 | grad 1.70 | tok/s 20221
step    400 | loss 1.7524 | lr 3.00e-04 | grad 2.08 | tok/s 19161
step    410 | loss 1.7723 | lr 3.00e-04 | grad 2.89 | tok/s 19347
step    420 | loss 1.5961 | lr 3.00e-04 | grad 4.97 | tok/s 20163
step    430 | loss 1.6038 | lr 3.00e-04 | grad 2.23 | tok/s 19858
step    440 | loss 1.7116 | lr 3.00e-04 | grad 2.86 | tok/s 19239
step    450 | loss 1.6359 | lr 3.00e-04 | grad 1.94 | tok/s 19433
step    460 | loss 1.6076 | lr 3.00e-04 | grad 2.41 | tok/s 19747
step    470 | loss 1.5689 | lr 3.00e-04 | grad 3.95 | tok/s 19605
step    480 | loss 1.5566 | lr 3.00e-04 | grad 3.22 | tok/s 20013
step    490 | loss 1.7114 | lr 3.00e-04 | grad 2.80 | tok/s 19224
step    500 | loss 1.8108 | lr 3.00e-04 | grad 2.02 | tok/s 19553
step    510 | loss 1.6831 | lr 3.00e-04 | grad 1.85 | tok/s 18684
step    520 | loss 1.5431 | lr 3.00e-04 | grad 2.58 | tok/s 19578
step    530 | loss 1.7160 | lr 3.00e-04 | grad 2.25 | tok/s 19242
step    540 | loss 1.5912 | lr 3.00e-04 | grad 1.91 | tok/s 18834
step    550 | loss 1.3855 | lr 3.00e-04 | grad 3.59 | tok/s 19715
step    560 | loss 1.4458 | lr 3.00e-04 | grad 2.28 | tok/s 20261
step    570 | loss 1.3501 | lr 3.00e-04 | grad 2.28 | tok/s 20253
step    580 | loss 1.3076 | lr 3.00e-04 | grad 1.69 | tok/s 20281
step    590 | loss 1.3425 | lr 3.00e-04 | grad 1.68 | tok/s 20265
step    600 | loss 1.2754 | lr 3.00e-04 | grad 1.90 | tok/s 20272
step    610 | loss 1.3118 | lr 3.00e-04 | grad 2.06 | tok/s 20277
step    620 | loss 1.3024 | lr 3.00e-04 | grad 2.19 | tok/s 20189
step    630 | loss 1.7153 | lr 3.00e-04 | grad 5.81 | tok/s 19069
step    640 | loss 1.7504 | lr 3.00e-04 | grad 2.16 | tok/s 19311
step    650 | loss 1.5633 | lr 3.00e-04 | grad 2.03 | tok/s 19301
step    660 | loss 1.6007 | lr 3.00e-04 | grad 2.17 | tok/s 20045
step    670 | loss 1.6513 | lr 3.00e-04 | grad 5.72 | tok/s 19382
step    680 | loss 1.6603 | lr 3.00e-04 | grad 2.66 | tok/s 19070
step    690 | loss 1.6105 | lr 3.00e-04 | grad 2.17 | tok/s 18932
step    700 | loss 1.4859 | lr 3.00e-04 | grad 1.64 | tok/s 19342
step    710 | loss 1.6617 | lr 3.00e-04 | grad 3.12 | tok/s 19035
step    720 | loss 1.3209 | lr 3.00e-04 | grad 2.09 | tok/s 19779
step    730 | loss 1.4936 | lr 3.00e-04 | grad 1.73 | tok/s 19485
step    740 | loss 1.7867 | lr 3.00e-04 | grad 4.53 | tok/s 20000
step    750 | loss 1.5364 | lr 3.00e-04 | grad 2.02 | tok/s 20227
step    760 | loss 1.5577 | lr 3.00e-04 | grad 4.53 | tok/s 19797
step    770 | loss 1.5967 | lr 3.00e-04 | grad 2.28 | tok/s 19474
step    780 | loss 1.5007 | lr 3.00e-04 | grad 2.28 | tok/s 19629
step    790 | loss 1.6368 | lr 3.00e-04 | grad 5.56 | tok/s 20028
step    800 | loss 1.3373 | lr 3.00e-04 | grad 1.43 | tok/s 19679
step    810 | loss 1.3386 | lr 3.00e-04 | grad 2.98 | tok/s 19032
step    820 | loss 1.4346 | lr 3.00e-04 | grad 2.27 | tok/s 19425
step    830 | loss 1.5121 | lr 3.00e-04 | grad 1.81 | tok/s 19156
step    840 | loss 1.6503 | lr 3.00e-04 | grad 2.14 | tok/s 19051
step    850 | loss 1.5728 | lr 3.00e-04 | grad 1.82 | tok/s 19448
step    860 | loss 1.6115 | lr 3.00e-04 | grad 2.80 | tok/s 19791
step    870 | loss 1.4220 | lr 3.00e-04 | grad 2.03 | tok/s 19934
step    880 | loss 1.6146 | lr 3.00e-04 | grad 2.17 | tok/s 19558
step    890 | loss 1.5093 | lr 3.00e-04 | grad 1.77 | tok/s 18876
step    900 | loss 1.5583 | lr 3.00e-04 | grad 1.73 | tok/s 19373
step    910 | loss 1.5486 | lr 3.00e-04 | grad 7.41 | tok/s 19173
step    920 | loss 1.5108 | lr 3.00e-04 | grad 2.23 | tok/s 19402
step    930 | loss 1.3995 | lr 3.00e-04 | grad 2.61 | tok/s 19652
step    940 | loss 1.3772 | lr 3.00e-04 | grad 2.12 | tok/s 19208
step    950 | loss 1.5172 | lr 3.00e-04 | grad 2.70 | tok/s 18876
step    960 | loss 1.4642 | lr 3.00e-04 | grad 1.93 | tok/s 19397
step    970 | loss 1.4909 | lr 3.00e-04 | grad 2.06 | tok/s 19403
step    980 | loss 1.9478 | lr 3.00e-04 | grad 3.70 | tok/s 20194
step    990 | loss 1.6036 | lr 3.00e-04 | grad 2.05 | tok/s 19367
step   1000 | loss 1.6093 | lr 3.00e-04 | grad 2.27 | tok/s 19435
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6093.pt
step   1010 | loss 1.3791 | lr 3.00e-04 | grad 2.03 | tok/s 9303
step   1020 | loss 1.2099 | lr 3.00e-04 | grad 1.65 | tok/s 20513
step   1030 | loss 1.6240 | lr 3.00e-04 | grad 2.22 | tok/s 19340
step   1040 | loss 2.2210 | lr 3.00e-04 | grad 5.22 | tok/s 19965
step   1050 | loss 1.5029 | lr 3.00e-04 | grad 2.17 | tok/s 19713
step   1060 | loss 1.1354 | lr 3.00e-04 | grad 1.58 | tok/s 20078
step   1070 | loss 1.5061 | lr 3.00e-04 | grad 1.91 | tok/s 19689
step   1080 | loss 1.2803 | lr 3.00e-04 | grad 2.03 | tok/s 20324
step   1090 | loss 1.2550 | lr 3.00e-04 | grad 1.84 | tok/s 20305
step   1100 | loss 1.2054 | lr 3.00e-04 | grad 2.02 | tok/s 20271
step   1110 | loss 1.1963 | lr 3.00e-04 | grad 1.81 | tok/s 20269
step   1120 | loss 1.5165 | lr 3.00e-04 | grad 3.80 | tok/s 19673
step   1130 | loss 1.6797 | lr 3.00e-04 | grad 3.64 | tok/s 19903
step   1140 | loss 1.7318 | lr 3.00e-04 | grad 1.60 | tok/s 20121
step   1150 | loss 1.6705 | lr 3.00e-04 | grad 2.30 | tok/s 19126
step   1160 | loss 1.8031 | lr 3.00e-04 | grad 8.06 | tok/s 19408
step   1170 | loss 1.4676 | lr 3.00e-04 | grad 2.14 | tok/s 19102
step   1180 | loss 1.3649 | lr 3.00e-04 | grad 2.39 | tok/s 19799
step   1190 | loss 1.5633 | lr 3.00e-04 | grad 4.91 | tok/s 20140
step   1200 | loss 1.1504 | lr 3.00e-04 | grad 5.19 | tok/s 20228
step   1210 | loss 1.4870 | lr 3.00e-04 | grad 2.66 | tok/s 18814
step   1220 | loss 1.3806 | lr 3.00e-04 | grad 1.91 | tok/s 19562
step   1230 | loss 1.3354 | lr 3.00e-04 | grad 1.49 | tok/s 20033
step   1240 | loss 1.3337 | lr 3.00e-04 | grad 1.73 | tok/s 19690
step   1250 | loss 1.5205 | lr 3.00e-04 | grad 5.62 | tok/s 19743
step   1260 | loss 1.4252 | lr 3.00e-04 | grad 2.91 | tok/s 19945
step   1270 | loss 1.3997 | lr 3.00e-04 | grad 1.78 | tok/s 19538
step   1280 | loss 1.4623 | lr 3.00e-04 | grad 2.11 | tok/s 19225
step   1290 | loss 1.3550 | lr 3.00e-04 | grad 1.88 | tok/s 19290
step   1300 | loss 1.6652 | lr 3.00e-04 | grad 4.53 | tok/s 18968
step   1310 | loss 1.5180 | lr 3.00e-04 | grad 2.03 | tok/s 19649
step   1320 | loss 1.5472 | lr 3.00e-04 | grad 3.41 | tok/s 19787
step   1330 | loss 1.4147 | lr 3.00e-04 | grad 2.33 | tok/s 19517
step   1340 | loss 1.6292 | lr 3.00e-04 | grad 2.17 | tok/s 19149
step   1350 | loss 1.4874 | lr 3.00e-04 | grad 4.66 | tok/s 19432
step   1360 | loss 1.4124 | lr 3.00e-04 | grad 1.81 | tok/s 18920
step   1370 | loss 1.6871 | lr 3.00e-04 | grad 3.22 | tok/s 19842
step   1380 | loss 1.4352 | lr 3.00e-04 | grad 2.78 | tok/s 18887
step   1390 | loss 1.3559 | lr 3.00e-04 | grad 2.75 | tok/s 19885
step   1400 | loss 1.5165 | lr 3.00e-04 | grad 1.59 | tok/s 19225
step   1410 | loss 1.4616 | lr 3.00e-04 | grad 4.06 | tok/s 18784
step   1420 | loss 1.2353 | lr 3.00e-04 | grad 7.84 | tok/s 19968
step   1430 | loss 1.5988 | lr 3.00e-04 | grad 3.23 | tok/s 17367
step   1440 | loss 1.4579 | lr 3.00e-04 | grad 2.30 | tok/s 19807

Training complete! Final step: 1448
