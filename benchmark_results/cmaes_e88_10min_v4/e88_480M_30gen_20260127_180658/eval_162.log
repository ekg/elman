Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min_v4/e88_480M_30gen_20260127_180658/eval_162/levelE88_100m_20260127_213422
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 473,310,408 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3645 | lr 3.00e-04 | grad 14.50 | tok/s 9652
step     20 | loss 3.3858 | lr 3.00e-04 | grad 8.00 | tok/s 19913
step     30 | loss 3.0899 | lr 3.00e-04 | grad 8.31 | tok/s 21009
step     40 | loss 4.6087 | lr 3.00e-04 | grad 28.00 | tok/s 21441
step     50 | loss 4.2881 | lr 3.00e-04 | grad 14.50 | tok/s 21648
step     60 | loss 3.2832 | lr 3.00e-04 | grad 9.12 | tok/s 21569
step     70 | loss 2.8893 | lr 3.00e-04 | grad 5.91 | tok/s 21531
step     80 | loss 2.6230 | lr 3.00e-04 | grad 11.25 | tok/s 21449
step     90 | loss 2.5149 | lr 3.00e-04 | grad 5.09 | tok/s 21439
step    100 | loss 2.2378 | lr 3.00e-04 | grad 5.12 | tok/s 21399
step    110 | loss 2.2362 | lr 3.00e-04 | grad 5.06 | tok/s 21250
step    120 | loss 2.6668 | lr 3.00e-04 | grad 3.25 | tok/s 20181
step    130 | loss 2.0550 | lr 3.00e-04 | grad 6.91 | tok/s 20682
step    140 | loss 2.3536 | lr 3.00e-04 | grad 8.81 | tok/s 20771
step    150 | loss 1.3394 | lr 3.00e-04 | grad 7.16 | tok/s 21222
step    160 | loss 2.2722 | lr 3.00e-04 | grad 3.25 | tok/s 20526
step    170 | loss 2.2889 | lr 3.00e-04 | grad 2.59 | tok/s 20189
step    180 | loss 1.7827 | lr 3.00e-04 | grad 4.12 | tok/s 20677
step    190 | loss 1.8798 | lr 3.00e-04 | grad 3.55 | tok/s 20327
step    200 | loss 1.5966 | lr 3.00e-04 | grad 2.44 | tok/s 21237
step    210 | loss 1.8564 | lr 3.00e-04 | grad 6.94 | tok/s 20154
step    220 | loss 2.1766 | lr 3.00e-04 | grad 5.03 | tok/s 20378
step    230 | loss 1.9876 | lr 3.00e-04 | grad 3.53 | tok/s 20326
step    240 | loss 2.2587 | lr 3.00e-04 | grad 7.88 | tok/s 20605
step    250 | loss 1.7341 | lr 3.00e-04 | grad 2.31 | tok/s 20501
step    260 | loss 1.8693 | lr 3.00e-04 | grad 4.16 | tok/s 21065
step    270 | loss 1.7970 | lr 3.00e-04 | grad 2.88 | tok/s 20605
step    280 | loss 1.7552 | lr 3.00e-04 | grad 2.44 | tok/s 19359
step    290 | loss 1.6541 | lr 3.00e-04 | grad 2.95 | tok/s 20023
step    300 | loss 1.9655 | lr 3.00e-04 | grad 3.08 | tok/s 20161
step    310 | loss 1.6543 | lr 3.00e-04 | grad 2.31 | tok/s 20058
step    320 | loss 1.8752 | lr 3.00e-04 | grad 5.19 | tok/s 20273
step    330 | loss 1.7135 | lr 3.00e-04 | grad 2.55 | tok/s 20476
step    340 | loss 2.0498 | lr 3.00e-04 | grad 2.89 | tok/s 20432
step    350 | loss 1.7002 | lr 3.00e-04 | grad 2.58 | tok/s 20995
step    360 | loss 1.5765 | lr 3.00e-04 | grad 2.31 | tok/s 20119
step    370 | loss 1.4661 | lr 3.00e-04 | grad 2.31 | tok/s 21194
step    380 | loss 1.1979 | lr 3.00e-04 | grad 2.23 | tok/s 21372
step    390 | loss 1.1079 | lr 3.00e-04 | grad 1.94 | tok/s 21368
step    400 | loss 1.7489 | lr 3.00e-04 | grad 2.17 | tok/s 20195
step    410 | loss 1.7775 | lr 3.00e-04 | grad 2.98 | tok/s 20413
step    420 | loss 1.5824 | lr 3.00e-04 | grad 4.81 | tok/s 21271
step    430 | loss 1.6088 | lr 3.00e-04 | grad 2.41 | tok/s 20957
step    440 | loss 1.7131 | lr 3.00e-04 | grad 3.11 | tok/s 20299
step    450 | loss 1.6380 | lr 3.00e-04 | grad 2.22 | tok/s 20516
step    460 | loss 1.6027 | lr 3.00e-04 | grad 2.53 | tok/s 20810
step    470 | loss 1.5668 | lr 3.00e-04 | grad 4.41 | tok/s 20642
step    480 | loss 1.5826 | lr 3.00e-04 | grad 3.56 | tok/s 21093
step    490 | loss 1.7112 | lr 3.00e-04 | grad 3.08 | tok/s 20290
step    500 | loss 1.8123 | lr 3.00e-04 | grad 2.16 | tok/s 20607
step    510 | loss 1.6848 | lr 3.00e-04 | grad 2.12 | tok/s 19697
step    520 | loss 1.5307 | lr 3.00e-04 | grad 2.61 | tok/s 20621
step    530 | loss 1.7191 | lr 3.00e-04 | grad 2.38 | tok/s 20282
step    540 | loss 1.5879 | lr 3.00e-04 | grad 2.11 | tok/s 19846
step    550 | loss 1.3732 | lr 3.00e-04 | grad 4.03 | tok/s 20788
step    560 | loss 1.4506 | lr 3.00e-04 | grad 2.39 | tok/s 21350
step    570 | loss 1.3481 | lr 3.00e-04 | grad 2.44 | tok/s 21337
step    580 | loss 1.3047 | lr 3.00e-04 | grad 1.84 | tok/s 21356
step    590 | loss 1.3356 | lr 3.00e-04 | grad 1.87 | tok/s 20416
step    600 | loss 1.2754 | lr 3.00e-04 | grad 2.06 | tok/s 21379
step    610 | loss 1.3104 | lr 3.00e-04 | grad 2.22 | tok/s 21332
step    620 | loss 1.2993 | lr 3.00e-04 | grad 2.48 | tok/s 21250
step    630 | loss 1.7340 | lr 3.00e-04 | grad 6.75 | tok/s 20108
step    640 | loss 1.7549 | lr 3.00e-04 | grad 2.41 | tok/s 20356
step    650 | loss 1.5613 | lr 3.00e-04 | grad 2.20 | tok/s 20365
step    660 | loss 1.6033 | lr 3.00e-04 | grad 2.33 | tok/s 21159
step    670 | loss 1.6568 | lr 3.00e-04 | grad 6.22 | tok/s 20446
step    680 | loss 1.6553 | lr 3.00e-04 | grad 2.80 | tok/s 20133
step    690 | loss 1.6063 | lr 3.00e-04 | grad 2.38 | tok/s 19951
step    700 | loss 1.4922 | lr 3.00e-04 | grad 1.70 | tok/s 20415
step    710 | loss 1.6691 | lr 3.00e-04 | grad 3.39 | tok/s 20095
step    720 | loss 1.3168 | lr 3.00e-04 | grad 2.17 | tok/s 20877
step    730 | loss 1.4953 | lr 3.00e-04 | grad 1.86 | tok/s 20513
step    740 | loss 1.7831 | lr 3.00e-04 | grad 4.38 | tok/s 21055
step    750 | loss 1.5273 | lr 3.00e-04 | grad 2.08 | tok/s 21298
step    760 | loss 1.5626 | lr 3.00e-04 | grad 4.78 | tok/s 20860
step    770 | loss 1.6136 | lr 3.00e-04 | grad 2.48 | tok/s 20512
step    780 | loss 1.4993 | lr 3.00e-04 | grad 2.41 | tok/s 20652
step    790 | loss 1.6338 | lr 3.00e-04 | grad 5.47 | tok/s 21102
step    800 | loss 1.3353 | lr 3.00e-04 | grad 1.67 | tok/s 20750
step    810 | loss 1.3402 | lr 3.00e-04 | grad 3.27 | tok/s 20076
step    820 | loss 1.4355 | lr 3.00e-04 | grad 2.44 | tok/s 20476
step    830 | loss 1.5134 | lr 3.00e-04 | grad 1.93 | tok/s 20168
step    840 | loss 1.6579 | lr 3.00e-04 | grad 2.31 | tok/s 20106
step    850 | loss 1.5727 | lr 3.00e-04 | grad 1.96 | tok/s 20535
step    860 | loss 1.6138 | lr 3.00e-04 | grad 3.56 | tok/s 20867
step    870 | loss 1.4127 | lr 3.00e-04 | grad 2.23 | tok/s 21017
step    880 | loss 1.6165 | lr 3.00e-04 | grad 2.39 | tok/s 20626
step    890 | loss 1.5112 | lr 3.00e-04 | grad 1.82 | tok/s 20537
step    900 | loss 1.5625 | lr 3.00e-04 | grad 1.91 | tok/s 20445
step    910 | loss 1.5572 | lr 3.00e-04 | grad 8.62 | tok/s 20234
step    920 | loss 1.5128 | lr 3.00e-04 | grad 2.53 | tok/s 20455
step    930 | loss 1.3990 | lr 3.00e-04 | grad 2.69 | tok/s 20726
step    940 | loss 1.3764 | lr 3.00e-04 | grad 2.28 | tok/s 20242
step    950 | loss 1.5168 | lr 3.00e-04 | grad 2.95 | tok/s 19299
step    960 | loss 1.4673 | lr 3.00e-04 | grad 2.12 | tok/s 20449
step    970 | loss 1.4918 | lr 3.00e-04 | grad 2.22 | tok/s 20485
step    980 | loss 1.9345 | lr 3.00e-04 | grad 3.73 | tok/s 21285
step    990 | loss 1.6092 | lr 3.00e-04 | grad 2.14 | tok/s 20427
step   1000 | loss 1.6195 | lr 3.00e-04 | grad 2.53 | tok/s 20468
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6195.pt
step   1010 | loss 1.3799 | lr 3.00e-04 | grad 3.12 | tok/s 9164
step   1020 | loss 1.2188 | lr 3.00e-04 | grad 2.27 | tok/s 21654
step   1030 | loss 1.6680 | lr 3.00e-04 | grad 2.95 | tok/s 20293
step   1040 | loss 2.2009 | lr 3.00e-04 | grad 4.12 | tok/s 21206
step   1050 | loss 1.5236 | lr 3.00e-04 | grad 2.62 | tok/s 20702
step   1060 | loss 1.0842 | lr 3.00e-04 | grad 2.27 | tok/s 21272
step   1070 | loss 1.5253 | lr 3.00e-04 | grad 2.55 | tok/s 20943
step   1080 | loss 1.2681 | lr 3.00e-04 | grad 1.66 | tok/s 21559
step   1090 | loss 1.2541 | lr 3.00e-04 | grad 2.05 | tok/s 21519
step   1100 | loss 1.2022 | lr 3.00e-04 | grad 1.89 | tok/s 21516
step   1110 | loss 1.2507 | lr 3.00e-04 | grad 2.91 | tok/s 21398
step   1120 | loss 1.5174 | lr 3.00e-04 | grad 3.81 | tok/s 21012
step   1130 | loss 1.8055 | lr 3.00e-04 | grad 8.75 | tok/s 21144
step   1140 | loss 1.5457 | lr 3.00e-04 | grad 1.88 | tok/s 21381
step   1150 | loss 1.7429 | lr 3.00e-04 | grad 3.34 | tok/s 20322
step   1160 | loss 1.7771 | lr 3.00e-04 | grad 2.47 | tok/s 20409
step   1170 | loss 1.4589 | lr 3.00e-04 | grad 1.76 | tok/s 20633
step   1180 | loss 1.4328 | lr 3.00e-04 | grad 3.69 | tok/s 21053
step   1190 | loss 1.5229 | lr 3.00e-04 | grad 3.36 | tok/s 21490
step   1200 | loss 1.1375 | lr 3.00e-04 | grad 2.38 | tok/s 21385
step   1210 | loss 1.5041 | lr 3.00e-04 | grad 2.03 | tok/s 20157
step   1220 | loss 1.3722 | lr 3.00e-04 | grad 2.52 | tok/s 20750
step   1230 | loss 1.3385 | lr 3.00e-04 | grad 2.06 | tok/s 21285
step   1240 | loss 1.3297 | lr 3.00e-04 | grad 2.02 | tok/s 20853
step   1250 | loss 1.5161 | lr 3.00e-04 | grad 3.09 | tok/s 21262
step   1260 | loss 1.4589 | lr 3.00e-04 | grad 2.36 | tok/s 21236
step   1270 | loss 1.3846 | lr 3.00e-04 | grad 1.99 | tok/s 20810
step   1280 | loss 1.4818 | lr 3.00e-04 | grad 1.86 | tok/s 20476
step   1290 | loss 1.3771 | lr 3.00e-04 | grad 2.55 | tok/s 20387
step   1300 | loss 1.6664 | lr 3.00e-04 | grad 8.88 | tok/s 20367
step   1310 | loss 1.5514 | lr 3.00e-04 | grad 2.22 | tok/s 20913
step   1320 | loss 1.5679 | lr 3.00e-04 | grad 2.42 | tok/s 21099
step   1330 | loss 1.4233 | lr 3.00e-04 | grad 2.86 | tok/s 20581
step   1340 | loss 1.6046 | lr 3.00e-04 | grad 1.77 | tok/s 20661
step   1350 | loss 1.5504 | lr 3.00e-04 | grad 8.56 | tok/s 20708
step   1360 | loss 1.3558 | lr 3.00e-04 | grad 2.08 | tok/s 20183
step   1370 | loss 1.7355 | lr 3.00e-04 | grad 2.80 | tok/s 21061
step   1380 | loss 1.4614 | lr 3.00e-04 | grad 3.94 | tok/s 20131
step   1390 | loss 1.4186 | lr 3.00e-04 | grad 5.03 | tok/s 20933
step   1400 | loss 1.4300 | lr 3.00e-04 | grad 2.55 | tok/s 20563
step   1410 | loss 1.4422 | lr 3.00e-04 | grad 4.78 | tok/s 20282
step   1420 | loss 1.3491 | lr 3.00e-04 | grad 5.47 | tok/s 21224
step   1430 | loss 1.5623 | lr 3.00e-04 | grad 2.08 | tok/s 20504
step   1440 | loss 1.4592 | lr 3.00e-04 | grad 2.30 | tok/s 21107
step   1450 | loss 1.5165 | lr 3.00e-04 | grad 2.36 | tok/s 20879
step   1460 | loss 1.5947 | lr 3.00e-04 | grad 2.05 | tok/s 20304
step   1470 | loss 1.3882 | lr 3.00e-04 | grad 3.80 | tok/s 20024
step   1480 | loss 1.3692 | lr 3.00e-04 | grad 1.73 | tok/s 21013
step   1490 | loss 2.0214 | lr 3.00e-04 | grad 2.06 | tok/s 20737
step   1500 | loss 1.4398 | lr 3.00e-04 | grad 4.12 | tok/s 20821
step   1510 | loss 1.2783 | lr 3.00e-04 | grad 2.23 | tok/s 19928
step   1520 | loss 1.5047 | lr 3.00e-04 | grad 2.64 | tok/s 20635
step   1530 | loss 1.4882 | lr 3.00e-04 | grad 4.59 | tok/s 20935

Training complete! Final step: 1530
