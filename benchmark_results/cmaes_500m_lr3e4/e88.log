Using device: cuda
Output directory: benchmark_results/cmaes_500m_lr3e4/e88_h68n16/levelE88_100m_20260125_024116
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 499,300,624 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.4961 | lr 3.00e-04 | grad 33.75 | tok/s 9398
step     20 | loss 3.5628 | lr 3.00e-04 | grad 16.62 | tok/s 19622
step     30 | loss 3.4721 | lr 3.00e-04 | grad 13.75 | tok/s 19534
step     40 | loss 3.4558 | lr 3.00e-04 | grad 47.50 | tok/s 20111
step     50 | loss 5.1671 | lr 3.00e-04 | grad 25.38 | tok/s 20832
step     60 | loss 3.9344 | lr 3.00e-04 | grad 22.00 | tok/s 20752
step     70 | loss 3.3322 | lr 3.00e-04 | grad 14.12 | tok/s 20651
step     80 | loss 3.1117 | lr 3.00e-04 | grad 12.81 | tok/s 20485
step     90 | loss 2.8176 | lr 3.00e-04 | grad 12.38 | tok/s 20397
step    100 | loss 2.6545 | lr 3.00e-04 | grad 9.44 | tok/s 20277
step    110 | loss 2.4442 | lr 3.00e-04 | grad 6.75 | tok/s 20046
step    120 | loss 3.0768 | lr 3.00e-04 | grad 5.62 | tok/s 19339
step    130 | loss 2.6137 | lr 3.00e-04 | grad 6.22 | tok/s 18814
step    140 | loss 2.3181 | lr 3.00e-04 | grad 9.00 | tok/s 18824
step    150 | loss 2.0623 | lr 3.00e-04 | grad 13.62 | tok/s 19329
step    160 | loss 2.2981 | lr 3.00e-04 | grad 8.12 | tok/s 19349
step    170 | loss 2.4782 | lr 3.00e-04 | grad 11.12 | tok/s 18231
step    180 | loss 2.0520 | lr 3.00e-04 | grad 10.19 | tok/s 18798
step    190 | loss 1.9313 | lr 3.00e-04 | grad 6.78 | tok/s 17961
step    200 | loss 1.9435 | lr 3.00e-04 | grad 4.69 | tok/s 19178
step    210 | loss 1.7499 | lr 3.00e-04 | grad 5.34 | tok/s 18554
step    220 | loss 2.4032 | lr 3.00e-04 | grad 6.53 | tok/s 17824
step    230 | loss 2.2797 | lr 3.00e-04 | grad 3.88 | tok/s 16893
step    240 | loss 2.0368 | lr 3.00e-04 | grad 6.56 | tok/s 17858
step    250 | loss 2.1942 | lr 3.00e-04 | grad 4.16 | tok/s 17864
step    260 | loss 1.9006 | lr 3.00e-04 | grad 3.66 | tok/s 18380
step    270 | loss 2.0072 | lr 3.00e-04 | grad 3.58 | tok/s 18387
step    280 | loss 1.8551 | lr 3.00e-04 | grad 3.70 | tok/s 17744
step    290 | loss 1.7849 | lr 3.00e-04 | grad 5.44 | tok/s 17045
step    300 | loss 1.8582 | lr 3.00e-04 | grad 3.53 | tok/s 17256
step    310 | loss 1.8757 | lr 3.00e-04 | grad 3.42 | tok/s 17634
step    320 | loss 1.7007 | lr 3.00e-04 | grad 4.72 | tok/s 16838
step    330 | loss 1.9160 | lr 3.00e-04 | grad 4.06 | tok/s 17646
step    340 | loss 2.0085 | lr 3.00e-04 | grad 5.06 | tok/s 17959
step    350 | loss 1.9537 | lr 3.00e-04 | grad 4.19 | tok/s 17616
step    360 | loss 1.6673 | lr 3.00e-04 | grad 4.47 | tok/s 17985
step    370 | loss 1.5308 | lr 3.00e-04 | grad 2.89 | tok/s 17649
step    380 | loss 1.5223 | lr 3.00e-04 | grad 3.42 | tok/s 18436
step    390 | loss 1.2213 | lr 3.00e-04 | grad 2.98 | tok/s 18517
step    400 | loss 1.1678 | lr 3.00e-04 | grad 2.86 | tok/s 18228
step    410 | loss 1.9147 | lr 3.00e-04 | grad 4.00 | tok/s 17588
step    420 | loss 1.7618 | lr 3.00e-04 | grad 3.20 | tok/s 17577
step    430 | loss 1.6724 | lr 3.00e-04 | grad 10.94 | tok/s 18464
step    440 | loss 1.6057 | lr 3.00e-04 | grad 3.55 | tok/s 17887
step    450 | loss 1.8035 | lr 3.00e-04 | grad 2.84 | tok/s 17626
step    460 | loss 1.6288 | lr 3.00e-04 | grad 7.00 | tok/s 17463
step    470 | loss 1.6599 | lr 3.00e-04 | grad 3.94 | tok/s 17486
step    480 | loss 1.6190 | lr 3.00e-04 | grad 3.61 | tok/s 18334
step    490 | loss 1.5816 | lr 3.00e-04 | grad 2.48 | tok/s 17711
step    500 | loss 1.6767 | lr 3.00e-04 | grad 2.62 | tok/s 17597
step    510 | loss 1.9377 | lr 3.00e-04 | grad 15.31 | tok/s 17316
step    520 | loss 1.7134 | lr 3.00e-04 | grad 3.34 | tok/s 16561
step    530 | loss 1.5506 | lr 3.00e-04 | grad 2.66 | tok/s 17554
step    540 | loss 1.7740 | lr 3.00e-04 | grad 2.84 | tok/s 17489
step    550 | loss 1.6252 | lr 3.00e-04 | grad 3.05 | tok/s 17080
step    560 | loss 1.4279 | lr 3.00e-04 | grad 3.00 | tok/s 17856
step    570 | loss 1.4787 | lr 3.00e-04 | grad 3.03 | tok/s 18456
step    580 | loss 1.3903 | lr 3.00e-04 | grad 2.92 | tok/s 18410
step    590 | loss 1.3411 | lr 3.00e-04 | grad 3.20 | tok/s 18378
step    600 | loss 1.3981 | lr 3.00e-04 | grad 3.16 | tok/s 18358
step    610 | loss 1.3275 | lr 3.00e-04 | grad 3.17 | tok/s 18427
step    620 | loss 1.3476 | lr 3.00e-04 | grad 2.78 | tok/s 18407
step    630 | loss 1.4242 | lr 3.00e-04 | grad 6.28 | tok/s 18109
step    640 | loss 1.7418 | lr 3.00e-04 | grad 4.41 | tok/s 17305
step    650 | loss 1.7169 | lr 3.00e-04 | grad 3.00 | tok/s 17181
step    660 | loss 1.5887 | lr 3.00e-04 | grad 3.78 | tok/s 17364
step    670 | loss 1.6077 | lr 3.00e-04 | grad 2.77 | tok/s 17968
step    680 | loss 1.7032 | lr 3.00e-04 | grad 3.03 | tok/s 17334
step    690 | loss 1.6955 | lr 3.00e-04 | grad 3.03 | tok/s 17220
step    700 | loss 1.6679 | lr 3.00e-04 | grad 2.81 | tok/s 17061
step    710 | loss 1.5185 | lr 3.00e-04 | grad 2.30 | tok/s 17524
step    720 | loss 1.7249 | lr 3.00e-04 | grad 3.52 | tok/s 17150
step    730 | loss 1.3667 | lr 3.00e-04 | grad 2.94 | tok/s 17916
step    740 | loss 1.4969 | lr 3.00e-04 | grad 2.61 | tok/s 17433
step    750 | loss 1.8233 | lr 3.00e-04 | grad 6.69 | tok/s 18123
step    760 | loss 1.5463 | lr 3.00e-04 | grad 2.86 | tok/s 18137
step    770 | loss 1.5617 | lr 3.00e-04 | grad 3.06 | tok/s 17708
step    780 | loss 1.5958 | lr 3.00e-04 | grad 2.88 | tok/s 17227
step    790 | loss 1.5288 | lr 3.00e-04 | grad 3.83 | tok/s 17625
step    800 | loss 1.6730 | lr 3.00e-04 | grad 6.88 | tok/s 18156
step    810 | loss 1.4294 | lr 3.00e-04 | grad 4.22 | tok/s 17577
step    820 | loss 1.3128 | lr 3.00e-04 | grad 4.41 | tok/s 17144
step    830 | loss 1.4161 | lr 3.00e-04 | grad 3.44 | tok/s 17376
step    840 | loss 1.5693 | lr 3.00e-04 | grad 2.66 | tok/s 17037
step    850 | loss 1.6464 | lr 3.00e-04 | grad 3.05 | tok/s 17106
step    860 | loss 1.6934 | lr 3.00e-04 | grad 3.30 | tok/s 17268
step    870 | loss 1.6114 | lr 3.00e-04 | grad 5.12 | tok/s 17418
step    880 | loss 1.4853 | lr 3.00e-04 | grad 2.88 | tok/s 18260
step    890 | loss 1.6592 | lr 3.00e-04 | grad 5.31 | tok/s 17336
step    900 | loss 1.5636 | lr 3.00e-04 | grad 2.72 | tok/s 17305
step    910 | loss 1.5512 | lr 3.00e-04 | grad 3.41 | tok/s 16901
step    920 | loss 1.5885 | lr 3.00e-04 | grad 2.31 | tok/s 17294
step    930 | loss 1.6056 | lr 3.00e-04 | grad 2.62 | tok/s 17352
step    940 | loss 1.4355 | lr 3.00e-04 | grad 3.89 | tok/s 17848
step    950 | loss 1.3863 | lr 3.00e-04 | grad 2.58 | tok/s 17080
step    960 | loss 1.5311 | lr 3.00e-04 | grad 2.95 | tok/s 16825
step    970 | loss 1.4962 | lr 3.00e-04 | grad 2.84 | tok/s 17031
step    980 | loss 1.4969 | lr 3.00e-04 | grad 3.50 | tok/s 17491
step    990 | loss 1.9310 | lr 3.00e-04 | grad 3.77 | tok/s 18223
step   1000 | loss 1.6393 | lr 3.00e-04 | grad 2.17 | tok/s 17470
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6393.pt
step   1010 | loss 1.6027 | lr 3.00e-04 | grad 3.11 | tok/s 9683
step   1020 | loss 1.3153 | lr 3.00e-04 | grad 2.66 | tok/s 17775
step   1030 | loss 1.2356 | lr 3.00e-04 | grad 3.11 | tok/s 18154
step   1040 | loss 1.3775 | lr 3.00e-04 | grad 2.39 | tok/s 18175
step   1050 | loss 1.7845 | lr 3.00e-04 | grad 7.19 | tok/s 17355
step   1060 | loss 2.0910 | lr 3.00e-04 | grad 2.23 | tok/s 18215
step   1070 | loss 1.5303 | lr 3.00e-04 | grad 3.59 | tok/s 17545
step   1080 | loss 1.1487 | lr 3.00e-04 | grad 3.41 | tok/s 17921
step   1090 | loss 1.4568 | lr 3.00e-04 | grad 4.19 | tok/s 17992
step   1100 | loss 1.2961 | lr 3.00e-04 | grad 2.92 | tok/s 18435
step   1110 | loss 1.2723 | lr 3.00e-04 | grad 2.50 | tok/s 18406
step   1120 | loss 1.2502 | lr 3.00e-04 | grad 2.75 | tok/s 18393
step   1130 | loss 1.3085 | lr 3.00e-04 | grad 2.36 | tok/s 17841
step   1140 | loss 1.5274 | lr 3.00e-04 | grad 4.41 | tok/s 18075
step   1150 | loss 1.8739 | lr 3.00e-04 | grad 5.72 | tok/s 17750
step   1160 | loss 1.4730 | lr 3.00e-04 | grad 2.70 | tok/s 17703
step   1170 | loss 1.6533 | lr 3.00e-04 | grad 2.47 | tok/s 17296
step   1180 | loss 1.8295 | lr 3.00e-04 | grad 2.94 | tok/s 17403
step   1190 | loss 1.4688 | lr 3.00e-04 | grad 2.11 | tok/s 16834
step   1200 | loss 1.5034 | lr 3.00e-04 | grad 5.25 | tok/s 17960
step   1210 | loss 1.5151 | lr 3.00e-04 | grad 2.20 | tok/s 18328
step   1220 | loss 1.1706 | lr 3.00e-04 | grad 2.50 | tok/s 18106
step   1230 | loss 1.5200 | lr 3.00e-04 | grad 2.36 | tok/s 17058
step   1240 | loss 1.3911 | lr 3.00e-04 | grad 3.06 | tok/s 17533
step   1250 | loss 1.3247 | lr 3.00e-04 | grad 3.00 | tok/s 17909
step   1260 | loss 1.3652 | lr 3.00e-04 | grad 2.28 | tok/s 17769
step   1270 | loss 1.5116 | lr 3.00e-04 | grad 3.48 | tok/s 17935
step   1280 | loss 1.4652 | lr 3.00e-04 | grad 3.09 | tok/s 17737
step   1290 | loss 1.4158 | lr 3.00e-04 | grad 2.73 | tok/s 17660
step   1300 | loss 1.4496 | lr 3.00e-04 | grad 2.28 | tok/s 17372
step   1310 | loss 1.3880 | lr 3.00e-04 | grad 2.62 | tok/s 17224
step   1320 | loss 1.7018 | lr 3.00e-04 | grad 3.89 | tok/s 17246
step   1330 | loss 1.5835 | lr 3.00e-04 | grad 3.08 | tok/s 17721
step   1340 | loss 1.5846 | lr 3.00e-04 | grad 2.95 | tok/s 17975

Training complete! Final step: 1340
