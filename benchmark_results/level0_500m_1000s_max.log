Using p50k_base tokenizer with vocab size 50,281
Loading data from data/fineweb_500mb.txt...
Loaded 113,733,867 tokens from cache: data/fineweb_500mb.txt.p50k_base.tokens.npy

Creating 0 model with ~500m parameters...
Created Level 0 model: dim=1024, depth=18, params=499,885,056

============================================================
Training: 0 (timeout=1000.0s)
Parameters: 499.89M
Vocab size: 50,281
============================================================
[0] step    1 | loss 11.0000 | ppl 59874.1 | grad 4.16 | 27855 tok/s | 1.9s | 1901ms/step
Traceback (most recent call last):
  File "/home/erikg/elman/benchmark_baselines.py", line 411, in <module>
    main()
  File "/home/erikg/elman/benchmark_baselines.py", line 375, in main
    results = train_model(model, dataset, args, model_name, output_dir, use_batched_dataset=use_batched)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/erikg/elman/benchmark_baselines.py", line 187, in train_model
    loss.backward()
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.99 GiB. GPU 0 has a total capacity of 47.38 GiB of which 2.93 GiB is free. Including non-PyTorch memory, this process has 44.44 GiB memory in use. Of the allocated memory 39.73 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
