Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b8k4/level87b8k4_100m_20260119_152329
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b8k4, 49,305,600 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.5673 | lr 2.70e-06 | grad 28.38 | tok/s 19449
step     20 | loss 5.5275 | lr 5.70e-06 | grad 12.06 | tok/s 49013
step     30 | loss 5.7469 | lr 8.70e-06 | grad 12.38 | tok/s 51754
step     40 | loss 5.6961 | lr 1.17e-05 | grad 14.56 | tok/s 51758
step     50 | loss 5.5964 | lr 1.47e-05 | grad 11.00 | tok/s 51703
step     60 | loss 5.4143 | lr 1.77e-05 | grad 43.75 | tok/s 50689
step     70 | loss 5.0922 | lr 2.07e-05 | grad 43.25 | tok/s 48802
step     80 | loss 4.8524 | lr 2.37e-05 | grad 12.62 | tok/s 50672
step     90 | loss 4.8726 | lr 2.67e-05 | grad 12.38 | tok/s 48829
step    100 | loss 4.5707 | lr 2.97e-05 | grad 9.19 | tok/s 49309
step    110 | loss 4.2157 | lr 3.27e-05 | grad 9.75 | tok/s 48520
step    120 | loss 4.0592 | lr 3.57e-05 | grad 10.88 | tok/s 48032
step    130 | loss 3.8802 | lr 3.87e-05 | grad 7.91 | tok/s 48940
step    140 | loss 3.5212 | lr 4.17e-05 | grad 8.56 | tok/s 49026
step    150 | loss 3.2688 | lr 4.47e-05 | grad 6.44 | tok/s 46745
step    160 | loss 3.1099 | lr 4.77e-05 | grad 4.69 | tok/s 47172
step    170 | loss 3.1949 | lr 5.07e-05 | grad 10.50 | tok/s 48816
step    180 | loss 3.2346 | lr 5.37e-05 | grad 7.03 | tok/s 48998
step    190 | loss 3.0067 | lr 5.67e-05 | grad 11.31 | tok/s 49697
step    200 | loss 2.9416 | lr 5.97e-05 | grad 7.47 | tok/s 50827
step    210 | loss 2.9869 | lr 6.27e-05 | grad 8.25 | tok/s 48523
step    220 | loss 3.0273 | lr 6.57e-05 | grad 5.84 | tok/s 50104
step    230 | loss 2.8057 | lr 6.87e-05 | grad 6.75 | tok/s 48502
step    240 | loss 2.9204 | lr 7.17e-05 | grad 7.34 | tok/s 49445
step    250 | loss 2.7918 | lr 7.47e-05 | grad 4.97 | tok/s 48681
step    260 | loss 2.7622 | lr 7.77e-05 | grad 6.09 | tok/s 46719
step    270 | loss 2.6699 | lr 8.07e-05 | grad 6.22 | tok/s 48404
step    280 | loss 2.6186 | lr 8.37e-05 | grad 6.19 | tok/s 48411
step    290 | loss 2.5953 | lr 8.67e-05 | grad 3.80 | tok/s 51072
step    300 | loss 2.5124 | lr 8.97e-05 | grad 3.53 | tok/s 50922
step    310 | loss 2.4427 | lr 9.27e-05 | grad 3.64 | tok/s 50844
step    320 | loss 2.4875 | lr 9.57e-05 | grad 8.94 | tok/s 48939
step    330 | loss 2.5842 | lr 9.87e-05 | grad 9.12 | tok/s 47789
step    340 | loss 2.5906 | lr 1.02e-04 | grad 5.34 | tok/s 48750
step    350 | loss 2.5684 | lr 1.05e-04 | grad 6.22 | tok/s 47412
step    360 | loss 2.5557 | lr 1.08e-04 | grad 6.31 | tok/s 47991
step    370 | loss 2.4521 | lr 1.11e-04 | grad 5.53 | tok/s 48858
step    380 | loss 2.8954 | lr 1.14e-04 | grad 4.59 | tok/s 50145
step    390 | loss 2.4657 | lr 1.17e-04 | grad 5.47 | tok/s 48289
step    400 | loss 2.5924 | lr 1.20e-04 | grad 10.12 | tok/s 49572
step    410 | loss 2.3376 | lr 1.23e-04 | grad 4.66 | tok/s 48060
step    420 | loss 2.4449 | lr 1.26e-04 | grad 3.62 | tok/s 47696
step    430 | loss 2.5367 | lr 1.29e-04 | grad 10.50 | tok/s 47577
step    440 | loss 2.7321 | lr 1.32e-04 | grad 4.19 | tok/s 49441
step    450 | loss 2.3793 | lr 1.35e-04 | grad 2.59 | tok/s 48083
step    460 | loss 2.3818 | lr 1.38e-04 | grad 3.61 | tok/s 48303
step    470 | loss 2.4016 | lr 1.41e-04 | grad 4.12 | tok/s 48816
step    480 | loss 2.2806 | lr 1.44e-04 | grad 2.39 | tok/s 47118
step    490 | loss 2.1982 | lr 1.47e-04 | grad 2.91 | tok/s 47987
step    500 | loss 3.0276 | lr 1.50e-04 | grad 57.50 | tok/s 49522
step    510 | loss 2.3405 | lr 1.53e-04 | grad 6.16 | tok/s 48372
step    520 | loss 2.4242 | lr 1.56e-04 | grad 2.83 | tok/s 49939
step    530 | loss 2.7167 | lr 1.59e-04 | grad 5.06 | tok/s 48841
step    540 | loss 2.3530 | lr 1.62e-04 | grad 3.27 | tok/s 48765
step    550 | loss 2.2062 | lr 1.65e-04 | grad 2.25 | tok/s 50205
step    560 | loss 2.0354 | lr 1.68e-04 | grad 1.69 | tok/s 50855
step    570 | loss 2.3340 | lr 1.71e-04 | grad 5.78 | tok/s 49713
step    580 | loss 2.6265 | lr 1.74e-04 | grad 3.02 | tok/s 48978
step    590 | loss 2.7651 | lr 1.77e-04 | grad 5.34 | tok/s 48043
step    600 | loss 2.3880 | lr 1.80e-04 | grad 5.31 | tok/s 48149
step    610 | loss 2.4517 | lr 1.83e-04 | grad 2.94 | tok/s 50489
step    620 | loss 2.2184 | lr 1.86e-04 | grad 1.52 | tok/s 47923
step    630 | loss 2.1919 | lr 1.89e-04 | grad 1.75 | tok/s 49440
step    640 | loss 2.7063 | lr 1.92e-04 | grad 2.00 | tok/s 49481
step    650 | loss 2.2265 | lr 1.95e-04 | grad 2.50 | tok/s 48594
step    660 | loss 2.5242 | lr 1.98e-04 | grad 6.94 | tok/s 47964
step    670 | loss 2.3983 | lr 2.01e-04 | grad 3.05 | tok/s 49637
step    680 | loss 2.3314 | lr 2.04e-04 | grad 2.84 | tok/s 47832
step    690 | loss 2.3257 | lr 2.07e-04 | grad 2.23 | tok/s 48266
step    700 | loss 2.3990 | lr 2.10e-04 | grad 2.16 | tok/s 48498
step    710 | loss 2.3376 | lr 2.13e-04 | grad 1.59 | tok/s 48772
step    720 | loss 2.4057 | lr 2.16e-04 | grad 4.66 | tok/s 48585
step    730 | loss 2.4628 | lr 2.19e-04 | grad 2.09 | tok/s 49049
step    740 | loss 2.2971 | lr 2.22e-04 | grad 3.44 | tok/s 48583
step    750 | loss 2.1271 | lr 2.25e-04 | grad 2.23 | tok/s 48074
step    760 | loss 2.6769 | lr 2.28e-04 | grad 4.12 | tok/s 48596
step    770 | loss 2.1042 | lr 2.31e-04 | grad 4.56 | tok/s 48301
step    780 | loss 2.1552 | lr 2.34e-04 | grad 1.99 | tok/s 48823
step    790 | loss 2.1418 | lr 2.37e-04 | grad 1.21 | tok/s 49275
step    800 | loss 2.1578 | lr 2.40e-04 | grad 1.66 | tok/s 49310
step    810 | loss 2.2616 | lr 2.43e-04 | grad 16.25 | tok/s 48886
step    820 | loss 2.7475 | lr 2.46e-04 | grad 2.23 | tok/s 50092
step    830 | loss 2.3061 | lr 2.49e-04 | grad 2.67 | tok/s 50918
step    840 | loss 2.0234 | lr 2.52e-04 | grad 1.34 | tok/s 50910
step    850 | loss 2.5065 | lr 2.55e-04 | grad 1.84 | tok/s 48448
step    860 | loss 2.2828 | lr 2.58e-04 | grad 1.52 | tok/s 47512
step    870 | loss 2.1641 | lr 2.61e-04 | grad 2.42 | tok/s 48875
step    880 | loss 2.2452 | lr 2.64e-04 | grad 2.17 | tok/s 48647
step    890 | loss 2.0665 | lr 2.67e-04 | grad 1.09 | tok/s 48631
step    900 | loss 2.5915 | lr 2.70e-04 | grad 1.10 | tok/s 47363
step    910 | loss 2.1174 | lr 2.73e-04 | grad 1.12 | tok/s 48204
step    920 | loss 2.0465 | lr 2.76e-04 | grad 1.00 | tok/s 48005
step    930 | loss 2.1452 | lr 2.79e-04 | grad 1.51 | tok/s 47955
step    940 | loss 2.0312 | lr 2.82e-04 | grad 1.72 | tok/s 47467
step    950 | loss 2.1167 | lr 2.85e-04 | grad 1.60 | tok/s 48612
step    960 | loss 1.8250 | lr 2.88e-04 | grad 0.63 | tok/s 50935
step    970 | loss 1.6308 | lr 2.91e-04 | grad 0.70 | tok/s 50878
step    980 | loss 1.8350 | lr 2.94e-04 | grad 2.36 | tok/s 49435
step    990 | loss 2.2579 | lr 2.97e-04 | grad 1.03 | tok/s 48140
step   1000 | loss 2.0592 | lr 3.00e-04 | grad 0.78 | tok/s 47042
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0592.pt
step   1010 | loss 2.3849 | lr 1.06e-06 | grad 4.25 | tok/s 41949
step   1020 | loss 1.9929 | lr 1.27e-06 | grad 0.79 | tok/s 46247
step   1030 | loss 2.2648 | lr 1.62e-06 | grad 0.84 | tok/s 45251
step   1040 | loss 1.9609 | lr 2.12e-06 | grad 1.55 | tok/s 46437
step   1050 | loss 1.9929 | lr 2.77e-06 | grad 0.88 | tok/s 46484
step   1060 | loss 2.4587 | lr 3.56e-06 | grad 3.83 | tok/s 46566
step   1070 | loss 2.5227 | lr 4.50e-06 | grad 1.08 | tok/s 46588
step   1080 | loss 2.6261 | lr 5.58e-06 | grad 1.31 | tok/s 46036
step   1090 | loss 2.3630 | lr 6.81e-06 | grad 0.98 | tok/s 46571
step   1100 | loss 2.0313 | lr 8.17e-06 | grad 0.96 | tok/s 46298
step   1110 | loss 2.1635 | lr 9.68e-06 | grad 1.12 | tok/s 47075
step   1120 | loss 2.6099 | lr 1.13e-05 | grad 1.01 | tok/s 47598
step   1130 | loss 2.0173 | lr 1.31e-05 | grad 0.78 | tok/s 45064
step   1140 | loss 1.9300 | lr 1.50e-05 | grad 0.84 | tok/s 46490
step   1150 | loss 2.2419 | lr 1.71e-05 | grad 1.24 | tok/s 46351
step   1160 | loss 1.8738 | lr 1.93e-05 | grad 0.86 | tok/s 45755
step   1170 | loss 2.2517 | lr 2.16e-05 | grad 1.13 | tok/s 46329
step   1180 | loss 1.9275 | lr 2.40e-05 | grad 0.94 | tok/s 48586
step   1190 | loss 1.9018 | lr 2.66e-05 | grad 0.74 | tok/s 48791
step   1200 | loss 1.8521 | lr 2.93e-05 | grad 0.71 | tok/s 48770
step   1210 | loss 1.8354 | lr 3.21e-05 | grad 0.70 | tok/s 48702
step   1220 | loss 1.8540 | lr 3.50e-05 | grad 0.89 | tok/s 48238
step   1230 | loss 1.7962 | lr 3.80e-05 | grad 0.79 | tok/s 46573
step   1240 | loss 1.9398 | lr 4.12e-05 | grad 0.80 | tok/s 45794
step   1250 | loss 2.0771 | lr 4.45e-05 | grad 4.25 | tok/s 47241
step   1260 | loss 2.0840 | lr 4.78e-05 | grad 2.28 | tok/s 47111
step   1270 | loss 2.1504 | lr 5.13e-05 | grad 1.36 | tok/s 46592
step   1280 | loss 2.0416 | lr 5.48e-05 | grad 0.95 | tok/s 46219
step   1290 | loss 1.9375 | lr 5.85e-05 | grad 1.00 | tok/s 46052
step   1300 | loss 1.9810 | lr 6.22e-05 | grad 0.85 | tok/s 45764
step   1310 | loss 2.1016 | lr 6.61e-05 | grad 0.93 | tok/s 45742
step   1320 | loss 2.0104 | lr 7.00e-05 | grad 1.23 | tok/s 46340
step   1330 | loss 1.9434 | lr 7.40e-05 | grad 0.84 | tok/s 46559
step   1340 | loss 1.9021 | lr 7.81e-05 | grad 1.09 | tok/s 46548
step   1350 | loss 1.9836 | lr 8.22e-05 | grad 2.34 | tok/s 47780
step   1360 | loss 1.8383 | lr 8.64e-05 | grad 0.81 | tok/s 45371
step   1370 | loss 1.9969 | lr 9.07e-05 | grad 0.63 | tok/s 46091
step   1380 | loss 2.0423 | lr 9.50e-05 | grad 1.11 | tok/s 46512
step   1390 | loss 1.9258 | lr 9.94e-05 | grad 1.58 | tok/s 45396
step   1400 | loss 2.0263 | lr 1.04e-04 | grad 8.81 | tok/s 47218
step   1410 | loss 2.0915 | lr 1.08e-04 | grad 1.06 | tok/s 47580
step   1420 | loss 2.0117 | lr 1.13e-04 | grad 1.08 | tok/s 45339
step   1430 | loss 1.8026 | lr 1.17e-04 | grad 1.08 | tok/s 44166
step   1440 | loss 1.7496 | lr 1.22e-04 | grad 0.88 | tok/s 46755
step   1450 | loss 1.8400 | lr 1.27e-04 | grad 2.19 | tok/s 47778
step   1460 | loss 1.8529 | lr 1.31e-04 | grad 0.75 | tok/s 44558
step   1470 | loss 1.9725 | lr 1.36e-04 | grad 2.95 | tok/s 46212
step   1480 | loss 1.8477 | lr 1.41e-04 | grad 2.17 | tok/s 46716
step   1490 | loss 2.0278 | lr 1.45e-04 | grad 2.41 | tok/s 46557
step   1500 | loss 2.1018 | lr 1.50e-04 | grad 2.20 | tok/s 45214
step   1510 | loss 2.0005 | lr 1.55e-04 | grad 1.33 | tok/s 47546
step   1520 | loss 1.9739 | lr 1.59e-04 | grad 1.25 | tok/s 47126
step   1530 | loss 1.9031 | lr 1.64e-04 | grad 0.84 | tok/s 46899
step   1540 | loss 1.8730 | lr 1.69e-04 | grad 0.75 | tok/s 46095
step   1550 | loss 1.8768 | lr 1.73e-04 | grad 2.39 | tok/s 47886
step   1560 | loss 2.3907 | lr 1.78e-04 | grad 1.33 | tok/s 46611
step   1570 | loss 1.8747 | lr 1.83e-04 | grad 1.34 | tok/s 45833
step   1580 | loss 2.0551 | lr 1.87e-04 | grad 4.09 | tok/s 47335
step   1590 | loss 1.7862 | lr 1.92e-04 | grad 0.95 | tok/s 46140
step   1600 | loss 1.8639 | lr 1.96e-04 | grad 1.09 | tok/s 45168
step   1610 | loss 1.7138 | lr 2.01e-04 | grad 0.96 | tok/s 47765
step   1620 | loss 1.8666 | lr 2.05e-04 | grad 0.85 | tok/s 47168
step   1630 | loss 1.8811 | lr 2.09e-04 | grad 1.02 | tok/s 47593
step   1640 | loss 1.8046 | lr 2.14e-04 | grad 0.85 | tok/s 45932
step   1650 | loss 1.8402 | lr 2.18e-04 | grad 1.54 | tok/s 45257
step   1660 | loss 1.8339 | lr 2.22e-04 | grad 0.89 | tok/s 45630
step   1670 | loss 1.9320 | lr 2.26e-04 | grad 2.02 | tok/s 47579
step   1680 | loss 2.3243 | lr 2.30e-04 | grad 0.85 | tok/s 47493
step   1690 | loss 1.8575 | lr 2.34e-04 | grad 1.23 | tok/s 46278
step   1700 | loss 2.3240 | lr 2.38e-04 | grad 1.19 | tok/s 47349
step   1710 | loss 1.9453 | lr 2.42e-04 | grad 1.16 | tok/s 46025
step   1720 | loss 1.9483 | lr 2.45e-04 | grad 1.20 | tok/s 46229
step   1730 | loss 2.1509 | lr 2.49e-04 | grad 1.19 | tok/s 46270
step   1740 | loss 1.8929 | lr 2.52e-04 | grad 0.80 | tok/s 46822
step   1750 | loss 1.8198 | lr 2.56e-04 | grad 0.76 | tok/s 45270
step   1760 | loss 2.1120 | lr 2.59e-04 | grad 0.90 | tok/s 45891
step   1770 | loss 1.9823 | lr 2.62e-04 | grad 0.83 | tok/s 47005
step   1780 | loss 1.8479 | lr 2.65e-04 | grad 1.25 | tok/s 45068
step   1790 | loss 2.0756 | lr 2.68e-04 | grad 1.04 | tok/s 45867
step   1800 | loss 1.7530 | lr 2.71e-04 | grad 0.76 | tok/s 46752
step   1810 | loss 1.8356 | lr 2.74e-04 | grad 0.95 | tok/s 46678

Training complete! Final step: 1813
