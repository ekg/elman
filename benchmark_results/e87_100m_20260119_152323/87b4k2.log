Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b4k2/level87b4k2_100m_20260119_152330
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b4k2, 52,770,176 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.7288 | lr 2.70e-06 | grad 44.75 | tok/s 17572
step     20 | loss 5.6649 | lr 5.70e-06 | grad 17.75 | tok/s 39381
step     30 | loss 5.8207 | lr 8.70e-06 | grad 17.75 | tok/s 41691
step     40 | loss 5.7684 | lr 1.17e-05 | grad 20.50 | tok/s 41677
step     50 | loss 5.6903 | lr 1.47e-05 | grad 15.44 | tok/s 41726
step     60 | loss 5.4889 | lr 1.77e-05 | grad 72.50 | tok/s 40811
step     70 | loss 5.1125 | lr 2.07e-05 | grad 91.00 | tok/s 39399
step     80 | loss 4.7681 | lr 2.37e-05 | grad 14.31 | tok/s 40922
step     90 | loss 4.7319 | lr 2.67e-05 | grad 16.38 | tok/s 39522
step    100 | loss 4.4198 | lr 2.97e-05 | grad 9.56 | tok/s 39862
step    110 | loss 4.1491 | lr 3.27e-05 | grad 11.69 | tok/s 39294
step    120 | loss 4.0540 | lr 3.57e-05 | grad 15.19 | tok/s 38714
step    130 | loss 3.9057 | lr 3.87e-05 | grad 5.72 | tok/s 39625
step    140 | loss 3.5695 | lr 4.17e-05 | grad 10.12 | tok/s 39719
step    150 | loss 3.3655 | lr 4.47e-05 | grad 6.84 | tok/s 37871
step    160 | loss 3.2024 | lr 4.77e-05 | grad 3.61 | tok/s 38134
step    170 | loss 3.3049 | lr 5.07e-05 | grad 11.00 | tok/s 39394
step    180 | loss 3.3213 | lr 5.37e-05 | grad 4.12 | tok/s 39502
step    190 | loss 3.1310 | lr 5.67e-05 | grad 4.91 | tok/s 40148
step    200 | loss 3.0372 | lr 5.97e-05 | grad 3.84 | tok/s 40914
step    210 | loss 3.0351 | lr 6.27e-05 | grad 4.53 | tok/s 39089
step    220 | loss 3.0574 | lr 6.57e-05 | grad 3.39 | tok/s 40411
step    230 | loss 2.8148 | lr 6.87e-05 | grad 5.91 | tok/s 39040
step    240 | loss 2.9202 | lr 7.17e-05 | grad 5.91 | tok/s 39842
step    250 | loss 2.7565 | lr 7.47e-05 | grad 3.62 | tok/s 39298
step    260 | loss 2.7284 | lr 7.77e-05 | grad 2.50 | tok/s 37719
step    270 | loss 2.6234 | lr 8.07e-05 | grad 6.09 | tok/s 39147
step    280 | loss 2.5587 | lr 8.37e-05 | grad 8.06 | tok/s 39016
step    290 | loss 2.5374 | lr 8.67e-05 | grad 2.70 | tok/s 41150
step    300 | loss 2.4362 | lr 8.97e-05 | grad 3.06 | tok/s 41152
step    310 | loss 2.3701 | lr 9.27e-05 | grad 3.12 | tok/s 41102
step    320 | loss 2.4349 | lr 9.57e-05 | grad 6.12 | tok/s 39641
step    330 | loss 2.5368 | lr 9.87e-05 | grad 4.91 | tok/s 38622
step    340 | loss 2.5340 | lr 1.02e-04 | grad 4.12 | tok/s 39480
step    350 | loss 2.5249 | lr 1.05e-04 | grad 3.58 | tok/s 38322
step    360 | loss 2.5083 | lr 1.08e-04 | grad 6.84 | tok/s 38817
step    370 | loss 2.3692 | lr 1.11e-04 | grad 3.42 | tok/s 39550
step    380 | loss 2.8469 | lr 1.14e-04 | grad 3.80 | tok/s 40570
step    390 | loss 2.4324 | lr 1.17e-04 | grad 3.75 | tok/s 39069
step    400 | loss 2.5272 | lr 1.20e-04 | grad 6.75 | tok/s 40040
step    410 | loss 2.2682 | lr 1.23e-04 | grad 4.09 | tok/s 38833
step    420 | loss 2.3724 | lr 1.26e-04 | grad 2.95 | tok/s 38590
step    430 | loss 2.4941 | lr 1.29e-04 | grad 3.42 | tok/s 38478
step    440 | loss 2.6599 | lr 1.32e-04 | grad 3.31 | tok/s 39963
step    450 | loss 2.3510 | lr 1.35e-04 | grad 3.47 | tok/s 38884
step    460 | loss 2.3268 | lr 1.38e-04 | grad 2.66 | tok/s 39071
step    470 | loss 2.3356 | lr 1.41e-04 | grad 2.67 | tok/s 39507
step    480 | loss 2.2509 | lr 1.44e-04 | grad 2.03 | tok/s 38133
step    490 | loss 2.1355 | lr 1.47e-04 | grad 1.70 | tok/s 38782
step    500 | loss 3.0334 | lr 1.50e-04 | grad 5.12 | tok/s 40016
step    510 | loss 2.2760 | lr 1.53e-04 | grad 8.56 | tok/s 39123
step    520 | loss 2.2483 | lr 1.56e-04 | grad 2.22 | tok/s 40344
step    530 | loss 2.6764 | lr 1.59e-04 | grad 5.94 | tok/s 39542
step    540 | loss 2.2669 | lr 1.62e-04 | grad 2.92 | tok/s 39442
step    550 | loss 2.0984 | lr 1.65e-04 | grad 1.30 | tok/s 40570
step    560 | loss 1.9211 | lr 1.68e-04 | grad 1.45 | tok/s 41137
step    570 | loss 2.2429 | lr 1.71e-04 | grad 3.48 | tok/s 40159
step    580 | loss 2.5507 | lr 1.74e-04 | grad 2.69 | tok/s 39594
step    590 | loss 2.7395 | lr 1.77e-04 | grad 5.88 | tok/s 38858
step    600 | loss 2.2935 | lr 1.80e-04 | grad 2.59 | tok/s 38967
step    610 | loss 2.3256 | lr 1.83e-04 | grad 2.22 | tok/s 40855
step    620 | loss 2.1625 | lr 1.86e-04 | grad 1.89 | tok/s 38746
step    630 | loss 2.1039 | lr 1.89e-04 | grad 1.53 | tok/s 39982
step    640 | loss 2.5771 | lr 1.92e-04 | grad 1.84 | tok/s 39997
step    650 | loss 2.1601 | lr 1.95e-04 | grad 5.28 | tok/s 39176
step    660 | loss 2.4554 | lr 1.98e-04 | grad 6.44 | tok/s 38716
step    670 | loss 2.2967 | lr 2.01e-04 | grad 2.73 | tok/s 40102
step    680 | loss 2.2514 | lr 2.04e-04 | grad 2.19 | tok/s 38665
step    690 | loss 2.2462 | lr 2.07e-04 | grad 1.70 | tok/s 39020
step    700 | loss 2.3019 | lr 2.10e-04 | grad 1.97 | tok/s 39190
step    710 | loss 2.2611 | lr 2.13e-04 | grad 1.52 | tok/s 39420
step    720 | loss 2.2299 | lr 2.16e-04 | grad 3.38 | tok/s 39240
step    730 | loss 2.3426 | lr 2.19e-04 | grad 1.95 | tok/s 39642
step    740 | loss 2.2101 | lr 2.22e-04 | grad 2.75 | tok/s 39256
step    750 | loss 2.0127 | lr 2.25e-04 | grad 2.25 | tok/s 38844
step    760 | loss 2.5391 | lr 2.28e-04 | grad 1.36 | tok/s 39271
step    770 | loss 2.0256 | lr 2.31e-04 | grad 1.84 | tok/s 39070
step    780 | loss 2.0488 | lr 2.34e-04 | grad 1.55 | tok/s 39496
step    790 | loss 2.0363 | lr 2.37e-04 | grad 1.26 | tok/s 39838
step    800 | loss 2.0082 | lr 2.40e-04 | grad 1.37 | tok/s 39857
step    810 | loss 2.0859 | lr 2.43e-04 | grad 2.70 | tok/s 39440
step    820 | loss 2.6866 | lr 2.46e-04 | grad 1.91 | tok/s 40528
step    830 | loss 2.2140 | lr 2.49e-04 | grad 1.12 | tok/s 41146
step    840 | loss 1.8946 | lr 2.52e-04 | grad 0.84 | tok/s 41179
step    850 | loss 2.4526 | lr 2.55e-04 | grad 1.74 | tok/s 39146
step    860 | loss 2.1463 | lr 2.58e-04 | grad 1.45 | tok/s 38341
step    870 | loss 2.0568 | lr 2.61e-04 | grad 1.00 | tok/s 39494
step    880 | loss 2.1193 | lr 2.64e-04 | grad 1.45 | tok/s 39306
step    890 | loss 1.9593 | lr 2.67e-04 | grad 0.99 | tok/s 39291
step    900 | loss 2.4448 | lr 2.70e-04 | grad 1.09 | tok/s 38268
step    910 | loss 2.0048 | lr 2.73e-04 | grad 1.09 | tok/s 38987
step    920 | loss 1.9444 | lr 2.76e-04 | grad 0.96 | tok/s 38851
step    930 | loss 2.0268 | lr 2.79e-04 | grad 1.43 | tok/s 38739
step    940 | loss 1.9451 | lr 2.82e-04 | grad 1.49 | tok/s 38384
step    950 | loss 2.0394 | lr 2.85e-04 | grad 1.55 | tok/s 39226
step    960 | loss 1.7336 | lr 2.88e-04 | grad 0.96 | tok/s 41185
step    970 | loss 1.5562 | lr 2.91e-04 | grad 0.75 | tok/s 41180
step    980 | loss 1.7587 | lr 2.94e-04 | grad 2.08 | tok/s 39999
step    990 | loss 2.1581 | lr 2.97e-04 | grad 1.01 | tok/s 38946
step   1000 | loss 1.9689 | lr 3.00e-04 | grad 0.79 | tok/s 38013
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9689.pt
step   1010 | loss 2.2668 | lr 1.06e-06 | grad 4.03 | tok/s 34711
step   1020 | loss 1.9196 | lr 1.27e-06 | grad 0.84 | tok/s 37607
step   1030 | loss 2.2376 | lr 1.62e-06 | grad 0.97 | tok/s 36992
step   1040 | loss 1.8920 | lr 2.12e-06 | grad 1.48 | tok/s 37808
step   1050 | loss 1.9093 | lr 2.77e-06 | grad 0.89 | tok/s 37912
step   1060 | loss 2.3142 | lr 3.56e-06 | grad 2.55 | tok/s 38014
step   1070 | loss 2.3821 | lr 4.50e-06 | grad 0.94 | tok/s 38138
step   1080 | loss 2.5807 | lr 5.58e-06 | grad 1.30 | tok/s 37651
step   1090 | loss 2.2886 | lr 6.81e-06 | grad 1.09 | tok/s 37874
step   1100 | loss 1.9580 | lr 8.17e-06 | grad 0.96 | tok/s 37708
step   1110 | loss 2.0483 | lr 9.68e-06 | grad 1.02 | tok/s 38324
step   1120 | loss 2.4109 | lr 1.13e-05 | grad 1.02 | tok/s 38744
step   1130 | loss 1.9665 | lr 1.31e-05 | grad 0.72 | tok/s 36897
step   1140 | loss 1.8668 | lr 1.50e-05 | grad 0.90 | tok/s 37729
step   1150 | loss 2.1703 | lr 1.71e-05 | grad 1.34 | tok/s 37734
step   1160 | loss 1.8008 | lr 1.93e-05 | grad 0.72 | tok/s 37311
step   1170 | loss 2.1801 | lr 2.16e-05 | grad 0.84 | tok/s 37730
step   1180 | loss 1.8831 | lr 2.40e-05 | grad 0.95 | tok/s 39707
step   1190 | loss 1.8503 | lr 2.66e-05 | grad 0.76 | tok/s 39715
step   1200 | loss 1.7989 | lr 2.93e-05 | grad 0.72 | tok/s 39676
step   1210 | loss 1.7829 | lr 3.21e-05 | grad 0.73 | tok/s 39708
step   1220 | loss 1.7924 | lr 3.50e-05 | grad 0.82 | tok/s 39391
step   1230 | loss 1.7316 | lr 3.80e-05 | grad 0.75 | tok/s 37984
step   1240 | loss 1.8745 | lr 4.12e-05 | grad 0.83 | tok/s 37339
step   1250 | loss 1.9886 | lr 4.45e-05 | grad 3.09 | tok/s 38453
step   1260 | loss 2.0254 | lr 4.78e-05 | grad 2.52 | tok/s 38418
step   1270 | loss 2.0772 | lr 5.13e-05 | grad 1.37 | tok/s 37943
step   1280 | loss 1.9570 | lr 5.48e-05 | grad 0.94 | tok/s 37525
step   1290 | loss 1.8722 | lr 5.85e-05 | grad 1.04 | tok/s 37331
step   1300 | loss 1.9139 | lr 6.22e-05 | grad 0.88 | tok/s 37099
step   1310 | loss 2.0059 | lr 6.61e-05 | grad 0.90 | tok/s 37092
step   1320 | loss 1.9435 | lr 7.00e-05 | grad 1.21 | tok/s 37807
step   1330 | loss 1.8749 | lr 7.40e-05 | grad 0.83 | tok/s 37826
step   1340 | loss 1.8106 | lr 7.81e-05 | grad 1.07 | tok/s 37842
step   1350 | loss 1.8964 | lr 8.22e-05 | grad 2.03 | tok/s 38862
step   1360 | loss 1.7728 | lr 8.64e-05 | grad 0.80 | tok/s 36877
step   1370 | loss 1.9155 | lr 9.07e-05 | grad 0.70 | tok/s 37443
step   1380 | loss 1.9611 | lr 9.50e-05 | grad 1.09 | tok/s 37750
step   1390 | loss 1.8648 | lr 9.94e-05 | grad 1.62 | tok/s 36825
step   1400 | loss 1.9081 | lr 1.04e-04 | grad 12.56 | tok/s 38351
step   1410 | loss 1.9289 | lr 1.08e-04 | grad 1.02 | tok/s 38699
step   1420 | loss 1.9297 | lr 1.13e-04 | grad 0.96 | tok/s 36797
step   1430 | loss 1.7371 | lr 1.17e-04 | grad 0.96 | tok/s 35915
step   1440 | loss 1.6828 | lr 1.22e-04 | grad 0.84 | tok/s 37934
step   1450 | loss 1.7693 | lr 1.27e-04 | grad 2.38 | tok/s 38804
step   1460 | loss 1.7944 | lr 1.31e-04 | grad 0.76 | tok/s 36175
step   1470 | loss 1.9099 | lr 1.36e-04 | grad 2.56 | tok/s 37470

Training complete! Final step: 1476
