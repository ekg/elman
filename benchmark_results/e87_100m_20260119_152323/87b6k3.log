Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b6k3/level87b6k3_100m_20260119_152330
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b6k3, 58,234,496 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6667 | lr 2.70e-06 | grad 28.38 | tok/s 17461
step     20 | loss 5.6482 | lr 5.70e-06 | grad 19.75 | tok/s 37150
step     30 | loss 5.7157 | lr 8.70e-06 | grad 17.88 | tok/s 39217
step     40 | loss 5.6461 | lr 1.17e-05 | grad 16.12 | tok/s 39135
step     50 | loss 5.5289 | lr 1.47e-05 | grad 15.25 | tok/s 39065
step     60 | loss 5.3634 | lr 1.77e-05 | grad 33.75 | tok/s 38207
step     70 | loss 4.9827 | lr 2.07e-05 | grad 36.25 | tok/s 36925
step     80 | loss 4.6703 | lr 2.37e-05 | grad 18.25 | tok/s 38356
step     90 | loss 4.6205 | lr 2.67e-05 | grad 11.44 | tok/s 36971
step    100 | loss 4.3279 | lr 2.97e-05 | grad 9.06 | tok/s 37338
step    110 | loss 4.0289 | lr 3.27e-05 | grad 19.38 | tok/s 36771
step    120 | loss 4.0285 | lr 3.57e-05 | grad 39.25 | tok/s 36270
step    130 | loss 3.8880 | lr 3.87e-05 | grad 7.62 | tok/s 36991
step    140 | loss 3.5498 | lr 4.17e-05 | grad 15.88 | tok/s 37026
step    150 | loss 3.3077 | lr 4.47e-05 | grad 7.50 | tok/s 35294
step    160 | loss 3.1485 | lr 4.77e-05 | grad 5.44 | tok/s 35607
step    170 | loss 3.2286 | lr 5.07e-05 | grad 10.31 | tok/s 36821
step    180 | loss 3.2626 | lr 5.37e-05 | grad 6.94 | tok/s 36926
step    190 | loss 3.0387 | lr 5.67e-05 | grad 6.88 | tok/s 37484
step    200 | loss 2.9515 | lr 5.97e-05 | grad 6.22 | tok/s 38338
step    210 | loss 2.9959 | lr 6.27e-05 | grad 9.94 | tok/s 36688
step    220 | loss 3.0102 | lr 6.57e-05 | grad 6.38 | tok/s 37904
step    230 | loss 2.7787 | lr 6.87e-05 | grad 5.69 | tok/s 36587
step    240 | loss 2.8891 | lr 7.17e-05 | grad 7.72 | tok/s 37334
step    250 | loss 2.7616 | lr 7.47e-05 | grad 5.25 | tok/s 36789
step    260 | loss 2.7271 | lr 7.77e-05 | grad 4.81 | tok/s 35312
step    270 | loss 2.6266 | lr 8.07e-05 | grad 6.62 | tok/s 36653
step    280 | loss 2.5795 | lr 8.37e-05 | grad 9.56 | tok/s 36565
step    290 | loss 2.5581 | lr 8.67e-05 | grad 6.69 | tok/s 38534
step    300 | loss 2.4842 | lr 8.97e-05 | grad 3.86 | tok/s 38550
step    310 | loss 2.4180 | lr 9.27e-05 | grad 4.28 | tok/s 38544
step    320 | loss 2.4513 | lr 9.57e-05 | grad 8.06 | tok/s 37099
step    330 | loss 2.5545 | lr 9.87e-05 | grad 7.69 | tok/s 36187
step    340 | loss 2.5742 | lr 1.02e-04 | grad 5.28 | tok/s 36959
step    350 | loss 2.5314 | lr 1.05e-04 | grad 8.81 | tok/s 35886
step    360 | loss 2.5455 | lr 1.08e-04 | grad 7.88 | tok/s 36339
step    370 | loss 2.4234 | lr 1.11e-04 | grad 6.25 | tok/s 37047
step    380 | loss 2.9052 | lr 1.14e-04 | grad 6.66 | tok/s 37984
step    390 | loss 2.4702 | lr 1.17e-04 | grad 4.66 | tok/s 36556
step    400 | loss 2.5610 | lr 1.20e-04 | grad 13.19 | tok/s 37538
step    410 | loss 2.3166 | lr 1.23e-04 | grad 4.41 | tok/s 36326
step    420 | loss 2.4155 | lr 1.26e-04 | grad 3.22 | tok/s 36110
step    430 | loss 2.5366 | lr 1.29e-04 | grad 4.72 | tok/s 36003
step    440 | loss 2.7288 | lr 1.32e-04 | grad 5.72 | tok/s 37435
step    450 | loss 2.3799 | lr 1.35e-04 | grad 3.98 | tok/s 36382
step    460 | loss 2.3564 | lr 1.38e-04 | grad 3.17 | tok/s 36528
step    470 | loss 2.3726 | lr 1.41e-04 | grad 3.17 | tok/s 36950
step    480 | loss 2.2519 | lr 1.44e-04 | grad 2.70 | tok/s 35690
step    490 | loss 2.1691 | lr 1.47e-04 | grad 1.73 | tok/s 36334
step    500 | loss 3.0317 | lr 1.50e-04 | grad 4.75 | tok/s 37453
step    510 | loss 2.3100 | lr 1.53e-04 | grad 3.75 | tok/s 36603
step    520 | loss 2.3182 | lr 1.56e-04 | grad 58.75 | tok/s 37760
step    530 | loss 2.7556 | lr 1.59e-04 | grad 7.41 | tok/s 36976
step    540 | loss 2.4062 | lr 1.62e-04 | grad 3.67 | tok/s 36923
step    550 | loss 2.1821 | lr 1.65e-04 | grad 1.72 | tok/s 37980
step    560 | loss 1.9858 | lr 1.68e-04 | grad 1.71 | tok/s 38485
step    570 | loss 2.3069 | lr 1.71e-04 | grad 3.67 | tok/s 37603
step    580 | loss 2.6267 | lr 1.74e-04 | grad 2.31 | tok/s 37107
step    590 | loss 2.7911 | lr 1.77e-04 | grad 3.77 | tok/s 36350
step    600 | loss 2.3514 | lr 1.80e-04 | grad 4.53 | tok/s 36444
step    610 | loss 2.4263 | lr 1.83e-04 | grad 2.55 | tok/s 38179
step    620 | loss 2.1997 | lr 1.86e-04 | grad 1.81 | tok/s 36251
step    630 | loss 2.1622 | lr 1.89e-04 | grad 1.80 | tok/s 37383
step    640 | loss 2.5658 | lr 1.92e-04 | grad 1.77 | tok/s 37425
step    650 | loss 2.1883 | lr 1.95e-04 | grad 2.50 | tok/s 36706
step    660 | loss 2.5090 | lr 1.98e-04 | grad 5.75 | tok/s 36198
step    670 | loss 2.3469 | lr 2.01e-04 | grad 2.14 | tok/s 37516
step    680 | loss 2.2892 | lr 2.04e-04 | grad 2.56 | tok/s 36164
step    690 | loss 2.3144 | lr 2.07e-04 | grad 2.28 | tok/s 36481
step    700 | loss 2.3649 | lr 2.10e-04 | grad 1.70 | tok/s 36658
step    710 | loss 2.3023 | lr 2.13e-04 | grad 1.60 | tok/s 36861
step    720 | loss 2.2538 | lr 2.16e-04 | grad 4.94 | tok/s 36670
step    730 | loss 2.3822 | lr 2.19e-04 | grad 1.94 | tok/s 37050
step    740 | loss 2.2710 | lr 2.22e-04 | grad 2.59 | tok/s 36721
step    750 | loss 2.0688 | lr 2.25e-04 | grad 1.91 | tok/s 36313
step    760 | loss 2.6615 | lr 2.28e-04 | grad 1.23 | tok/s 36773
step    770 | loss 2.0316 | lr 2.31e-04 | grad 1.46 | tok/s 36515
step    780 | loss 2.0609 | lr 2.34e-04 | grad 1.49 | tok/s 36937
step    790 | loss 2.0599 | lr 2.37e-04 | grad 1.09 | tok/s 37225
step    800 | loss 2.0354 | lr 2.40e-04 | grad 1.34 | tok/s 37267
step    810 | loss 2.0892 | lr 2.43e-04 | grad 2.56 | tok/s 36918
step    820 | loss 2.6487 | lr 2.46e-04 | grad 2.31 | tok/s 37874
step    830 | loss 2.1864 | lr 2.49e-04 | grad 0.94 | tok/s 38462
step    840 | loss 1.8928 | lr 2.52e-04 | grad 0.84 | tok/s 38479
step    850 | loss 2.4198 | lr 2.55e-04 | grad 1.61 | tok/s 36583
step    860 | loss 2.1358 | lr 2.58e-04 | grad 1.23 | tok/s 35854
step    870 | loss 2.0679 | lr 2.61e-04 | grad 0.97 | tok/s 36938
step    880 | loss 2.1322 | lr 2.64e-04 | grad 1.52 | tok/s 36742
step    890 | loss 1.9790 | lr 2.67e-04 | grad 1.01 | tok/s 36720
step    900 | loss 2.4586 | lr 2.70e-04 | grad 1.24 | tok/s 35760
step    910 | loss 2.0204 | lr 2.73e-04 | grad 1.09 | tok/s 36414
step    920 | loss 1.9754 | lr 2.76e-04 | grad 0.95 | tok/s 36302
step    930 | loss 2.0499 | lr 2.79e-04 | grad 1.45 | tok/s 36231
step    940 | loss 1.9571 | lr 2.82e-04 | grad 1.52 | tok/s 35852
step    950 | loss 2.0608 | lr 2.85e-04 | grad 1.44 | tok/s 36689
step    960 | loss 1.7522 | lr 2.88e-04 | grad 0.62 | tok/s 38480
step    970 | loss 1.5706 | lr 2.91e-04 | grad 0.58 | tok/s 38486
step    980 | loss 1.7668 | lr 2.94e-04 | grad 2.06 | tok/s 37401
step    990 | loss 2.1749 | lr 2.97e-04 | grad 1.04 | tok/s 36393
step   1000 | loss 1.9878 | lr 3.00e-04 | grad 0.72 | tok/s 35507
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9878.pt
step   1010 | loss 2.3279 | lr 1.06e-06 | grad 11.50 | tok/s 32588
step   1020 | loss 1.9563 | lr 1.27e-06 | grad 0.82 | tok/s 35159
step   1030 | loss 2.2294 | lr 1.62e-06 | grad 0.94 | tok/s 34542
step   1040 | loss 1.9009 | lr 2.12e-06 | grad 1.48 | tok/s 35309
step   1050 | loss 1.9862 | lr 2.77e-06 | grad 0.88 | tok/s 35431
step   1060 | loss 2.3274 | lr 3.56e-06 | grad 2.28 | tok/s 35508
step   1070 | loss 2.3890 | lr 4.50e-06 | grad 0.91 | tok/s 35603
step   1080 | loss 2.5514 | lr 5.58e-06 | grad 1.27 | tok/s 35141
step   1090 | loss 2.3035 | lr 6.81e-06 | grad 1.04 | tok/s 35415
step   1100 | loss 2.0077 | lr 8.17e-06 | grad 0.94 | tok/s 35171
step   1110 | loss 2.1304 | lr 9.68e-06 | grad 0.96 | tok/s 35802
step   1120 | loss 2.4366 | lr 1.13e-05 | grad 1.01 | tok/s 36245
step   1130 | loss 1.9887 | lr 1.31e-05 | grad 0.73 | tok/s 34448
step   1140 | loss 1.8874 | lr 1.50e-05 | grad 0.85 | tok/s 35301
step   1150 | loss 2.1814 | lr 1.71e-05 | grad 1.27 | tok/s 35249
step   1160 | loss 1.8461 | lr 1.93e-05 | grad 0.66 | tok/s 34881
step   1170 | loss 2.2017 | lr 2.16e-05 | grad 0.85 | tok/s 35254
step   1180 | loss 1.8962 | lr 2.40e-05 | grad 0.95 | tok/s 37080
step   1190 | loss 1.8630 | lr 2.66e-05 | grad 0.72 | tok/s 37050
step   1200 | loss 1.8134 | lr 2.93e-05 | grad 0.67 | tok/s 37109
step   1210 | loss 1.7988 | lr 3.21e-05 | grad 0.68 | tok/s 37084
step   1220 | loss 1.8034 | lr 3.50e-05 | grad 0.75 | tok/s 36751
step   1230 | loss 1.7750 | lr 3.80e-05 | grad 0.71 | tok/s 35468
step   1240 | loss 1.8829 | lr 4.12e-05 | grad 0.78 | tok/s 34849
step   1250 | loss 2.0056 | lr 4.45e-05 | grad 3.08 | tok/s 35919
step   1260 | loss 2.0114 | lr 4.78e-05 | grad 2.83 | tok/s 35896
step   1270 | loss 2.0888 | lr 5.13e-05 | grad 1.34 | tok/s 35425
step   1280 | loss 1.9669 | lr 5.48e-05 | grad 0.87 | tok/s 34998
step   1290 | loss 1.8880 | lr 5.85e-05 | grad 1.02 | tok/s 34909
step   1300 | loss 1.9261 | lr 6.22e-05 | grad 0.84 | tok/s 34690
step   1310 | loss 2.0011 | lr 6.61e-05 | grad 0.90 | tok/s 34667
step   1320 | loss 1.9546 | lr 7.00e-05 | grad 1.20 | tok/s 35269
step   1330 | loss 1.8812 | lr 7.40e-05 | grad 0.82 | tok/s 35332
step   1340 | loss 1.8190 | lr 7.81e-05 | grad 1.02 | tok/s 35352
step   1350 | loss 1.8998 | lr 8.22e-05 | grad 2.02 | tok/s 36270
step   1360 | loss 1.7795 | lr 8.64e-05 | grad 0.81 | tok/s 34467
step   1370 | loss 1.9280 | lr 9.07e-05 | grad 0.69 | tok/s 34996
step   1380 | loss 1.9707 | lr 9.50e-05 | grad 1.04 | tok/s 35280

Training complete! Final step: 1383
