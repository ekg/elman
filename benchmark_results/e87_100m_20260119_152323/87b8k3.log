Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b8k3/level87b8k3_100m_20260119_152440
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b8k3, 49,305,600 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.5572 | lr 2.70e-06 | grad 25.50 | tok/s 19450
step     20 | loss 5.5074 | lr 5.70e-06 | grad 11.62 | tok/s 48541
step     30 | loss 5.7209 | lr 8.70e-06 | grad 10.62 | tok/s 51516
step     40 | loss 5.6842 | lr 1.17e-05 | grad 12.12 | tok/s 51486
step     50 | loss 5.5998 | lr 1.47e-05 | grad 10.44 | tok/s 51463
step     60 | loss 5.4116 | lr 1.77e-05 | grad 23.00 | tok/s 50275
step     70 | loss 5.0531 | lr 2.07e-05 | grad 18.62 | tok/s 48555
step     80 | loss 4.7953 | lr 2.37e-05 | grad 16.38 | tok/s 50553
step     90 | loss 4.8200 | lr 2.67e-05 | grad 10.06 | tok/s 48736
step    100 | loss 4.5754 | lr 2.97e-05 | grad 8.88 | tok/s 49192
step    110 | loss 4.2663 | lr 3.27e-05 | grad 13.94 | tok/s 48575
step    120 | loss 4.0984 | lr 3.57e-05 | grad 16.25 | tok/s 47895
step    130 | loss 3.9410 | lr 3.87e-05 | grad 11.19 | tok/s 48902
step    140 | loss 3.5933 | lr 4.17e-05 | grad 6.69 | tok/s 48999
step    150 | loss 3.3569 | lr 4.47e-05 | grad 6.56 | tok/s 46769
step    160 | loss 3.1841 | lr 4.77e-05 | grad 5.19 | tok/s 47154
step    170 | loss 3.2633 | lr 5.07e-05 | grad 13.94 | tok/s 48823
step    180 | loss 3.2814 | lr 5.37e-05 | grad 6.38 | tok/s 48931
step    190 | loss 3.0582 | lr 5.67e-05 | grad 8.12 | tok/s 49530
step    200 | loss 2.9654 | lr 5.97e-05 | grad 4.53 | tok/s 50895
step    210 | loss 3.0112 | lr 6.27e-05 | grad 7.84 | tok/s 48683
step    220 | loss 3.0493 | lr 6.57e-05 | grad 4.59 | tok/s 50110
step    230 | loss 2.8083 | lr 6.87e-05 | grad 5.38 | tok/s 48357
step    240 | loss 2.9176 | lr 7.17e-05 | grad 7.97 | tok/s 49353
step    250 | loss 2.7935 | lr 7.47e-05 | grad 5.50 | tok/s 48639
step    260 | loss 2.7673 | lr 7.77e-05 | grad 4.88 | tok/s 46709
step    270 | loss 2.6815 | lr 8.07e-05 | grad 4.94 | tok/s 48488
step    280 | loss 2.6250 | lr 8.37e-05 | grad 5.31 | tok/s 48416
step    290 | loss 2.6080 | lr 8.67e-05 | grad 4.22 | tok/s 50977
step    300 | loss 2.5340 | lr 8.97e-05 | grad 3.97 | tok/s 50912
step    310 | loss 2.4806 | lr 9.27e-05 | grad 4.78 | tok/s 50946
step    320 | loss 2.5268 | lr 9.57e-05 | grad 13.44 | tok/s 49029
step    330 | loss 2.6087 | lr 9.87e-05 | grad 5.44 | tok/s 47880
step    340 | loss 2.6018 | lr 1.02e-04 | grad 6.41 | tok/s 48838
step    350 | loss 2.5765 | lr 1.05e-04 | grad 5.38 | tok/s 47428
step    360 | loss 2.5633 | lr 1.08e-04 | grad 9.94 | tok/s 48038
step    370 | loss 2.4684 | lr 1.11e-04 | grad 5.84 | tok/s 48779
step    380 | loss 2.9103 | lr 1.14e-04 | grad 4.34 | tok/s 50003
step    390 | loss 2.5082 | lr 1.17e-04 | grad 12.00 | tok/s 48152
step    400 | loss 2.6366 | lr 1.20e-04 | grad 8.69 | tok/s 49395
step    410 | loss 2.4322 | lr 1.23e-04 | grad 9.19 | tok/s 47907
step    420 | loss 2.4447 | lr 1.26e-04 | grad 3.88 | tok/s 47571
step    430 | loss 2.5629 | lr 1.29e-04 | grad 4.47 | tok/s 47414
step    440 | loss 2.7426 | lr 1.32e-04 | grad 4.78 | tok/s 49308
step    450 | loss 2.4041 | lr 1.35e-04 | grad 4.69 | tok/s 47936
step    460 | loss 2.4022 | lr 1.38e-04 | grad 2.92 | tok/s 48124
step    470 | loss 2.4024 | lr 1.41e-04 | grad 4.16 | tok/s 48680
step    480 | loss 2.3119 | lr 1.44e-04 | grad 3.20 | tok/s 46918
step    490 | loss 2.2155 | lr 1.47e-04 | grad 2.03 | tok/s 47817
step    500 | loss 3.0608 | lr 1.50e-04 | grad 5.03 | tok/s 49264
step    510 | loss 2.3661 | lr 1.53e-04 | grad 5.47 | tok/s 48181
step    520 | loss 2.3945 | lr 1.56e-04 | grad 4.00 | tok/s 49745
step    530 | loss 2.7283 | lr 1.59e-04 | grad 5.53 | tok/s 48651
step    540 | loss 2.3181 | lr 1.62e-04 | grad 19.62 | tok/s 48583
step    550 | loss 2.2514 | lr 1.65e-04 | grad 2.17 | tok/s 49929
step    560 | loss 2.0588 | lr 1.68e-04 | grad 2.45 | tok/s 50618
step    570 | loss 2.3251 | lr 1.71e-04 | grad 5.25 | tok/s 49470
step    580 | loss 2.6166 | lr 1.74e-04 | grad 4.25 | tok/s 48806
step    590 | loss 2.7947 | lr 1.77e-04 | grad 3.11 | tok/s 47840
step    600 | loss 2.3786 | lr 1.80e-04 | grad 3.84 | tok/s 47948
step    610 | loss 2.4754 | lr 1.83e-04 | grad 3.30 | tok/s 50269
step    620 | loss 2.2404 | lr 1.86e-04 | grad 3.95 | tok/s 47696
step    630 | loss 2.2121 | lr 1.89e-04 | grad 1.91 | tok/s 49125
step    640 | loss 2.6545 | lr 1.92e-04 | grad 3.50 | tok/s 49226
step    650 | loss 2.2563 | lr 1.95e-04 | grad 4.09 | tok/s 48256
step    660 | loss 2.5333 | lr 1.98e-04 | grad 5.88 | tok/s 47640
step    670 | loss 2.3809 | lr 2.01e-04 | grad 3.25 | tok/s 49277
step    680 | loss 2.3360 | lr 2.04e-04 | grad 4.56 | tok/s 47540
step    690 | loss 2.3354 | lr 2.07e-04 | grad 5.75 | tok/s 47959
step    700 | loss 2.4116 | lr 2.10e-04 | grad 1.97 | tok/s 48216
step    710 | loss 2.3435 | lr 2.13e-04 | grad 2.03 | tok/s 48486
step    720 | loss 2.3444 | lr 2.16e-04 | grad 5.28 | tok/s 48288
step    730 | loss 2.4506 | lr 2.19e-04 | grad 2.19 | tok/s 48782
step    740 | loss 2.3359 | lr 2.22e-04 | grad 4.12 | tok/s 48306
step    750 | loss 2.1665 | lr 2.25e-04 | grad 1.91 | tok/s 47760
step    760 | loss 2.6865 | lr 2.28e-04 | grad 1.26 | tok/s 48307
step    770 | loss 2.1083 | lr 2.31e-04 | grad 1.80 | tok/s 48061
step    780 | loss 2.1471 | lr 2.34e-04 | grad 1.64 | tok/s 48617
step    790 | loss 2.1423 | lr 2.37e-04 | grad 1.27 | tok/s 48985
step    800 | loss 2.1198 | lr 2.40e-04 | grad 1.68 | tok/s 49017
step    810 | loss 2.1536 | lr 2.43e-04 | grad 2.95 | tok/s 48546
step    820 | loss 2.8009 | lr 2.46e-04 | grad 2.03 | tok/s 49781
step    830 | loss 2.3381 | lr 2.49e-04 | grad 1.35 | tok/s 50634
step    840 | loss 2.0103 | lr 2.52e-04 | grad 1.12 | tok/s 50555
step    850 | loss 2.4593 | lr 2.55e-04 | grad 1.95 | tok/s 48091
step    860 | loss 2.2217 | lr 2.58e-04 | grad 1.55 | tok/s 47176
step    870 | loss 2.1602 | lr 2.61e-04 | grad 1.36 | tok/s 48544
step    880 | loss 2.2452 | lr 2.64e-04 | grad 2.25 | tok/s 48329
step    890 | loss 2.1650 | lr 2.67e-04 | grad 1.02 | tok/s 48301
step    900 | loss 2.5732 | lr 2.70e-04 | grad 1.12 | tok/s 47066
step    910 | loss 2.0980 | lr 2.73e-04 | grad 1.05 | tok/s 47896
step    920 | loss 2.0415 | lr 2.76e-04 | grad 0.98 | tok/s 47816
step    930 | loss 2.1404 | lr 2.79e-04 | grad 1.48 | tok/s 47625
step    940 | loss 2.0229 | lr 2.82e-04 | grad 1.53 | tok/s 47163
step    950 | loss 2.1126 | lr 2.85e-04 | grad 1.56 | tok/s 48279
step    960 | loss 1.8361 | lr 2.88e-04 | grad 0.92 | tok/s 50618
step    970 | loss 1.6376 | lr 2.91e-04 | grad 0.70 | tok/s 50590
step    980 | loss 1.8251 | lr 2.94e-04 | grad 2.03 | tok/s 49136
step    990 | loss 2.2573 | lr 2.97e-04 | grad 1.13 | tok/s 47840
step   1000 | loss 2.0371 | lr 3.00e-04 | grad 0.79 | tok/s 46724
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0371.pt
step   1010 | loss 2.3712 | lr 1.06e-06 | grad 9.31 | tok/s 43128
step   1020 | loss 1.9940 | lr 1.27e-06 | grad 0.98 | tok/s 45696
step   1030 | loss 2.2690 | lr 1.62e-06 | grad 0.87 | tok/s 44965
step   1040 | loss 1.9591 | lr 2.12e-06 | grad 3.38 | tok/s 45957
step   1050 | loss 1.9994 | lr 2.77e-06 | grad 1.01 | tok/s 46093
step   1060 | loss 2.4258 | lr 3.56e-06 | grad 3.66 | tok/s 46181
step   1070 | loss 2.4879 | lr 4.50e-06 | grad 1.09 | tok/s 46314
step   1080 | loss 2.5943 | lr 5.58e-06 | grad 2.25 | tok/s 45774
step   1090 | loss 2.3482 | lr 6.81e-06 | grad 1.01 | tok/s 46153
step   1100 | loss 2.0157 | lr 8.17e-06 | grad 0.88 | tok/s 45812
step   1110 | loss 2.1641 | lr 9.68e-06 | grad 1.05 | tok/s 46544
step   1120 | loss 2.5298 | lr 1.13e-05 | grad 1.00 | tok/s 47218
step   1130 | loss 2.0247 | lr 1.31e-05 | grad 0.72 | tok/s 44864
step   1140 | loss 1.9323 | lr 1.50e-05 | grad 0.84 | tok/s 45953
step   1150 | loss 2.2204 | lr 1.71e-05 | grad 1.77 | tok/s 45842
step   1160 | loss 1.8751 | lr 1.93e-05 | grad 0.68 | tok/s 45388
step   1170 | loss 2.2643 | lr 2.16e-05 | grad 0.79 | tok/s 45967
step   1180 | loss 1.9348 | lr 2.40e-05 | grad 0.90 | tok/s 48411
step   1190 | loss 1.9014 | lr 2.66e-05 | grad 0.71 | tok/s 48459
step   1200 | loss 1.8552 | lr 2.93e-05 | grad 0.70 | tok/s 48302
step   1210 | loss 1.8471 | lr 3.21e-05 | grad 0.68 | tok/s 48321
step   1220 | loss 1.8555 | lr 3.50e-05 | grad 10.56 | tok/s 47851
step   1230 | loss 1.8046 | lr 3.80e-05 | grad 0.75 | tok/s 46176
step   1240 | loss 1.9461 | lr 4.12e-05 | grad 0.82 | tok/s 45346
step   1250 | loss 2.0613 | lr 4.45e-05 | grad 3.05 | tok/s 46729
step   1260 | loss 2.0893 | lr 4.78e-05 | grad 2.55 | tok/s 46755
step   1270 | loss 2.1450 | lr 5.13e-05 | grad 1.27 | tok/s 46221
step   1280 | loss 2.0353 | lr 5.48e-05 | grad 0.92 | tok/s 45814
step   1290 | loss 1.9417 | lr 5.85e-05 | grad 0.95 | tok/s 45554
step   1300 | loss 1.9744 | lr 6.22e-05 | grad 0.85 | tok/s 45209
step   1310 | loss 2.0818 | lr 6.61e-05 | grad 0.89 | tok/s 45107
step   1320 | loss 2.0101 | lr 7.00e-05 | grad 1.26 | tok/s 45986
step   1330 | loss 1.9469 | lr 7.40e-05 | grad 0.86 | tok/s 46147
step   1340 | loss 1.8978 | lr 7.81e-05 | grad 1.04 | tok/s 46029
step   1350 | loss 1.9729 | lr 8.22e-05 | grad 2.09 | tok/s 47235
step   1360 | loss 1.8379 | lr 8.64e-05 | grad 0.77 | tok/s 44869
step   1370 | loss 1.9882 | lr 9.07e-05 | grad 0.94 | tok/s 45551
step   1380 | loss 2.0436 | lr 9.50e-05 | grad 1.05 | tok/s 45987
step   1390 | loss 1.9282 | lr 9.94e-05 | grad 1.73 | tok/s 44916
step   1400 | loss 1.9989 | lr 1.04e-04 | grad 8.38 | tok/s 46659
step   1410 | loss 2.0619 | lr 1.08e-04 | grad 1.08 | tok/s 47106
step   1420 | loss 2.0132 | lr 1.13e-04 | grad 0.98 | tok/s 44961
step   1430 | loss 1.7947 | lr 1.17e-04 | grad 0.97 | tok/s 43756
step   1440 | loss 1.7452 | lr 1.22e-04 | grad 0.87 | tok/s 46196
step   1450 | loss 1.8341 | lr 1.27e-04 | grad 2.17 | tok/s 47164
step   1460 | loss 1.8496 | lr 1.31e-04 | grad 0.72 | tok/s 44054
step   1470 | loss 1.9619 | lr 1.36e-04 | grad 2.31 | tok/s 45762
step   1480 | loss 1.8661 | lr 1.41e-04 | grad 2.12 | tok/s 46217
step   1490 | loss 2.0194 | lr 1.45e-04 | grad 2.44 | tok/s 46171
step   1500 | loss 2.0898 | lr 1.50e-04 | grad 1.94 | tok/s 44996
step   1510 | loss 2.0052 | lr 1.55e-04 | grad 1.29 | tok/s 47161
step   1520 | loss 1.9751 | lr 1.59e-04 | grad 1.24 | tok/s 46653
step   1530 | loss 1.9012 | lr 1.64e-04 | grad 0.79 | tok/s 46357
step   1540 | loss 1.8613 | lr 1.69e-04 | grad 0.75 | tok/s 45445
step   1550 | loss 1.8731 | lr 1.73e-04 | grad 2.45 | tok/s 47317
step   1560 | loss 2.4140 | lr 1.78e-04 | grad 1.83 | tok/s 46043
step   1570 | loss 1.9166 | lr 1.83e-04 | grad 1.37 | tok/s 45249
step   1580 | loss 2.0842 | lr 1.87e-04 | grad 1.38 | tok/s 46894
step   1590 | loss 1.7927 | lr 1.92e-04 | grad 0.90 | tok/s 45620
step   1600 | loss 1.8769 | lr 1.96e-04 | grad 1.04 | tok/s 44903
step   1610 | loss 1.7188 | lr 2.01e-04 | grad 1.01 | tok/s 47468
step   1620 | loss 1.8801 | lr 2.05e-04 | grad 0.85 | tok/s 46819
step   1630 | loss 1.9189 | lr 2.09e-04 | grad 1.51 | tok/s 47135
step   1640 | loss 1.8163 | lr 2.14e-04 | grad 0.74 | tok/s 45572
step   1650 | loss 1.8390 | lr 2.18e-04 | grad 1.45 | tok/s 44829
step   1660 | loss 1.8418 | lr 2.22e-04 | grad 0.87 | tok/s 45170
step   1670 | loss 1.9460 | lr 2.26e-04 | grad 2.58 | tok/s 47113
step   1680 | loss 2.3394 | lr 2.30e-04 | grad 0.84 | tok/s 47130
step   1690 | loss 1.8460 | lr 2.34e-04 | grad 1.22 | tok/s 46066
step   1700 | loss 2.3062 | lr 2.38e-04 | grad 1.15 | tok/s 46914
step   1710 | loss 1.9417 | lr 2.42e-04 | grad 1.07 | tok/s 45625
step   1720 | loss 1.9298 | lr 2.45e-04 | grad 1.16 | tok/s 45827
step   1730 | loss 2.0993 | lr 2.49e-04 | grad 1.17 | tok/s 45815
step   1740 | loss 1.9005 | lr 2.52e-04 | grad 0.84 | tok/s 46502
step   1750 | loss 1.8185 | lr 2.56e-04 | grad 0.79 | tok/s 44846
step   1760 | loss 2.1416 | lr 2.59e-04 | grad 0.82 | tok/s 45484
step   1770 | loss 1.9781 | lr 2.62e-04 | grad 0.82 | tok/s 46471
step   1780 | loss 1.8408 | lr 2.65e-04 | grad 1.16 | tok/s 44765
step   1790 | loss 2.0691 | lr 2.68e-04 | grad 0.91 | tok/s 45631
step   1800 | loss 1.7635 | lr 2.71e-04 | grad 0.80 | tok/s 46391

Training complete! Final step: 1802
