Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b6k2/level87b6k2_100m_20260119_152330
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b6k2, 58,234,496 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6398 | lr 2.70e-06 | grad 50.25 | tok/s 17815
step     20 | loss 5.6170 | lr 5.70e-06 | grad 20.88 | tok/s 37473
step     30 | loss 5.7014 | lr 8.70e-06 | grad 16.75 | tok/s 39656
step     40 | loss 5.6560 | lr 1.17e-05 | grad 17.88 | tok/s 39636
step     50 | loss 5.5532 | lr 1.47e-05 | grad 15.69 | tok/s 39656
step     60 | loss 5.4002 | lr 1.77e-05 | grad 38.75 | tok/s 38791
step     70 | loss 5.0626 | lr 2.07e-05 | grad 33.75 | tok/s 37349
step     80 | loss 4.7428 | lr 2.37e-05 | grad 15.25 | tok/s 38801
step     90 | loss 4.6874 | lr 2.67e-05 | grad 10.81 | tok/s 37447
step    100 | loss 4.3920 | lr 2.97e-05 | grad 10.69 | tok/s 37848
step    110 | loss 4.0950 | lr 3.27e-05 | grad 23.50 | tok/s 37228
step    120 | loss 4.1422 | lr 3.57e-05 | grad 58.50 | tok/s 36701
step    130 | loss 4.0303 | lr 3.87e-05 | grad 8.06 | tok/s 37519
step    140 | loss 3.7249 | lr 4.17e-05 | grad 88.00 | tok/s 37536
step    150 | loss 3.6521 | lr 4.47e-05 | grad 8.19 | tok/s 35788
step    160 | loss 3.4734 | lr 4.77e-05 | grad 22.62 | tok/s 36050
step    170 | loss 3.5024 | lr 5.07e-05 | grad 21.00 | tok/s 37332
step    180 | loss 3.4947 | lr 5.37e-05 | grad 23.25 | tok/s 37332
step    190 | loss 3.2857 | lr 5.67e-05 | grad 9.19 | tok/s 37912
step    200 | loss 3.1635 | lr 5.97e-05 | grad 58.75 | tok/s 38787
step    210 | loss 3.2091 | lr 6.27e-05 | grad 8.31 | tok/s 37200
step    220 | loss 3.1779 | lr 6.57e-05 | grad 13.94 | tok/s 38392
step    230 | loss 2.9429 | lr 6.87e-05 | grad 25.62 | tok/s 37080
step    240 | loss 3.0727 | lr 7.17e-05 | grad 7.12 | tok/s 37798
step    250 | loss 2.9428 | lr 7.47e-05 | grad 8.69 | tok/s 37197
step    260 | loss 2.8774 | lr 7.77e-05 | grad 12.56 | tok/s 35879
step    270 | loss 2.7677 | lr 8.07e-05 | grad 6.31 | tok/s 37162
step    280 | loss 2.7090 | lr 8.37e-05 | grad 9.50 | tok/s 37121
step    290 | loss 2.6721 | lr 8.67e-05 | grad 2.61 | tok/s 39205
step    300 | loss 2.5789 | lr 8.97e-05 | grad 3.91 | tok/s 39146
step    310 | loss 2.5067 | lr 9.27e-05 | grad 4.28 | tok/s 39150
step    320 | loss 2.5808 | lr 9.57e-05 | grad 23.50 | tok/s 37739
step    330 | loss 2.6547 | lr 9.87e-05 | grad 9.44 | tok/s 36775
step    340 | loss 2.6595 | lr 1.02e-04 | grad 5.72 | tok/s 37565
step    350 | loss 2.6182 | lr 1.05e-04 | grad 4.81 | tok/s 36514
step    360 | loss 2.6188 | lr 1.08e-04 | grad 11.62 | tok/s 36898
step    370 | loss 2.4951 | lr 1.11e-04 | grad 4.53 | tok/s 37658
step    380 | loss 2.9483 | lr 1.14e-04 | grad 5.47 | tok/s 38631
step    390 | loss 2.5232 | lr 1.17e-04 | grad 4.47 | tok/s 37195
step    400 | loss 2.6250 | lr 1.20e-04 | grad 7.31 | tok/s 38103
step    410 | loss 2.3687 | lr 1.23e-04 | grad 5.12 | tok/s 36993
step    420 | loss 2.4568 | lr 1.26e-04 | grad 2.80 | tok/s 36733
step    430 | loss 2.5570 | lr 1.29e-04 | grad 3.33 | tok/s 36628
step    440 | loss 2.7131 | lr 1.32e-04 | grad 4.97 | tok/s 38094
step    450 | loss 2.4170 | lr 1.35e-04 | grad 2.66 | tok/s 37013
step    460 | loss 2.3852 | lr 1.38e-04 | grad 2.67 | tok/s 37169
step    470 | loss 2.4047 | lr 1.41e-04 | grad 3.59 | tok/s 37610
step    480 | loss 2.2873 | lr 1.44e-04 | grad 2.91 | tok/s 36270
step    490 | loss 2.1992 | lr 1.47e-04 | grad 2.45 | tok/s 36964
step    500 | loss 3.0506 | lr 1.50e-04 | grad 3.69 | tok/s 38128
step    510 | loss 2.2999 | lr 1.53e-04 | grad 6.97 | tok/s 37207
step    520 | loss 2.3236 | lr 1.56e-04 | grad 3.02 | tok/s 38423
step    530 | loss 2.7024 | lr 1.59e-04 | grad 4.41 | tok/s 37637
step    540 | loss 2.3032 | lr 1.62e-04 | grad 4.38 | tok/s 37617
step    550 | loss 2.1930 | lr 1.65e-04 | grad 1.46 | tok/s 38672
step    560 | loss 2.0034 | lr 1.68e-04 | grad 1.97 | tok/s 39145
step    570 | loss 2.2813 | lr 1.71e-04 | grad 3.31 | tok/s 38256
step    580 | loss 2.5923 | lr 1.74e-04 | grad 2.19 | tok/s 37744
step    590 | loss 2.7670 | lr 1.77e-04 | grad 3.56 | tok/s 36975
step    600 | loss 2.3406 | lr 1.80e-04 | grad 2.39 | tok/s 37084
step    610 | loss 2.3442 | lr 1.83e-04 | grad 2.34 | tok/s 38895
step    620 | loss 2.2030 | lr 1.86e-04 | grad 1.84 | tok/s 36903
step    630 | loss 2.1699 | lr 1.89e-04 | grad 1.70 | tok/s 38064
step    640 | loss 2.5690 | lr 1.92e-04 | grad 1.84 | tok/s 38101
step    650 | loss 2.1857 | lr 1.95e-04 | grad 2.62 | tok/s 37343
step    660 | loss 2.4728 | lr 1.98e-04 | grad 6.38 | tok/s 36855
step    670 | loss 2.3437 | lr 2.01e-04 | grad 3.16 | tok/s 38195
step    680 | loss 2.2985 | lr 2.04e-04 | grad 3.17 | tok/s 36810
step    690 | loss 2.2778 | lr 2.07e-04 | grad 1.85 | tok/s 37104
step    700 | loss 2.3531 | lr 2.10e-04 | grad 1.70 | tok/s 37321
step    710 | loss 2.2881 | lr 2.13e-04 | grad 1.62 | tok/s 37441
step    720 | loss 2.2611 | lr 2.16e-04 | grad 6.03 | tok/s 37287
step    730 | loss 2.3640 | lr 2.19e-04 | grad 1.90 | tok/s 37646
step    740 | loss 2.2232 | lr 2.22e-04 | grad 2.50 | tok/s 37347
step    750 | loss 2.0521 | lr 2.25e-04 | grad 1.94 | tok/s 36866
step    760 | loss 2.5831 | lr 2.28e-04 | grad 1.30 | tok/s 37363
step    770 | loss 2.0301 | lr 2.31e-04 | grad 1.55 | tok/s 37122
step    780 | loss 2.0884 | lr 2.34e-04 | grad 1.83 | tok/s 37552
step    790 | loss 2.0527 | lr 2.37e-04 | grad 1.43 | tok/s 37853
step    800 | loss 2.0362 | lr 2.40e-04 | grad 1.34 | tok/s 37874
step    810 | loss 2.0852 | lr 2.43e-04 | grad 2.66 | tok/s 37487
step    820 | loss 2.6499 | lr 2.46e-04 | grad 1.88 | tok/s 38466
step    830 | loss 2.1743 | lr 2.49e-04 | grad 1.09 | tok/s 39125
step    840 | loss 1.8808 | lr 2.52e-04 | grad 0.79 | tok/s 39115
step    850 | loss 2.4636 | lr 2.55e-04 | grad 1.68 | tok/s 37200
step    860 | loss 2.1691 | lr 2.58e-04 | grad 1.34 | tok/s 36410
step    870 | loss 2.0889 | lr 2.61e-04 | grad 1.06 | tok/s 37533
step    880 | loss 2.1353 | lr 2.64e-04 | grad 1.48 | tok/s 37346
step    890 | loss 1.9862 | lr 2.67e-04 | grad 1.05 | tok/s 37308
step    900 | loss 2.4587 | lr 2.70e-04 | grad 1.20 | tok/s 36340
step    910 | loss 2.0340 | lr 2.73e-04 | grad 1.16 | tok/s 37036
step    920 | loss 1.9824 | lr 2.76e-04 | grad 0.91 | tok/s 36898
step    930 | loss 2.0536 | lr 2.79e-04 | grad 1.44 | tok/s 36833
step    940 | loss 1.9621 | lr 2.82e-04 | grad 1.45 | tok/s 36456
step    950 | loss 2.0598 | lr 2.85e-04 | grad 1.48 | tok/s 37331
step    960 | loss 1.7535 | lr 2.88e-04 | grad 0.63 | tok/s 39118
step    970 | loss 1.5702 | lr 2.91e-04 | grad 0.62 | tok/s 39111
step    980 | loss 1.7680 | lr 2.94e-04 | grad 2.16 | tok/s 37993
step    990 | loss 2.1969 | lr 2.97e-04 | grad 0.97 | tok/s 36961
step   1000 | loss 1.9827 | lr 3.00e-04 | grad 0.75 | tok/s 36065
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9827.pt
step   1010 | loss 2.2958 | lr 1.06e-06 | grad 3.28 | tok/s 33900
step   1020 | loss 1.9299 | lr 1.27e-06 | grad 0.91 | tok/s 35722
step   1030 | loss 2.2148 | lr 1.62e-06 | grad 0.84 | tok/s 35093
step   1040 | loss 1.9011 | lr 2.12e-06 | grad 1.49 | tok/s 35876
step   1050 | loss 1.9350 | lr 2.77e-06 | grad 0.83 | tok/s 36068
step   1060 | loss 2.3255 | lr 3.56e-06 | grad 2.33 | tok/s 36113
step   1070 | loss 2.3791 | lr 4.50e-06 | grad 0.90 | tok/s 36224
step   1080 | loss 2.5856 | lr 5.58e-06 | grad 1.29 | tok/s 35737
step   1090 | loss 2.2942 | lr 6.81e-06 | grad 1.05 | tok/s 35992
step   1100 | loss 1.9653 | lr 8.17e-06 | grad 0.93 | tok/s 35855
step   1110 | loss 2.0888 | lr 9.68e-06 | grad 0.96 | tok/s 36405
step   1120 | loss 2.4274 | lr 1.13e-05 | grad 0.95 | tok/s 36885
step   1130 | loss 1.9815 | lr 1.31e-05 | grad 0.74 | tok/s 35085
step   1140 | loss 1.8765 | lr 1.50e-05 | grad 0.85 | tok/s 35884
step   1150 | loss 2.1538 | lr 1.71e-05 | grad 1.23 | tok/s 35835
step   1160 | loss 1.8228 | lr 1.93e-05 | grad 0.68 | tok/s 35557
step   1170 | loss 2.2133 | lr 2.16e-05 | grad 0.84 | tok/s 35852
step   1180 | loss 1.8994 | lr 2.40e-05 | grad 0.95 | tok/s 37760
step   1190 | loss 1.8638 | lr 2.66e-05 | grad 0.71 | tok/s 37729
step   1200 | loss 1.8113 | lr 2.93e-05 | grad 0.67 | tok/s 37733
step   1210 | loss 1.8045 | lr 3.21e-05 | grad 0.64 | tok/s 37697
step   1220 | loss 1.8137 | lr 3.50e-05 | grad 0.80 | tok/s 37418
step   1230 | loss 1.7487 | lr 3.80e-05 | grad 0.71 | tok/s 36102
step   1240 | loss 1.8725 | lr 4.12e-05 | grad 0.81 | tok/s 35486
step   1250 | loss 2.0083 | lr 4.45e-05 | grad 3.12 | tok/s 36515
step   1260 | loss 2.0153 | lr 4.78e-05 | grad 2.34 | tok/s 36385
step   1270 | loss 2.0875 | lr 5.13e-05 | grad 1.28 | tok/s 36019
step   1280 | loss 1.9640 | lr 5.48e-05 | grad 0.96 | tok/s 35530
step   1290 | loss 1.8892 | lr 5.85e-05 | grad 1.00 | tok/s 35465
step   1300 | loss 1.9262 | lr 6.22e-05 | grad 0.85 | tok/s 35180
step   1310 | loss 2.0160 | lr 6.61e-05 | grad 0.92 | tok/s 35155
step   1320 | loss 1.9550 | lr 7.00e-05 | grad 1.23 | tok/s 35689
step   1330 | loss 1.8886 | lr 7.40e-05 | grad 0.80 | tok/s 35832
step   1340 | loss 1.8494 | lr 7.81e-05 | grad 1.05 | tok/s 35770
step   1350 | loss 1.8969 | lr 8.22e-05 | grad 2.02 | tok/s 36764
step   1360 | loss 1.7815 | lr 8.64e-05 | grad 0.77 | tok/s 34943
step   1370 | loss 1.9234 | lr 9.07e-05 | grad 0.70 | tok/s 35502
step   1380 | loss 1.9744 | lr 9.50e-05 | grad 1.04 | tok/s 35779
step   1390 | loss 1.8716 | lr 9.94e-05 | grad 1.62 | tok/s 34859
step   1400 | loss 1.9136 | lr 1.04e-04 | grad 9.25 | tok/s 36332

Training complete! Final step: 1405
