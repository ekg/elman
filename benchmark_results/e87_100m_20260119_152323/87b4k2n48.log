Using device: cuda
Output directory: benchmark_results/e87_100m_20260119_152323/87b4k2n48/level87b4k2n48_100m_20260119_152330
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 87b4k2n48, 50,432,000 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.8096 | lr 2.70e-06 | grad 114.50 | tok/s 13292
step     20 | loss 5.7409 | lr 5.70e-06 | grad 17.88 | tok/s 21651
step     30 | loss 5.8469 | lr 8.70e-06 | grad 20.50 | tok/s 22944
step     40 | loss 5.8136 | lr 1.17e-05 | grad 20.00 | tok/s 22911
step     50 | loss 5.7610 | lr 1.47e-05 | grad 17.75 | tok/s 22899
step     60 | loss 5.6305 | lr 1.77e-05 | grad 53.25 | tok/s 22420
step     70 | loss 5.4351 | lr 2.07e-05 | grad 187.00 | tok/s 21654
step     80 | loss 5.1717 | lr 2.37e-05 | grad 16.00 | tok/s 22480
step     90 | loss 5.0729 | lr 2.67e-05 | grad 19.62 | tok/s 21711
step    100 | loss 4.7022 | lr 2.97e-05 | grad 10.31 | tok/s 21911
step    110 | loss 4.3132 | lr 3.27e-05 | grad 16.38 | tok/s 21536
step    120 | loss 4.1601 | lr 3.57e-05 | grad 12.31 | tok/s 21221
step    130 | loss 3.9380 | lr 3.87e-05 | grad 12.50 | tok/s 21717
step    140 | loss 3.5645 | lr 4.17e-05 | grad 11.50 | tok/s 21748
step    150 | loss 3.3721 | lr 4.47e-05 | grad 6.34 | tok/s 20731
step    160 | loss 3.2191 | lr 4.77e-05 | grad 6.34 | tok/s 20910
step    170 | loss 3.3051 | lr 5.07e-05 | grad 45.75 | tok/s 21643
step    180 | loss 3.3327 | lr 5.37e-05 | grad 5.16 | tok/s 21721
step    190 | loss 3.1441 | lr 5.67e-05 | grad 9.44 | tok/s 22059
step    200 | loss 3.0391 | lr 5.97e-05 | grad 6.59 | tok/s 22574
step    210 | loss 3.0678 | lr 6.27e-05 | grad 5.59 | tok/s 21606
step    220 | loss 3.0977 | lr 6.57e-05 | grad 4.88 | tok/s 22326
step    230 | loss 2.8470 | lr 6.87e-05 | grad 4.16 | tok/s 21537
step    240 | loss 2.9594 | lr 7.17e-05 | grad 7.06 | tok/s 21985
step    250 | loss 2.8096 | lr 7.47e-05 | grad 3.39 | tok/s 21673
step    260 | loss 2.7711 | lr 7.77e-05 | grad 3.34 | tok/s 20798
step    270 | loss 2.6704 | lr 8.07e-05 | grad 5.94 | tok/s 21582
step    280 | loss 2.6076 | lr 8.37e-05 | grad 5.78 | tok/s 21535
step    290 | loss 2.5782 | lr 8.67e-05 | grad 2.81 | tok/s 22728
step    300 | loss 2.4890 | lr 8.97e-05 | grad 3.34 | tok/s 22727
step    310 | loss 2.4105 | lr 9.27e-05 | grad 2.86 | tok/s 22721
step    320 | loss 2.5237 | lr 9.57e-05 | grad 23.88 | tok/s 21878
step    330 | loss 2.5689 | lr 9.87e-05 | grad 5.22 | tok/s 21335
step    340 | loss 2.5681 | lr 1.02e-04 | grad 4.62 | tok/s 21788
step    350 | loss 2.5417 | lr 1.05e-04 | grad 3.30 | tok/s 21165
step    360 | loss 2.5284 | lr 1.08e-04 | grad 7.59 | tok/s 21422
step    370 | loss 2.4202 | lr 1.11e-04 | grad 6.41 | tok/s 21846
step    380 | loss 2.8963 | lr 1.14e-04 | grad 4.50 | tok/s 22405
step    390 | loss 2.4838 | lr 1.17e-04 | grad 2.88 | tok/s 21556
step    400 | loss 2.5488 | lr 1.20e-04 | grad 7.34 | tok/s 22134
step    410 | loss 2.3222 | lr 1.23e-04 | grad 4.59 | tok/s 21435
step    420 | loss 2.4109 | lr 1.26e-04 | grad 2.91 | tok/s 21309
step    430 | loss 2.5154 | lr 1.29e-04 | grad 4.19 | tok/s 21222
step    440 | loss 2.6652 | lr 1.32e-04 | grad 3.94 | tok/s 22085
step    450 | loss 2.3576 | lr 1.35e-04 | grad 4.38 | tok/s 21480
step    460 | loss 2.3356 | lr 1.38e-04 | grad 2.28 | tok/s 21554
step    470 | loss 2.3317 | lr 1.41e-04 | grad 2.25 | tok/s 21820
step    480 | loss 2.2506 | lr 1.44e-04 | grad 1.97 | tok/s 21029
step    490 | loss 2.1390 | lr 1.47e-04 | grad 1.72 | tok/s 21437
step    500 | loss 3.0148 | lr 1.50e-04 | grad 3.45 | tok/s 22098
step    510 | loss 2.2601 | lr 1.53e-04 | grad 3.19 | tok/s 21598
step    520 | loss 2.2552 | lr 1.56e-04 | grad 2.72 | tok/s 22291
step    530 | loss 2.6633 | lr 1.59e-04 | grad 4.66 | tok/s 21821
step    540 | loss 2.2754 | lr 1.62e-04 | grad 4.19 | tok/s 21794
step    550 | loss 2.1170 | lr 1.65e-04 | grad 1.66 | tok/s 22407
step    560 | loss 1.9355 | lr 1.68e-04 | grad 1.55 | tok/s 22725
step    570 | loss 2.2321 | lr 1.71e-04 | grad 3.84 | tok/s 22195
step    580 | loss 2.5300 | lr 1.74e-04 | grad 2.38 | tok/s 21894
step    590 | loss 2.6883 | lr 1.77e-04 | grad 2.98 | tok/s 21446
step    600 | loss 2.2807 | lr 1.80e-04 | grad 2.27 | tok/s 21503
step    610 | loss 2.3206 | lr 1.83e-04 | grad 3.09 | tok/s 22556
step    620 | loss 2.1735 | lr 1.86e-04 | grad 3.23 | tok/s 21378
step    630 | loss 2.1221 | lr 1.89e-04 | grad 1.72 | tok/s 22069
step    640 | loss 2.5591 | lr 1.92e-04 | grad 1.90 | tok/s 22084
step    650 | loss 2.1559 | lr 1.95e-04 | grad 2.52 | tok/s 21668
step    660 | loss 2.4397 | lr 1.98e-04 | grad 5.38 | tok/s 21376
step    670 | loss 2.3050 | lr 2.01e-04 | grad 2.95 | tok/s 22141
step    680 | loss 2.2422 | lr 2.04e-04 | grad 2.27 | tok/s 21340
step    690 | loss 2.2380 | lr 2.07e-04 | grad 2.45 | tok/s 21525
step    700 | loss 2.3201 | lr 2.10e-04 | grad 1.86 | tok/s 21630
step    710 | loss 2.2596 | lr 2.13e-04 | grad 1.64 | tok/s 21756
step    720 | loss 2.1979 | lr 2.16e-04 | grad 3.38 | tok/s 21661
step    730 | loss 2.3133 | lr 2.19e-04 | grad 2.02 | tok/s 21874
step    740 | loss 2.2169 | lr 2.22e-04 | grad 3.20 | tok/s 21670
step    750 | loss 2.0394 | lr 2.25e-04 | grad 2.17 | tok/s 21426
step    760 | loss 2.4581 | lr 2.28e-04 | grad 1.22 | tok/s 21695
step    770 | loss 2.0025 | lr 2.31e-04 | grad 1.60 | tok/s 21552
step    780 | loss 2.0493 | lr 2.34e-04 | grad 1.52 | tok/s 21802
step    790 | loss 2.0654 | lr 2.37e-04 | grad 1.31 | tok/s 21991
step    800 | loss 1.9952 | lr 2.40e-04 | grad 1.30 | tok/s 22000
step    810 | loss 2.0217 | lr 2.43e-04 | grad 2.84 | tok/s 21777
step    820 | loss 2.6687 | lr 2.46e-04 | grad 2.17 | tok/s 22350

Training complete! Final step: 826
