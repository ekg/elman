E21 10-minute comparison: 600s per model
Batch size: 64, Seq len: 512
============================================================
Launching E1_d1280x6 on GPU 0...
Launching Mamba2_50M on GPU 1...
Launching minLSTM on GPU 2...
Launching E18A on GPU 3...
Launching E21 on GPU 4...
Launching E21S on GPU 5...
Launching E21T on GPU 6...
Launching E21L on GPU 7...

All 8 experiments launched! Running for 600s each...
Monitoring progress...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  6/8 completed...
  8/8 completed...

============================================================
FINAL RESULTS - 10 minutes training time
============================================================
FINAL E1_d1280x6: steps=1739, tokens=57.0M, loss=1.4503, tok/s=95.0K
FINAL Mamba2_50M: steps=1052, tokens=34.5M, loss=1.3873, tok/s=57.5K
FINAL minLSTM: steps=1079, tokens=35.4M, loss=1.6074, tok/s=58.9K
FINAL E18A: steps=1833, tokens=60.1M, loss=1.4770, tok/s=100.1K
FINAL E21: steps=184, tokens=6.0M, loss=2.2989, tok/s=10.0K
FINAL E21S: steps=371, tokens=12.2M, loss=1.9354, tok/s=20.2K
FINAL E21T: steps=23, tokens=0.8M, loss=2.9810, tok/s=1.2K
FINAL E21L: steps=24, tokens=0.8M, loss=2.9486, tok/s=1.3K

============================================================
SUMMARY TABLE
============================================================
Model               Loss      Tok/s       Params
--------------------------------------------------
E1_d1280x6        1.4503      95.0K   49,505,280
Mamba2_50M        1.3873      57.5K   50,928,750
minLSTM           1.6074      58.9K   29,836,800
E18A              1.4770     100.1K   49,505,280
E21               2.2989      10.0K   75,333,984
E21S              1.9354      20.2K   56,997,984
E21T              2.9810       1.2K   75,333,984
E21L              2.9486       1.3K   75,333,984
============================================================
