Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_160/levelE88_100m_20260126_140146
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,624,544 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.3027 | lr 3.00e-04 | grad 24.12 | tok/s 6222
step     20 | loss 3.0512 | lr 3.00e-04 | grad 9.75 | tok/s 18842
step     30 | loss 2.6908 | lr 3.00e-04 | grad 7.31 | tok/s 19063
step     40 | loss 2.4586 | lr 3.00e-04 | grad 5.19 | tok/s 18228
step     50 | loss 3.0351 | lr 3.00e-04 | grad 21.50 | tok/s 18524
step     60 | loss 2.0595 | lr 3.00e-04 | grad 4.12 | tok/s 19070
step     70 | loss 1.8558 | lr 3.00e-04 | grad 5.53 | tok/s 19324
step     80 | loss 6.4293 | lr 3.00e-04 | grad 52.25 | tok/s 19395
step     90 | loss 5.4197 | lr 3.00e-04 | grad 9.75 | tok/s 19707
step    100 | loss 4.1509 | lr 3.00e-04 | grad 8.12 | tok/s 19672
step    110 | loss 3.6529 | lr 3.00e-04 | grad 19.00 | tok/s 19627
step    120 | loss 3.2043 | lr 3.00e-04 | grad 12.00 | tok/s 19605
step    130 | loss 2.9754 | lr 3.00e-04 | grad 13.50 | tok/s 19593
step    140 | loss 2.7611 | lr 3.00e-04 | grad 11.69 | tok/s 19540
step    150 | loss 2.7407 | lr 3.00e-04 | grad 10.88 | tok/s 19548
step    160 | loss 2.4633 | lr 3.00e-04 | grad 10.38 | tok/s 19531
step    170 | loss 2.4733 | lr 3.00e-04 | grad 13.06 | tok/s 19533
step    180 | loss 2.2799 | lr 3.00e-04 | grad 7.72 | tok/s 19520
step    190 | loss 2.3742 | lr 3.00e-04 | grad 7.38 | tok/s 19531
step    200 | loss 2.1094 | lr 3.00e-04 | grad 5.81 | tok/s 19522
step    210 | loss 2.1283 | lr 3.00e-04 | grad 7.22 | tok/s 19516
step    220 | loss 2.1576 | lr 3.00e-04 | grad 4.09 | tok/s 19245
step    230 | loss 2.1215 | lr 3.00e-04 | grad 3.80 | tok/s 17297
step    240 | loss 2.3073 | lr 3.00e-04 | grad 5.44 | tok/s 18087
step    250 | loss 2.1005 | lr 3.00e-04 | grad 3.02 | tok/s 18605
step    260 | loss 1.5280 | lr 3.00e-04 | grad 3.67 | tok/s 19186
step    270 | loss 2.0804 | lr 3.00e-04 | grad 3.53 | tok/s 18923
step    280 | loss 2.2432 | lr 3.00e-04 | grad 5.94 | tok/s 18561
step    290 | loss 1.4837 | lr 3.00e-04 | grad 35.25 | tok/s 19570
step    300 | loss 0.5721 | lr 3.00e-04 | grad 4.94 | tok/s 19514
step    310 | loss 2.3871 | lr 3.00e-04 | grad 4.75 | tok/s 19188
step    320 | loss 1.9085 | lr 3.00e-04 | grad 6.50 | tok/s 18740
step    330 | loss 1.9508 | lr 3.00e-04 | grad 3.33 | tok/s 18104
step    340 | loss 2.2672 | lr 3.00e-04 | grad 3.53 | tok/s 18388
step    350 | loss 1.8640 | lr 3.00e-04 | grad 4.56 | tok/s 18848
step    360 | loss 1.2054 | lr 3.00e-04 | grad 8.69 | tok/s 19265
step    370 | loss 1.7947 | lr 3.00e-04 | grad 3.03 | tok/s 17474
step    380 | loss 1.7454 | lr 3.00e-04 | grad 3.33 | tok/s 18592
step    390 | loss 1.5321 | lr 3.00e-04 | grad 2.61 | tok/s 19409
step    400 | loss 1.4815 | lr 3.00e-04 | grad 3.09 | tok/s 19247
step    410 | loss 1.2633 | lr 3.00e-04 | grad 2.44 | tok/s 18840
step    420 | loss 1.8104 | lr 3.00e-04 | grad 5.03 | tok/s 17980
step    430 | loss 2.1551 | lr 3.00e-04 | grad 3.42 | tok/s 19151
step    440 | loss 2.1502 | lr 3.00e-04 | grad 4.50 | tok/s 18092
step    450 | loss 2.1440 | lr 3.00e-04 | grad 3.00 | tok/s 18727
step    460 | loss 1.7232 | lr 3.00e-04 | grad 3.52 | tok/s 18323
step    470 | loss 1.8462 | lr 3.00e-04 | grad 3.17 | tok/s 18858
step    480 | loss 2.2655 | lr 3.00e-04 | grad 7.44 | tok/s 18906
step    490 | loss 1.7931 | lr 3.00e-04 | grad 2.70 | tok/s 17824
step    500 | loss 1.6683 | lr 3.00e-04 | grad 4.03 | tok/s 19048
step    510 | loss 1.7019 | lr 3.00e-04 | grad 2.88 | tok/s 19297
step    520 | loss 1.6501 | lr 3.00e-04 | grad 2.45 | tok/s 19253
step    530 | loss 1.9287 | lr 3.00e-04 | grad 2.67 | tok/s 18551
step    540 | loss 1.7334 | lr 3.00e-04 | grad 2.91 | tok/s 18521
step    550 | loss 1.5718 | lr 3.00e-04 | grad 3.41 | tok/s 18123
step    560 | loss 1.7344 | lr 3.00e-04 | grad 3.00 | tok/s 17668
step    570 | loss 1.6656 | lr 3.00e-04 | grad 3.84 | tok/s 18166
step    580 | loss 1.5494 | lr 3.00e-04 | grad 2.59 | tok/s 18095
step    590 | loss 1.8604 | lr 3.00e-04 | grad 3.48 | tok/s 18532
step    600 | loss 1.8359 | lr 3.00e-04 | grad 2.52 | tok/s 17910
step    610 | loss 1.6234 | lr 3.00e-04 | grad 2.78 | tok/s 18840
step    620 | loss 1.5484 | lr 3.00e-04 | grad 2.69 | tok/s 17828
step    630 | loss 1.6507 | lr 3.00e-04 | grad 4.91 | tok/s 18017
step    640 | loss 1.8124 | lr 3.00e-04 | grad 2.84 | tok/s 18460
step    650 | loss 1.6915 | lr 3.00e-04 | grad 3.08 | tok/s 18579
step    660 | loss 1.7002 | lr 3.00e-04 | grad 2.67 | tok/s 18649
step    670 | loss 1.9197 | lr 3.00e-04 | grad 3.66 | tok/s 18769
step    680 | loss 1.7331 | lr 3.00e-04 | grad 2.62 | tok/s 18397
step    690 | loss 1.8485 | lr 3.00e-04 | grad 3.55 | tok/s 18975
step    700 | loss 1.4057 | lr 3.00e-04 | grad 3.22 | tok/s 19341
step    710 | loss 1.5866 | lr 3.00e-04 | grad 2.72 | tok/s 18050
step    720 | loss 1.4741 | lr 3.00e-04 | grad 3.61 | tok/s 17807
step    730 | loss 1.2870 | lr 3.00e-04 | grad 3.16 | tok/s 19267
step    740 | loss 1.5061 | lr 3.00e-04 | grad 2.56 | tok/s 19020
step    750 | loss 1.2101 | lr 3.00e-04 | grad 2.89 | tok/s 19298
step    760 | loss 1.1212 | lr 3.00e-04 | grad 2.39 | tok/s 19278
step    770 | loss 1.0694 | lr 3.00e-04 | grad 2.36 | tok/s 19392
step    780 | loss 1.0068 | lr 3.00e-04 | grad 2.19 | tok/s 19365
step    790 | loss 1.1344 | lr 3.00e-04 | grad 3.55 | tok/s 18782
step    800 | loss 1.8215 | lr 3.00e-04 | grad 5.84 | tok/s 18698
step    810 | loss 1.7162 | lr 3.00e-04 | grad 2.41 | tok/s 18539
step    820 | loss 1.7308 | lr 3.00e-04 | grad 4.34 | tok/s 17835

Training complete! Final step: 822
