Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_93/levelE88_100m_20260126_133524
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 477,310,688 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.1108 | lr 3.00e-04 | grad 17.50 | tok/s 9257
step     20 | loss 3.2355 | lr 3.00e-04 | grad 8.25 | tok/s 17260
step     30 | loss 3.2151 | lr 3.00e-04 | grad 11.19 | tok/s 18226
step     40 | loss 4.9844 | lr 3.00e-04 | grad 56.50 | tok/s 18514
step     50 | loss 4.9164 | lr 3.00e-04 | grad 21.00 | tok/s 18677
step     60 | loss 3.6290 | lr 3.00e-04 | grad 15.44 | tok/s 18617
step     70 | loss 3.0069 | lr 3.00e-04 | grad 10.38 | tok/s 18555
step     80 | loss 2.6151 | lr 3.00e-04 | grad 7.16 | tok/s 18525
step     90 | loss 2.5988 | lr 3.00e-04 | grad 7.22 | tok/s 18463
step    100 | loss 2.3559 | lr 3.00e-04 | grad 4.19 | tok/s 18452
step    110 | loss 2.3564 | lr 3.00e-04 | grad 4.94 | tok/s 18327
step    120 | loss 2.7646 | lr 3.00e-04 | grad 3.55 | tok/s 17393
step    130 | loss 2.1494 | lr 3.00e-04 | grad 7.31 | tok/s 17783
step    140 | loss 2.3705 | lr 3.00e-04 | grad 9.31 | tok/s 17820
step    150 | loss 1.4611 | lr 3.00e-04 | grad 6.94 | tok/s 18258
step    160 | loss 2.3526 | lr 3.00e-04 | grad 3.20 | tok/s 17645
step    170 | loss 2.3271 | lr 3.00e-04 | grad 2.88 | tok/s 17428
step    180 | loss 1.8083 | lr 3.00e-04 | grad 4.16 | tok/s 17860
step    190 | loss 1.9410 | lr 3.00e-04 | grad 3.75 | tok/s 17477
step    200 | loss 1.6638 | lr 3.00e-04 | grad 2.58 | tok/s 18284
step    210 | loss 1.9053 | lr 3.00e-04 | grad 8.94 | tok/s 17348
step    220 | loss 2.2199 | lr 3.00e-04 | grad 5.16 | tok/s 17547
step    230 | loss 2.0559 | lr 3.00e-04 | grad 3.47 | tok/s 17540
step    240 | loss 2.2527 | lr 3.00e-04 | grad 6.91 | tok/s 17770
step    250 | loss 1.7794 | lr 3.00e-04 | grad 2.19 | tok/s 17647
step    260 | loss 1.9092 | lr 3.00e-04 | grad 3.88 | tok/s 18121
step    270 | loss 1.8305 | lr 3.00e-04 | grad 2.80 | tok/s 17697
step    280 | loss 1.7837 | lr 3.00e-04 | grad 2.28 | tok/s 16640
step    290 | loss 1.6809 | lr 3.00e-04 | grad 2.88 | tok/s 17197
step    300 | loss 1.9784 | lr 3.00e-04 | grad 2.66 | tok/s 17296
step    310 | loss 1.6748 | lr 3.00e-04 | grad 2.42 | tok/s 17209
step    320 | loss 1.8949 | lr 3.00e-04 | grad 4.81 | tok/s 17397
step    330 | loss 1.7296 | lr 3.00e-04 | grad 2.53 | tok/s 17629
step    340 | loss 2.0597 | lr 3.00e-04 | grad 2.80 | tok/s 17512
step    350 | loss 1.7056 | lr 3.00e-04 | grad 2.53 | tok/s 18016
step    360 | loss 1.5917 | lr 3.00e-04 | grad 2.45 | tok/s 17228
step    370 | loss 1.4822 | lr 3.00e-04 | grad 2.28 | tok/s 18165
step    380 | loss 1.2064 | lr 3.00e-04 | grad 2.03 | tok/s 18308
step    390 | loss 1.1107 | lr 3.00e-04 | grad 2.02 | tok/s 18312

Training complete! Final step: 393
