Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_59/levelE88_100m_20260126_132213
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,750,256 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.0830 | lr 3.00e-04 | grad 18.62 | tok/s 6076
step     20 | loss 2.8293 | lr 3.00e-04 | grad 10.56 | tok/s 16303
step     30 | loss 2.6837 | lr 3.00e-04 | grad 7.44 | tok/s 16509
step     40 | loss 2.4493 | lr 3.00e-04 | grad 4.50 | tok/s 15776
step     50 | loss 3.0731 | lr 3.00e-04 | grad 15.19 | tok/s 16068
step     60 | loss 2.0683 | lr 3.00e-04 | grad 4.91 | tok/s 16541
step     70 | loss 1.8598 | lr 3.00e-04 | grad 5.47 | tok/s 16770
step     80 | loss 6.6392 | lr 3.00e-04 | grad 91.00 | tok/s 16861
step     90 | loss 5.8670 | lr 3.00e-04 | grad 12.94 | tok/s 17139
step    100 | loss 4.3798 | lr 3.00e-04 | grad 10.19 | tok/s 17103
step    110 | loss 3.6907 | lr 3.00e-04 | grad 14.19 | tok/s 17092
step    120 | loss 3.2858 | lr 3.00e-04 | grad 13.19 | tok/s 17094
step    130 | loss 3.0772 | lr 3.00e-04 | grad 16.50 | tok/s 17031
step    140 | loss 2.7811 | lr 3.00e-04 | grad 10.94 | tok/s 16999
step    150 | loss 2.7320 | lr 3.00e-04 | grad 11.81 | tok/s 16995
step    160 | loss 2.3868 | lr 3.00e-04 | grad 9.56 | tok/s 16972
step    170 | loss 2.4712 | lr 3.00e-04 | grad 12.00 | tok/s 16964
step    180 | loss 2.2753 | lr 3.00e-04 | grad 8.31 | tok/s 16970
step    190 | loss 2.4404 | lr 3.00e-04 | grad 7.50 | tok/s 16973
step    200 | loss 2.1136 | lr 3.00e-04 | grad 5.94 | tok/s 16943
step    210 | loss 2.1241 | lr 3.00e-04 | grad 8.94 | tok/s 16966
step    220 | loss 2.1658 | lr 3.00e-04 | grad 4.19 | tok/s 16726
step    230 | loss 2.0584 | lr 3.00e-04 | grad 5.06 | tok/s 16554
step    240 | loss 2.3215 | lr 3.00e-04 | grad 5.44 | tok/s 15713
step    250 | loss 2.1104 | lr 3.00e-04 | grad 3.00 | tok/s 16131
step    260 | loss 1.5434 | lr 3.00e-04 | grad 3.38 | tok/s 16639
step    270 | loss 2.0893 | lr 3.00e-04 | grad 3.14 | tok/s 16424
step    280 | loss 2.2552 | lr 3.00e-04 | grad 5.31 | tok/s 16113
step    290 | loss 1.3919 | lr 3.00e-04 | grad 3.83 | tok/s 16942
step    300 | loss 0.5560 | lr 3.00e-04 | grad 3.72 | tok/s 16915
step    310 | loss 2.3997 | lr 3.00e-04 | grad 4.16 | tok/s 16643
step    320 | loss 1.9076 | lr 3.00e-04 | grad 6.28 | tok/s 16294
step    330 | loss 1.9557 | lr 3.00e-04 | grad 3.25 | tok/s 15717
step    340 | loss 2.3025 | lr 3.00e-04 | grad 3.16 | tok/s 15964
step    350 | loss 1.8542 | lr 3.00e-04 | grad 4.03 | tok/s 16377
step    360 | loss 1.1727 | lr 3.00e-04 | grad 10.06 | tok/s 16716
step    370 | loss 1.8029 | lr 3.00e-04 | grad 2.84 | tok/s 15169
step    380 | loss 1.7721 | lr 3.00e-04 | grad 2.92 | tok/s 16167
step    390 | loss 1.5280 | lr 3.00e-04 | grad 2.44 | tok/s 16861
step    400 | loss 1.4813 | lr 3.00e-04 | grad 2.98 | tok/s 16711
step    410 | loss 1.2640 | lr 3.00e-04 | grad 2.31 | tok/s 16342
step    420 | loss 1.8139 | lr 3.00e-04 | grad 4.78 | tok/s 15607
step    430 | loss 2.1784 | lr 3.00e-04 | grad 3.25 | tok/s 16609
step    440 | loss 2.1581 | lr 3.00e-04 | grad 4.25 | tok/s 15706
step    450 | loss 2.0731 | lr 3.00e-04 | grad 2.97 | tok/s 16256
step    460 | loss 1.7247 | lr 3.00e-04 | grad 3.47 | tok/s 15898
step    470 | loss 1.8362 | lr 3.00e-04 | grad 2.73 | tok/s 16410
step    480 | loss 2.2445 | lr 3.00e-04 | grad 7.53 | tok/s 16413
step    490 | loss 1.7938 | lr 3.00e-04 | grad 2.83 | tok/s 15511
step    500 | loss 1.6800 | lr 3.00e-04 | grad 3.84 | tok/s 16580
step    510 | loss 1.7087 | lr 3.00e-04 | grad 2.77 | tok/s 16806
step    520 | loss 1.6569 | lr 3.00e-04 | grad 2.33 | tok/s 16750
step    530 | loss 1.9147 | lr 3.00e-04 | grad 2.66 | tok/s 16116
step    540 | loss 1.7435 | lr 3.00e-04 | grad 2.53 | tok/s 14789
step    550 | loss 1.5747 | lr 3.00e-04 | grad 3.16 | tok/s 15771
step    560 | loss 1.7236 | lr 3.00e-04 | grad 2.94 | tok/s 15369
step    570 | loss 1.6635 | lr 3.00e-04 | grad 3.84 | tok/s 15794
step    580 | loss 1.5507 | lr 3.00e-04 | grad 2.44 | tok/s 15729
step    590 | loss 1.8636 | lr 3.00e-04 | grad 3.42 | tok/s 16141
step    600 | loss 1.8221 | lr 3.00e-04 | grad 2.47 | tok/s 15602
step    610 | loss 1.6211 | lr 3.00e-04 | grad 2.64 | tok/s 16369
step    620 | loss 1.5511 | lr 3.00e-04 | grad 2.69 | tok/s 15541
step    630 | loss 1.6575 | lr 3.00e-04 | grad 4.62 | tok/s 15647
step    640 | loss 1.8077 | lr 3.00e-04 | grad 2.62 | tok/s 16068
step    650 | loss 1.6892 | lr 3.00e-04 | grad 2.77 | tok/s 16162
step    660 | loss 1.7022 | lr 3.00e-04 | grad 2.22 | tok/s 16206
step    670 | loss 1.9269 | lr 3.00e-04 | grad 3.52 | tok/s 16320
step    680 | loss 1.7323 | lr 3.00e-04 | grad 2.62 | tok/s 15999
step    690 | loss 1.8240 | lr 3.00e-04 | grad 3.47 | tok/s 16537
step    700 | loss 1.4165 | lr 3.00e-04 | grad 3.31 | tok/s 16880
step    710 | loss 1.5986 | lr 3.00e-04 | grad 2.59 | tok/s 15740

Training complete! Final step: 717
