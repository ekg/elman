Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_47/levelE88_100m_20260126_131537
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 495,711,890 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2519 | lr 3.00e-04 | grad 11.19 | tok/s 5298
step     20 | loss 2.5753 | lr 3.00e-04 | grad 4.69 | tok/s 10550
step     30 | loss 2.5064 | lr 3.00e-04 | grad 2.50 | tok/s 10622
step     40 | loss 2.3178 | lr 3.00e-04 | grad 2.56 | tok/s 10155
step     50 | loss 2.9476 | lr 3.00e-04 | grad 8.31 | tok/s 10297
step     60 | loss 2.0674 | lr 3.00e-04 | grad 3.08 | tok/s 10610
step     70 | loss 1.9198 | lr 3.00e-04 | grad 3.53 | tok/s 10732
step     80 | loss 5.1342 | lr 3.00e-04 | grad 60.25 | tok/s 10771
step     90 | loss 5.0072 | lr 3.00e-04 | grad 8.06 | tok/s 10952
step    100 | loss 4.2025 | lr 3.00e-04 | grad 7.50 | tok/s 10886
step    110 | loss 3.7544 | lr 3.00e-04 | grad 14.38 | tok/s 10888
step    120 | loss 3.3529 | lr 3.00e-04 | grad 11.50 | tok/s 10892
step    130 | loss 2.9778 | lr 3.00e-04 | grad 13.12 | tok/s 10894
step    140 | loss 2.5961 | lr 3.00e-04 | grad 7.59 | tok/s 10893
step    150 | loss 2.7287 | lr 3.00e-04 | grad 8.94 | tok/s 10895
step    160 | loss 2.2869 | lr 3.00e-04 | grad 6.75 | tok/s 10892
step    170 | loss 2.3689 | lr 3.00e-04 | grad 9.19 | tok/s 10896
step    180 | loss 2.1778 | lr 3.00e-04 | grad 4.06 | tok/s 10896
step    190 | loss 2.3619 | lr 3.00e-04 | grad 5.59 | tok/s 10893
step    200 | loss 2.0304 | lr 3.00e-04 | grad 4.28 | tok/s 10841
step    210 | loss 2.0180 | lr 3.00e-04 | grad 4.25 | tok/s 10895
step    220 | loss 2.1504 | lr 3.00e-04 | grad 2.19 | tok/s 10727
step    230 | loss 2.0286 | lr 3.00e-04 | grad 2.62 | tok/s 10049
step    240 | loss 2.2477 | lr 3.00e-04 | grad 3.36 | tok/s 10068
step    250 | loss 2.0810 | lr 3.00e-04 | grad 1.77 | tok/s 10342
step    260 | loss 1.5722 | lr 3.00e-04 | grad 2.02 | tok/s 10680
step    270 | loss 2.0817 | lr 3.00e-04 | grad 1.88 | tok/s 10522
step    280 | loss 2.2505 | lr 3.00e-04 | grad 3.83 | tok/s 10308
step    290 | loss 1.4084 | lr 3.00e-04 | grad 2.86 | tok/s 10855
step    300 | loss 0.5786 | lr 3.00e-04 | grad 2.33 | tok/s 10849
step    310 | loss 2.4021 | lr 3.00e-04 | grad 2.78 | tok/s 10662
step    320 | loss 1.9542 | lr 3.00e-04 | grad 3.88 | tok/s 10458
step    330 | loss 1.9254 | lr 3.00e-04 | grad 2.00 | tok/s 10115
step    340 | loss 2.2367 | lr 3.00e-04 | grad 1.89 | tok/s 10255
step    350 | loss 1.8830 | lr 3.00e-04 | grad 2.91 | tok/s 10501
step    360 | loss 1.2076 | lr 3.00e-04 | grad 5.16 | tok/s 10740
step    370 | loss 1.7953 | lr 3.00e-04 | grad 1.84 | tok/s 9725
step    380 | loss 1.7622 | lr 3.00e-04 | grad 1.75 | tok/s 10372
step    390 | loss 1.5329 | lr 3.00e-04 | grad 1.39 | tok/s 10822
step    400 | loss 1.4950 | lr 3.00e-04 | grad 1.83 | tok/s 10728
step    410 | loss 1.2878 | lr 3.00e-04 | grad 1.45 | tok/s 10156
step    420 | loss 1.7976 | lr 3.00e-04 | grad 3.17 | tok/s 10039
step    430 | loss 2.1340 | lr 3.00e-04 | grad 2.11 | tok/s 10676
step    440 | loss 2.1322 | lr 3.00e-04 | grad 3.16 | tok/s 10101
step    450 | loss 1.8791 | lr 3.00e-04 | grad 1.95 | tok/s 10436
step    460 | loss 1.6963 | lr 3.00e-04 | grad 1.91 | tok/s 10228

Training complete! Final step: 464
