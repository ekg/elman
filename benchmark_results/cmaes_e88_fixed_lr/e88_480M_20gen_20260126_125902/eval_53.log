Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_53/levelE88_100m_20260126_131854
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,161,168 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1131 | lr 3.00e-04 | grad 19.62 | tok/s 6099
step     20 | loss 2.9724 | lr 3.00e-04 | grad 12.06 | tok/s 15581
step     30 | loss 2.6967 | lr 3.00e-04 | grad 7.00 | tok/s 15739
step     40 | loss 2.4958 | lr 3.00e-04 | grad 7.00 | tok/s 15091
step     50 | loss 3.0709 | lr 3.00e-04 | grad 14.12 | tok/s 15288
step     60 | loss 2.1168 | lr 3.00e-04 | grad 4.06 | tok/s 15756
step     70 | loss 1.9130 | lr 3.00e-04 | grad 5.59 | tok/s 15919
step     80 | loss 6.4301 | lr 3.00e-04 | grad 103.00 | tok/s 16046
step     90 | loss 5.9390 | lr 3.00e-04 | grad 12.88 | tok/s 16255
step    100 | loss 4.4718 | lr 3.00e-04 | grad 10.50 | tok/s 16290
step    110 | loss 3.7924 | lr 3.00e-04 | grad 24.62 | tok/s 16274
step    120 | loss 3.3556 | lr 3.00e-04 | grad 16.00 | tok/s 16254
step    130 | loss 3.0937 | lr 3.00e-04 | grad 20.88 | tok/s 15133
step    140 | loss 2.8358 | lr 3.00e-04 | grad 13.38 | tok/s 16225
step    150 | loss 2.8584 | lr 3.00e-04 | grad 19.25 | tok/s 16209
step    160 | loss 2.4112 | lr 3.00e-04 | grad 11.06 | tok/s 16222
step    170 | loss 2.4486 | lr 3.00e-04 | grad 15.00 | tok/s 16169
step    180 | loss 2.3607 | lr 3.00e-04 | grad 10.44 | tok/s 16172
step    190 | loss 2.4547 | lr 3.00e-04 | grad 9.31 | tok/s 16202
step    200 | loss 2.1555 | lr 3.00e-04 | grad 7.97 | tok/s 16207
step    210 | loss 2.2115 | lr 3.00e-04 | grad 11.12 | tok/s 16183
step    220 | loss 2.2128 | lr 3.00e-04 | grad 4.19 | tok/s 15955
step    230 | loss 2.0647 | lr 3.00e-04 | grad 4.12 | tok/s 15807
step    240 | loss 2.3207 | lr 3.00e-04 | grad 5.16 | tok/s 14958
step    250 | loss 2.1253 | lr 3.00e-04 | grad 2.83 | tok/s 15440
step    260 | loss 1.5653 | lr 3.00e-04 | grad 3.19 | tok/s 15892
step    270 | loss 2.0963 | lr 3.00e-04 | grad 3.00 | tok/s 15693
step    280 | loss 2.2738 | lr 3.00e-04 | grad 5.53 | tok/s 15357
step    290 | loss 1.3885 | lr 3.00e-04 | grad 3.98 | tok/s 16169
step    300 | loss 0.5524 | lr 3.00e-04 | grad 2.95 | tok/s 16150
step    310 | loss 2.4227 | lr 3.00e-04 | grad 3.98 | tok/s 15896
step    320 | loss 1.9475 | lr 3.00e-04 | grad 6.09 | tok/s 15593
step    330 | loss 1.9660 | lr 3.00e-04 | grad 3.12 | tok/s 15003
step    340 | loss 2.2852 | lr 3.00e-04 | grad 2.95 | tok/s 15256
step    350 | loss 1.8774 | lr 3.00e-04 | grad 4.41 | tok/s 15613
step    360 | loss 1.1950 | lr 3.00e-04 | grad 6.91 | tok/s 15976
step    370 | loss 1.8168 | lr 3.00e-04 | grad 2.80 | tok/s 14502
step    380 | loss 1.7826 | lr 3.00e-04 | grad 2.91 | tok/s 15436
step    390 | loss 1.5488 | lr 3.00e-04 | grad 2.36 | tok/s 16119
step    400 | loss 1.4997 | lr 3.00e-04 | grad 2.80 | tok/s 15963
step    410 | loss 1.2798 | lr 3.00e-04 | grad 2.20 | tok/s 15614
step    420 | loss 1.8302 | lr 3.00e-04 | grad 4.62 | tok/s 14920
step    430 | loss 2.1574 | lr 3.00e-04 | grad 3.19 | tok/s 15938
step    440 | loss 2.1684 | lr 3.00e-04 | grad 4.22 | tok/s 15011
step    450 | loss 2.0228 | lr 3.00e-04 | grad 2.89 | tok/s 15545
step    460 | loss 1.7302 | lr 3.00e-04 | grad 3.20 | tok/s 15219
step    470 | loss 1.8467 | lr 3.00e-04 | grad 2.77 | tok/s 15660
step    480 | loss 2.2622 | lr 3.00e-04 | grad 6.72 | tok/s 15750
step    490 | loss 1.8013 | lr 3.00e-04 | grad 2.55 | tok/s 14839
step    500 | loss 1.6891 | lr 3.00e-04 | grad 3.67 | tok/s 15847
step    510 | loss 1.7212 | lr 3.00e-04 | grad 2.67 | tok/s 16053
step    520 | loss 1.6667 | lr 3.00e-04 | grad 2.23 | tok/s 15983
step    530 | loss 1.9229 | lr 3.00e-04 | grad 2.58 | tok/s 15376
step    540 | loss 1.7535 | lr 3.00e-04 | grad 2.41 | tok/s 15420
step    550 | loss 1.5801 | lr 3.00e-04 | grad 3.31 | tok/s 15055
step    560 | loss 1.7355 | lr 3.00e-04 | grad 2.73 | tok/s 14682
step    570 | loss 1.6765 | lr 3.00e-04 | grad 3.75 | tok/s 15091
step    580 | loss 1.5515 | lr 3.00e-04 | grad 2.30 | tok/s 14083
step    590 | loss 1.8715 | lr 3.00e-04 | grad 3.22 | tok/s 15446
step    600 | loss 1.8355 | lr 3.00e-04 | grad 2.36 | tok/s 14907
step    610 | loss 1.6312 | lr 3.00e-04 | grad 2.58 | tok/s 15666
step    620 | loss 1.5574 | lr 3.00e-04 | grad 2.56 | tok/s 14859
step    630 | loss 1.6734 | lr 3.00e-04 | grad 4.50 | tok/s 14962
step    640 | loss 1.8245 | lr 3.00e-04 | grad 2.52 | tok/s 15363
step    650 | loss 1.6825 | lr 3.00e-04 | grad 2.61 | tok/s 15409
step    660 | loss 1.7040 | lr 3.00e-04 | grad 2.14 | tok/s 15495
step    670 | loss 1.9309 | lr 3.00e-04 | grad 3.34 | tok/s 15620
step    680 | loss 1.7413 | lr 3.00e-04 | grad 2.50 | tok/s 15271

Training complete! Final step: 685
