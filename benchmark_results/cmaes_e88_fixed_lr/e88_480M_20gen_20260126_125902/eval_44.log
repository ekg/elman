Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_44/levelE88_100m_20260126_131536
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 495,111,000 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.6486 | lr 3.00e-04 | grad 31.50 | tok/s 5812
step     20 | loss 2.7180 | lr 3.00e-04 | grad 12.50 | tok/s 13057
step     30 | loss 2.6305 | lr 3.00e-04 | grad 6.41 | tok/s 13199
step     40 | loss 2.4510 | lr 3.00e-04 | grad 4.62 | tok/s 12619
step     50 | loss 3.2501 | lr 3.00e-04 | grad 23.00 | tok/s 12797
step     60 | loss 2.1797 | lr 3.00e-04 | grad 6.09 | tok/s 13231
step     70 | loss 1.9874 | lr 3.00e-04 | grad 7.22 | tok/s 12722
step     80 | loss 5.9569 | lr 3.00e-04 | grad 207.00 | tok/s 13453
step     90 | loss 6.0582 | lr 3.00e-04 | grad 18.62 | tok/s 13669
step    100 | loss 5.1479 | lr 3.00e-04 | grad 22.12 | tok/s 13695
step    110 | loss 4.7433 | lr 3.00e-04 | grad 43.50 | tok/s 13703
step    120 | loss 4.2035 | lr 3.00e-04 | grad 40.50 | tok/s 13648
step    130 | loss 3.8812 | lr 3.00e-04 | grad 48.25 | tok/s 13622
step    140 | loss 3.1333 | lr 3.00e-04 | grad 29.38 | tok/s 12981
step    150 | loss 3.6487 | lr 3.00e-04 | grad 36.00 | tok/s 13603
step    160 | loss 2.7221 | lr 3.00e-04 | grad 34.50 | tok/s 13578
step    170 | loss 2.7821 | lr 3.00e-04 | grad 30.62 | tok/s 13595
step    180 | loss 2.5527 | lr 3.00e-04 | grad 7.59 | tok/s 13604
step    190 | loss 2.9047 | lr 3.00e-04 | grad 9.38 | tok/s 13601
step    200 | loss 2.3536 | lr 3.00e-04 | grad 18.88 | tok/s 13600
step    210 | loss 2.4504 | lr 3.00e-04 | grad 15.19 | tok/s 13585
step    220 | loss 2.3885 | lr 3.00e-04 | grad 3.30 | tok/s 13419
step    230 | loss 2.2802 | lr 3.00e-04 | grad 3.95 | tok/s 13264
step    240 | loss 2.3300 | lr 3.00e-04 | grad 4.94 | tok/s 12589
step    250 | loss 2.1582 | lr 3.00e-04 | grad 2.50 | tok/s 12937
step    260 | loss 1.6195 | lr 3.00e-04 | grad 2.92 | tok/s 13358
step    270 | loss 2.1529 | lr 3.00e-04 | grad 2.56 | tok/s 13176
step    280 | loss 2.3177 | lr 3.00e-04 | grad 5.19 | tok/s 12917
step    290 | loss 1.5310 | lr 3.00e-04 | grad 4.44 | tok/s 13610
step    300 | loss 0.6628 | lr 3.00e-04 | grad 2.88 | tok/s 13593
step    310 | loss 2.4416 | lr 3.00e-04 | grad 3.34 | tok/s 13358
step    320 | loss 1.9986 | lr 3.00e-04 | grad 6.62 | tok/s 13083
step    330 | loss 2.0019 | lr 3.00e-04 | grad 2.88 | tok/s 12627
step    340 | loss 2.3435 | lr 3.00e-04 | grad 2.98 | tok/s 12840
step    350 | loss 1.9872 | lr 3.00e-04 | grad 7.28 | tok/s 13156
step    360 | loss 1.3600 | lr 3.00e-04 | grad 6.88 | tok/s 13451
step    370 | loss 1.8599 | lr 3.00e-04 | grad 2.42 | tok/s 12204
step    380 | loss 1.8484 | lr 3.00e-04 | grad 2.30 | tok/s 12996
step    390 | loss 1.5861 | lr 3.00e-04 | grad 1.85 | tok/s 13562
step    400 | loss 1.5407 | lr 3.00e-04 | grad 2.39 | tok/s 13449
step    410 | loss 1.3237 | lr 3.00e-04 | grad 1.90 | tok/s 13155
step    420 | loss 1.8675 | lr 3.00e-04 | grad 4.50 | tok/s 12100
step    430 | loss 2.2280 | lr 3.00e-04 | grad 2.64 | tok/s 13375
step    440 | loss 2.2040 | lr 3.00e-04 | grad 4.03 | tok/s 12649
step    450 | loss 2.0092 | lr 3.00e-04 | grad 2.61 | tok/s 13074
step    460 | loss 1.7433 | lr 3.00e-04 | grad 2.50 | tok/s 12820
step    470 | loss 1.8824 | lr 3.00e-04 | grad 2.16 | tok/s 13185
step    480 | loss 2.3166 | lr 3.00e-04 | grad 6.47 | tok/s 13205
step    490 | loss 1.8329 | lr 3.00e-04 | grad 2.27 | tok/s 12480
step    500 | loss 1.7285 | lr 3.00e-04 | grad 3.06 | tok/s 13323
step    510 | loss 1.7522 | lr 3.00e-04 | grad 2.09 | tok/s 13509
step    520 | loss 1.7054 | lr 3.00e-04 | grad 1.87 | tok/s 13491
step    530 | loss 1.9699 | lr 3.00e-04 | grad 2.25 | tok/s 12954
step    540 | loss 1.7732 | lr 3.00e-04 | grad 1.99 | tok/s 12956
step    550 | loss 1.6017 | lr 3.00e-04 | grad 2.77 | tok/s 12691
step    560 | loss 1.7661 | lr 3.00e-04 | grad 2.30 | tok/s 12354
step    570 | loss 1.7075 | lr 3.00e-04 | grad 3.28 | tok/s 12690

Training complete! Final step: 577
