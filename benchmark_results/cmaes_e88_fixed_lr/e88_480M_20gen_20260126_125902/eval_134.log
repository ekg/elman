Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_134/levelE88_100m_20260126_135152
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 474,988,672 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.2724 | lr 3.00e-04 | grad 20.38 | tok/s 9483
step     20 | loss 3.3279 | lr 3.00e-04 | grad 8.75 | tok/s 18806
step     30 | loss 3.1870 | lr 3.00e-04 | grad 10.94 | tok/s 19851
step     40 | loss 4.9148 | lr 3.00e-04 | grad 46.50 | tok/s 20279
step     50 | loss 4.5302 | lr 3.00e-04 | grad 19.25 | tok/s 20513
step     60 | loss 3.4761 | lr 3.00e-04 | grad 10.56 | tok/s 20427
step     70 | loss 2.9072 | lr 3.00e-04 | grad 6.69 | tok/s 20380
step     80 | loss 2.6569 | lr 3.00e-04 | grad 7.44 | tok/s 20341
step     90 | loss 2.5007 | lr 3.00e-04 | grad 5.88 | tok/s 20304
step    100 | loss 2.2868 | lr 3.00e-04 | grad 5.59 | tok/s 20283
step    110 | loss 2.2865 | lr 3.00e-04 | grad 5.03 | tok/s 20115
step    120 | loss 2.7538 | lr 3.00e-04 | grad 3.34 | tok/s 19146
step    130 | loss 2.1047 | lr 3.00e-04 | grad 7.16 | tok/s 19595
step    140 | loss 2.3764 | lr 3.00e-04 | grad 8.75 | tok/s 19639
step    150 | loss 1.3951 | lr 3.00e-04 | grad 7.94 | tok/s 20107
step    160 | loss 2.3575 | lr 3.00e-04 | grad 3.31 | tok/s 19476
step    170 | loss 2.3200 | lr 3.00e-04 | grad 2.77 | tok/s 19190
step    180 | loss 1.8071 | lr 3.00e-04 | grad 4.28 | tok/s 19642
step    190 | loss 1.9177 | lr 3.00e-04 | grad 4.03 | tok/s 19270
step    200 | loss 1.6320 | lr 3.00e-04 | grad 2.78 | tok/s 20138
step    210 | loss 1.8767 | lr 3.00e-04 | grad 9.44 | tok/s 19138
step    220 | loss 2.2232 | lr 3.00e-04 | grad 4.62 | tok/s 19319
step    230 | loss 2.0287 | lr 3.00e-04 | grad 3.44 | tok/s 19320
step    240 | loss 2.2723 | lr 3.00e-04 | grad 7.78 | tok/s 19569
step    250 | loss 1.7635 | lr 3.00e-04 | grad 2.52 | tok/s 19422
step    260 | loss 1.8935 | lr 3.00e-04 | grad 4.06 | tok/s 19971
step    270 | loss 1.8172 | lr 3.00e-04 | grad 3.11 | tok/s 19530
step    280 | loss 1.7749 | lr 3.00e-04 | grad 2.53 | tok/s 18351
step    290 | loss 1.6762 | lr 3.00e-04 | grad 3.11 | tok/s 18987
step    300 | loss 1.9731 | lr 3.00e-04 | grad 2.92 | tok/s 19100
step    310 | loss 1.6679 | lr 3.00e-04 | grad 2.55 | tok/s 19030
step    320 | loss 1.8849 | lr 3.00e-04 | grad 5.00 | tok/s 19262
step    330 | loss 1.7283 | lr 3.00e-04 | grad 2.75 | tok/s 19455
step    340 | loss 2.0619 | lr 3.00e-04 | grad 2.97 | tok/s 19367
step    350 | loss 1.6891 | lr 3.00e-04 | grad 2.77 | tok/s 19919
step    360 | loss 1.5819 | lr 3.00e-04 | grad 2.44 | tok/s 19078
step    370 | loss 1.4709 | lr 3.00e-04 | grad 2.39 | tok/s 20085
step    380 | loss 1.1891 | lr 3.00e-04 | grad 2.12 | tok/s 20262
step    390 | loss 1.1000 | lr 3.00e-04 | grad 2.14 | tok/s 20233
step    400 | loss 1.7610 | lr 3.00e-04 | grad 2.33 | tok/s 19210
step    410 | loss 1.7730 | lr 3.00e-04 | grad 3.17 | tok/s 19341
step    420 | loss 1.5945 | lr 3.00e-04 | grad 4.50 | tok/s 20182
step    430 | loss 1.6001 | lr 3.00e-04 | grad 2.89 | tok/s 19845

Training complete! Final step: 432
