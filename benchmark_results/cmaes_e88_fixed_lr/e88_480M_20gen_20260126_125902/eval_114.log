Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_114/levelE88_100m_20260126_134517
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,920,064 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2512 | lr 3.00e-04 | grad 18.88 | tok/s 6421
step     20 | loss 3.0924 | lr 3.00e-04 | grad 11.06 | tok/s 19106
step     30 | loss 2.6847 | lr 3.00e-04 | grad 7.03 | tok/s 19328
step     40 | loss 2.4215 | lr 3.00e-04 | grad 4.69 | tok/s 18519
step     50 | loss 3.0277 | lr 3.00e-04 | grad 20.38 | tok/s 18772
step     60 | loss 2.0194 | lr 3.00e-04 | grad 5.12 | tok/s 19349
step     70 | loss 1.8472 | lr 3.00e-04 | grad 5.28 | tok/s 19623
step     80 | loss 6.9921 | lr 3.00e-04 | grad 59.75 | tok/s 19728
step     90 | loss 6.1071 | lr 3.00e-04 | grad 10.75 | tok/s 20059
step    100 | loss 4.1994 | lr 3.00e-04 | grad 8.94 | tok/s 20025
step    110 | loss 3.4230 | lr 3.00e-04 | grad 16.75 | tok/s 20017
step    120 | loss 3.1803 | lr 3.00e-04 | grad 13.81 | tok/s 20000
step    130 | loss 2.9764 | lr 3.00e-04 | grad 13.75 | tok/s 19982
step    140 | loss 2.8377 | lr 3.00e-04 | grad 10.88 | tok/s 19986
step    150 | loss 2.7883 | lr 3.00e-04 | grad 16.88 | tok/s 19974
step    160 | loss 2.4270 | lr 3.00e-04 | grad 10.94 | tok/s 19939
step    170 | loss 2.4269 | lr 3.00e-04 | grad 14.38 | tok/s 19952
step    180 | loss 2.3617 | lr 3.00e-04 | grad 8.56 | tok/s 19914
step    190 | loss 2.4885 | lr 3.00e-04 | grad 6.94 | tok/s 19947
step    200 | loss 2.0979 | lr 3.00e-04 | grad 5.59 | tok/s 19914
step    210 | loss 2.1780 | lr 3.00e-04 | grad 11.25 | tok/s 19937
step    220 | loss 2.1783 | lr 3.00e-04 | grad 5.00 | tok/s 18244
step    230 | loss 2.0888 | lr 3.00e-04 | grad 4.41 | tok/s 19602
step    240 | loss 2.3132 | lr 3.00e-04 | grad 5.94 | tok/s 18637
step    250 | loss 2.1000 | lr 3.00e-04 | grad 3.22 | tok/s 19128
step    260 | loss 1.5307 | lr 3.00e-04 | grad 3.56 | tok/s 19724
step    270 | loss 2.0829 | lr 3.00e-04 | grad 3.56 | tok/s 19454
step    280 | loss 2.2345 | lr 3.00e-04 | grad 5.12 | tok/s 19081
step    290 | loss 1.4862 | lr 3.00e-04 | grad 4.06 | tok/s 20094
step    300 | loss 0.5922 | lr 3.00e-04 | grad 4.34 | tok/s 20070
step    310 | loss 2.4138 | lr 3.00e-04 | grad 4.28 | tok/s 19724
step    320 | loss 1.9156 | lr 3.00e-04 | grad 6.81 | tok/s 19316
step    330 | loss 1.9515 | lr 3.00e-04 | grad 3.42 | tok/s 18648
step    340 | loss 2.2845 | lr 3.00e-04 | grad 3.69 | tok/s 18976
step    350 | loss 1.8362 | lr 3.00e-04 | grad 4.28 | tok/s 19421
step    360 | loss 1.1814 | lr 3.00e-04 | grad 10.06 | tok/s 19822
step    370 | loss 1.7963 | lr 3.00e-04 | grad 2.95 | tok/s 17982
step    380 | loss 1.7648 | lr 3.00e-04 | grad 3.41 | tok/s 19169
step    390 | loss 1.5189 | lr 3.00e-04 | grad 2.78 | tok/s 20009
step    400 | loss 1.4859 | lr 3.00e-04 | grad 3.31 | tok/s 19842
step    410 | loss 1.2619 | lr 3.00e-04 | grad 2.44 | tok/s 19409
step    420 | loss 1.8079 | lr 3.00e-04 | grad 5.38 | tok/s 18526
step    430 | loss 2.1476 | lr 3.00e-04 | grad 3.55 | tok/s 19733
step    440 | loss 2.1558 | lr 3.00e-04 | grad 4.47 | tok/s 18637
step    450 | loss 2.0712 | lr 3.00e-04 | grad 3.03 | tok/s 19279
step    460 | loss 1.7115 | lr 3.00e-04 | grad 3.53 | tok/s 18869
step    470 | loss 1.8281 | lr 3.00e-04 | grad 3.28 | tok/s 19435
step    480 | loss 2.2550 | lr 3.00e-04 | grad 7.34 | tok/s 19471
step    490 | loss 1.7921 | lr 3.00e-04 | grad 3.02 | tok/s 18383
step    500 | loss 1.6637 | lr 3.00e-04 | grad 4.38 | tok/s 19607
step    510 | loss 1.7088 | lr 3.00e-04 | grad 3.08 | tok/s 19883
step    520 | loss 1.6475 | lr 3.00e-04 | grad 2.55 | tok/s 19817
step    530 | loss 1.8944 | lr 3.00e-04 | grad 2.77 | tok/s 19098
step    540 | loss 1.7306 | lr 3.00e-04 | grad 2.91 | tok/s 19075
step    550 | loss 1.5681 | lr 3.00e-04 | grad 3.23 | tok/s 18696
step    560 | loss 1.7121 | lr 3.00e-04 | grad 3.12 | tok/s 18213
step    570 | loss 1.6618 | lr 3.00e-04 | grad 4.12 | tok/s 18724
step    580 | loss 1.5381 | lr 3.00e-04 | grad 2.70 | tok/s 18647
step    590 | loss 1.8468 | lr 3.00e-04 | grad 3.55 | tok/s 19127
step    600 | loss 1.8286 | lr 3.00e-04 | grad 2.59 | tok/s 18475
step    610 | loss 1.6206 | lr 3.00e-04 | grad 2.88 | tok/s 19416
step    620 | loss 1.5461 | lr 3.00e-04 | grad 2.75 | tok/s 18405
step    630 | loss 1.6554 | lr 3.00e-04 | grad 4.75 | tok/s 18548
step    640 | loss 1.8187 | lr 3.00e-04 | grad 2.64 | tok/s 19054
step    650 | loss 1.6831 | lr 3.00e-04 | grad 3.06 | tok/s 19146
step    660 | loss 1.6936 | lr 3.00e-04 | grad 2.33 | tok/s 19234
step    670 | loss 1.9154 | lr 3.00e-04 | grad 4.00 | tok/s 19370
step    680 | loss 1.7329 | lr 3.00e-04 | grad 2.88 | tok/s 18970
step    690 | loss 1.8293 | lr 3.00e-04 | grad 3.72 | tok/s 19646
step    700 | loss 1.3999 | lr 3.00e-04 | grad 3.30 | tok/s 20013
step    710 | loss 1.5917 | lr 3.00e-04 | grad 2.95 | tok/s 18690
step    720 | loss 1.4709 | lr 3.00e-04 | grad 3.81 | tok/s 18413
step    730 | loss 1.2798 | lr 3.00e-04 | grad 3.27 | tok/s 19949
step    740 | loss 1.4951 | lr 3.00e-04 | grad 2.75 | tok/s 19684
step    750 | loss 1.1905 | lr 3.00e-04 | grad 2.77 | tok/s 20000
step    760 | loss 1.0984 | lr 3.00e-04 | grad 2.48 | tok/s 19996
step    770 | loss 1.0505 | lr 3.00e-04 | grad 2.44 | tok/s 20010
step    780 | loss 0.9889 | lr 3.00e-04 | grad 2.38 | tok/s 19990
step    790 | loss 1.1248 | lr 3.00e-04 | grad 3.86 | tok/s 19367
step    800 | loss 1.8200 | lr 3.00e-04 | grad 6.34 | tok/s 19299
step    810 | loss 1.7056 | lr 3.00e-04 | grad 2.58 | tok/s 19208
step    820 | loss 1.7120 | lr 3.00e-04 | grad 4.47 | tok/s 17070
step    830 | loss 1.4884 | lr 3.00e-04 | grad 2.56 | tok/s 19638
step    840 | loss 1.3460 | lr 3.00e-04 | grad 2.48 | tok/s 19842

Training complete! Final step: 844
