Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_112/levelE88_100m_20260126_134159
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,585,324 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.2273 | lr 3.00e-04 | grad 21.62 | tok/s 9411
step     20 | loss 3.3339 | lr 3.00e-04 | grad 8.75 | tok/s 18133
step     30 | loss 3.2198 | lr 3.00e-04 | grad 11.50 | tok/s 19089
step     40 | loss 4.8725 | lr 3.00e-04 | grad 40.50 | tok/s 19414
step     50 | loss 4.7359 | lr 3.00e-04 | grad 20.00 | tok/s 19597
step     60 | loss 3.5351 | lr 3.00e-04 | grad 12.12 | tok/s 19514
step     70 | loss 2.9472 | lr 3.00e-04 | grad 7.72 | tok/s 19464
step     80 | loss 2.6720 | lr 3.00e-04 | grad 7.31 | tok/s 19400
step     90 | loss 2.5499 | lr 3.00e-04 | grad 6.28 | tok/s 19339
step    100 | loss 2.3463 | lr 3.00e-04 | grad 4.25 | tok/s 19312
step    110 | loss 2.3436 | lr 3.00e-04 | grad 5.06 | tok/s 19186
step    120 | loss 2.7737 | lr 3.00e-04 | grad 3.12 | tok/s 18247
step    130 | loss 2.1148 | lr 3.00e-04 | grad 6.94 | tok/s 18649
step    140 | loss 2.3654 | lr 3.00e-04 | grad 9.12 | tok/s 18743
step    150 | loss 1.4345 | lr 3.00e-04 | grad 8.31 | tok/s 19209
step    160 | loss 2.3636 | lr 3.00e-04 | grad 3.17 | tok/s 18514
step    170 | loss 2.3145 | lr 3.00e-04 | grad 2.75 | tok/s 18194
step    180 | loss 1.8014 | lr 3.00e-04 | grad 4.25 | tok/s 18635
step    190 | loss 1.9154 | lr 3.00e-04 | grad 3.92 | tok/s 18294
step    200 | loss 1.6329 | lr 3.00e-04 | grad 2.83 | tok/s 19167
step    210 | loss 1.8888 | lr 3.00e-04 | grad 7.66 | tok/s 18169
step    220 | loss 2.1911 | lr 3.00e-04 | grad 4.00 | tok/s 17762
step    230 | loss 2.0033 | lr 3.00e-04 | grad 3.53 | tok/s 18323
step    240 | loss 2.2784 | lr 3.00e-04 | grad 7.59 | tok/s 18566
step    250 | loss 1.7569 | lr 3.00e-04 | grad 2.53 | tok/s 18452
step    260 | loss 1.8996 | lr 3.00e-04 | grad 4.16 | tok/s 18964
step    270 | loss 1.8242 | lr 3.00e-04 | grad 3.08 | tok/s 18519
step    280 | loss 1.7731 | lr 3.00e-04 | grad 2.41 | tok/s 17397
step    290 | loss 1.6705 | lr 3.00e-04 | grad 3.03 | tok/s 17988
step    300 | loss 1.9728 | lr 3.00e-04 | grad 2.86 | tok/s 18137
step    310 | loss 1.6728 | lr 3.00e-04 | grad 2.45 | tok/s 18071
step    320 | loss 1.8787 | lr 3.00e-04 | grad 4.38 | tok/s 18254
step    330 | loss 1.7236 | lr 3.00e-04 | grad 2.61 | tok/s 18458
step    340 | loss 2.0382 | lr 3.00e-04 | grad 2.73 | tok/s 18373
step    350 | loss 1.7034 | lr 3.00e-04 | grad 2.67 | tok/s 18914
step    360 | loss 1.5845 | lr 3.00e-04 | grad 2.52 | tok/s 18080
step    370 | loss 1.4860 | lr 3.00e-04 | grad 2.42 | tok/s 19041
step    380 | loss 1.2170 | lr 3.00e-04 | grad 2.14 | tok/s 19230
step    390 | loss 1.1279 | lr 3.00e-04 | grad 1.90 | tok/s 19229
step    400 | loss 1.7699 | lr 3.00e-04 | grad 2.27 | tok/s 18214
step    410 | loss 1.7756 | lr 3.00e-04 | grad 3.02 | tok/s 18371

Training complete! Final step: 411
