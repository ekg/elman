Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_139/levelE88_100m_20260126_135510
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,391,466 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2708 | lr 3.00e-04 | grad 21.38 | tok/s 6216
step     20 | loss 3.1729 | lr 3.00e-04 | grad 11.75 | tok/s 18211
step     30 | loss 2.7535 | lr 3.00e-04 | grad 7.69 | tok/s 18425
step     40 | loss 2.4845 | lr 3.00e-04 | grad 4.88 | tok/s 17602
step     50 | loss 3.1196 | lr 3.00e-04 | grad 19.88 | tok/s 17888
step     60 | loss 2.1002 | lr 3.00e-04 | grad 4.47 | tok/s 18440
step     70 | loss 1.8671 | lr 3.00e-04 | grad 5.88 | tok/s 18638
step     80 | loss 6.7795 | lr 3.00e-04 | grad 52.75 | tok/s 18734
step     90 | loss 5.7550 | lr 3.00e-04 | grad 9.62 | tok/s 19082
step    100 | loss 4.1366 | lr 3.00e-04 | grad 7.59 | tok/s 19068
step    110 | loss 3.5471 | lr 3.00e-04 | grad 13.00 | tok/s 18989
step    120 | loss 3.2505 | lr 3.00e-04 | grad 34.25 | tok/s 19012
step    130 | loss 2.9511 | lr 3.00e-04 | grad 13.56 | tok/s 18944
step    140 | loss 2.7496 | lr 3.00e-04 | grad 10.19 | tok/s 18917
step    150 | loss 2.6969 | lr 3.00e-04 | grad 9.69 | tok/s 18926
step    160 | loss 2.4020 | lr 3.00e-04 | grad 10.69 | tok/s 18930
step    170 | loss 2.4658 | lr 3.00e-04 | grad 12.06 | tok/s 18928
step    180 | loss 2.3075 | lr 3.00e-04 | grad 8.44 | tok/s 18927
step    190 | loss 2.4289 | lr 3.00e-04 | grad 10.31 | tok/s 18918
step    200 | loss 2.1213 | lr 3.00e-04 | grad 5.47 | tok/s 18896
step    210 | loss 2.1616 | lr 3.00e-04 | grad 10.00 | tok/s 18882
step    220 | loss 2.1812 | lr 3.00e-04 | grad 4.47 | tok/s 18600
step    230 | loss 2.1143 | lr 3.00e-04 | grad 4.31 | tok/s 17072
step    240 | loss 2.3257 | lr 3.00e-04 | grad 6.12 | tok/s 17528
step    250 | loss 2.1175 | lr 3.00e-04 | grad 3.19 | tok/s 17997
step    260 | loss 1.5331 | lr 3.00e-04 | grad 3.56 | tok/s 18511
step    270 | loss 2.0854 | lr 3.00e-04 | grad 3.47 | tok/s 18317
step    280 | loss 2.2598 | lr 3.00e-04 | grad 7.12 | tok/s 17924
step    290 | loss 1.5525 | lr 3.00e-04 | grad 4.47 | tok/s 18869
step    300 | loss 0.5828 | lr 3.00e-04 | grad 3.42 | tok/s 18860
step    310 | loss 2.4304 | lr 3.00e-04 | grad 4.88 | tok/s 18508
step    320 | loss 1.9317 | lr 3.00e-04 | grad 6.62 | tok/s 18149
step    330 | loss 1.9519 | lr 3.00e-04 | grad 3.33 | tok/s 17551
step    340 | loss 2.2767 | lr 3.00e-04 | grad 3.62 | tok/s 17812
step    350 | loss 1.8347 | lr 3.00e-04 | grad 4.41 | tok/s 18243
step    360 | loss 1.1810 | lr 3.00e-04 | grad 9.06 | tok/s 18619
step    370 | loss 1.8023 | lr 3.00e-04 | grad 3.14 | tok/s 16903
step    380 | loss 1.7484 | lr 3.00e-04 | grad 3.30 | tok/s 18005
step    390 | loss 1.5236 | lr 3.00e-04 | grad 2.86 | tok/s 18791
step    400 | loss 1.4854 | lr 3.00e-04 | grad 3.33 | tok/s 18627
step    410 | loss 1.2620 | lr 3.00e-04 | grad 2.36 | tok/s 18232
step    420 | loss 1.8154 | lr 3.00e-04 | grad 5.19 | tok/s 17421
step    430 | loss 2.1698 | lr 3.00e-04 | grad 3.69 | tok/s 18513
step    440 | loss 2.1522 | lr 3.00e-04 | grad 4.47 | tok/s 17535
step    450 | loss 2.1194 | lr 3.00e-04 | grad 2.98 | tok/s 18100
step    460 | loss 1.7134 | lr 3.00e-04 | grad 3.58 | tok/s 17765
step    470 | loss 1.8261 | lr 3.00e-04 | grad 3.55 | tok/s 18273
step    480 | loss 2.2708 | lr 3.00e-04 | grad 7.31 | tok/s 18262
step    490 | loss 1.7802 | lr 3.00e-04 | grad 2.89 | tok/s 17288
step    500 | loss 1.6735 | lr 3.00e-04 | grad 4.41 | tok/s 18432
step    510 | loss 1.7025 | lr 3.00e-04 | grad 3.12 | tok/s 18664
step    520 | loss 1.6515 | lr 3.00e-04 | grad 2.64 | tok/s 18635
step    530 | loss 1.8931 | lr 3.00e-04 | grad 2.84 | tok/s 17906
step    540 | loss 1.7318 | lr 3.00e-04 | grad 2.95 | tok/s 17934
step    550 | loss 1.5698 | lr 3.00e-04 | grad 3.17 | tok/s 17581
step    560 | loss 1.7207 | lr 3.00e-04 | grad 3.00 | tok/s 17100
step    570 | loss 1.6668 | lr 3.00e-04 | grad 4.25 | tok/s 17569
step    580 | loss 1.5422 | lr 3.00e-04 | grad 2.72 | tok/s 17535
step    590 | loss 1.8430 | lr 3.00e-04 | grad 3.45 | tok/s 17942
step    600 | loss 1.8278 | lr 3.00e-04 | grad 2.56 | tok/s 17335
step    610 | loss 1.6258 | lr 3.00e-04 | grad 2.86 | tok/s 18227
step    620 | loss 1.5473 | lr 3.00e-04 | grad 2.83 | tok/s 17292
step    630 | loss 1.6519 | lr 3.00e-04 | grad 4.84 | tok/s 17435
step    640 | loss 1.8099 | lr 3.00e-04 | grad 2.78 | tok/s 17878
step    650 | loss 1.6743 | lr 3.00e-04 | grad 3.38 | tok/s 17967
step    660 | loss 1.6926 | lr 3.00e-04 | grad 2.34 | tok/s 18093
step    670 | loss 1.9166 | lr 3.00e-04 | grad 3.62 | tok/s 18177
step    680 | loss 1.7324 | lr 3.00e-04 | grad 2.84 | tok/s 17799
step    690 | loss 1.8153 | lr 3.00e-04 | grad 3.95 | tok/s 18412
step    700 | loss 1.3897 | lr 3.00e-04 | grad 3.53 | tok/s 18755
step    710 | loss 1.5915 | lr 3.00e-04 | grad 3.00 | tok/s 17536
step    720 | loss 1.4610 | lr 3.00e-04 | grad 4.09 | tok/s 17297
step    730 | loss 1.2790 | lr 3.00e-04 | grad 3.34 | tok/s 18715
step    740 | loss 1.4869 | lr 3.00e-04 | grad 2.72 | tok/s 18481
step    750 | loss 1.1852 | lr 3.00e-04 | grad 2.91 | tok/s 18781
step    760 | loss 1.0961 | lr 3.00e-04 | grad 2.45 | tok/s 18765
step    770 | loss 1.0424 | lr 3.00e-04 | grad 2.44 | tok/s 18762
step    780 | loss 0.9828 | lr 3.00e-04 | grad 2.22 | tok/s 18752
step    790 | loss 1.1226 | lr 3.00e-04 | grad 3.80 | tok/s 18178

Training complete! Final step: 796
