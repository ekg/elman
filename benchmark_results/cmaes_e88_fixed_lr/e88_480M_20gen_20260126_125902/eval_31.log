Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_31/levelE88_100m_20260126_130900
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 472,077,538 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.2401 | lr 3.00e-04 | grad 12.00 | tok/s 7053
step     20 | loss 2.7299 | lr 3.00e-04 | grad 2.70 | tok/s 10223
step     30 | loss 2.9926 | lr 3.00e-04 | grad 5.22 | tok/s 10762
step     40 | loss 4.1765 | lr 3.00e-04 | grad 30.62 | tok/s 10921
step     50 | loss 4.4993 | lr 3.00e-04 | grad 14.00 | tok/s 11011
step     60 | loss 3.6625 | lr 3.00e-04 | grad 10.69 | tok/s 10960
step     70 | loss 2.8882 | lr 3.00e-04 | grad 6.03 | tok/s 10947
step     80 | loss 2.5498 | lr 3.00e-04 | grad 6.00 | tok/s 10939
step     90 | loss 2.3786 | lr 3.00e-04 | grad 3.48 | tok/s 10933
step    100 | loss 2.1424 | lr 3.00e-04 | grad 2.05 | tok/s 10919
step    110 | loss 2.2409 | lr 3.00e-04 | grad 2.23 | tok/s 10833
step    120 | loss 2.6599 | lr 3.00e-04 | grad 1.41 | tok/s 10307
step    130 | loss 2.1236 | lr 3.00e-04 | grad 4.00 | tok/s 10345
step    140 | loss 2.3724 | lr 3.00e-04 | grad 5.88 | tok/s 10581
step    150 | loss 1.3935 | lr 3.00e-04 | grad 4.12 | tok/s 10849
step    160 | loss 2.3393 | lr 3.00e-04 | grad 1.65 | tok/s 10478
step    170 | loss 2.2738 | lr 3.00e-04 | grad 1.25 | tok/s 10310
step    180 | loss 1.8584 | lr 3.00e-04 | grad 2.27 | tok/s 10565
step    190 | loss 1.9298 | lr 3.00e-04 | grad 1.58 | tok/s 10380
step    200 | loss 1.6794 | lr 3.00e-04 | grad 1.36 | tok/s 10851
step    210 | loss 1.8919 | lr 3.00e-04 | grad 3.58 | tok/s 10307
step    220 | loss 2.2020 | lr 3.00e-04 | grad 2.08 | tok/s 10420
step    230 | loss 1.9495 | lr 3.00e-04 | grad 2.20 | tok/s 10409

Training complete! Final step: 235
