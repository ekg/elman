Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_87/levelE88_100m_20260126_133207
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,920,064 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2297 | lr 3.00e-04 | grad 19.12 | tok/s 6293
step     20 | loss 3.0634 | lr 3.00e-04 | grad 12.19 | tok/s 17979
step     30 | loss 2.7403 | lr 3.00e-04 | grad 8.94 | tok/s 18117
step     40 | loss 2.5302 | lr 3.00e-04 | grad 5.25 | tok/s 17358
step     50 | loss 3.1176 | lr 3.00e-04 | grad 16.62 | tok/s 17625
step     60 | loss 2.0997 | lr 3.00e-04 | grad 4.56 | tok/s 18166
step     70 | loss 1.8716 | lr 3.00e-04 | grad 5.78 | tok/s 18325
step     80 | loss 6.7169 | lr 3.00e-04 | grad 63.25 | tok/s 18418
step     90 | loss 5.9255 | lr 3.00e-04 | grad 10.75 | tok/s 18731
step    100 | loss 4.2134 | lr 3.00e-04 | grad 9.38 | tok/s 18691
step    110 | loss 3.5360 | lr 3.00e-04 | grad 24.50 | tok/s 18673
step    120 | loss 3.2183 | lr 3.00e-04 | grad 12.19 | tok/s 18684
step    130 | loss 3.0163 | lr 3.00e-04 | grad 14.69 | tok/s 18666
step    140 | loss 2.7074 | lr 3.00e-04 | grad 11.19 | tok/s 18668
step    150 | loss 2.7435 | lr 3.00e-04 | grad 11.06 | tok/s 18617
step    160 | loss 2.4429 | lr 3.00e-04 | grad 13.25 | tok/s 18600
step    170 | loss 2.5933 | lr 3.00e-04 | grad 13.31 | tok/s 18602
step    180 | loss 2.3492 | lr 3.00e-04 | grad 14.06 | tok/s 18574
step    190 | loss 2.4335 | lr 3.00e-04 | grad 21.12 | tok/s 18575
step    200 | loss 2.1810 | lr 3.00e-04 | grad 6.16 | tok/s 18553
step    210 | loss 2.1841 | lr 3.00e-04 | grad 7.69 | tok/s 18524
step    220 | loss 2.2084 | lr 3.00e-04 | grad 4.53 | tok/s 16730
step    230 | loss 2.0767 | lr 3.00e-04 | grad 4.56 | tok/s 18089
step    240 | loss 2.3231 | lr 3.00e-04 | grad 6.09 | tok/s 17177
step    250 | loss 2.1153 | lr 3.00e-04 | grad 3.06 | tok/s 17653
step    260 | loss 1.5403 | lr 3.00e-04 | grad 3.53 | tok/s 18183
step    270 | loss 2.1075 | lr 3.00e-04 | grad 3.45 | tok/s 17929
step    280 | loss 2.2464 | lr 3.00e-04 | grad 5.72 | tok/s 17620
step    290 | loss 1.4463 | lr 3.00e-04 | grad 3.75 | tok/s 18519
step    300 | loss 0.5667 | lr 3.00e-04 | grad 2.84 | tok/s 18499
step    310 | loss 2.4424 | lr 3.00e-04 | grad 4.56 | tok/s 18213
step    320 | loss 1.9398 | lr 3.00e-04 | grad 6.69 | tok/s 17820
step    330 | loss 1.9491 | lr 3.00e-04 | grad 3.44 | tok/s 17195
step    340 | loss 2.2907 | lr 3.00e-04 | grad 3.53 | tok/s 17470
step    350 | loss 1.8451 | lr 3.00e-04 | grad 4.12 | tok/s 17901
step    360 | loss 1.1716 | lr 3.00e-04 | grad 8.31 | tok/s 18313
step    370 | loss 1.7944 | lr 3.00e-04 | grad 2.95 | tok/s 16611
step    380 | loss 1.7489 | lr 3.00e-04 | grad 3.02 | tok/s 17658
step    390 | loss 1.5235 | lr 3.00e-04 | grad 2.70 | tok/s 18452
step    400 | loss 1.4851 | lr 3.00e-04 | grad 3.14 | tok/s 18312
step    410 | loss 1.2625 | lr 3.00e-04 | grad 2.33 | tok/s 17889
step    420 | loss 1.8139 | lr 3.00e-04 | grad 5.12 | tok/s 17099
step    430 | loss 2.1576 | lr 3.00e-04 | grad 3.62 | tok/s 18199
step    440 | loss 2.1587 | lr 3.00e-04 | grad 4.50 | tok/s 17193
step    450 | loss 2.0798 | lr 3.00e-04 | grad 3.02 | tok/s 17812
step    460 | loss 1.7122 | lr 3.00e-04 | grad 3.50 | tok/s 17434
step    470 | loss 1.8222 | lr 3.00e-04 | grad 3.27 | tok/s 17956
step    480 | loss 2.2725 | lr 3.00e-04 | grad 7.53 | tok/s 17958
step    490 | loss 1.7916 | lr 3.00e-04 | grad 3.39 | tok/s 16971
step    500 | loss 1.6755 | lr 3.00e-04 | grad 4.16 | tok/s 18135
step    510 | loss 1.7112 | lr 3.00e-04 | grad 3.02 | tok/s 18363
step    520 | loss 1.6535 | lr 3.00e-04 | grad 2.58 | tok/s 18311
step    530 | loss 1.9079 | lr 3.00e-04 | grad 2.72 | tok/s 17629
step    540 | loss 1.7412 | lr 3.00e-04 | grad 2.84 | tok/s 17623
step    550 | loss 1.5687 | lr 3.00e-04 | grad 3.31 | tok/s 17262
step    560 | loss 1.7246 | lr 3.00e-04 | grad 3.11 | tok/s 16804
step    570 | loss 1.6669 | lr 3.00e-04 | grad 4.00 | tok/s 17240
step    580 | loss 1.5402 | lr 3.00e-04 | grad 2.72 | tok/s 17182
step    590 | loss 1.8551 | lr 3.00e-04 | grad 3.47 | tok/s 17647
step    600 | loss 1.8301 | lr 3.00e-04 | grad 2.59 | tok/s 17071
step    610 | loss 1.6155 | lr 3.00e-04 | grad 2.88 | tok/s 17915
step    620 | loss 1.5511 | lr 3.00e-04 | grad 2.83 | tok/s 16996
step    630 | loss 1.6492 | lr 3.00e-04 | grad 4.75 | tok/s 17143
step    640 | loss 1.8058 | lr 3.00e-04 | grad 2.67 | tok/s 17609
step    650 | loss 1.6751 | lr 3.00e-04 | grad 3.12 | tok/s 17656
step    660 | loss 1.6972 | lr 3.00e-04 | grad 2.39 | tok/s 17757
step    670 | loss 1.9093 | lr 3.00e-04 | grad 3.75 | tok/s 17860
step    680 | loss 1.7325 | lr 3.00e-04 | grad 2.75 | tok/s 17483
step    690 | loss 1.8251 | lr 3.00e-04 | grad 3.77 | tok/s 18116
step    700 | loss 1.4026 | lr 3.00e-04 | grad 3.41 | tok/s 18500
step    710 | loss 1.6015 | lr 3.00e-04 | grad 2.84 | tok/s 17265
step    720 | loss 1.4779 | lr 3.00e-04 | grad 3.91 | tok/s 17009
step    730 | loss 1.2756 | lr 3.00e-04 | grad 3.44 | tok/s 18409
step    740 | loss 1.4990 | lr 3.00e-04 | grad 2.70 | tok/s 18163
step    750 | loss 1.1866 | lr 3.00e-04 | grad 2.86 | tok/s 18454
step    760 | loss 1.0998 | lr 3.00e-04 | grad 2.42 | tok/s 18457
step    770 | loss 1.0479 | lr 3.00e-04 | grad 2.41 | tok/s 18488
step    780 | loss 0.9917 | lr 3.00e-04 | grad 2.27 | tok/s 18458

Training complete! Final step: 783
