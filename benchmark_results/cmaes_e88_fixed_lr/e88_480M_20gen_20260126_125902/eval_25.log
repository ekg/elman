Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_25/levelE88_100m_20260126_130900
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,174,448 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2081 | lr 3.00e-04 | grad 17.88 | tok/s 6118
step     20 | loss 3.2735 | lr 3.00e-04 | grad 10.62 | tok/s 17917
step     30 | loss 2.7582 | lr 3.00e-04 | grad 7.38 | tok/s 18121
step     40 | loss 2.4937 | lr 3.00e-04 | grad 5.31 | tok/s 17310
step     50 | loss 3.0275 | lr 3.00e-04 | grad 21.88 | tok/s 17605
step     60 | loss 2.0804 | lr 3.00e-04 | grad 7.69 | tok/s 18169
step     70 | loss 1.8772 | lr 3.00e-04 | grad 5.69 | tok/s 18305
step     80 | loss 6.3130 | lr 3.00e-04 | grad 53.75 | tok/s 18427
step     90 | loss 5.4372 | lr 3.00e-04 | grad 10.06 | tok/s 18679
step    100 | loss 4.2524 | lr 3.00e-04 | grad 8.06 | tok/s 18670
step    110 | loss 3.4066 | lr 3.00e-04 | grad 12.62 | tok/s 18661
step    120 | loss 3.2915 | lr 3.00e-04 | grad 12.94 | tok/s 18623
step    130 | loss 2.9923 | lr 3.00e-04 | grad 13.44 | tok/s 18580
step    140 | loss 2.7686 | lr 3.00e-04 | grad 12.31 | tok/s 18517
step    150 | loss 2.7077 | lr 3.00e-04 | grad 10.81 | tok/s 18524
step    160 | loss 2.4207 | lr 3.00e-04 | grad 9.88 | tok/s 18440
step    170 | loss 2.4909 | lr 3.00e-04 | grad 13.19 | tok/s 18472
step    180 | loss 2.3316 | lr 3.00e-04 | grad 8.94 | tok/s 18481
step    190 | loss 2.3784 | lr 3.00e-04 | grad 7.88 | tok/s 18423
step    200 | loss 2.1410 | lr 3.00e-04 | grad 5.88 | tok/s 18426
step    210 | loss 2.1841 | lr 3.00e-04 | grad 8.06 | tok/s 18375
step    220 | loss 2.1634 | lr 3.00e-04 | grad 4.06 | tok/s 18188
step    230 | loss 2.1114 | lr 3.00e-04 | grad 3.78 | tok/s 16559
step    240 | loss 2.3125 | lr 3.00e-04 | grad 5.44 | tok/s 17102
step    250 | loss 2.1000 | lr 3.00e-04 | grad 3.08 | tok/s 17560
step    260 | loss 1.5351 | lr 3.00e-04 | grad 3.50 | tok/s 18064
step    270 | loss 2.0743 | lr 3.00e-04 | grad 3.36 | tok/s 17867
step    280 | loss 2.2619 | lr 3.00e-04 | grad 5.72 | tok/s 17520
step    290 | loss 1.3912 | lr 3.00e-04 | grad 19.50 | tok/s 18483
step    300 | loss 0.6033 | lr 3.00e-04 | grad 2.64 | tok/s 18421
step    310 | loss 2.4164 | lr 3.00e-04 | grad 4.69 | tok/s 18074
step    320 | loss 1.9287 | lr 3.00e-04 | grad 6.38 | tok/s 17707
step    330 | loss 1.9436 | lr 3.00e-04 | grad 3.27 | tok/s 17100
step    340 | loss 2.2898 | lr 3.00e-04 | grad 3.38 | tok/s 17358
step    350 | loss 1.8777 | lr 3.00e-04 | grad 4.56 | tok/s 17797
step    360 | loss 1.2111 | lr 3.00e-04 | grad 8.75 | tok/s 18214
step    370 | loss 1.7987 | lr 3.00e-04 | grad 2.92 | tok/s 16563
step    380 | loss 1.7610 | lr 3.00e-04 | grad 3.34 | tok/s 17590
step    390 | loss 1.5265 | lr 3.00e-04 | grad 2.45 | tok/s 18375
step    400 | loss 1.4812 | lr 3.00e-04 | grad 2.91 | tok/s 18191
step    410 | loss 1.2631 | lr 3.00e-04 | grad 2.39 | tok/s 17812
step    420 | loss 1.8152 | lr 3.00e-04 | grad 4.94 | tok/s 16974
step    430 | loss 2.1806 | lr 3.00e-04 | grad 3.33 | tok/s 18057
step    440 | loss 2.1602 | lr 3.00e-04 | grad 4.44 | tok/s 17106
step    450 | loss 2.0490 | lr 3.00e-04 | grad 3.00 | tok/s 17664
step    460 | loss 1.7347 | lr 3.00e-04 | grad 3.47 | tok/s 17305
step    470 | loss 1.8379 | lr 3.00e-04 | grad 3.06 | tok/s 17853
step    480 | loss 2.2930 | lr 3.00e-04 | grad 7.41 | tok/s 17851
step    490 | loss 1.7967 | lr 3.00e-04 | grad 2.86 | tok/s 16847
step    500 | loss 1.6688 | lr 3.00e-04 | grad 4.00 | tok/s 18026
step    510 | loss 1.7083 | lr 3.00e-04 | grad 2.75 | tok/s 18251
step    520 | loss 1.6541 | lr 3.00e-04 | grad 2.36 | tok/s 18231
step    530 | loss 1.9257 | lr 3.00e-04 | grad 2.61 | tok/s 17515
step    540 | loss 1.7337 | lr 3.00e-04 | grad 2.62 | tok/s 17525
step    550 | loss 1.5694 | lr 3.00e-04 | grad 3.20 | tok/s 17174
step    560 | loss 1.7314 | lr 3.00e-04 | grad 2.97 | tok/s 16734
step    570 | loss 1.6697 | lr 3.00e-04 | grad 3.97 | tok/s 17181
step    580 | loss 1.5520 | lr 3.00e-04 | grad 2.56 | tok/s 17126
step    590 | loss 1.8618 | lr 3.00e-04 | grad 3.42 | tok/s 17557
step    600 | loss 1.8467 | lr 3.00e-04 | grad 2.44 | tok/s 16933
step    610 | loss 1.6287 | lr 3.00e-04 | grad 2.73 | tok/s 17817
step    620 | loss 1.5499 | lr 3.00e-04 | grad 2.70 | tok/s 16884
step    630 | loss 1.6571 | lr 3.00e-04 | grad 4.91 | tok/s 17015
step    640 | loss 1.8126 | lr 3.00e-04 | grad 2.73 | tok/s 17476
step    650 | loss 1.6880 | lr 3.00e-04 | grad 3.08 | tok/s 17562
step    660 | loss 1.6993 | lr 3.00e-04 | grad 2.53 | tok/s 17643
step    670 | loss 1.9306 | lr 3.00e-04 | grad 3.75 | tok/s 17765
step    680 | loss 1.7364 | lr 3.00e-04 | grad 2.59 | tok/s 17409
step    690 | loss 1.8451 | lr 3.00e-04 | grad 3.52 | tok/s 17989
step    700 | loss 1.4333 | lr 3.00e-04 | grad 3.22 | tok/s 18372
step    710 | loss 1.6005 | lr 3.00e-04 | grad 2.67 | tok/s 17140
step    720 | loss 1.4867 | lr 3.00e-04 | grad 3.59 | tok/s 16878
step    730 | loss 1.2877 | lr 3.00e-04 | grad 3.25 | tok/s 18294
step    740 | loss 1.5139 | lr 3.00e-04 | grad 2.59 | tok/s 18073
step    750 | loss 1.2192 | lr 3.00e-04 | grad 2.78 | tok/s 18368
step    760 | loss 1.1225 | lr 3.00e-04 | grad 2.31 | tok/s 18360
step    770 | loss 1.0689 | lr 3.00e-04 | grad 2.20 | tok/s 18373

Training complete! Final step: 779
