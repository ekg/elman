Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_96/levelE88_100m_20260126_133524
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,920,064 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2512 | lr 3.00e-04 | grad 18.88 | tok/s 6274
step     20 | loss 3.0924 | lr 3.00e-04 | grad 11.06 | tok/s 18665
step     30 | loss 2.6847 | lr 3.00e-04 | grad 7.03 | tok/s 18857
step     40 | loss 2.4215 | lr 3.00e-04 | grad 4.69 | tok/s 18039
step     50 | loss 3.0277 | lr 3.00e-04 | grad 20.38 | tok/s 18282
step     60 | loss 2.0194 | lr 3.00e-04 | grad 5.12 | tok/s 18876
step     70 | loss 1.8472 | lr 3.00e-04 | grad 5.28 | tok/s 19098
step     80 | loss 6.9921 | lr 3.00e-04 | grad 59.75 | tok/s 19181
step     90 | loss 6.1071 | lr 3.00e-04 | grad 10.75 | tok/s 19496
step    100 | loss 4.1994 | lr 3.00e-04 | grad 8.94 | tok/s 19467
step    110 | loss 3.4230 | lr 3.00e-04 | grad 16.75 | tok/s 19427
step    120 | loss 3.1803 | lr 3.00e-04 | grad 13.81 | tok/s 19364
step    130 | loss 2.9764 | lr 3.00e-04 | grad 13.75 | tok/s 19332
step    140 | loss 2.8377 | lr 3.00e-04 | grad 10.88 | tok/s 19300
step    150 | loss 2.7883 | lr 3.00e-04 | grad 16.88 | tok/s 19288
step    160 | loss 2.4270 | lr 3.00e-04 | grad 10.94 | tok/s 19292
step    170 | loss 2.4269 | lr 3.00e-04 | grad 14.38 | tok/s 19278
step    180 | loss 2.3617 | lr 3.00e-04 | grad 8.56 | tok/s 19259
step    190 | loss 2.4885 | lr 3.00e-04 | grad 6.94 | tok/s 19234
step    200 | loss 2.0979 | lr 3.00e-04 | grad 5.59 | tok/s 19226
step    210 | loss 2.1780 | lr 3.00e-04 | grad 11.25 | tok/s 19225
step    220 | loss 2.1783 | lr 3.00e-04 | grad 5.00 | tok/s 17212
step    230 | loss 2.0888 | lr 3.00e-04 | grad 4.41 | tok/s 18761
step    240 | loss 2.3132 | lr 3.00e-04 | grad 5.94 | tok/s 17798
step    250 | loss 2.1000 | lr 3.00e-04 | grad 3.22 | tok/s 18287
step    260 | loss 1.5307 | lr 3.00e-04 | grad 3.56 | tok/s 18870
step    270 | loss 2.0829 | lr 3.00e-04 | grad 3.56 | tok/s 18597
step    280 | loss 2.2345 | lr 3.00e-04 | grad 5.12 | tok/s 18250
step    290 | loss 1.4862 | lr 3.00e-04 | grad 4.06 | tok/s 19230
step    300 | loss 0.5922 | lr 3.00e-04 | grad 4.34 | tok/s 19186
step    310 | loss 2.4138 | lr 3.00e-04 | grad 4.28 | tok/s 18870
step    320 | loss 1.9156 | lr 3.00e-04 | grad 6.81 | tok/s 18489
step    330 | loss 1.9515 | lr 3.00e-04 | grad 3.42 | tok/s 17850
step    340 | loss 2.2845 | lr 3.00e-04 | grad 3.69 | tok/s 18148
step    350 | loss 1.8362 | lr 3.00e-04 | grad 4.28 | tok/s 18599
step    360 | loss 1.1814 | lr 3.00e-04 | grad 10.06 | tok/s 18982
step    370 | loss 1.7963 | lr 3.00e-04 | grad 2.95 | tok/s 17216
step    380 | loss 1.7648 | lr 3.00e-04 | grad 3.41 | tok/s 18353
step    390 | loss 1.5189 | lr 3.00e-04 | grad 2.78 | tok/s 19179
step    400 | loss 1.4859 | lr 3.00e-04 | grad 3.31 | tok/s 19013
step    410 | loss 1.2619 | lr 3.00e-04 | grad 2.44 | tok/s 18559
step    420 | loss 1.8079 | lr 3.00e-04 | grad 5.38 | tok/s 17729
step    430 | loss 2.1476 | lr 3.00e-04 | grad 3.55 | tok/s 18877
step    440 | loss 2.1558 | lr 3.00e-04 | grad 4.47 | tok/s 17803
step    450 | loss 2.0712 | lr 3.00e-04 | grad 3.03 | tok/s 18447
step    460 | loss 1.7115 | lr 3.00e-04 | grad 3.53 | tok/s 18053
step    470 | loss 1.8281 | lr 3.00e-04 | grad 3.28 | tok/s 18610
step    480 | loss 2.2550 | lr 3.00e-04 | grad 7.34 | tok/s 18588
step    490 | loss 1.7921 | lr 3.00e-04 | grad 3.02 | tok/s 17581
step    500 | loss 1.6637 | lr 3.00e-04 | grad 4.38 | tok/s 18790
step    510 | loss 1.7088 | lr 3.00e-04 | grad 3.08 | tok/s 19025
step    520 | loss 1.6475 | lr 3.00e-04 | grad 2.55 | tok/s 19000
step    530 | loss 1.8944 | lr 3.00e-04 | grad 2.77 | tok/s 18275
step    540 | loss 1.7306 | lr 3.00e-04 | grad 2.91 | tok/s 18267
step    550 | loss 1.5681 | lr 3.00e-04 | grad 3.23 | tok/s 17892
step    560 | loss 1.7121 | lr 3.00e-04 | grad 3.12 | tok/s 17394
step    570 | loss 1.6618 | lr 3.00e-04 | grad 4.12 | tok/s 17893
step    580 | loss 1.5381 | lr 3.00e-04 | grad 2.70 | tok/s 17837
step    590 | loss 1.8468 | lr 3.00e-04 | grad 3.55 | tok/s 18277
step    600 | loss 1.8286 | lr 3.00e-04 | grad 2.59 | tok/s 17627
step    610 | loss 1.6206 | lr 3.00e-04 | grad 2.88 | tok/s 18545
step    620 | loss 1.5461 | lr 3.00e-04 | grad 2.75 | tok/s 17584
step    630 | loss 1.6554 | lr 3.00e-04 | grad 4.75 | tok/s 17714
step    640 | loss 1.8187 | lr 3.00e-04 | grad 2.64 | tok/s 18186
step    650 | loss 1.6831 | lr 3.00e-04 | grad 3.06 | tok/s 18280
step    660 | loss 1.6936 | lr 3.00e-04 | grad 2.33 | tok/s 18386
step    670 | loss 1.9154 | lr 3.00e-04 | grad 4.00 | tok/s 18533
step    680 | loss 1.7329 | lr 3.00e-04 | grad 2.88 | tok/s 18148
step    690 | loss 1.8293 | lr 3.00e-04 | grad 3.72 | tok/s 18788
step    700 | loss 1.3999 | lr 3.00e-04 | grad 3.30 | tok/s 19126
step    710 | loss 1.5917 | lr 3.00e-04 | grad 2.95 | tok/s 17864
step    720 | loss 1.4709 | lr 3.00e-04 | grad 3.81 | tok/s 17585
step    730 | loss 1.2798 | lr 3.00e-04 | grad 3.27 | tok/s 19071
step    740 | loss 1.4951 | lr 3.00e-04 | grad 2.75 | tok/s 18832
step    750 | loss 1.1905 | lr 3.00e-04 | grad 2.77 | tok/s 19146
step    760 | loss 1.0984 | lr 3.00e-04 | grad 2.48 | tok/s 19145
step    770 | loss 1.0505 | lr 3.00e-04 | grad 2.44 | tok/s 19152
step    780 | loss 0.9889 | lr 3.00e-04 | grad 2.38 | tok/s 19117
step    790 | loss 1.1248 | lr 3.00e-04 | grad 3.86 | tok/s 18520
step    800 | loss 1.8200 | lr 3.00e-04 | grad 6.34 | tok/s 18476
step    810 | loss 1.7056 | lr 3.00e-04 | grad 2.58 | tok/s 18370

Training complete! Final step: 811
