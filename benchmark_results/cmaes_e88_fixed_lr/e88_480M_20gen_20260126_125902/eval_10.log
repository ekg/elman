Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_10/levelE88_100m_20260126_130226
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 469,027,980 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.7131 | lr 3.00e-04 | grad 9.81 | tok/s 5293
step     20 | loss 3.5566 | lr 3.00e-04 | grad 67.00 | tok/s 7136
step     30 | loss 4.7945 | lr 3.00e-04 | grad 41.00 | tok/s 7374
step     40 | loss 4.2081 | lr 3.00e-04 | grad 20.62 | tok/s 7346
step     50 | loss 3.9706 | lr 3.00e-04 | grad 15.00 | tok/s 7336
step     60 | loss 3.3228 | lr 3.00e-04 | grad 3.67 | tok/s 7148
step     70 | loss 2.7437 | lr 3.00e-04 | grad 5.28 | tok/s 7032
step     80 | loss 2.6394 | lr 3.00e-04 | grad 2.41 | tok/s 7227
step     90 | loss 2.7131 | lr 3.00e-04 | grad 13.94 | tok/s 6871
step    100 | loss 2.3513 | lr 3.00e-04 | grad 6.31 | tok/s 7006
step    110 | loss 2.4651 | lr 3.00e-04 | grad 1.91 | tok/s 7059
step    120 | loss 2.5851 | lr 3.00e-04 | grad 2.34 | tok/s 7068
step    130 | loss 2.5046 | lr 3.00e-04 | grad 1.62 | tok/s 7227
step    140 | loss 2.2453 | lr 3.00e-04 | grad 3.66 | tok/s 7063
step    150 | loss 2.2686 | lr 3.00e-04 | grad 1.59 | tok/s 6954

Training complete! Final step: 158
