Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_105/levelE88_100m_20260126_134159
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 483,867,642 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.0500 | lr 3.00e-04 | grad 21.25 | tok/s 6267
step     20 | loss 2.8695 | lr 3.00e-04 | grad 12.31 | tok/s 16822
step     30 | loss 2.7303 | lr 3.00e-04 | grad 7.97 | tok/s 16988
step     40 | loss 2.5462 | lr 3.00e-04 | grad 5.50 | tok/s 16250
step     50 | loss 3.0943 | lr 3.00e-04 | grad 16.00 | tok/s 16466
step     60 | loss 2.0854 | lr 3.00e-04 | grad 4.22 | tok/s 16963
step     70 | loss 1.9158 | lr 3.00e-04 | grad 5.38 | tok/s 17152
step     80 | loss 6.6820 | lr 3.00e-04 | grad 93.50 | tok/s 17210
step     90 | loss 6.1650 | lr 3.00e-04 | grad 12.38 | tok/s 17504
step    100 | loss 4.4608 | lr 3.00e-04 | grad 11.06 | tok/s 17464
step    110 | loss 3.8287 | lr 3.00e-04 | grad 28.00 | tok/s 17427
step    120 | loss 3.3966 | lr 3.00e-04 | grad 16.75 | tok/s 17395
step    130 | loss 3.0807 | lr 3.00e-04 | grad 20.12 | tok/s 16364
step    140 | loss 2.7870 | lr 3.00e-04 | grad 13.06 | tok/s 17377
step    150 | loss 2.9193 | lr 3.00e-04 | grad 19.38 | tok/s 17348
step    160 | loss 2.4654 | lr 3.00e-04 | grad 11.19 | tok/s 17330
step    170 | loss 2.5078 | lr 3.00e-04 | grad 14.88 | tok/s 17318
step    180 | loss 2.3927 | lr 3.00e-04 | grad 9.69 | tok/s 17332
step    190 | loss 2.4877 | lr 3.00e-04 | grad 7.38 | tok/s 17328
step    200 | loss 2.2448 | lr 3.00e-04 | grad 7.47 | tok/s 17332
step    210 | loss 2.2632 | lr 3.00e-04 | grad 12.88 | tok/s 17335
step    220 | loss 2.2298 | lr 3.00e-04 | grad 4.09 | tok/s 17088
step    230 | loss 2.1002 | lr 3.00e-04 | grad 3.86 | tok/s 16893
step    240 | loss 2.3257 | lr 3.00e-04 | grad 5.22 | tok/s 15986
step    250 | loss 2.1289 | lr 3.00e-04 | grad 2.89 | tok/s 16468
step    260 | loss 1.5656 | lr 3.00e-04 | grad 3.23 | tok/s 16961
step    270 | loss 2.1199 | lr 3.00e-04 | grad 3.16 | tok/s 16752
step    280 | loss 2.2663 | lr 3.00e-04 | grad 5.38 | tok/s 16428
step    290 | loss 1.4051 | lr 3.00e-04 | grad 3.14 | tok/s 17267
step    300 | loss 0.5585 | lr 3.00e-04 | grad 3.66 | tok/s 17271
step    310 | loss 2.4363 | lr 3.00e-04 | grad 4.06 | tok/s 16964
step    320 | loss 1.9536 | lr 3.00e-04 | grad 6.06 | tok/s 16613
step    330 | loss 1.9718 | lr 3.00e-04 | grad 3.08 | tok/s 16070
step    340 | loss 2.2994 | lr 3.00e-04 | grad 3.16 | tok/s 16302
step    350 | loss 1.8692 | lr 3.00e-04 | grad 4.94 | tok/s 16727
step    360 | loss 1.1848 | lr 3.00e-04 | grad 7.97 | tok/s 17060
step    370 | loss 1.8227 | lr 3.00e-04 | grad 2.80 | tok/s 15513
step    380 | loss 1.7798 | lr 3.00e-04 | grad 3.09 | tok/s 16477
step    390 | loss 1.5424 | lr 3.00e-04 | grad 2.36 | tok/s 17194
step    400 | loss 1.5063 | lr 3.00e-04 | grad 2.92 | tok/s 17040
step    410 | loss 1.2782 | lr 3.00e-04 | grad 2.22 | tok/s 16676
step    420 | loss 1.8227 | lr 3.00e-04 | grad 4.72 | tok/s 15913
step    430 | loss 2.1852 | lr 3.00e-04 | grad 3.36 | tok/s 16981
step    440 | loss 2.1740 | lr 3.00e-04 | grad 4.44 | tok/s 16026
step    450 | loss 2.0238 | lr 3.00e-04 | grad 2.92 | tok/s 16567
step    460 | loss 1.7268 | lr 3.00e-04 | grad 3.11 | tok/s 16273
step    470 | loss 1.8555 | lr 3.00e-04 | grad 2.89 | tok/s 16721
step    480 | loss 2.2810 | lr 3.00e-04 | grad 6.97 | tok/s 16773
step    490 | loss 1.8000 | lr 3.00e-04 | grad 2.84 | tok/s 15839
step    500 | loss 1.6922 | lr 3.00e-04 | grad 3.77 | tok/s 16895
step    510 | loss 1.7178 | lr 3.00e-04 | grad 2.72 | tok/s 17152
step    520 | loss 1.6680 | lr 3.00e-04 | grad 2.34 | tok/s 17096
step    530 | loss 1.9196 | lr 3.00e-04 | grad 2.59 | tok/s 16418
step    540 | loss 1.7522 | lr 3.00e-04 | grad 2.56 | tok/s 16467
step    550 | loss 1.5832 | lr 3.00e-04 | grad 3.17 | tok/s 16112
step    560 | loss 1.7385 | lr 3.00e-04 | grad 2.86 | tok/s 15704
step    570 | loss 1.6866 | lr 3.00e-04 | grad 3.83 | tok/s 15288
step    580 | loss 1.5591 | lr 3.00e-04 | grad 2.42 | tok/s 16045
step    590 | loss 1.8825 | lr 3.00e-04 | grad 3.36 | tok/s 16459
step    600 | loss 1.8360 | lr 3.00e-04 | grad 2.41 | tok/s 15943
step    610 | loss 1.6379 | lr 3.00e-04 | grad 2.52 | tok/s 16725
step    620 | loss 1.5584 | lr 3.00e-04 | grad 2.56 | tok/s 15869
step    630 | loss 1.6687 | lr 3.00e-04 | grad 4.81 | tok/s 15960
step    640 | loss 1.8328 | lr 3.00e-04 | grad 2.58 | tok/s 16418
step    650 | loss 1.6904 | lr 3.00e-04 | grad 2.77 | tok/s 16492
step    660 | loss 1.7107 | lr 3.00e-04 | grad 2.19 | tok/s 16540
step    670 | loss 1.9288 | lr 3.00e-04 | grad 3.36 | tok/s 16674
step    680 | loss 1.7423 | lr 3.00e-04 | grad 2.58 | tok/s 16346
step    690 | loss 1.8500 | lr 3.00e-04 | grad 3.88 | tok/s 16899
step    700 | loss 1.4401 | lr 3.00e-04 | grad 3.33 | tok/s 17229
step    710 | loss 1.6059 | lr 3.00e-04 | grad 2.56 | tok/s 16105
step    720 | loss 1.4805 | lr 3.00e-04 | grad 3.48 | tok/s 15872
step    730 | loss 1.2931 | lr 3.00e-04 | grad 3.09 | tok/s 17190

Training complete! Final step: 732
