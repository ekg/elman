Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_35/levelE88_100m_20260126_131218
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 466,674,272 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 3.9879 | lr 3.00e-04 | grad 5.34 | tok/s 5150
step     20 | loss 3.3501 | lr 3.00e-04 | grad 24.00 | tok/s 6815
step     30 | loss 4.4011 | lr 3.00e-04 | grad 14.56 | tok/s 7047
step     40 | loss 3.3789 | lr 3.00e-04 | grad 11.25 | tok/s 7033
step     50 | loss 2.9601 | lr 3.00e-04 | grad 4.53 | tok/s 7013
step     60 | loss 2.8994 | lr 3.00e-04 | grad 1.93 | tok/s 6850
step     70 | loss 2.5822 | lr 3.00e-04 | grad 2.88 | tok/s 6729
step     80 | loss 2.5039 | lr 3.00e-04 | grad 1.92 | tok/s 6914
step     90 | loss 2.4719 | lr 3.00e-04 | grad 6.47 | tok/s 6581
step    100 | loss 2.2175 | lr 3.00e-04 | grad 3.03 | tok/s 6704
step    110 | loss 2.3196 | lr 3.00e-04 | grad 1.49 | tok/s 6751
step    120 | loss 2.3668 | lr 3.00e-04 | grad 1.48 | tok/s 6748
step    130 | loss 2.3239 | lr 3.00e-04 | grad 1.59 | tok/s 6906
step    140 | loss 2.1344 | lr 3.00e-04 | grad 3.78 | tok/s 6736
step    150 | loss 2.1458 | lr 3.00e-04 | grad 1.29 | tok/s 6632

Training complete! Final step: 151
