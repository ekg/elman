Using device: cuda
Output directory: benchmark_results/cmaes_e88_fixed_lr/e88_480M_20gen_20260126_125902/eval_151/levelE88_100m_20260126_135828
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,325,414 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.0745 | lr 3.00e-04 | grad 18.88 | tok/s 6349
step     20 | loss 2.9450 | lr 3.00e-04 | grad 9.62 | tok/s 18690
step     30 | loss 2.7102 | lr 3.00e-04 | grad 7.78 | tok/s 18894
step     40 | loss 2.4892 | lr 3.00e-04 | grad 5.59 | tok/s 18075
step     50 | loss 3.0378 | lr 3.00e-04 | grad 13.44 | tok/s 18326
step     60 | loss 2.1054 | lr 3.00e-04 | grad 4.16 | tok/s 18904
step     70 | loss 1.8602 | lr 3.00e-04 | grad 5.62 | tok/s 19081
step     80 | loss 6.4670 | lr 3.00e-04 | grad 60.75 | tok/s 19204
step     90 | loss 5.5471 | lr 3.00e-04 | grad 10.31 | tok/s 19518
step    100 | loss 4.2671 | lr 3.00e-04 | grad 8.25 | tok/s 19458
step    110 | loss 3.5027 | lr 3.00e-04 | grad 11.88 | tok/s 19444
step    120 | loss 3.3027 | lr 3.00e-04 | grad 11.50 | tok/s 19430
step    130 | loss 3.0080 | lr 3.00e-04 | grad 16.00 | tok/s 19390
step    140 | loss 2.6349 | lr 3.00e-04 | grad 11.38 | tok/s 19379
step    150 | loss 2.7682 | lr 3.00e-04 | grad 10.88 | tok/s 19369
step    160 | loss 2.3806 | lr 3.00e-04 | grad 9.25 | tok/s 19350
step    170 | loss 2.4594 | lr 3.00e-04 | grad 14.50 | tok/s 19310
step    180 | loss 2.3460 | lr 3.00e-04 | grad 9.69 | tok/s 19306
step    190 | loss 2.4209 | lr 3.00e-04 | grad 7.03 | tok/s 19303
step    200 | loss 2.0559 | lr 3.00e-04 | grad 5.56 | tok/s 19261
step    210 | loss 2.1370 | lr 3.00e-04 | grad 7.91 | tok/s 19265
step    220 | loss 2.1446 | lr 3.00e-04 | grad 4.03 | tok/s 19003
step    230 | loss 2.0160 | lr 3.00e-04 | grad 4.09 | tok/s 18802
step    240 | loss 2.2985 | lr 3.00e-04 | grad 5.34 | tok/s 17885
step    250 | loss 2.1044 | lr 3.00e-04 | grad 2.88 | tok/s 18360
step    260 | loss 1.5178 | lr 3.00e-04 | grad 3.28 | tok/s 18915
step    270 | loss 2.0797 | lr 3.00e-04 | grad 3.20 | tok/s 18673
step    280 | loss 2.2351 | lr 3.00e-04 | grad 5.56 | tok/s 18327
step    290 | loss 1.3761 | lr 3.00e-04 | grad 6.12 | tok/s 19283
step    300 | loss 0.5800 | lr 3.00e-04 | grad 4.81 | tok/s 19283
step    310 | loss 2.4103 | lr 3.00e-04 | grad 3.91 | tok/s 18951
step    320 | loss 1.9109 | lr 3.00e-04 | grad 6.28 | tok/s 18548
step    330 | loss 1.9408 | lr 3.00e-04 | grad 3.16 | tok/s 17878
step    340 | loss 2.2906 | lr 3.00e-04 | grad 3.22 | tok/s 18173
step    350 | loss 1.8387 | lr 3.00e-04 | grad 4.06 | tok/s 18646
step    360 | loss 1.1661 | lr 3.00e-04 | grad 8.12 | tok/s 19020
step    370 | loss 1.7993 | lr 3.00e-04 | grad 2.88 | tok/s 17259
step    380 | loss 1.7511 | lr 3.00e-04 | grad 2.98 | tok/s 18383
step    390 | loss 1.5190 | lr 3.00e-04 | grad 2.50 | tok/s 19169
step    400 | loss 1.4779 | lr 3.00e-04 | grad 2.95 | tok/s 18998
step    410 | loss 1.2576 | lr 3.00e-04 | grad 2.27 | tok/s 18597
step    420 | loss 1.8060 | lr 3.00e-04 | grad 4.72 | tok/s 17760
step    430 | loss 2.1509 | lr 3.00e-04 | grad 3.31 | tok/s 18904
step    440 | loss 2.1496 | lr 3.00e-04 | grad 4.47 | tok/s 17866
step    450 | loss 2.0648 | lr 3.00e-04 | grad 2.89 | tok/s 18475
step    460 | loss 1.7197 | lr 3.00e-04 | grad 3.14 | tok/s 18097
step    470 | loss 1.8363 | lr 3.00e-04 | grad 2.95 | tok/s 18627
step    480 | loss 2.2250 | lr 3.00e-04 | grad 7.25 | tok/s 18650
step    490 | loss 1.7943 | lr 3.00e-04 | grad 2.70 | tok/s 17616
step    500 | loss 1.6707 | lr 3.00e-04 | grad 3.73 | tok/s 18823
step    510 | loss 1.7044 | lr 3.00e-04 | grad 2.67 | tok/s 19065
step    520 | loss 1.6514 | lr 3.00e-04 | grad 2.34 | tok/s 19031
step    530 | loss 1.9201 | lr 3.00e-04 | grad 2.67 | tok/s 18311
step    540 | loss 1.7341 | lr 3.00e-04 | grad 2.53 | tok/s 18326
step    550 | loss 1.5696 | lr 3.00e-04 | grad 3.17 | tok/s 16252
step    560 | loss 1.7223 | lr 3.00e-04 | grad 2.91 | tok/s 17515
step    570 | loss 1.6666 | lr 3.00e-04 | grad 3.80 | tok/s 17993
step    580 | loss 1.5420 | lr 3.00e-04 | grad 2.56 | tok/s 17900
step    590 | loss 1.8497 | lr 3.00e-04 | grad 3.44 | tok/s 18358
step    600 | loss 1.8365 | lr 3.00e-04 | grad 2.45 | tok/s 17744
step    610 | loss 1.6247 | lr 3.00e-04 | grad 2.66 | tok/s 18650
step    620 | loss 1.5446 | lr 3.00e-04 | grad 2.70 | tok/s 17668
step    630 | loss 1.6523 | lr 3.00e-04 | grad 4.56 | tok/s 17804
step    640 | loss 1.8016 | lr 3.00e-04 | grad 2.64 | tok/s 18284
step    650 | loss 1.6702 | lr 3.00e-04 | grad 2.89 | tok/s 18387
step    660 | loss 1.6943 | lr 3.00e-04 | grad 2.39 | tok/s 18470
step    670 | loss 1.9242 | lr 3.00e-04 | grad 7.81 | tok/s 18604
step    680 | loss 1.7242 | lr 3.00e-04 | grad 2.64 | tok/s 18216
step    690 | loss 1.8308 | lr 3.00e-04 | grad 3.66 | tok/s 18838
step    700 | loss 1.4186 | lr 3.00e-04 | grad 3.25 | tok/s 19221
step    710 | loss 1.5983 | lr 3.00e-04 | grad 2.59 | tok/s 17976
step    720 | loss 1.4753 | lr 3.00e-04 | grad 3.64 | tok/s 17718
step    730 | loss 1.2789 | lr 3.00e-04 | grad 3.06 | tok/s 19187
step    740 | loss 1.4940 | lr 3.00e-04 | grad 2.62 | tok/s 18904
step    750 | loss 1.1938 | lr 3.00e-04 | grad 2.81 | tok/s 19186
step    760 | loss 1.0991 | lr 3.00e-04 | grad 2.31 | tok/s 19199
step    770 | loss 1.0504 | lr 3.00e-04 | grad 2.23 | tok/s 19207
step    780 | loss 0.9935 | lr 3.00e-04 | grad 2.28 | tok/s 19214
step    790 | loss 1.1245 | lr 3.00e-04 | grad 3.47 | tok/s 18614
step    800 | loss 1.8275 | lr 3.00e-04 | grad 5.75 | tok/s 18560
step    810 | loss 1.7086 | lr 3.00e-04 | grad 2.45 | tok/s 18454

Training complete! Final step: 814
