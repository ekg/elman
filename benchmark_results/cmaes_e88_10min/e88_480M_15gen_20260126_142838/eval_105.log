Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_105/levelE88_100m_20260126_164244
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 483,867,642 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.0500 | lr 3.00e-04 | grad 21.25 | tok/s 6037
step     20 | loss 2.8695 | lr 3.00e-04 | grad 12.31 | tok/s 16870
step     30 | loss 2.7303 | lr 3.00e-04 | grad 7.97 | tok/s 17012
step     40 | loss 2.5462 | lr 3.00e-04 | grad 5.50 | tok/s 16307
step     50 | loss 3.0943 | lr 3.00e-04 | grad 16.00 | tok/s 16519
step     60 | loss 2.0854 | lr 3.00e-04 | grad 4.22 | tok/s 16984
step     70 | loss 1.9158 | lr 3.00e-04 | grad 5.38 | tok/s 17175
step     80 | loss 6.6820 | lr 3.00e-04 | grad 93.50 | tok/s 17269
step     90 | loss 6.1650 | lr 3.00e-04 | grad 12.38 | tok/s 17541
step    100 | loss 4.4608 | lr 3.00e-04 | grad 11.06 | tok/s 17467
step    110 | loss 3.8287 | lr 3.00e-04 | grad 28.00 | tok/s 17470
step    120 | loss 3.3966 | lr 3.00e-04 | grad 16.75 | tok/s 17426
step    130 | loss 3.0807 | lr 3.00e-04 | grad 20.12 | tok/s 17376
step    140 | loss 2.7870 | lr 3.00e-04 | grad 13.06 | tok/s 17327
step    150 | loss 2.9193 | lr 3.00e-04 | grad 19.38 | tok/s 17282
step    160 | loss 2.4654 | lr 3.00e-04 | grad 11.19 | tok/s 17326
step    170 | loss 2.5078 | lr 3.00e-04 | grad 14.88 | tok/s 17317
step    180 | loss 2.3927 | lr 3.00e-04 | grad 9.69 | tok/s 17314
step    190 | loss 2.4877 | lr 3.00e-04 | grad 7.38 | tok/s 17308
step    200 | loss 2.2448 | lr 3.00e-04 | grad 7.47 | tok/s 17295
step    210 | loss 2.2632 | lr 3.00e-04 | grad 12.88 | tok/s 17246
step    220 | loss 2.2298 | lr 3.00e-04 | grad 4.09 | tok/s 17086
step    230 | loss 2.1002 | lr 3.00e-04 | grad 3.86 | tok/s 15502
step    240 | loss 2.3257 | lr 3.00e-04 | grad 5.22 | tok/s 16008
step    250 | loss 2.1289 | lr 3.00e-04 | grad 2.89 | tok/s 16395
step    260 | loss 1.5656 | lr 3.00e-04 | grad 3.23 | tok/s 16904
step    270 | loss 2.1199 | lr 3.00e-04 | grad 3.16 | tok/s 16703
step    280 | loss 2.2663 | lr 3.00e-04 | grad 5.38 | tok/s 16329
step    290 | loss 1.4051 | lr 3.00e-04 | grad 3.14 | tok/s 17212
step    300 | loss 0.5585 | lr 3.00e-04 | grad 3.66 | tok/s 17154
step    310 | loss 2.4363 | lr 3.00e-04 | grad 4.06 | tok/s 16869
step    320 | loss 1.9536 | lr 3.00e-04 | grad 6.06 | tok/s 16505
step    330 | loss 1.9718 | lr 3.00e-04 | grad 3.08 | tok/s 15952
step    340 | loss 2.2994 | lr 3.00e-04 | grad 3.16 | tok/s 16205
step    350 | loss 1.8692 | lr 3.00e-04 | grad 4.94 | tok/s 16604
step    360 | loss 1.1848 | lr 3.00e-04 | grad 7.97 | tok/s 16941
step    370 | loss 1.8227 | lr 3.00e-04 | grad 2.80 | tok/s 15399
step    380 | loss 1.7798 | lr 3.00e-04 | grad 3.09 | tok/s 16392
step    390 | loss 1.5424 | lr 3.00e-04 | grad 2.36 | tok/s 17094
step    400 | loss 1.5063 | lr 3.00e-04 | grad 2.92 | tok/s 16958
step    410 | loss 1.2782 | lr 3.00e-04 | grad 2.22 | tok/s 14827
step    420 | loss 1.8227 | lr 3.00e-04 | grad 4.72 | tok/s 15876
step    430 | loss 2.1852 | lr 3.00e-04 | grad 3.36 | tok/s 16903
step    440 | loss 2.1740 | lr 3.00e-04 | grad 4.44 | tok/s 15939
step    450 | loss 2.0238 | lr 3.00e-04 | grad 2.92 | tok/s 16492
step    460 | loss 1.7268 | lr 3.00e-04 | grad 3.11 | tok/s 16126
step    470 | loss 1.8555 | lr 3.00e-04 | grad 2.89 | tok/s 16664
step    480 | loss 2.2810 | lr 3.00e-04 | grad 6.97 | tok/s 16659
step    490 | loss 1.8000 | lr 3.00e-04 | grad 2.84 | tok/s 15733
step    500 | loss 1.6922 | lr 3.00e-04 | grad 3.77 | tok/s 16781
step    510 | loss 1.7178 | lr 3.00e-04 | grad 2.72 | tok/s 17027
step    520 | loss 1.6680 | lr 3.00e-04 | grad 2.34 | tok/s 16966
step    530 | loss 1.9196 | lr 3.00e-04 | grad 2.59 | tok/s 16341
step    540 | loss 1.7522 | lr 3.00e-04 | grad 2.56 | tok/s 16330
step    550 | loss 1.5832 | lr 3.00e-04 | grad 3.17 | tok/s 15992
step    560 | loss 1.7385 | lr 3.00e-04 | grad 2.86 | tok/s 14340
step    570 | loss 1.6866 | lr 3.00e-04 | grad 3.83 | tok/s 16066
step    580 | loss 1.5591 | lr 3.00e-04 | grad 2.42 | tok/s 15984
step    590 | loss 1.8825 | lr 3.00e-04 | grad 3.36 | tok/s 16403
step    600 | loss 1.8360 | lr 3.00e-04 | grad 2.41 | tok/s 15809
step    610 | loss 1.6379 | lr 3.00e-04 | grad 2.52 | tok/s 16651
step    620 | loss 1.5584 | lr 3.00e-04 | grad 2.56 | tok/s 15742
step    630 | loss 1.6687 | lr 3.00e-04 | grad 4.81 | tok/s 15862
step    640 | loss 1.8328 | lr 3.00e-04 | grad 2.58 | tok/s 16278
step    650 | loss 1.6904 | lr 3.00e-04 | grad 2.77 | tok/s 16357
step    660 | loss 1.7107 | lr 3.00e-04 | grad 2.19 | tok/s 16468
step    670 | loss 1.9288 | lr 3.00e-04 | grad 3.36 | tok/s 16534
step    680 | loss 1.7423 | lr 3.00e-04 | grad 2.58 | tok/s 16212
step    690 | loss 1.8500 | lr 3.00e-04 | grad 3.88 | tok/s 16755
step    700 | loss 1.4401 | lr 3.00e-04 | grad 3.33 | tok/s 17085
step    710 | loss 1.6059 | lr 3.00e-04 | grad 2.56 | tok/s 15949
step    720 | loss 1.4805 | lr 3.00e-04 | grad 3.48 | tok/s 15766
step    730 | loss 1.2931 | lr 3.00e-04 | grad 3.09 | tok/s 17097
step    740 | loss 1.5087 | lr 3.00e-04 | grad 2.48 | tok/s 16859
step    750 | loss 1.2098 | lr 3.00e-04 | grad 2.64 | tok/s 17136
step    760 | loss 1.1132 | lr 3.00e-04 | grad 2.34 | tok/s 17087
step    770 | loss 1.0616 | lr 3.00e-04 | grad 2.31 | tok/s 17117
step    780 | loss 0.9968 | lr 3.00e-04 | grad 2.17 | tok/s 17107
step    790 | loss 1.1350 | lr 3.00e-04 | grad 3.59 | tok/s 16598
step    800 | loss 1.8315 | lr 3.00e-04 | grad 5.69 | tok/s 16521
step    810 | loss 1.7146 | lr 3.00e-04 | grad 2.25 | tok/s 16438
step    820 | loss 1.7280 | lr 3.00e-04 | grad 4.28 | tok/s 15818
step    830 | loss 1.5107 | lr 3.00e-04 | grad 2.48 | tok/s 16969
step    840 | loss 1.3630 | lr 3.00e-04 | grad 2.31 | tok/s 17153
step    850 | loss 1.5827 | lr 3.00e-04 | grad 2.27 | tok/s 17048
step    860 | loss 1.4811 | lr 3.00e-04 | grad 3.98 | tok/s 16836
step    870 | loss 1.5148 | lr 3.00e-04 | grad 2.81 | tok/s 16240
step    880 | loss 1.6980 | lr 3.00e-04 | grad 2.92 | tok/s 16296
step    890 | loss 1.6967 | lr 3.00e-04 | grad 3.19 | tok/s 16536
step    900 | loss 1.5805 | lr 3.00e-04 | grad 2.67 | tok/s 16560
step    910 | loss 1.4340 | lr 3.00e-04 | grad 4.12 | tok/s 16200
step    920 | loss 1.5424 | lr 3.00e-04 | grad 3.72 | tok/s 16858
step    930 | loss 1.6163 | lr 3.00e-04 | grad 3.66 | tok/s 16113
step    940 | loss 1.3954 | lr 3.00e-04 | grad 2.03 | tok/s 16971
step    950 | loss 1.4984 | lr 3.00e-04 | grad 2.67 | tok/s 17072
step    960 | loss 1.3344 | lr 3.00e-04 | grad 2.77 | tok/s 17064
step    970 | loss 1.7592 | lr 3.00e-04 | grad 3.80 | tok/s 16069
step    980 | loss 1.6578 | lr 3.00e-04 | grad 2.53 | tok/s 16494
step    990 | loss 1.4631 | lr 3.00e-04 | grad 2.31 | tok/s 16774
step   1000 | loss 1.8710 | lr 3.00e-04 | grad 9.31 | tok/s 15098
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8710.pt
step   1010 | loss 1.7400 | lr 3.00e-04 | grad 2.66 | tok/s 5870
step   1020 | loss 1.7001 | lr 3.00e-04 | grad 2.73 | tok/s 15905
step   1030 | loss 1.4190 | lr 3.00e-04 | grad 1.94 | tok/s 16501
step   1040 | loss 1.5351 | lr 3.00e-04 | grad 4.41 | tok/s 16884
step   1050 | loss 1.6139 | lr 3.00e-04 | grad 2.45 | tok/s 15884
step   1060 | loss 1.7147 | lr 3.00e-04 | grad 2.62 | tok/s 16977
step   1070 | loss 1.6605 | lr 3.00e-04 | grad 3.62 | tok/s 16729
step   1080 | loss 1.4178 | lr 3.00e-04 | grad 2.59 | tok/s 15486
step   1090 | loss 1.0150 | lr 3.00e-04 | grad 1.77 | tok/s 17110
step   1100 | loss 1.5817 | lr 3.00e-04 | grad 3.12 | tok/s 16435
step   1110 | loss 1.4322 | lr 3.00e-04 | grad 2.23 | tok/s 17238
step   1120 | loss 1.3431 | lr 3.00e-04 | grad 2.42 | tok/s 17238
step   1130 | loss 1.2869 | lr 3.00e-04 | grad 2.25 | tok/s 17235
step   1140 | loss 1.2974 | lr 3.00e-04 | grad 2.02 | tok/s 17325
step   1150 | loss 1.2937 | lr 3.00e-04 | grad 2.20 | tok/s 17262
step   1160 | loss 1.2157 | lr 3.00e-04 | grad 2.16 | tok/s 17207
step   1170 | loss 1.2598 | lr 3.00e-04 | grad 2.30 | tok/s 17220
step   1180 | loss 1.3183 | lr 3.00e-04 | grad 1.91 | tok/s 17306
step   1190 | loss 1.2244 | lr 3.00e-04 | grad 2.33 | tok/s 17305
step   1200 | loss 1.2253 | lr 3.00e-04 | grad 2.02 | tok/s 17211
step   1210 | loss 1.2630 | lr 3.00e-04 | grad 1.80 | tok/s 17281
step   1220 | loss 1.2933 | lr 3.00e-04 | grad 2.00 | tok/s 17215
step   1230 | loss 1.2646 | lr 3.00e-04 | grad 1.88 | tok/s 17300
step   1240 | loss 1.2660 | lr 3.00e-04 | grad 2.98 | tok/s 17126
step   1250 | loss 1.8004 | lr 3.00e-04 | grad 2.61 | tok/s 16376
step   1260 | loss 1.3197 | lr 3.00e-04 | grad 3.59 | tok/s 16163
step   1270 | loss 1.7799 | lr 3.00e-04 | grad 3.06 | tok/s 16128
step   1280 | loss 1.5961 | lr 3.00e-04 | grad 2.91 | tok/s 16864
step   1290 | loss 1.5000 | lr 3.00e-04 | grad 2.86 | tok/s 16576
step   1300 | loss 1.5297 | lr 3.00e-04 | grad 2.33 | tok/s 16427
step   1310 | loss 1.4672 | lr 3.00e-04 | grad 2.05 | tok/s 17215
step   1320 | loss 1.6003 | lr 3.00e-04 | grad 2.55 | tok/s 17031
step   1330 | loss 1.5441 | lr 3.00e-04 | grad 2.19 | tok/s 17061
step   1340 | loss 1.5988 | lr 3.00e-04 | grad 3.58 | tok/s 16076
step   1350 | loss 1.7380 | lr 3.00e-04 | grad 2.58 | tok/s 15913
step   1360 | loss 1.4863 | lr 3.00e-04 | grad 1.92 | tok/s 16692
step   1370 | loss 1.5240 | lr 3.00e-04 | grad 7.56 | tok/s 16492
step   1380 | loss 1.6078 | lr 3.00e-04 | grad 3.34 | tok/s 15831
step   1390 | loss 1.4906 | lr 3.00e-04 | grad 5.03 | tok/s 16731
step   1400 | loss 1.3851 | lr 3.00e-04 | grad 1.80 | tok/s 16319
step   1410 | loss 1.5369 | lr 3.00e-04 | grad 7.91 | tok/s 16280
step   1420 | loss 1.6433 | lr 3.00e-04 | grad 2.25 | tok/s 16234
step   1430 | loss 1.3454 | lr 3.00e-04 | grad 2.39 | tok/s 16408
step   1440 | loss 1.1413 | lr 3.00e-04 | grad 2.22 | tok/s 16251
step   1450 | loss 1.1742 | lr 3.00e-04 | grad 2.62 | tok/s 17051
step   1460 | loss 1.7111 | lr 3.00e-04 | grad 2.47 | tok/s 16163
step   1470 | loss 1.5291 | lr 3.00e-04 | grad 2.27 | tok/s 17168
step   1480 | loss 1.8497 | lr 3.00e-04 | grad 3.88 | tok/s 16950
step   1490 | loss 1.5766 | lr 3.00e-04 | grad 2.89 | tok/s 17184
step   1500 | loss 1.3126 | lr 3.00e-04 | grad 2.20 | tok/s 17277
step   1510 | loss 1.5568 | lr 3.00e-04 | grad 2.94 | tok/s 17010
step   1520 | loss 1.4444 | lr 3.00e-04 | grad 3.17 | tok/s 16740
step   1530 | loss 1.4560 | lr 3.00e-04 | grad 3.39 | tok/s 17108
step   1540 | loss 1.6460 | lr 3.00e-04 | grad 2.81 | tok/s 16043
step   1550 | loss 1.2765 | lr 3.00e-04 | grad 2.94 | tok/s 17147
step   1560 | loss 1.6012 | lr 3.00e-04 | grad 2.12 | tok/s 16206
step   1570 | loss 1.2783 | lr 3.00e-04 | grad 2.41 | tok/s 17099
step   1580 | loss 1.7223 | lr 3.00e-04 | grad 5.06 | tok/s 17008
step   1590 | loss 1.6074 | lr 3.00e-04 | grad 2.36 | tok/s 16214
step   1600 | loss 0.9209 | lr 3.00e-04 | grad 1.30 | tok/s 17305
step   1610 | loss 1.1163 | lr 3.00e-04 | grad 2.03 | tok/s 16318
step   1620 | loss 1.4095 | lr 3.00e-04 | grad 3.47 | tok/s 16042
step   1630 | loss 1.3700 | lr 3.00e-04 | grad 1.93 | tok/s 16809
step   1640 | loss 1.3705 | lr 3.00e-04 | grad 2.33 | tok/s 16251
step   1650 | loss 1.5792 | lr 3.00e-04 | grad 2.89 | tok/s 15386
step   1660 | loss 1.3276 | lr 3.00e-04 | grad 1.78 | tok/s 17253
step   1670 | loss 1.4217 | lr 3.00e-04 | grad 3.97 | tok/s 16550
step   1680 | loss 1.6948 | lr 3.00e-04 | grad 2.02 | tok/s 15875
step   1690 | loss 1.5077 | lr 3.00e-04 | grad 3.70 | tok/s 16596
step   1700 | loss 1.5198 | lr 3.00e-04 | grad 2.16 | tok/s 15441
step   1710 | loss 1.4443 | lr 3.00e-04 | grad 2.27 | tok/s 16448
step   1720 | loss 1.5330 | lr 3.00e-04 | grad 2.97 | tok/s 17181
step   1730 | loss 1.2024 | lr 3.00e-04 | grad 2.72 | tok/s 17265
step   1740 | loss 1.4145 | lr 3.00e-04 | grad 2.72 | tok/s 16656
step   1750 | loss 1.5539 | lr 3.00e-04 | grad 2.88 | tok/s 16702
step   1760 | loss 1.5805 | lr 3.00e-04 | grad 2.27 | tok/s 16625
step   1770 | loss 1.4583 | lr 3.00e-04 | grad 2.47 | tok/s 16280
step   1780 | loss 1.5003 | lr 3.00e-04 | grad 2.05 | tok/s 16834
step   1790 | loss 1.4377 | lr 3.00e-04 | grad 3.17 | tok/s 16632
step   1800 | loss 1.5910 | lr 3.00e-04 | grad 2.12 | tok/s 16356
step   1810 | loss 1.4803 | lr 3.00e-04 | grad 3.75 | tok/s 16042
step   1820 | loss 1.4896 | lr 3.00e-04 | grad 5.25 | tok/s 16591
step   1830 | loss 1.4729 | lr 3.00e-04 | grad 4.19 | tok/s 16890
step   1840 | loss 1.4971 | lr 3.00e-04 | grad 1.95 | tok/s 16135
step   1850 | loss 1.2849 | lr 3.00e-04 | grad 2.08 | tok/s 17223
step   1860 | loss 1.3733 | lr 3.00e-04 | grad 2.91 | tok/s 16317
step   1870 | loss 1.3777 | lr 3.00e-04 | grad 1.88 | tok/s 16702
step   1880 | loss 1.2827 | lr 3.00e-04 | grad 2.89 | tok/s 15453
step   1890 | loss 1.5462 | lr 3.00e-04 | grad 2.17 | tok/s 15539
step   1900 | loss 1.4008 | lr 3.00e-04 | grad 2.50 | tok/s 16649
step   1910 | loss 1.4976 | lr 3.00e-04 | grad 2.30 | tok/s 15779
step   1920 | loss 1.3887 | lr 3.00e-04 | grad 2.09 | tok/s 17254
step   1930 | loss 1.4721 | lr 3.00e-04 | grad 3.41 | tok/s 15912
step   1940 | loss 1.4648 | lr 3.00e-04 | grad 2.48 | tok/s 17173
step   1950 | loss 1.8843 | lr 3.00e-04 | grad 3.59 | tok/s 17096
step   1960 | loss 1.4582 | lr 3.00e-04 | grad 4.44 | tok/s 17293
step   1970 | loss 1.5007 | lr 3.00e-04 | grad 2.47 | tok/s 16864
step   1980 | loss 1.5738 | lr 3.00e-04 | grad 2.42 | tok/s 16130
step   1990 | loss 1.6388 | lr 3.00e-04 | grad 2.72 | tok/s 16369
step   2000 | loss 1.5171 | lr 3.00e-04 | grad 2.73 | tok/s 16671
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5171.pt
step   2010 | loss 1.1136 | lr 3.00e-04 | grad 2.62 | tok/s 6978
step   2020 | loss 1.3208 | lr 3.00e-04 | grad 3.12 | tok/s 15874
step   2030 | loss 1.0081 | lr 3.00e-04 | grad 1.79 | tok/s 17465
step   2040 | loss 1.3138 | lr 3.00e-04 | grad 2.36 | tok/s 17399
step   2050 | loss 1.2401 | lr 3.00e-04 | grad 2.64 | tok/s 16903
step   2060 | loss 1.5994 | lr 3.00e-04 | grad 3.22 | tok/s 16193
step   2070 | loss 1.7517 | lr 3.00e-04 | grad 7.59 | tok/s 16483
step   2080 | loss 2.2309 | lr 3.00e-04 | grad 4.31 | tok/s 17332
step   2090 | loss 1.6766 | lr 3.00e-04 | grad 2.34 | tok/s 16996
step   2100 | loss 1.4098 | lr 3.00e-04 | grad 2.17 | tok/s 17050
step   2110 | loss 1.5116 | lr 3.00e-04 | grad 3.62 | tok/s 16192
step   2120 | loss 0.8975 | lr 3.00e-04 | grad 4.22 | tok/s 17401
step   2130 | loss 1.1888 | lr 3.00e-04 | grad 4.84 | tok/s 16771
step   2140 | loss 1.5174 | lr 3.00e-04 | grad 2.25 | tok/s 16791
step   2150 | loss 1.3055 | lr 3.00e-04 | grad 2.27 | tok/s 17340
step   2160 | loss 1.1816 | lr 3.00e-04 | grad 1.71 | tok/s 17336
step   2170 | loss 1.2504 | lr 3.00e-04 | grad 1.95 | tok/s 17323
step   2180 | loss 1.1955 | lr 3.00e-04 | grad 1.80 | tok/s 17315
step   2190 | loss 1.2014 | lr 3.00e-04 | grad 1.84 | tok/s 17334
step   2200 | loss 1.1995 | lr 3.00e-04 | grad 1.91 | tok/s 17334
step   2210 | loss 1.1460 | lr 3.00e-04 | grad 1.91 | tok/s 17329
step   2220 | loss 1.1436 | lr 3.00e-04 | grad 1.73 | tok/s 17311
step   2230 | loss 1.3871 | lr 3.00e-04 | grad 1.93 | tok/s 16995
step   2240 | loss 1.3405 | lr 3.00e-04 | grad 2.52 | tok/s 17246
step   2250 | loss 1.5777 | lr 3.00e-04 | grad 4.69 | tok/s 16797
step   2260 | loss 1.6151 | lr 3.00e-04 | grad 2.09 | tok/s 16733
step   2270 | loss 1.9818 | lr 3.00e-04 | grad 4.47 | tok/s 17335
step   2280 | loss 1.4379 | lr 3.00e-04 | grad 2.44 | tok/s 17116
step   2290 | loss 1.4025 | lr 3.00e-04 | grad 8.19 | tok/s 16711
step   2300 | loss 1.4997 | lr 3.00e-04 | grad 3.72 | tok/s 17068
step   2310 | loss 1.4841 | lr 3.00e-04 | grad 2.28 | tok/s 16194
step   2320 | loss 1.7659 | lr 3.00e-04 | grad 4.84 | tok/s 16261
step   2330 | loss 1.6885 | lr 3.00e-04 | grad 2.17 | tok/s 16563
step   2340 | loss 1.4303 | lr 3.00e-04 | grad 2.56 | tok/s 16030
step   2350 | loss 1.3985 | lr 3.00e-04 | grad 2.73 | tok/s 16858
step   2360 | loss 1.3083 | lr 3.00e-04 | grad 2.19 | tok/s 17304
step   2370 | loss 1.5029 | lr 3.00e-04 | grad 3.20 | tok/s 16958
step   2380 | loss 1.4957 | lr 3.00e-04 | grad 3.84 | tok/s 17345
step   2390 | loss 1.1280 | lr 3.00e-04 | grad 2.06 | tok/s 17295
step   2400 | loss 1.0804 | lr 3.00e-04 | grad 1.77 | tok/s 17332
step   2410 | loss 1.1731 | lr 3.00e-04 | grad 2.62 | tok/s 16588
step   2420 | loss 1.4827 | lr 3.00e-04 | grad 2.41 | tok/s 16392
step   2430 | loss 1.3965 | lr 3.00e-04 | grad 2.17 | tok/s 16516

Training complete! Final step: 2438
