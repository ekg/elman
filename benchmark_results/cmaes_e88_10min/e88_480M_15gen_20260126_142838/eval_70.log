Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_70/levelE88_100m_20260126_155109
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,857,680 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0346 | lr 3.00e-04 | grad 6.38 | tok/s 5288
step     20 | loss 3.3364 | lr 3.00e-04 | grad 25.12 | tok/s 7107
step     30 | loss 4.4205 | lr 3.00e-04 | grad 15.50 | tok/s 7347
step     40 | loss 3.3641 | lr 3.00e-04 | grad 6.69 | tok/s 7337
step     50 | loss 2.9747 | lr 3.00e-04 | grad 3.64 | tok/s 7338
step     60 | loss 2.8705 | lr 3.00e-04 | grad 2.09 | tok/s 7150
step     70 | loss 2.5665 | lr 3.00e-04 | grad 3.00 | tok/s 7059
step     80 | loss 2.5353 | lr 3.00e-04 | grad 2.20 | tok/s 7251
step     90 | loss 2.4785 | lr 3.00e-04 | grad 7.06 | tok/s 6889
step    100 | loss 2.1973 | lr 3.00e-04 | grad 2.66 | tok/s 7006
step    110 | loss 2.3039 | lr 3.00e-04 | grad 1.45 | tok/s 7065
step    120 | loss 2.3569 | lr 3.00e-04 | grad 1.48 | tok/s 7059
step    130 | loss 2.3078 | lr 3.00e-04 | grad 1.61 | tok/s 7230
step    140 | loss 2.1218 | lr 3.00e-04 | grad 3.70 | tok/s 7067
step    150 | loss 2.1348 | lr 3.00e-04 | grad 1.38 | tok/s 6951
step    160 | loss 2.0208 | lr 3.00e-04 | grad 3.56 | tok/s 6919
step    170 | loss 2.1219 | lr 3.00e-04 | grad 1.53 | tok/s 7123
step    180 | loss 1.9910 | lr 3.00e-04 | grad 1.05 | tok/s 6889
step    190 | loss 1.7479 | lr 3.00e-04 | grad 1.12 | tok/s 7346
step    200 | loss 1.5740 | lr 3.00e-04 | grad 2.59 | tok/s 7228
step    210 | loss 1.9803 | lr 3.00e-04 | grad 1.47 | tok/s 7088
step    220 | loss 1.9326 | lr 3.00e-04 | grad 1.62 | tok/s 7111
step    230 | loss 1.7877 | lr 3.00e-04 | grad 1.34 | tok/s 7035
step    240 | loss 1.8087 | lr 3.00e-04 | grad 1.59 | tok/s 7133
step    250 | loss 1.8263 | lr 3.00e-04 | grad 1.18 | tok/s 7032
step    260 | loss 1.9730 | lr 3.00e-04 | grad 1.03 | tok/s 7001
step    270 | loss 1.9043 | lr 3.00e-04 | grad 1.20 | tok/s 7041
step    280 | loss 1.5858 | lr 3.00e-04 | grad 0.93 | tok/s 7070
step    290 | loss 1.5162 | lr 3.00e-04 | grad 1.09 | tok/s 7351
step    300 | loss 1.4591 | lr 3.00e-04 | grad 1.10 | tok/s 7350
step    310 | loss 1.4571 | lr 3.00e-04 | grad 0.99 | tok/s 7272
step    320 | loss 1.7268 | lr 3.00e-04 | grad 1.30 | tok/s 6933
step    330 | loss 1.7247 | lr 3.00e-04 | grad 1.47 | tok/s 7107
step    340 | loss 1.8264 | lr 3.00e-04 | grad 1.24 | tok/s 6998
step    350 | loss 1.6636 | lr 3.00e-04 | grad 0.90 | tok/s 6941
step    360 | loss 1.7654 | lr 3.00e-04 | grad 1.70 | tok/s 7054
step    370 | loss 1.5859 | lr 3.00e-04 | grad 0.93 | tok/s 7114
step    380 | loss 1.8824 | lr 3.00e-04 | grad 1.48 | tok/s 7301
step    390 | loss 1.5868 | lr 3.00e-04 | grad 1.99 | tok/s 7070
step    400 | loss 1.8821 | lr 3.00e-04 | grad 1.52 | tok/s 7140
step    410 | loss 1.4634 | lr 3.00e-04 | grad 1.55 | tok/s 7033
step    420 | loss 1.6785 | lr 3.00e-04 | grad 2.95 | tok/s 6923
step    430 | loss 1.6730 | lr 3.00e-04 | grad 1.32 | tok/s 7078
step    440 | loss 1.7664 | lr 3.00e-04 | grad 1.20 | tok/s 7179
step    450 | loss 1.5826 | lr 3.00e-04 | grad 1.02 | tok/s 6972
step    460 | loss 1.6489 | lr 3.00e-04 | grad 1.23 | tok/s 7021
step    470 | loss 1.5452 | lr 3.00e-04 | grad 0.97 | tok/s 7092
step    480 | loss 1.5568 | lr 3.00e-04 | grad 1.00 | tok/s 6858
step    490 | loss 1.8309 | lr 3.00e-04 | grad 2.52 | tok/s 7233
step    500 | loss 1.7677 | lr 3.00e-04 | grad 1.23 | tok/s 7000
step    510 | loss 1.3626 | lr 3.00e-04 | grad 1.03 | tok/s 7282
step    520 | loss 1.6593 | lr 3.00e-04 | grad 1.03 | tok/s 6990
step    530 | loss 1.9458 | lr 3.00e-04 | grad 6.47 | tok/s 7171

Training complete! Final step: 533
