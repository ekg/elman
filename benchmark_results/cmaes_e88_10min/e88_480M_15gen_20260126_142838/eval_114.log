Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_114/levelE88_100m_20260126_165303
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 494,316,128 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.1032 | lr 3.00e-04 | grad 4.50 | tok/s 4225
step     20 | loss 2.5388 | lr 3.00e-04 | grad 2.20 | tok/s 6855
step     30 | loss 2.4655 | lr 3.00e-04 | grad 1.20 | tok/s 6920
step     40 | loss 2.2758 | lr 3.00e-04 | grad 1.49 | tok/s 6619
step     50 | loss 2.7996 | lr 3.00e-04 | grad 4.88 | tok/s 6721
step     60 | loss 2.0719 | lr 3.00e-04 | grad 3.38 | tok/s 6961
step     70 | loss 1.9466 | lr 3.00e-04 | grad 2.34 | tok/s 7034
step     80 | loss 4.6890 | lr 3.00e-04 | grad 22.75 | tok/s 7070
step     90 | loss 4.4396 | lr 3.00e-04 | grad 4.25 | tok/s 7199
step    100 | loss 3.6981 | lr 3.00e-04 | grad 3.86 | tok/s 7197
step    110 | loss 3.1726 | lr 3.00e-04 | grad 4.81 | tok/s 7191
step    120 | loss 2.8396 | lr 3.00e-04 | grad 5.72 | tok/s 7192
step    130 | loss 2.6046 | lr 3.00e-04 | grad 5.72 | tok/s 7192
step    140 | loss 2.4204 | lr 3.00e-04 | grad 4.47 | tok/s 7187
step    150 | loss 2.3469 | lr 3.00e-04 | grad 3.39 | tok/s 7186
step    160 | loss 2.0801 | lr 3.00e-04 | grad 3.31 | tok/s 7181
step    170 | loss 2.1333 | lr 3.00e-04 | grad 4.25 | tok/s 7182
step    180 | loss 1.9721 | lr 3.00e-04 | grad 1.93 | tok/s 7198
step    190 | loss 2.0940 | lr 3.00e-04 | grad 2.12 | tok/s 7188
step    200 | loss 1.8639 | lr 3.00e-04 | grad 2.20 | tok/s 7191
step    210 | loss 1.9054 | lr 3.00e-04 | grad 2.47 | tok/s 7196
step    220 | loss 2.0569 | lr 3.00e-04 | grad 1.66 | tok/s 7112
step    230 | loss 2.0757 | lr 3.00e-04 | grad 1.79 | tok/s 6851
step    240 | loss 2.2144 | lr 3.00e-04 | grad 2.27 | tok/s 6665
step    250 | loss 2.0500 | lr 3.00e-04 | grad 1.28 | tok/s 6851
step    260 | loss 1.5852 | lr 3.00e-04 | grad 1.55 | tok/s 7079
step    270 | loss 2.0442 | lr 3.00e-04 | grad 1.45 | tok/s 6985
step    280 | loss 2.2228 | lr 3.00e-04 | grad 2.95 | tok/s 6848
step    290 | loss 1.4344 | lr 3.00e-04 | grad 1.98 | tok/s 7204
step    300 | loss 0.5751 | lr 3.00e-04 | grad 1.84 | tok/s 7206
step    310 | loss 2.3963 | lr 3.00e-04 | grad 3.11 | tok/s 7085
step    320 | loss 1.9668 | lr 3.00e-04 | grad 2.92 | tok/s 6938
step    330 | loss 1.8846 | lr 3.00e-04 | grad 1.52 | tok/s 6704
step    340 | loss 2.1881 | lr 3.00e-04 | grad 1.34 | tok/s 6802
step    350 | loss 1.8693 | lr 3.00e-04 | grad 2.61 | tok/s 6977
step    360 | loss 1.2618 | lr 3.00e-04 | grad 4.12 | tok/s 7136
step    370 | loss 1.7829 | lr 3.00e-04 | grad 1.44 | tok/s 6455
step    380 | loss 1.7281 | lr 3.00e-04 | grad 1.43 | tok/s 6887
step    390 | loss 1.5122 | lr 3.00e-04 | grad 1.08 | tok/s 7203
step    400 | loss 1.4769 | lr 3.00e-04 | grad 1.39 | tok/s 7134
step    410 | loss 1.2854 | lr 3.00e-04 | grad 1.20 | tok/s 6869
step    420 | loss 1.7721 | lr 3.00e-04 | grad 2.56 | tok/s 6651
step    430 | loss 2.0760 | lr 3.00e-04 | grad 1.64 | tok/s 7082
step    440 | loss 2.0880 | lr 3.00e-04 | grad 2.52 | tok/s 6693
step    450 | loss 1.8286 | lr 3.00e-04 | grad 1.54 | tok/s 6929
step    460 | loss 1.6802 | lr 3.00e-04 | grad 1.67 | tok/s 6779
step    470 | loss 1.7718 | lr 3.00e-04 | grad 1.33 | tok/s 6989
step    480 | loss 2.1315 | lr 3.00e-04 | grad 4.03 | tok/s 6995
step    490 | loss 1.7283 | lr 3.00e-04 | grad 1.35 | tok/s 6600
step    500 | loss 1.6410 | lr 3.00e-04 | grad 2.02 | tok/s 7053
step    510 | loss 1.6690 | lr 3.00e-04 | grad 1.28 | tok/s 7141
step    520 | loss 1.6443 | lr 3.00e-04 | grad 1.20 | tok/s 7122
step    530 | loss 1.8409 | lr 3.00e-04 | grad 1.48 | tok/s 6853
step    540 | loss 1.6964 | lr 3.00e-04 | grad 1.24 | tok/s 6864
step    550 | loss 1.5389 | lr 3.00e-04 | grad 1.65 | tok/s 6597
step    560 | loss 1.6782 | lr 3.00e-04 | grad 1.53 | tok/s 6539
step    570 | loss 1.6083 | lr 3.00e-04 | grad 2.20 | tok/s 6718
step    580 | loss 1.5125 | lr 3.00e-04 | grad 1.24 | tok/s 6699
step    590 | loss 1.7947 | lr 3.00e-04 | grad 1.88 | tok/s 6866
step    600 | loss 1.7711 | lr 3.00e-04 | grad 1.30 | tok/s 6634
step    610 | loss 1.5880 | lr 3.00e-04 | grad 1.36 | tok/s 6975
step    620 | loss 1.5138 | lr 3.00e-04 | grad 1.36 | tok/s 6611
step    630 | loss 1.6230 | lr 3.00e-04 | grad 2.67 | tok/s 6662
step    640 | loss 1.7564 | lr 3.00e-04 | grad 1.47 | tok/s 6844
step    650 | loss 1.6176 | lr 3.00e-04 | grad 1.47 | tok/s 6887
step    660 | loss 1.6558 | lr 3.00e-04 | grad 1.21 | tok/s 6905
step    670 | loss 1.8059 | lr 3.00e-04 | grad 2.72 | tok/s 6967
step    680 | loss 1.6759 | lr 3.00e-04 | grad 1.42 | tok/s 6818
step    690 | loss 1.7658 | lr 3.00e-04 | grad 1.95 | tok/s 7054
step    700 | loss 1.3960 | lr 3.00e-04 | grad 1.88 | tok/s 7173
step    710 | loss 1.5429 | lr 3.00e-04 | grad 1.48 | tok/s 6708
step    720 | loss 1.4247 | lr 3.00e-04 | grad 1.94 | tok/s 6614
step    730 | loss 1.2934 | lr 3.00e-04 | grad 1.71 | tok/s 7183
step    740 | loss 1.4621 | lr 3.00e-04 | grad 1.43 | tok/s 7082
step    750 | loss 1.1917 | lr 3.00e-04 | grad 1.52 | tok/s 7197
step    760 | loss 1.0960 | lr 3.00e-04 | grad 1.46 | tok/s 7188
step    770 | loss 1.0372 | lr 3.00e-04 | grad 1.23 | tok/s 7194
step    780 | loss 0.9776 | lr 3.00e-04 | grad 1.32 | tok/s 7190
step    790 | loss 1.0970 | lr 3.00e-04 | grad 2.22 | tok/s 6966
step    800 | loss 1.7443 | lr 3.00e-04 | grad 3.64 | tok/s 6947
step    810 | loss 1.6466 | lr 3.00e-04 | grad 1.29 | tok/s 6910
step    820 | loss 1.6496 | lr 3.00e-04 | grad 2.34 | tok/s 6632
step    830 | loss 1.4625 | lr 3.00e-04 | grad 1.60 | tok/s 7116
step    840 | loss 1.3640 | lr 3.00e-04 | grad 1.42 | tok/s 7192
step    850 | loss 1.5210 | lr 3.00e-04 | grad 1.30 | tok/s 7164
step    860 | loss 1.4388 | lr 3.00e-04 | grad 2.39 | tok/s 7078
step    870 | loss 1.4634 | lr 3.00e-04 | grad 1.66 | tok/s 6816
step    880 | loss 1.5954 | lr 3.00e-04 | grad 1.49 | tok/s 6843
step    890 | loss 1.6270 | lr 3.00e-04 | grad 1.88 | tok/s 6948
step    900 | loss 1.4969 | lr 3.00e-04 | grad 1.62 | tok/s 6945
step    910 | loss 1.3800 | lr 3.00e-04 | grad 2.42 | tok/s 6789
step    920 | loss 1.4813 | lr 3.00e-04 | grad 2.39 | tok/s 7069
step    930 | loss 1.5453 | lr 3.00e-04 | grad 2.39 | tok/s 6745
step    940 | loss 1.3540 | lr 3.00e-04 | grad 1.13 | tok/s 7112
step    950 | loss 1.4378 | lr 3.00e-04 | grad 1.73 | tok/s 7140
step    960 | loss 1.2886 | lr 3.00e-04 | grad 1.57 | tok/s 7150
step    970 | loss 1.6683 | lr 3.00e-04 | grad 2.31 | tok/s 6720
step    980 | loss 1.5756 | lr 3.00e-04 | grad 1.41 | tok/s 6924
step    990 | loss 1.4169 | lr 3.00e-04 | grad 1.27 | tok/s 7039
step   1000 | loss 1.7840 | lr 3.00e-04 | grad 6.88 | tok/s 6589
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7840.pt
step   1010 | loss 1.6217 | lr 3.00e-04 | grad 2.09 | tok/s 4268
step   1020 | loss 1.5807 | lr 3.00e-04 | grad 1.15 | tok/s 6588
step   1030 | loss 1.4215 | lr 3.00e-04 | grad 1.29 | tok/s 6860

Training complete! Final step: 1034
