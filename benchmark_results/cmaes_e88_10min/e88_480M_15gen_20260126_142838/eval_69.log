Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_69/levelE88_100m_20260126_155110
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 477,920,414 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.4812 | lr 3.00e-04 | grad 9.12 | tok/s 6852
step     20 | loss 2.7080 | lr 3.00e-04 | grad 2.61 | tok/s 9694
step     30 | loss 3.0445 | lr 3.00e-04 | grad 6.00 | tok/s 10204
step     40 | loss 4.1683 | lr 3.00e-04 | grad 37.00 | tok/s 10365
step     50 | loss 4.7116 | lr 3.00e-04 | grad 18.62 | tok/s 10475
step     60 | loss 3.9136 | lr 3.00e-04 | grad 14.94 | tok/s 10438
step     70 | loss 3.0989 | lr 3.00e-04 | grad 9.38 | tok/s 10402
step     80 | loss 2.7297 | lr 3.00e-04 | grad 6.91 | tok/s 10396
step     90 | loss 2.4748 | lr 3.00e-04 | grad 4.69 | tok/s 10376
step    100 | loss 2.3113 | lr 3.00e-04 | grad 2.31 | tok/s 10367
step    110 | loss 2.3245 | lr 3.00e-04 | grad 2.44 | tok/s 10300
step    120 | loss 2.7039 | lr 3.00e-04 | grad 1.37 | tok/s 9803
step    130 | loss 2.1608 | lr 3.00e-04 | grad 4.22 | tok/s 9930
step    140 | loss 2.4208 | lr 3.00e-04 | grad 7.19 | tok/s 10061
step    150 | loss 1.5204 | lr 3.00e-04 | grad 3.88 | tok/s 10301
step    160 | loss 2.3648 | lr 3.00e-04 | grad 1.61 | tok/s 9974
step    170 | loss 2.3080 | lr 3.00e-04 | grad 1.27 | tok/s 9831
step    180 | loss 1.9993 | lr 3.00e-04 | grad 2.31 | tok/s 10061
step    190 | loss 1.9657 | lr 3.00e-04 | grad 1.49 | tok/s 9876
step    200 | loss 1.7288 | lr 3.00e-04 | grad 1.37 | tok/s 10332
step    210 | loss 1.9391 | lr 3.00e-04 | grad 3.41 | tok/s 9803
step    220 | loss 2.2359 | lr 3.00e-04 | grad 1.90 | tok/s 9910
step    230 | loss 1.9618 | lr 3.00e-04 | grad 2.17 | tok/s 9889
step    240 | loss 2.3011 | lr 3.00e-04 | grad 4.03 | tok/s 10028
step    250 | loss 1.8168 | lr 3.00e-04 | grad 1.24 | tok/s 9946
step    260 | loss 1.9339 | lr 3.00e-04 | grad 2.34 | tok/s 10250
step    270 | loss 1.8572 | lr 3.00e-04 | grad 1.48 | tok/s 10018
step    280 | loss 1.8087 | lr 3.00e-04 | grad 1.44 | tok/s 9349
step    290 | loss 1.7071 | lr 3.00e-04 | grad 1.74 | tok/s 9718
step    300 | loss 1.9944 | lr 3.00e-04 | grad 1.52 | tok/s 9787
step    310 | loss 1.6889 | lr 3.00e-04 | grad 1.44 | tok/s 9759
step    320 | loss 1.8976 | lr 3.00e-04 | grad 2.19 | tok/s 9876
step    330 | loss 1.7360 | lr 3.00e-04 | grad 1.34 | tok/s 9985
step    340 | loss 2.0401 | lr 3.00e-04 | grad 1.60 | tok/s 9929
step    350 | loss 1.7549 | lr 3.00e-04 | grad 1.59 | tok/s 10228
step    360 | loss 1.5947 | lr 3.00e-04 | grad 1.52 | tok/s 9772
step    370 | loss 1.5159 | lr 3.00e-04 | grad 1.33 | tok/s 10302
step    380 | loss 1.2549 | lr 3.00e-04 | grad 1.36 | tok/s 10400
step    390 | loss 1.1437 | lr 3.00e-04 | grad 1.23 | tok/s 10406
step    400 | loss 1.7607 | lr 3.00e-04 | grad 1.42 | tok/s 9853
step    410 | loss 1.7567 | lr 3.00e-04 | grad 1.81 | tok/s 9940
step    420 | loss 1.6635 | lr 3.00e-04 | grad 2.53 | tok/s 10368
step    430 | loss 1.6336 | lr 3.00e-04 | grad 1.46 | tok/s 10204
step    440 | loss 1.7052 | lr 3.00e-04 | grad 1.72 | tok/s 9870
step    450 | loss 1.6390 | lr 3.00e-04 | grad 1.20 | tok/s 9990
step    460 | loss 1.6118 | lr 3.00e-04 | grad 1.62 | tok/s 10141
step    470 | loss 1.5784 | lr 3.00e-04 | grad 2.69 | tok/s 10055
step    480 | loss 1.6060 | lr 3.00e-04 | grad 2.23 | tok/s 10292
step    490 | loss 1.7043 | lr 3.00e-04 | grad 1.83 | tok/s 9886
step    500 | loss 1.8034 | lr 3.00e-04 | grad 1.40 | tok/s 10068
step    510 | loss 1.6765 | lr 3.00e-04 | grad 1.15 | tok/s 9604
step    520 | loss 1.5411 | lr 3.00e-04 | grad 1.62 | tok/s 10063
step    530 | loss 1.7196 | lr 3.00e-04 | grad 1.66 | tok/s 9883
step    540 | loss 1.6094 | lr 3.00e-04 | grad 1.27 | tok/s 9671
step    550 | loss 1.3604 | lr 3.00e-04 | grad 2.16 | tok/s 10107
step    560 | loss 1.4492 | lr 3.00e-04 | grad 1.40 | tok/s 10304
step    570 | loss 1.3545 | lr 3.00e-04 | grad 1.41 | tok/s 10411
step    580 | loss 1.3158 | lr 3.00e-04 | grad 1.08 | tok/s 10415
step    590 | loss 1.3485 | lr 3.00e-04 | grad 1.05 | tok/s 10418
step    600 | loss 1.2875 | lr 3.00e-04 | grad 1.33 | tok/s 10421
step    610 | loss 1.3163 | lr 3.00e-04 | grad 1.21 | tok/s 10425
step    620 | loss 1.3037 | lr 3.00e-04 | grad 1.34 | tok/s 10374
step    630 | loss 1.6512 | lr 3.00e-04 | grad 3.36 | tok/s 9789
step    640 | loss 1.7369 | lr 3.00e-04 | grad 1.30 | tok/s 9928
step    650 | loss 1.5562 | lr 3.00e-04 | grad 1.36 | tok/s 9911
step    660 | loss 1.6008 | lr 3.00e-04 | grad 1.36 | tok/s 10299
step    670 | loss 1.6241 | lr 3.00e-04 | grad 4.31 | tok/s 9944
step    680 | loss 1.6379 | lr 3.00e-04 | grad 1.56 | tok/s 9774
step    690 | loss 1.5657 | lr 3.00e-04 | grad 1.59 | tok/s 9718
step    700 | loss 1.4859 | lr 3.00e-04 | grad 1.17 | tok/s 9932
step    710 | loss 1.6336 | lr 3.00e-04 | grad 2.36 | tok/s 9752
step    720 | loss 1.3120 | lr 3.00e-04 | grad 1.19 | tok/s 10159
step    730 | loss 1.4656 | lr 3.00e-04 | grad 1.19 | tok/s 9996
step    740 | loss 1.7768 | lr 3.00e-04 | grad 2.59 | tok/s 10268
step    750 | loss 1.5392 | lr 3.00e-04 | grad 1.16 | tok/s 10386

Training complete! Final step: 754
