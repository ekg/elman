Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_91/levelE88_100m_20260126_162203
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,857,680 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0182 | lr 3.00e-04 | grad 5.16 | tok/s 5230
step     20 | loss 3.3693 | lr 3.00e-04 | grad 26.62 | tok/s 6944
step     30 | loss 4.4936 | lr 3.00e-04 | grad 15.38 | tok/s 7230
step     40 | loss 3.3640 | lr 3.00e-04 | grad 6.09 | tok/s 7159
step     50 | loss 2.9773 | lr 3.00e-04 | grad 3.69 | tok/s 7137
step     60 | loss 2.8762 | lr 3.00e-04 | grad 2.19 | tok/s 6968
step     70 | loss 2.5772 | lr 3.00e-04 | grad 3.22 | tok/s 6825
step     80 | loss 2.4756 | lr 3.00e-04 | grad 2.31 | tok/s 7035
step     90 | loss 2.4641 | lr 3.00e-04 | grad 7.25 | tok/s 6661
step    100 | loss 2.2037 | lr 3.00e-04 | grad 2.08 | tok/s 6809
step    110 | loss 2.3080 | lr 3.00e-04 | grad 1.41 | tok/s 6847
step    120 | loss 2.3532 | lr 3.00e-04 | grad 1.47 | tok/s 6857
step    130 | loss 2.3145 | lr 3.00e-04 | grad 1.59 | tok/s 7014
step    140 | loss 2.1184 | lr 3.00e-04 | grad 4.06 | tok/s 6867
step    150 | loss 2.1363 | lr 3.00e-04 | grad 1.34 | tok/s 6756
step    160 | loss 2.0259 | lr 3.00e-04 | grad 2.89 | tok/s 6720
step    170 | loss 2.1220 | lr 3.00e-04 | grad 1.53 | tok/s 6925
step    180 | loss 1.9933 | lr 3.00e-04 | grad 1.10 | tok/s 6687
step    190 | loss 1.7477 | lr 3.00e-04 | grad 1.02 | tok/s 7110
step    200 | loss 1.5767 | lr 3.00e-04 | grad 2.59 | tok/s 6991
step    210 | loss 1.9863 | lr 3.00e-04 | grad 1.45 | tok/s 6867
step    220 | loss 1.9325 | lr 3.00e-04 | grad 1.60 | tok/s 6888
step    230 | loss 1.7896 | lr 3.00e-04 | grad 1.35 | tok/s 6804
step    240 | loss 1.8071 | lr 3.00e-04 | grad 1.52 | tok/s 6921
step    250 | loss 1.8265 | lr 3.00e-04 | grad 1.16 | tok/s 6801
step    260 | loss 1.9683 | lr 3.00e-04 | grad 1.01 | tok/s 6806
step    270 | loss 1.9030 | lr 3.00e-04 | grad 1.19 | tok/s 6824
step    280 | loss 1.5836 | lr 3.00e-04 | grad 0.90 | tok/s 6882
step    290 | loss 1.5123 | lr 3.00e-04 | grad 1.08 | tok/s 7134
step    300 | loss 1.4573 | lr 3.00e-04 | grad 1.13 | tok/s 7122
step    310 | loss 1.4551 | lr 3.00e-04 | grad 0.98 | tok/s 7124
step    320 | loss 1.7211 | lr 3.00e-04 | grad 1.27 | tok/s 6737
step    330 | loss 1.7220 | lr 3.00e-04 | grad 1.49 | tok/s 6900
step    340 | loss 1.8205 | lr 3.00e-04 | grad 1.27 | tok/s 6777
step    350 | loss 1.6646 | lr 3.00e-04 | grad 0.91 | tok/s 6715
step    360 | loss 1.7645 | lr 3.00e-04 | grad 1.75 | tok/s 6833
step    370 | loss 1.5845 | lr 3.00e-04 | grad 0.90 | tok/s 6890
step    380 | loss 1.8830 | lr 3.00e-04 | grad 1.54 | tok/s 7065
step    390 | loss 1.5886 | lr 3.00e-04 | grad 2.05 | tok/s 6849
step    400 | loss 1.8856 | lr 3.00e-04 | grad 1.55 | tok/s 6920
step    410 | loss 1.4688 | lr 3.00e-04 | grad 1.59 | tok/s 6845
step    420 | loss 1.6715 | lr 3.00e-04 | grad 2.94 | tok/s 6713
step    430 | loss 1.6740 | lr 3.00e-04 | grad 1.47 | tok/s 6868
step    440 | loss 1.7604 | lr 3.00e-04 | grad 1.20 | tok/s 6964
step    450 | loss 1.5817 | lr 3.00e-04 | grad 1.05 | tok/s 6781
step    460 | loss 1.6479 | lr 3.00e-04 | grad 1.18 | tok/s 6803
step    470 | loss 1.5456 | lr 3.00e-04 | grad 0.99 | tok/s 6901
step    480 | loss 1.5556 | lr 3.00e-04 | grad 1.02 | tok/s 6647
step    490 | loss 1.8051 | lr 3.00e-04 | grad 2.53 | tok/s 7034
step    500 | loss 1.7586 | lr 3.00e-04 | grad 1.20 | tok/s 6796
step    510 | loss 1.3639 | lr 3.00e-04 | grad 1.34 | tok/s 7067

Training complete! Final step: 517
