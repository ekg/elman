Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_7/levelE88_100m_20260126_142845
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,666,728 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.4886 | lr 3.00e-04 | grad 28.12 | tok/s 5921
step     20 | loss 2.7292 | lr 3.00e-04 | grad 12.19 | tok/s 14189
step     30 | loss 2.6223 | lr 3.00e-04 | grad 5.66 | tok/s 14354
step     40 | loss 2.4392 | lr 3.00e-04 | grad 4.66 | tok/s 13754
step     50 | loss 3.1941 | lr 3.00e-04 | grad 18.50 | tok/s 13965
step     60 | loss 2.1370 | lr 3.00e-04 | grad 5.28 | tok/s 14399
step     70 | loss 1.9911 | lr 3.00e-04 | grad 6.19 | tok/s 14552
step     80 | loss 6.0306 | lr 3.00e-04 | grad 190.00 | tok/s 14644
step     90 | loss 6.1584 | lr 3.00e-04 | grad 18.50 | tok/s 14860
step    100 | loss 5.1692 | lr 3.00e-04 | grad 19.75 | tok/s 14876
step    110 | loss 4.6877 | lr 3.00e-04 | grad 35.75 | tok/s 14827
step    120 | loss 4.1221 | lr 3.00e-04 | grad 36.50 | tok/s 14795
step    130 | loss 3.7972 | lr 3.00e-04 | grad 42.75 | tok/s 13719
step    140 | loss 3.0869 | lr 3.00e-04 | grad 27.88 | tok/s 14727
step    150 | loss 3.5541 | lr 3.00e-04 | grad 37.50 | tok/s 14711
step    160 | loss 2.6590 | lr 3.00e-04 | grad 27.50 | tok/s 14623
step    170 | loss 2.7745 | lr 3.00e-04 | grad 29.25 | tok/s 14604
step    180 | loss 2.4729 | lr 3.00e-04 | grad 7.47 | tok/s 14566
step    190 | loss 2.7875 | lr 3.00e-04 | grad 14.44 | tok/s 14537
step    200 | loss 2.3796 | lr 3.00e-04 | grad 19.62 | tok/s 14510
step    210 | loss 2.3205 | lr 3.00e-04 | grad 12.44 | tok/s 14469
step    220 | loss 2.3697 | lr 3.00e-04 | grad 3.27 | tok/s 14266
step    230 | loss 2.1546 | lr 3.00e-04 | grad 3.55 | tok/s 14099
step    240 | loss 2.3212 | lr 3.00e-04 | grad 5.16 | tok/s 13329
step    250 | loss 2.1447 | lr 3.00e-04 | grad 2.52 | tok/s 13669
step    260 | loss 1.6135 | lr 3.00e-04 | grad 2.78 | tok/s 14085
step    270 | loss 2.1518 | lr 3.00e-04 | grad 2.50 | tok/s 13406
step    280 | loss 2.3082 | lr 3.00e-04 | grad 4.88 | tok/s 13591
step    290 | loss 1.5249 | lr 3.00e-04 | grad 4.69 | tok/s 14284
step    300 | loss 0.7146 | lr 3.00e-04 | grad 2.72 | tok/s 14263
step    310 | loss 2.4318 | lr 3.00e-04 | grad 3.23 | tok/s 13998
step    320 | loss 1.9852 | lr 3.00e-04 | grad 5.75 | tok/s 13694
step    330 | loss 1.9912 | lr 3.00e-04 | grad 2.80 | tok/s 13186
step    340 | loss 2.3182 | lr 3.00e-04 | grad 2.77 | tok/s 13366
step    350 | loss 1.9473 | lr 3.00e-04 | grad 5.69 | tok/s 13706
step    360 | loss 1.2853 | lr 3.00e-04 | grad 6.47 | tok/s 14003
step    370 | loss 1.8524 | lr 3.00e-04 | grad 2.39 | tok/s 12674
step    380 | loss 1.8279 | lr 3.00e-04 | grad 2.38 | tok/s 13500
step    390 | loss 1.5876 | lr 3.00e-04 | grad 1.83 | tok/s 14065
step    400 | loss 1.5298 | lr 3.00e-04 | grad 2.34 | tok/s 13947
step    410 | loss 1.3200 | lr 3.00e-04 | grad 1.95 | tok/s 13047
step    420 | loss 1.8583 | lr 3.00e-04 | grad 4.16 | tok/s 13024
step    430 | loss 2.2168 | lr 3.00e-04 | grad 2.72 | tok/s 13837
step    440 | loss 2.1971 | lr 3.00e-04 | grad 4.00 | tok/s 13077
step    450 | loss 1.9355 | lr 3.00e-04 | grad 2.69 | tok/s 13497
step    460 | loss 1.7588 | lr 3.00e-04 | grad 2.67 | tok/s 13211
step    470 | loss 1.8673 | lr 3.00e-04 | grad 2.12 | tok/s 13613
step    480 | loss 2.3161 | lr 3.00e-04 | grad 6.28 | tok/s 13593
step    490 | loss 1.8247 | lr 3.00e-04 | grad 2.27 | tok/s 12833
step    500 | loss 1.7198 | lr 3.00e-04 | grad 3.09 | tok/s 13697
step    510 | loss 1.7452 | lr 3.00e-04 | grad 2.22 | tok/s 13841
step    520 | loss 1.6982 | lr 3.00e-04 | grad 1.86 | tok/s 13833
step    530 | loss 1.9584 | lr 3.00e-04 | grad 2.31 | tok/s 13304
step    540 | loss 1.7643 | lr 3.00e-04 | grad 2.02 | tok/s 13337
step    550 | loss 1.5955 | lr 3.00e-04 | grad 2.81 | tok/s 13020
step    560 | loss 1.7558 | lr 3.00e-04 | grad 2.36 | tok/s 11998
step    570 | loss 1.6976 | lr 3.00e-04 | grad 3.42 | tok/s 13023
step    580 | loss 1.5692 | lr 3.00e-04 | grad 1.97 | tok/s 12990
step    590 | loss 1.8983 | lr 3.00e-04 | grad 2.84 | tok/s 13342
step    600 | loss 1.8392 | lr 3.00e-04 | grad 2.12 | tok/s 12895
step    610 | loss 1.6463 | lr 3.00e-04 | grad 2.17 | tok/s 13511
step    620 | loss 1.5701 | lr 3.00e-04 | grad 2.17 | tok/s 12799
step    630 | loss 1.6872 | lr 3.00e-04 | grad 3.95 | tok/s 12931
step    640 | loss 1.8384 | lr 3.00e-04 | grad 2.28 | tok/s 13283
step    650 | loss 1.6894 | lr 3.00e-04 | grad 2.33 | tok/s 13314
step    660 | loss 1.7236 | lr 3.00e-04 | grad 1.95 | tok/s 13351
step    670 | loss 1.9702 | lr 3.00e-04 | grad 3.03 | tok/s 13449
step    680 | loss 1.7564 | lr 3.00e-04 | grad 2.25 | tok/s 13218
step    690 | loss 1.8678 | lr 3.00e-04 | grad 3.02 | tok/s 13660
step    700 | loss 1.4793 | lr 3.00e-04 | grad 2.83 | tok/s 13304
step    710 | loss 1.6069 | lr 3.00e-04 | grad 2.20 | tok/s 12980
step    720 | loss 1.4924 | lr 3.00e-04 | grad 3.44 | tok/s 12793
step    730 | loss 1.3203 | lr 3.00e-04 | grad 2.45 | tok/s 13891
step    740 | loss 1.5283 | lr 3.00e-04 | grad 2.22 | tok/s 13710
step    750 | loss 1.2298 | lr 3.00e-04 | grad 2.33 | tok/s 13941
step    760 | loss 1.1373 | lr 3.00e-04 | grad 2.05 | tok/s 13898
step    770 | loss 1.0852 | lr 3.00e-04 | grad 1.88 | tok/s 13880
step    780 | loss 1.0178 | lr 3.00e-04 | grad 1.91 | tok/s 13867
step    790 | loss 1.1494 | lr 3.00e-04 | grad 2.98 | tok/s 13485
step    800 | loss 1.8386 | lr 3.00e-04 | grad 5.19 | tok/s 13424
step    810 | loss 1.7300 | lr 3.00e-04 | grad 1.98 | tok/s 13372
step    820 | loss 1.7376 | lr 3.00e-04 | grad 3.56 | tok/s 12817
step    830 | loss 1.5124 | lr 3.00e-04 | grad 2.19 | tok/s 13733
step    840 | loss 1.3993 | lr 3.00e-04 | grad 2.11 | tok/s 13443
step    850 | loss 1.6303 | lr 3.00e-04 | grad 1.92 | tok/s 13821
step    860 | loss 1.4965 | lr 3.00e-04 | grad 3.17 | tok/s 13671
step    870 | loss 1.5252 | lr 3.00e-04 | grad 2.52 | tok/s 13184
step    880 | loss 1.7048 | lr 3.00e-04 | grad 2.64 | tok/s 13215
step    890 | loss 1.7067 | lr 3.00e-04 | grad 2.72 | tok/s 13392
step    900 | loss 1.5851 | lr 3.00e-04 | grad 2.45 | tok/s 13392
step    910 | loss 1.4482 | lr 3.00e-04 | grad 3.94 | tok/s 13121
step    920 | loss 1.5463 | lr 3.00e-04 | grad 3.23 | tok/s 13613
step    930 | loss 1.6192 | lr 3.00e-04 | grad 3.34 | tok/s 13028
step    940 | loss 1.4003 | lr 3.00e-04 | grad 1.75 | tok/s 13726
step    950 | loss 1.5259 | lr 3.00e-04 | grad 2.56 | tok/s 13804
step    960 | loss 1.3519 | lr 3.00e-04 | grad 2.31 | tok/s 13867
step    970 | loss 1.7649 | lr 3.00e-04 | grad 3.28 | tok/s 13036
step    980 | loss 1.6675 | lr 3.00e-04 | grad 2.27 | tok/s 12950
step    990 | loss 1.4700 | lr 3.00e-04 | grad 1.96 | tok/s 13542
step   1000 | loss 1.8629 | lr 3.00e-04 | grad 6.81 | tok/s 13012
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8629.pt
step   1010 | loss 1.6491 | lr 3.00e-04 | grad 3.14 | tok/s 6447
step   1020 | loss 1.6732 | lr 3.00e-04 | grad 1.78 | tok/s 12808
step   1030 | loss 1.4767 | lr 3.00e-04 | grad 1.89 | tok/s 13311
step   1040 | loss 1.4957 | lr 3.00e-04 | grad 2.08 | tok/s 13716
step   1050 | loss 1.6447 | lr 3.00e-04 | grad 2.92 | tok/s 12683
step   1060 | loss 1.7521 | lr 3.00e-04 | grad 3.12 | tok/s 13748
step   1070 | loss 1.6684 | lr 3.00e-04 | grad 2.56 | tok/s 13649
step   1080 | loss 1.4138 | lr 3.00e-04 | grad 1.81 | tok/s 12377
step   1090 | loss 1.1284 | lr 3.00e-04 | grad 1.12 | tok/s 13630
step   1100 | loss 1.4578 | lr 3.00e-04 | grad 3.11 | tok/s 13243
step   1110 | loss 1.4779 | lr 3.00e-04 | grad 1.89 | tok/s 13904
step   1120 | loss 1.3474 | lr 3.00e-04 | grad 1.90 | tok/s 13405
step   1130 | loss 1.3033 | lr 3.00e-04 | grad 1.77 | tok/s 13872
step   1140 | loss 1.2913 | lr 3.00e-04 | grad 1.95 | tok/s 13906
step   1150 | loss 1.3085 | lr 3.00e-04 | grad 1.67 | tok/s 13888
step   1160 | loss 1.2257 | lr 3.00e-04 | grad 1.69 | tok/s 13897
step   1170 | loss 1.2554 | lr 3.00e-04 | grad 2.02 | tok/s 13897
step   1180 | loss 1.3542 | lr 3.00e-04 | grad 1.57 | tok/s 13896
step   1190 | loss 1.2378 | lr 3.00e-04 | grad 2.02 | tok/s 13834
step   1200 | loss 1.2295 | lr 3.00e-04 | grad 1.85 | tok/s 13826
step   1210 | loss 1.2810 | lr 3.00e-04 | grad 1.95 | tok/s 13827
step   1220 | loss 1.3025 | lr 3.00e-04 | grad 1.80 | tok/s 13826
step   1230 | loss 1.2671 | lr 3.00e-04 | grad 1.64 | tok/s 13826
step   1240 | loss 1.2292 | lr 3.00e-04 | grad 1.49 | tok/s 13821
step   1250 | loss 1.8380 | lr 3.00e-04 | grad 3.36 | tok/s 13088
step   1260 | loss 1.3895 | lr 3.00e-04 | grad 2.12 | tok/s 12965
step   1270 | loss 1.6783 | lr 3.00e-04 | grad 4.56 | tok/s 12450
step   1280 | loss 1.6434 | lr 3.00e-04 | grad 1.70 | tok/s 13308
step   1290 | loss 1.4941 | lr 3.00e-04 | grad 1.99 | tok/s 13251
step   1300 | loss 1.5356 | lr 3.00e-04 | grad 2.33 | tok/s 13359
step   1310 | loss 1.4733 | lr 3.00e-04 | grad 2.23 | tok/s 13604
step   1320 | loss 1.6008 | lr 3.00e-04 | grad 2.11 | tok/s 13627
step   1330 | loss 1.6210 | lr 3.00e-04 | grad 2.25 | tok/s 13659
step   1340 | loss 1.5053 | lr 3.00e-04 | grad 9.25 | tok/s 13047
step   1350 | loss 1.7463 | lr 3.00e-04 | grad 2.28 | tok/s 12632
step   1360 | loss 1.5057 | lr 3.00e-04 | grad 2.20 | tok/s 13359
step   1370 | loss 1.4251 | lr 3.00e-04 | grad 1.53 | tok/s 13217
step   1380 | loss 1.6783 | lr 3.00e-04 | grad 2.25 | tok/s 12670
step   1390 | loss 1.5338 | lr 3.00e-04 | grad 1.69 | tok/s 13463
step   1400 | loss 1.4326 | lr 3.00e-04 | grad 1.69 | tok/s 12984
step   1410 | loss 1.4749 | lr 3.00e-04 | grad 3.03 | tok/s 12643
step   1420 | loss 1.6925 | lr 3.00e-04 | grad 3.58 | tok/s 13104
step   1430 | loss 1.3611 | lr 3.00e-04 | grad 1.88 | tok/s 13273
step   1440 | loss 1.1616 | lr 3.00e-04 | grad 1.73 | tok/s 13748
step   1450 | loss 1.1774 | lr 3.00e-04 | grad 4.06 | tok/s 13798
step   1460 | loss 1.6825 | lr 3.00e-04 | grad 2.02 | tok/s 13031
step   1470 | loss 1.5388 | lr 3.00e-04 | grad 1.77 | tok/s 13503
step   1480 | loss 1.8312 | lr 3.00e-04 | grad 3.67 | tok/s 13568
step   1490 | loss 1.5813 | lr 3.00e-04 | grad 1.64 | tok/s 13805
step   1500 | loss 1.3607 | lr 3.00e-04 | grad 1.73 | tok/s 13900
step   1510 | loss 1.5361 | lr 3.00e-04 | grad 1.85 | tok/s 13732
step   1520 | loss 1.4422 | lr 3.00e-04 | grad 3.16 | tok/s 13442
step   1530 | loss 1.4633 | lr 3.00e-04 | grad 1.62 | tok/s 13754
step   1540 | loss 1.6544 | lr 3.00e-04 | grad 2.23 | tok/s 12865
step   1550 | loss 1.3006 | lr 3.00e-04 | grad 2.12 | tok/s 13352
step   1560 | loss 1.6023 | lr 3.00e-04 | grad 2.48 | tok/s 13013
step   1570 | loss 1.2984 | lr 3.00e-04 | grad 2.08 | tok/s 13881
step   1580 | loss 1.7455 | lr 3.00e-04 | grad 3.84 | tok/s 13545
step   1590 | loss 1.6212 | lr 3.00e-04 | grad 2.25 | tok/s 13035
step   1600 | loss 1.0161 | lr 3.00e-04 | grad 1.41 | tok/s 13914
step   1610 | loss 1.0376 | lr 3.00e-04 | grad 2.36 | tok/s 13438
step   1620 | loss 1.4212 | lr 3.00e-04 | grad 2.86 | tok/s 12565
step   1630 | loss 1.3522 | lr 3.00e-04 | grad 2.02 | tok/s 13444
step   1640 | loss 1.3443 | lr 3.00e-04 | grad 2.25 | tok/s 13178
step   1650 | loss 1.5495 | lr 3.00e-04 | grad 2.36 | tok/s 12589
step   1660 | loss 1.4070 | lr 3.00e-04 | grad 1.68 | tok/s 13490
step   1670 | loss 1.3737 | lr 3.00e-04 | grad 5.56 | tok/s 13427
step   1680 | loss 1.7498 | lr 3.00e-04 | grad 1.84 | tok/s 12911
step   1690 | loss 1.5047 | lr 3.00e-04 | grad 3.72 | tok/s 12756
step   1700 | loss 1.5196 | lr 3.00e-04 | grad 2.23 | tok/s 13500
step   1710 | loss 1.4302 | lr 3.00e-04 | grad 2.14 | tok/s 13186
step   1720 | loss 1.5403 | lr 3.00e-04 | grad 2.70 | tok/s 13743
step   1730 | loss 1.2401 | lr 3.00e-04 | grad 2.67 | tok/s 13918
step   1740 | loss 1.3529 | lr 3.00e-04 | grad 2.55 | tok/s 13554
step   1750 | loss 1.5939 | lr 3.00e-04 | grad 2.38 | tok/s 13308
step   1760 | loss 1.5577 | lr 3.00e-04 | grad 2.27 | tok/s 13318
step   1770 | loss 1.4507 | lr 3.00e-04 | grad 2.19 | tok/s 13118
step   1780 | loss 1.5239 | lr 3.00e-04 | grad 1.73 | tok/s 13657
step   1790 | loss 1.4132 | lr 3.00e-04 | grad 1.59 | tok/s 13297
step   1800 | loss 1.6416 | lr 3.00e-04 | grad 2.64 | tok/s 13467
step   1810 | loss 1.4784 | lr 3.00e-04 | grad 1.91 | tok/s 12924
step   1820 | loss 1.4991 | lr 3.00e-04 | grad 4.81 | tok/s 13102
step   1830 | loss 1.4227 | lr 3.00e-04 | grad 2.20 | tok/s 13131
step   1840 | loss 1.5584 | lr 3.00e-04 | grad 2.52 | tok/s 13047
step   1850 | loss 1.3150 | lr 3.00e-04 | grad 1.67 | tok/s 13694
step   1860 | loss 1.3348 | lr 3.00e-04 | grad 2.11 | tok/s 13241
step   1870 | loss 1.4348 | lr 3.00e-04 | grad 2.72 | tok/s 13289
step   1880 | loss 1.2512 | lr 3.00e-04 | grad 2.08 | tok/s 13004
step   1890 | loss 1.5320 | lr 3.00e-04 | grad 1.77 | tok/s 12353
step   1900 | loss 1.4076 | lr 3.00e-04 | grad 2.31 | tok/s 13337
step   1910 | loss 1.4826 | lr 3.00e-04 | grad 2.62 | tok/s 12665
step   1920 | loss 1.4100 | lr 3.00e-04 | grad 1.91 | tok/s 13857
step   1930 | loss 1.4697 | lr 3.00e-04 | grad 2.42 | tok/s 13033
step   1940 | loss 1.4526 | lr 3.00e-04 | grad 1.92 | tok/s 13583
step   1950 | loss 1.9083 | lr 3.00e-04 | grad 3.50 | tok/s 13755
step   1960 | loss 1.4878 | lr 3.00e-04 | grad 3.95 | tok/s 13863
step   1970 | loss 1.5517 | lr 3.00e-04 | grad 2.41 | tok/s 13537
step   1980 | loss 1.5679 | lr 3.00e-04 | grad 1.99 | tok/s 12436
step   1990 | loss 1.6655 | lr 3.00e-04 | grad 16.50 | tok/s 13198
step   2000 | loss 1.5071 | lr 3.00e-04 | grad 1.94 | tok/s 13403
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5071.pt

Training complete! Final step: 2000
