Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_27/levelE88_100m_20260126_145940
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 490,865,152 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.7803 | lr 3.00e-04 | grad 8.00 | tok/s 4435
step     20 | loss 2.6837 | lr 3.00e-04 | grad 4.47 | tok/s 7390
step     30 | loss 2.5554 | lr 3.00e-04 | grad 1.66 | tok/s 7450
step     40 | loss 2.3559 | lr 3.00e-04 | grad 2.05 | tok/s 7124
step     50 | loss 3.1538 | lr 3.00e-04 | grad 10.31 | tok/s 7215
step     60 | loss 2.1659 | lr 3.00e-04 | grad 6.47 | tok/s 7433
step     70 | loss 2.0934 | lr 3.00e-04 | grad 3.22 | tok/s 7508
step     80 | loss 4.7944 | lr 3.00e-04 | grad 69.50 | tok/s 7373
step     90 | loss 4.9567 | lr 3.00e-04 | grad 8.69 | tok/s 7687
step    100 | loss 4.5689 | lr 3.00e-04 | grad 11.94 | tok/s 7673
step    110 | loss 4.2994 | lr 3.00e-04 | grad 19.00 | tok/s 7682
step    120 | loss 3.9532 | lr 3.00e-04 | grad 21.12 | tok/s 7667
step    130 | loss 3.6760 | lr 3.00e-04 | grad 23.38 | tok/s 7657
step    140 | loss 2.8929 | lr 3.00e-04 | grad 13.94 | tok/s 7646
step    150 | loss 3.2884 | lr 3.00e-04 | grad 15.25 | tok/s 7541
step    160 | loss 2.4777 | lr 3.00e-04 | grad 12.19 | tok/s 7660
step    170 | loss 2.6195 | lr 3.00e-04 | grad 9.88 | tok/s 7655
step    180 | loss 2.3325 | lr 3.00e-04 | grad 3.98 | tok/s 7653
step    190 | loss 2.4956 | lr 3.00e-04 | grad 2.52 | tok/s 7665
step    200 | loss 2.2408 | lr 3.00e-04 | grad 5.84 | tok/s 7658
step    210 | loss 2.1446 | lr 3.00e-04 | grad 4.94 | tok/s 7666
step    220 | loss 2.3052 | lr 3.00e-04 | grad 1.62 | tok/s 7561
step    230 | loss 2.2960 | lr 3.00e-04 | grad 1.98 | tok/s 7472
step    240 | loss 2.2835 | lr 3.00e-04 | grad 2.64 | tok/s 6931
step    250 | loss 2.1322 | lr 3.00e-04 | grad 1.29 | tok/s 7306
step    260 | loss 1.6847 | lr 3.00e-04 | grad 1.70 | tok/s 7547
step    270 | loss 2.1280 | lr 3.00e-04 | grad 1.39 | tok/s 7448
step    280 | loss 2.2780 | lr 3.00e-04 | grad 2.84 | tok/s 7313
step    290 | loss 1.6541 | lr 3.00e-04 | grad 2.55 | tok/s 7690
step    300 | loss 0.6547 | lr 3.00e-04 | grad 1.70 | tok/s 7682
step    310 | loss 2.4779 | lr 3.00e-04 | grad 2.38 | tok/s 7392
step    320 | loss 2.0748 | lr 3.00e-04 | grad 3.62 | tok/s 7397
step    330 | loss 1.9577 | lr 3.00e-04 | grad 1.59 | tok/s 7148
step    340 | loss 2.2870 | lr 3.00e-04 | grad 1.41 | tok/s 7265
step    350 | loss 1.9506 | lr 3.00e-04 | grad 3.20 | tok/s 7444
step    360 | loss 1.3787 | lr 3.00e-04 | grad 3.38 | tok/s 7612
step    370 | loss 1.8476 | lr 3.00e-04 | grad 1.52 | tok/s 6907
step    380 | loss 1.8082 | lr 3.00e-04 | grad 1.40 | tok/s 7356
step    390 | loss 1.5853 | lr 3.00e-04 | grad 1.13 | tok/s 7635
step    400 | loss 1.5486 | lr 3.00e-04 | grad 1.49 | tok/s 7613
step    410 | loss 1.3542 | lr 3.00e-04 | grad 1.36 | tok/s 7440
step    420 | loss 1.8374 | lr 3.00e-04 | grad 2.67 | tok/s 7111
step    430 | loss 2.1535 | lr 3.00e-04 | grad 1.67 | tok/s 7569
step    440 | loss 2.1563 | lr 3.00e-04 | grad 2.55 | tok/s 7154
step    450 | loss 1.9220 | lr 3.00e-04 | grad 1.63 | tok/s 7392
step    460 | loss 1.7370 | lr 3.00e-04 | grad 1.88 | tok/s 7244
step    470 | loss 1.8319 | lr 3.00e-04 | grad 1.39 | tok/s 7357
step    480 | loss 2.2457 | lr 3.00e-04 | grad 4.25 | tok/s 7468
step    490 | loss 1.7969 | lr 3.00e-04 | grad 1.46 | tok/s 7061
step    500 | loss 1.7124 | lr 3.00e-04 | grad 2.06 | tok/s 7544
step    510 | loss 1.7271 | lr 3.00e-04 | grad 1.38 | tok/s 7652
step    520 | loss 1.7028 | lr 3.00e-04 | grad 1.25 | tok/s 7635
step    530 | loss 1.9148 | lr 3.00e-04 | grad 1.55 | tok/s 7335
step    540 | loss 1.7454 | lr 3.00e-04 | grad 1.32 | tok/s 7340
step    550 | loss 1.5836 | lr 3.00e-04 | grad 1.91 | tok/s 7051
step    560 | loss 1.7431 | lr 3.00e-04 | grad 1.59 | tok/s 7000
step    570 | loss 1.6645 | lr 3.00e-04 | grad 2.30 | tok/s 7194
step    580 | loss 1.5616 | lr 3.00e-04 | grad 1.30 | tok/s 7165
step    590 | loss 1.8666 | lr 3.00e-04 | grad 1.95 | tok/s 7350
step    600 | loss 1.8160 | lr 3.00e-04 | grad 1.39 | tok/s 7104
step    610 | loss 1.6356 | lr 3.00e-04 | grad 1.43 | tok/s 7480
step    620 | loss 1.5534 | lr 3.00e-04 | grad 1.42 | tok/s 7084
step    630 | loss 1.6694 | lr 3.00e-04 | grad 2.84 | tok/s 7093
step    640 | loss 1.8088 | lr 3.00e-04 | grad 1.58 | tok/s 7330
step    650 | loss 1.6638 | lr 3.00e-04 | grad 1.54 | tok/s 7370
step    660 | loss 1.7026 | lr 3.00e-04 | grad 1.37 | tok/s 7400
step    670 | loss 1.8841 | lr 3.00e-04 | grad 2.75 | tok/s 7452
step    680 | loss 1.7234 | lr 3.00e-04 | grad 1.49 | tok/s 7291
step    690 | loss 1.8138 | lr 3.00e-04 | grad 2.09 | tok/s 7551
step    700 | loss 1.4662 | lr 3.00e-04 | grad 1.95 | tok/s 7701
step    710 | loss 1.5844 | lr 3.00e-04 | grad 1.55 | tok/s 7090
step    720 | loss 1.4705 | lr 3.00e-04 | grad 1.94 | tok/s 7074
step    730 | loss 1.3310 | lr 3.00e-04 | grad 1.77 | tok/s 7672
step    740 | loss 1.5151 | lr 3.00e-04 | grad 1.59 | tok/s 7572
step    750 | loss 1.2377 | lr 3.00e-04 | grad 1.68 | tok/s 7696
step    760 | loss 1.1451 | lr 3.00e-04 | grad 1.49 | tok/s 7691
step    770 | loss 1.0820 | lr 3.00e-04 | grad 1.34 | tok/s 7689
step    780 | loss 1.0218 | lr 3.00e-04 | grad 1.37 | tok/s 7690
step    790 | loss 1.1340 | lr 3.00e-04 | grad 2.22 | tok/s 7319
step    800 | loss 1.8131 | lr 3.00e-04 | grad 4.16 | tok/s 7435
step    810 | loss 1.6841 | lr 3.00e-04 | grad 1.39 | tok/s 7395
step    820 | loss 1.6936 | lr 3.00e-04 | grad 2.56 | tok/s 7097
step    830 | loss 1.5148 | lr 3.00e-04 | grad 1.65 | tok/s 7610
step    840 | loss 1.4147 | lr 3.00e-04 | grad 1.53 | tok/s 7698
step    850 | loss 1.5953 | lr 3.00e-04 | grad 1.42 | tok/s 7658
step    860 | loss 1.4967 | lr 3.00e-04 | grad 2.50 | tok/s 7579
step    870 | loss 1.5005 | lr 3.00e-04 | grad 1.80 | tok/s 7247
step    880 | loss 1.6559 | lr 3.00e-04 | grad 1.64 | tok/s 7327
step    890 | loss 1.6671 | lr 3.00e-04 | grad 1.95 | tok/s 7432
step    900 | loss 1.5400 | lr 3.00e-04 | grad 1.72 | tok/s 7426
step    910 | loss 1.4166 | lr 3.00e-04 | grad 2.55 | tok/s 7270
step    920 | loss 1.5341 | lr 3.00e-04 | grad 2.52 | tok/s 7559
step    930 | loss 1.5864 | lr 3.00e-04 | grad 2.53 | tok/s 7217
step    940 | loss 1.4027 | lr 3.00e-04 | grad 1.20 | tok/s 7609
step    950 | loss 1.4807 | lr 3.00e-04 | grad 1.66 | tok/s 7536
step    960 | loss 1.3381 | lr 3.00e-04 | grad 1.70 | tok/s 7663
step    970 | loss 1.7024 | lr 3.00e-04 | grad 2.42 | tok/s 7205
step    980 | loss 1.6232 | lr 3.00e-04 | grad 1.49 | tok/s 7394
step    990 | loss 1.4541 | lr 3.00e-04 | grad 1.41 | tok/s 7525
step   1000 | loss 1.8191 | lr 3.00e-04 | grad 6.50 | tok/s 7221
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8191.pt
step   1010 | loss 1.6576 | lr 3.00e-04 | grad 2.06 | tok/s 4716
step   1020 | loss 1.6214 | lr 3.00e-04 | grad 1.19 | tok/s 7077
step   1030 | loss 1.4473 | lr 3.00e-04 | grad 1.34 | tok/s 7241
step   1040 | loss 1.4733 | lr 3.00e-04 | grad 1.39 | tok/s 7604
step   1050 | loss 1.5845 | lr 3.00e-04 | grad 2.09 | tok/s 7032
step   1060 | loss 1.7168 | lr 3.00e-04 | grad 2.25 | tok/s 7597
step   1070 | loss 1.6590 | lr 3.00e-04 | grad 1.84 | tok/s 7559
step   1080 | loss 1.3832 | lr 3.00e-04 | grad 1.34 | tok/s 6862
step   1090 | loss 1.0988 | lr 3.00e-04 | grad 0.95 | tok/s 7546
step   1100 | loss 1.4132 | lr 3.00e-04 | grad 2.23 | tok/s 7194

Training complete! Final step: 1105
