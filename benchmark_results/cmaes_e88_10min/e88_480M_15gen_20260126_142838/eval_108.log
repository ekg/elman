Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_108/levelE88_100m_20260126_164244
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 493,855,648 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.1286 | lr 3.00e-04 | grad 5.22 | tok/s 4339
step     20 | loss 2.5414 | lr 3.00e-04 | grad 2.56 | tok/s 7359
step     30 | loss 2.4691 | lr 3.00e-04 | grad 1.59 | tok/s 7430
step     40 | loss 2.2673 | lr 3.00e-04 | grad 1.84 | tok/s 7133
step     50 | loss 2.8065 | lr 3.00e-04 | grad 5.81 | tok/s 7208
step     60 | loss 2.0483 | lr 3.00e-04 | grad 4.34 | tok/s 7427
step     70 | loss 1.9276 | lr 3.00e-04 | grad 2.33 | tok/s 7519
step     80 | loss 4.6993 | lr 3.00e-04 | grad 24.00 | tok/s 7564
step     90 | loss 4.3585 | lr 3.00e-04 | grad 4.41 | tok/s 7689
step    100 | loss 3.6546 | lr 3.00e-04 | grad 3.95 | tok/s 7682
step    110 | loss 3.2116 | lr 3.00e-04 | grad 5.03 | tok/s 7689
step    120 | loss 2.8660 | lr 3.00e-04 | grad 5.28 | tok/s 7683
step    130 | loss 2.6470 | lr 3.00e-04 | grad 6.59 | tok/s 7268
step    140 | loss 2.4350 | lr 3.00e-04 | grad 4.97 | tok/s 7671
step    150 | loss 2.3917 | lr 3.00e-04 | grad 3.80 | tok/s 7650
step    160 | loss 2.0983 | lr 3.00e-04 | grad 3.34 | tok/s 7668
step    170 | loss 2.1399 | lr 3.00e-04 | grad 4.69 | tok/s 7659
step    180 | loss 1.9886 | lr 3.00e-04 | grad 2.78 | tok/s 7663
step    190 | loss 2.1083 | lr 3.00e-04 | grad 2.72 | tok/s 7653
step    200 | loss 1.8715 | lr 3.00e-04 | grad 2.19 | tok/s 7672
step    210 | loss 1.8981 | lr 3.00e-04 | grad 3.52 | tok/s 7671
step    220 | loss 2.0563 | lr 3.00e-04 | grad 1.90 | tok/s 7576
step    230 | loss 2.0598 | lr 3.00e-04 | grad 2.05 | tok/s 7491
step    240 | loss 2.2132 | lr 3.00e-04 | grad 2.52 | tok/s 7103
step    250 | loss 2.0513 | lr 3.00e-04 | grad 1.41 | tok/s 7315
step    260 | loss 1.5842 | lr 3.00e-04 | grad 1.64 | tok/s 7540
step    270 | loss 2.0497 | lr 3.00e-04 | grad 1.50 | tok/s 7440
step    280 | loss 2.2150 | lr 3.00e-04 | grad 3.16 | tok/s 7299
step    290 | loss 1.3947 | lr 3.00e-04 | grad 1.84 | tok/s 7672
step    300 | loss 0.5824 | lr 3.00e-04 | grad 1.62 | tok/s 7675
step    310 | loss 2.3829 | lr 3.00e-04 | grad 2.91 | tok/s 7552
step    320 | loss 1.9625 | lr 3.00e-04 | grad 3.14 | tok/s 7399
step    330 | loss 1.8869 | lr 3.00e-04 | grad 1.63 | tok/s 7141
step    340 | loss 2.1942 | lr 3.00e-04 | grad 1.40 | tok/s 7264
step    350 | loss 1.8504 | lr 3.00e-04 | grad 3.08 | tok/s 7455
step    360 | loss 1.2469 | lr 3.00e-04 | grad 4.09 | tok/s 7614
step    370 | loss 1.7770 | lr 3.00e-04 | grad 1.55 | tok/s 6897
step    380 | loss 1.7316 | lr 3.00e-04 | grad 1.44 | tok/s 7363
step    390 | loss 1.5110 | lr 3.00e-04 | grad 1.13 | tok/s 7676
step    400 | loss 1.4772 | lr 3.00e-04 | grad 1.46 | tok/s 7614
step    410 | loss 1.2812 | lr 3.00e-04 | grad 1.25 | tok/s 7458
step    420 | loss 1.7694 | lr 3.00e-04 | grad 2.56 | tok/s 7110
step    430 | loss 2.0798 | lr 3.00e-04 | grad 1.74 | tok/s 7573
step    440 | loss 2.0839 | lr 3.00e-04 | grad 2.45 | tok/s 7149
step    450 | loss 1.8299 | lr 3.00e-04 | grad 1.59 | tok/s 7400
step    460 | loss 1.6849 | lr 3.00e-04 | grad 1.83 | tok/s 7247
step    470 | loss 1.7711 | lr 3.00e-04 | grad 1.37 | tok/s 7468
step    480 | loss 2.1277 | lr 3.00e-04 | grad 4.12 | tok/s 7478
step    490 | loss 1.7216 | lr 3.00e-04 | grad 1.45 | tok/s 7062
step    500 | loss 1.6403 | lr 3.00e-04 | grad 2.05 | tok/s 7550
step    510 | loss 1.6665 | lr 3.00e-04 | grad 1.34 | tok/s 7661
step    520 | loss 1.6307 | lr 3.00e-04 | grad 1.25 | tok/s 7640
step    530 | loss 1.8393 | lr 3.00e-04 | grad 1.54 | tok/s 7336
step    540 | loss 1.6940 | lr 3.00e-04 | grad 1.30 | tok/s 7342
step    550 | loss 1.5391 | lr 3.00e-04 | grad 1.80 | tok/s 7193
step    560 | loss 1.6836 | lr 3.00e-04 | grad 1.59 | tok/s 7002
step    570 | loss 1.6093 | lr 3.00e-04 | grad 2.33 | tok/s 7188
step    580 | loss 1.5091 | lr 3.00e-04 | grad 1.29 | tok/s 7156
step    590 | loss 1.7784 | lr 3.00e-04 | grad 1.63 | tok/s 6704
step    600 | loss 1.7492 | lr 3.00e-04 | grad 1.19 | tok/s 7147
step    610 | loss 1.5644 | lr 3.00e-04 | grad 1.40 | tok/s 7287
step    620 | loss 1.5555 | lr 3.00e-04 | grad 1.35 | tok/s 7270
step    630 | loss 1.5962 | lr 3.00e-04 | grad 1.74 | tok/s 7167
step    640 | loss 1.8080 | lr 3.00e-04 | grad 3.94 | tok/s 7245
step    650 | loss 1.5709 | lr 3.00e-04 | grad 1.90 | tok/s 7426
step    660 | loss 1.6592 | lr 3.00e-04 | grad 1.88 | tok/s 7467
step    670 | loss 1.8318 | lr 3.00e-04 | grad 2.16 | tok/s 7442
step    680 | loss 1.6645 | lr 3.00e-04 | grad 1.95 | tok/s 7251
step    690 | loss 1.7341 | lr 3.00e-04 | grad 1.70 | tok/s 7633
step    700 | loss 1.3667 | lr 3.00e-04 | grad 1.61 | tok/s 7717
step    710 | loss 1.5599 | lr 3.00e-04 | grad 2.27 | tok/s 7056
step    720 | loss 1.4489 | lr 3.00e-04 | grad 1.73 | tok/s 7220
step    730 | loss 1.2746 | lr 3.00e-04 | grad 1.93 | tok/s 7701
step    740 | loss 1.4087 | lr 3.00e-04 | grad 1.36 | tok/s 7600
step    750 | loss 1.1880 | lr 3.00e-04 | grad 1.24 | tok/s 7713
step    760 | loss 1.0798 | lr 3.00e-04 | grad 1.26 | tok/s 7719
step    770 | loss 1.0360 | lr 3.00e-04 | grad 1.42 | tok/s 7708
step    780 | loss 0.9511 | lr 3.00e-04 | grad 1.30 | tok/s 7707
step    790 | loss 1.1448 | lr 3.00e-04 | grad 1.95 | tok/s 7385
step    800 | loss 1.8251 | lr 3.00e-04 | grad 3.55 | tok/s 7541
step    810 | loss 1.6079 | lr 3.00e-04 | grad 3.61 | tok/s 7291
step    820 | loss 1.6173 | lr 3.00e-04 | grad 4.97 | tok/s 7228
step    830 | loss 1.4235 | lr 3.00e-04 | grad 1.58 | tok/s 7628
step    840 | loss 1.4156 | lr 3.00e-04 | grad 3.12 | tok/s 7716
step    850 | loss 1.5043 | lr 3.00e-04 | grad 2.55 | tok/s 7434
step    860 | loss 1.4282 | lr 3.00e-04 | grad 1.82 | tok/s 7570
step    870 | loss 1.4605 | lr 3.00e-04 | grad 1.52 | tok/s 7408
step    880 | loss 1.6049 | lr 3.00e-04 | grad 1.80 | tok/s 7342
step    890 | loss 1.5784 | lr 3.00e-04 | grad 1.32 | tok/s 7364
step    900 | loss 1.5082 | lr 3.00e-04 | grad 1.29 | tok/s 7385
step    910 | loss 1.3953 | lr 3.00e-04 | grad 1.49 | tok/s 7450
step    920 | loss 1.5128 | lr 3.00e-04 | grad 2.45 | tok/s 7590
step    930 | loss 1.4969 | lr 3.00e-04 | grad 1.16 | tok/s 7164
step    940 | loss 1.3362 | lr 3.00e-04 | grad 1.31 | tok/s 7720
step    950 | loss 1.4456 | lr 3.00e-04 | grad 1.55 | tok/s 7673
step    960 | loss 1.2885 | lr 3.00e-04 | grad 1.42 | tok/s 7636
step    970 | loss 1.7413 | lr 3.00e-04 | grad 2.31 | tok/s 7259
step    980 | loss 1.5085 | lr 3.00e-04 | grad 1.52 | tok/s 7359
step    990 | loss 1.4303 | lr 3.00e-04 | grad 1.27 | tok/s 7585
step   1000 | loss 1.7925 | lr 3.00e-04 | grad 6.16 | tok/s 7299
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7925.pt
step   1010 | loss 1.6138 | lr 3.00e-04 | grad 2.33 | tok/s 4360
step   1020 | loss 1.5947 | lr 3.00e-04 | grad 1.45 | tok/s 7077
step   1030 | loss 1.3606 | lr 3.00e-04 | grad 1.27 | tok/s 7266
step   1040 | loss 1.4587 | lr 3.00e-04 | grad 1.45 | tok/s 7438
step   1050 | loss 1.5685 | lr 3.00e-04 | grad 1.70 | tok/s 7223
step   1060 | loss 1.6193 | lr 3.00e-04 | grad 1.39 | tok/s 7538
step   1070 | loss 1.6141 | lr 3.00e-04 | grad 1.57 | tok/s 7392
step   1080 | loss 1.3525 | lr 3.00e-04 | grad 1.61 | tok/s 6945
step   1090 | loss 0.9758 | lr 3.00e-04 | grad 3.41 | tok/s 7673
step   1100 | loss 1.4813 | lr 3.00e-04 | grad 1.62 | tok/s 7384

Training complete! Final step: 1105
