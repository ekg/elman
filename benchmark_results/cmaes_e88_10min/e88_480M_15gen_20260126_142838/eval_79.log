Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_79/levelE88_100m_20260126_160128
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 466,780,624 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0592 | lr 3.00e-04 | grad 8.06 | tok/s 6096
step     20 | loss 2.6880 | lr 3.00e-04 | grad 2.03 | tok/s 8411
step     30 | loss 3.0767 | lr 3.00e-04 | grad 4.69 | tok/s 8861
step     40 | loss 4.0969 | lr 3.00e-04 | grad 23.50 | tok/s 9014
step     50 | loss 4.2500 | lr 3.00e-04 | grad 9.06 | tok/s 9121
step     60 | loss 3.3438 | lr 3.00e-04 | grad 7.06 | tok/s 9111
step     70 | loss 2.7071 | lr 3.00e-04 | grad 4.25 | tok/s 9089
step     80 | loss 2.4025 | lr 3.00e-04 | grad 3.50 | tok/s 9083
step     90 | loss 2.2776 | lr 3.00e-04 | grad 3.09 | tok/s 9078
step    100 | loss 2.1134 | lr 3.00e-04 | grad 2.16 | tok/s 9061
step    110 | loss 2.1655 | lr 3.00e-04 | grad 2.03 | tok/s 9014
step    120 | loss 2.6589 | lr 3.00e-04 | grad 1.38 | tok/s 8573
step    130 | loss 2.1141 | lr 3.00e-04 | grad 3.73 | tok/s 8779
step    140 | loss 2.3627 | lr 3.00e-04 | grad 5.81 | tok/s 8796
step    150 | loss 1.3664 | lr 3.00e-04 | grad 4.00 | tok/s 9001
step    160 | loss 2.3184 | lr 3.00e-04 | grad 1.59 | tok/s 8717
step    170 | loss 2.2669 | lr 3.00e-04 | grad 1.20 | tok/s 8576
step    180 | loss 1.8670 | lr 3.00e-04 | grad 2.19 | tok/s 8789
step    190 | loss 1.9219 | lr 3.00e-04 | grad 1.65 | tok/s 8628
step    200 | loss 1.6738 | lr 3.00e-04 | grad 1.34 | tok/s 9032
step    210 | loss 1.8949 | lr 3.00e-04 | grad 3.67 | tok/s 8560
step    220 | loss 2.2047 | lr 3.00e-04 | grad 2.81 | tok/s 8651
step    230 | loss 1.9150 | lr 3.00e-04 | grad 1.95 | tok/s 8642
step    240 | loss 2.2346 | lr 3.00e-04 | grad 3.81 | tok/s 8753
step    250 | loss 1.7719 | lr 3.00e-04 | grad 1.25 | tok/s 8694
step    260 | loss 1.8884 | lr 3.00e-04 | grad 2.39 | tok/s 8942
step    270 | loss 1.8203 | lr 3.00e-04 | grad 1.42 | tok/s 8745
step    280 | loss 1.7687 | lr 3.00e-04 | grad 1.43 | tok/s 8209
step    290 | loss 1.6651 | lr 3.00e-04 | grad 1.66 | tok/s 8498
step    300 | loss 1.9563 | lr 3.00e-04 | grad 1.56 | tok/s 8558
step    310 | loss 1.6649 | lr 3.00e-04 | grad 1.40 | tok/s 8504
step    320 | loss 1.8661 | lr 3.00e-04 | grad 2.22 | tok/s 8610
step    330 | loss 1.7094 | lr 3.00e-04 | grad 1.40 | tok/s 8704
step    340 | loss 2.0021 | lr 3.00e-04 | grad 1.64 | tok/s 8665
step    350 | loss 1.7100 | lr 3.00e-04 | grad 1.54 | tok/s 8917
step    360 | loss 1.5676 | lr 3.00e-04 | grad 1.45 | tok/s 8537
step    370 | loss 1.4883 | lr 3.00e-04 | grad 1.31 | tok/s 8994
step    380 | loss 1.2278 | lr 3.00e-04 | grad 1.39 | tok/s 9074
step    390 | loss 1.1213 | lr 3.00e-04 | grad 1.25 | tok/s 9064
step    400 | loss 1.7352 | lr 3.00e-04 | grad 1.42 | tok/s 8596
step    410 | loss 1.7325 | lr 3.00e-04 | grad 1.87 | tok/s 8676
step    420 | loss 1.6231 | lr 3.00e-04 | grad 2.44 | tok/s 9052
step    430 | loss 1.5933 | lr 3.00e-04 | grad 1.47 | tok/s 8901
step    440 | loss 1.6794 | lr 3.00e-04 | grad 1.75 | tok/s 8622
step    450 | loss 1.6125 | lr 3.00e-04 | grad 1.22 | tok/s 8719
step    460 | loss 1.5779 | lr 3.00e-04 | grad 1.64 | tok/s 8851
step    470 | loss 1.5526 | lr 3.00e-04 | grad 2.42 | tok/s 8789
step    480 | loss 1.5722 | lr 3.00e-04 | grad 2.14 | tok/s 8973
step    490 | loss 1.6794 | lr 3.00e-04 | grad 1.83 | tok/s 8616
step    500 | loss 1.7806 | lr 3.00e-04 | grad 1.40 | tok/s 8754
step    510 | loss 1.6663 | lr 3.00e-04 | grad 1.13 | tok/s 8360
step    520 | loss 1.5243 | lr 3.00e-04 | grad 1.62 | tok/s 8764
step    530 | loss 1.6946 | lr 3.00e-04 | grad 1.62 | tok/s 8614
step    540 | loss 1.5869 | lr 3.00e-04 | grad 1.45 | tok/s 8434
step    550 | loss 1.3613 | lr 3.00e-04 | grad 1.43 | tok/s 8852
step    560 | loss 1.4078 | lr 3.00e-04 | grad 1.39 | tok/s 9072
step    570 | loss 1.3279 | lr 3.00e-04 | grad 1.18 | tok/s 9069
step    580 | loss 1.2815 | lr 3.00e-04 | grad 1.05 | tok/s 9062
step    590 | loss 1.3418 | lr 3.00e-04 | grad 1.49 | tok/s 9067
step    600 | loss 1.2656 | lr 3.00e-04 | grad 1.19 | tok/s 9066
step    610 | loss 1.2928 | lr 3.00e-04 | grad 1.05 | tok/s 9068
step    620 | loss 1.3479 | lr 3.00e-04 | grad 2.92 | tok/s 8942
step    630 | loss 1.6326 | lr 3.00e-04 | grad 2.34 | tok/s 8580
step    640 | loss 1.6615 | lr 3.00e-04 | grad 1.62 | tok/s 8644
step    650 | loss 1.5342 | lr 3.00e-04 | grad 1.90 | tok/s 8664

Training complete! Final step: 657
