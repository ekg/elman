Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_110/levelE88_100m_20260126_164243
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 464,867,228 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.3379 | lr 3.00e-04 | grad 11.88 | tok/s 7354
step     20 | loss 2.7256 | lr 3.00e-04 | grad 2.38 | tok/s 10863
step     30 | loss 3.0459 | lr 3.00e-04 | grad 4.94 | tok/s 11441
step     40 | loss 4.2308 | lr 3.00e-04 | grad 35.50 | tok/s 11649
step     50 | loss 4.6439 | lr 3.00e-04 | grad 16.88 | tok/s 11766
step     60 | loss 3.7921 | lr 3.00e-04 | grad 12.38 | tok/s 11735
step     70 | loss 2.9858 | lr 3.00e-04 | grad 7.62 | tok/s 11730
step     80 | loss 2.6128 | lr 3.00e-04 | grad 5.47 | tok/s 11705
step     90 | loss 2.4258 | lr 3.00e-04 | grad 4.03 | tok/s 11668
step    100 | loss 2.2166 | lr 3.00e-04 | grad 1.98 | tok/s 11677
step    110 | loss 2.2682 | lr 3.00e-04 | grad 2.34 | tok/s 11587
step    120 | loss 2.7114 | lr 3.00e-04 | grad 1.47 | tok/s 11022
step    130 | loss 2.1426 | lr 3.00e-04 | grad 4.47 | tok/s 11053
step    140 | loss 2.3872 | lr 3.00e-04 | grad 6.12 | tok/s 11313
step    150 | loss 1.4062 | lr 3.00e-04 | grad 3.98 | tok/s 11572
step    160 | loss 2.3281 | lr 3.00e-04 | grad 1.68 | tok/s 11199
step    170 | loss 2.2943 | lr 3.00e-04 | grad 1.33 | tok/s 11038
step    180 | loss 1.9026 | lr 3.00e-04 | grad 2.38 | tok/s 11296
step    190 | loss 1.9470 | lr 3.00e-04 | grad 1.62 | tok/s 11088
step    200 | loss 1.7043 | lr 3.00e-04 | grad 1.36 | tok/s 11607
step    210 | loss 1.9125 | lr 3.00e-04 | grad 3.88 | tok/s 10996
step    220 | loss 2.2158 | lr 3.00e-04 | grad 2.58 | tok/s 11114
step    230 | loss 1.9449 | lr 3.00e-04 | grad 2.22 | tok/s 11113
step    240 | loss 2.2758 | lr 3.00e-04 | grad 4.03 | tok/s 11248
step    250 | loss 1.7949 | lr 3.00e-04 | grad 1.30 | tok/s 11177
step    260 | loss 1.9164 | lr 3.00e-04 | grad 2.56 | tok/s 11489
step    270 | loss 1.8452 | lr 3.00e-04 | grad 1.55 | tok/s 11230
step    280 | loss 1.7901 | lr 3.00e-04 | grad 1.51 | tok/s 10539
step    290 | loss 1.6869 | lr 3.00e-04 | grad 1.72 | tok/s 10902
step    300 | loss 1.9766 | lr 3.00e-04 | grad 1.62 | tok/s 10981
step    310 | loss 1.6782 | lr 3.00e-04 | grad 1.43 | tok/s 10937
step    320 | loss 1.8870 | lr 3.00e-04 | grad 2.27 | tok/s 11081
step    330 | loss 1.7286 | lr 3.00e-04 | grad 1.41 | tok/s 11190
step    340 | loss 2.0204 | lr 3.00e-04 | grad 1.68 | tok/s 11142
step    350 | loss 1.7540 | lr 3.00e-04 | grad 1.59 | tok/s 11469
step    360 | loss 1.5892 | lr 3.00e-04 | grad 1.56 | tok/s 10970
step    370 | loss 1.5042 | lr 3.00e-04 | grad 1.34 | tok/s 11557
step    380 | loss 1.2426 | lr 3.00e-04 | grad 1.40 | tok/s 11667
step    390 | loss 1.1386 | lr 3.00e-04 | grad 1.28 | tok/s 11670
step    400 | loss 1.7492 | lr 3.00e-04 | grad 1.49 | tok/s 11061
step    410 | loss 1.7478 | lr 3.00e-04 | grad 1.94 | tok/s 11165
step    420 | loss 1.6423 | lr 3.00e-04 | grad 3.00 | tok/s 11646
step    430 | loss 1.6224 | lr 3.00e-04 | grad 1.54 | tok/s 11452
step    440 | loss 1.6951 | lr 3.00e-04 | grad 1.77 | tok/s 11101
step    450 | loss 1.6308 | lr 3.00e-04 | grad 1.22 | tok/s 11213
step    460 | loss 1.6004 | lr 3.00e-04 | grad 1.71 | tok/s 11391
step    470 | loss 1.5639 | lr 3.00e-04 | grad 2.56 | tok/s 11307
step    480 | loss 1.5703 | lr 3.00e-04 | grad 2.23 | tok/s 11571
step    490 | loss 1.6935 | lr 3.00e-04 | grad 1.90 | tok/s 11106
step    500 | loss 1.7949 | lr 3.00e-04 | grad 1.44 | tok/s 11305
step    510 | loss 1.6783 | lr 3.00e-04 | grad 1.20 | tok/s 10800
step    520 | loss 1.5390 | lr 3.00e-04 | grad 1.70 | tok/s 11294
step    530 | loss 1.7094 | lr 3.00e-04 | grad 1.70 | tok/s 11128
step    540 | loss 1.6029 | lr 3.00e-04 | grad 1.34 | tok/s 10896
step    550 | loss 1.3524 | lr 3.00e-04 | grad 2.22 | tok/s 11417
step    560 | loss 1.4438 | lr 3.00e-04 | grad 1.41 | tok/s 11776
step    570 | loss 1.3483 | lr 3.00e-04 | grad 1.42 | tok/s 11770
step    580 | loss 1.3094 | lr 3.00e-04 | grad 1.14 | tok/s 11504
step    590 | loss 1.3421 | lr 3.00e-04 | grad 1.10 | tok/s 11724
step    600 | loss 1.2794 | lr 3.00e-04 | grad 1.36 | tok/s 11721
step    610 | loss 1.3117 | lr 3.00e-04 | grad 1.25 | tok/s 11716
step    620 | loss 1.2994 | lr 3.00e-04 | grad 1.34 | tok/s 11681
step    630 | loss 1.6299 | lr 3.00e-04 | grad 3.42 | tok/s 11018
step    640 | loss 1.7353 | lr 3.00e-04 | grad 1.38 | tok/s 11169
step    650 | loss 1.5498 | lr 3.00e-04 | grad 1.46 | tok/s 11168
step    660 | loss 1.5997 | lr 3.00e-04 | grad 1.48 | tok/s 11616
step    670 | loss 1.6174 | lr 3.00e-04 | grad 4.34 | tok/s 11242
step    680 | loss 1.6285 | lr 3.00e-04 | grad 1.59 | tok/s 11057
step    690 | loss 1.5609 | lr 3.00e-04 | grad 1.53 | tok/s 10959
step    700 | loss 1.4779 | lr 3.00e-04 | grad 1.21 | tok/s 11203
step    710 | loss 1.6281 | lr 3.00e-04 | grad 2.56 | tok/s 11032
step    720 | loss 1.3062 | lr 3.00e-04 | grad 1.25 | tok/s 11474
step    730 | loss 1.4622 | lr 3.00e-04 | grad 1.23 | tok/s 11287
step    740 | loss 1.7647 | lr 3.00e-04 | grad 2.73 | tok/s 11617
step    750 | loss 1.5390 | lr 3.00e-04 | grad 1.19 | tok/s 11723
step    760 | loss 1.5278 | lr 3.00e-04 | grad 2.73 | tok/s 11483
step    770 | loss 1.5699 | lr 3.00e-04 | grad 1.60 | tok/s 11280
step    780 | loss 1.4847 | lr 3.00e-04 | grad 1.48 | tok/s 11356
step    790 | loss 1.6249 | lr 3.00e-04 | grad 3.48 | tok/s 11613
step    800 | loss 1.3204 | lr 3.00e-04 | grad 1.02 | tok/s 11392
step    810 | loss 1.3047 | lr 3.00e-04 | grad 2.17 | tok/s 11004
step    820 | loss 1.4184 | lr 3.00e-04 | grad 1.53 | tok/s 11238
step    830 | loss 1.4887 | lr 3.00e-04 | grad 1.14 | tok/s 11045
step    840 | loss 1.6043 | lr 3.00e-04 | grad 1.25 | tok/s 11039

Training complete! Final step: 847
