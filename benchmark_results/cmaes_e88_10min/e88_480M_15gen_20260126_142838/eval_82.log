Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_82/levelE88_100m_20260126_161145
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,500,480 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.1237 | lr 3.00e-04 | grad 17.00 | tok/s 6154
step     20 | loss 2.8042 | lr 3.00e-04 | grad 9.81 | tok/s 16093
step     30 | loss 2.6382 | lr 3.00e-04 | grad 5.56 | tok/s 16300
step     40 | loss 2.4220 | lr 3.00e-04 | grad 4.38 | tok/s 15600
step     50 | loss 2.9635 | lr 3.00e-04 | grad 11.06 | tok/s 15844
step     60 | loss 2.0776 | lr 3.00e-04 | grad 4.47 | tok/s 16343
step     70 | loss 1.8934 | lr 3.00e-04 | grad 5.34 | tok/s 16503
step     80 | loss 6.1241 | lr 3.00e-04 | grad 102.00 | tok/s 16598
step     90 | loss 5.8559 | lr 3.00e-04 | grad 13.75 | tok/s 16906
step    100 | loss 4.5073 | lr 3.00e-04 | grad 10.69 | tok/s 16882
step    110 | loss 3.8751 | lr 3.00e-04 | grad 18.50 | tok/s 16851
step    120 | loss 3.4357 | lr 3.00e-04 | grad 18.75 | tok/s 16836
step    130 | loss 3.1024 | lr 3.00e-04 | grad 19.25 | tok/s 15583
step    140 | loss 2.8464 | lr 3.00e-04 | grad 12.50 | tok/s 16860
step    150 | loss 2.8963 | lr 3.00e-04 | grad 18.25 | tok/s 16842
step    160 | loss 2.3654 | lr 3.00e-04 | grad 11.12 | tok/s 16823
step    170 | loss 2.5147 | lr 3.00e-04 | grad 16.12 | tok/s 16796
step    180 | loss 2.3267 | lr 3.00e-04 | grad 8.94 | tok/s 16808
step    190 | loss 2.4508 | lr 3.00e-04 | grad 11.50 | tok/s 16777
step    200 | loss 2.1718 | lr 3.00e-04 | grad 8.44 | tok/s 16796
step    210 | loss 2.1588 | lr 3.00e-04 | grad 7.50 | tok/s 16787
step    220 | loss 2.2003 | lr 3.00e-04 | grad 3.38 | tok/s 16540
step    230 | loss 2.0934 | lr 3.00e-04 | grad 4.16 | tok/s 16396
step    240 | loss 2.3069 | lr 3.00e-04 | grad 4.69 | tok/s 15565
step    250 | loss 2.1130 | lr 3.00e-04 | grad 2.61 | tok/s 15975
step    260 | loss 1.5568 | lr 3.00e-04 | grad 2.95 | tok/s 16468
step    270 | loss 2.1079 | lr 3.00e-04 | grad 2.80 | tok/s 16249
step    280 | loss 2.2687 | lr 3.00e-04 | grad 5.44 | tok/s 15958
step    290 | loss 1.4281 | lr 3.00e-04 | grad 3.66 | tok/s 16801
step    300 | loss 0.5622 | lr 3.00e-04 | grad 2.44 | tok/s 16789
step    310 | loss 2.4101 | lr 3.00e-04 | grad 3.81 | tok/s 16468
step    320 | loss 1.9493 | lr 3.00e-04 | grad 5.69 | tok/s 16159
step    330 | loss 1.9667 | lr 3.00e-04 | grad 2.94 | tok/s 15579
step    340 | loss 2.3124 | lr 3.00e-04 | grad 2.86 | tok/s 15849
step    350 | loss 1.8794 | lr 3.00e-04 | grad 4.69 | tok/s 16248
step    360 | loss 1.2243 | lr 3.00e-04 | grad 7.94 | tok/s 16617
step    370 | loss 1.8173 | lr 3.00e-04 | grad 2.59 | tok/s 15057
step    380 | loss 1.7836 | lr 3.00e-04 | grad 2.55 | tok/s 16062
step    390 | loss 1.5428 | lr 3.00e-04 | grad 2.17 | tok/s 16748
step    400 | loss 1.4981 | lr 3.00e-04 | grad 2.53 | tok/s 16618
step    410 | loss 1.2779 | lr 3.00e-04 | grad 2.00 | tok/s 16231
step    420 | loss 1.8276 | lr 3.00e-04 | grad 4.34 | tok/s 15511
step    430 | loss 2.1729 | lr 3.00e-04 | grad 2.94 | tok/s 16547
step    440 | loss 2.1632 | lr 3.00e-04 | grad 4.25 | tok/s 15588
step    450 | loss 2.0190 | lr 3.00e-04 | grad 2.78 | tok/s 16154
step    460 | loss 1.7355 | lr 3.00e-04 | grad 2.73 | tok/s 15812
step    470 | loss 1.8510 | lr 3.00e-04 | grad 2.42 | tok/s 16293
step    480 | loss 2.2658 | lr 3.00e-04 | grad 6.72 | tok/s 16294
step    490 | loss 1.8018 | lr 3.00e-04 | grad 2.70 | tok/s 15416
step    500 | loss 1.6924 | lr 3.00e-04 | grad 3.36 | tok/s 16443
step    510 | loss 1.7200 | lr 3.00e-04 | grad 2.33 | tok/s 16664
step    520 | loss 1.6684 | lr 3.00e-04 | grad 2.09 | tok/s 16631
step    530 | loss 1.9265 | lr 3.00e-04 | grad 2.44 | tok/s 16000
step    540 | loss 1.7496 | lr 3.00e-04 | grad 2.20 | tok/s 16002
step    550 | loss 1.5793 | lr 3.00e-04 | grad 2.92 | tok/s 15648
step    560 | loss 1.7373 | lr 3.00e-04 | grad 2.56 | tok/s 15239
step    570 | loss 1.6851 | lr 3.00e-04 | grad 3.70 | tok/s 15665
step    580 | loss 1.5585 | lr 3.00e-04 | grad 2.14 | tok/s 14409
step    590 | loss 1.8811 | lr 3.00e-04 | grad 3.08 | tok/s 16013
step    600 | loss 1.8359 | lr 3.00e-04 | grad 2.28 | tok/s 15470
step    610 | loss 1.6273 | lr 3.00e-04 | grad 2.34 | tok/s 16259
step    620 | loss 1.5544 | lr 3.00e-04 | grad 2.41 | tok/s 15407
step    630 | loss 1.6659 | lr 3.00e-04 | grad 4.22 | tok/s 15530
step    640 | loss 1.8120 | lr 3.00e-04 | grad 2.44 | tok/s 15950
step    650 | loss 1.6792 | lr 3.00e-04 | grad 2.55 | tok/s 16028
step    660 | loss 1.7060 | lr 3.00e-04 | grad 2.03 | tok/s 16091
step    670 | loss 1.9339 | lr 3.00e-04 | grad 3.12 | tok/s 16208
step    680 | loss 1.7410 | lr 3.00e-04 | grad 2.38 | tok/s 15878
step    690 | loss 1.8482 | lr 3.00e-04 | grad 3.17 | tok/s 16400
step    700 | loss 1.4333 | lr 3.00e-04 | grad 3.00 | tok/s 16721
step    710 | loss 1.6000 | lr 3.00e-04 | grad 2.33 | tok/s 15644
step    720 | loss 1.4882 | lr 3.00e-04 | grad 3.31 | tok/s 15405
step    730 | loss 1.2935 | lr 3.00e-04 | grad 2.66 | tok/s 16707
step    740 | loss 1.5160 | lr 3.00e-04 | grad 2.33 | tok/s 16489
step    750 | loss 1.2140 | lr 3.00e-04 | grad 2.56 | tok/s 16757
step    760 | loss 1.1207 | lr 3.00e-04 | grad 2.16 | tok/s 16767
step    770 | loss 1.0716 | lr 3.00e-04 | grad 2.02 | tok/s 16752
step    780 | loss 1.0037 | lr 3.00e-04 | grad 2.05 | tok/s 16742
step    790 | loss 1.1422 | lr 3.00e-04 | grad 3.25 | tok/s 16254
step    800 | loss 1.8206 | lr 3.00e-04 | grad 5.38 | tok/s 16176
step    810 | loss 1.7133 | lr 3.00e-04 | grad 2.08 | tok/s 16088
step    820 | loss 1.7252 | lr 3.00e-04 | grad 3.84 | tok/s 15451
step    830 | loss 1.4974 | lr 3.00e-04 | grad 2.22 | tok/s 15872
step    840 | loss 1.3718 | lr 3.00e-04 | grad 2.20 | tok/s 16743
step    850 | loss 1.6223 | lr 3.00e-04 | grad 2.00 | tok/s 16676
step    860 | loss 1.4834 | lr 3.00e-04 | grad 3.41 | tok/s 16479
step    870 | loss 1.5082 | lr 3.00e-04 | grad 2.55 | tok/s 15882
step    880 | loss 1.6970 | lr 3.00e-04 | grad 2.62 | tok/s 15958
step    890 | loss 1.6968 | lr 3.00e-04 | grad 2.88 | tok/s 16168
step    900 | loss 1.5794 | lr 3.00e-04 | grad 2.44 | tok/s 16185
step    910 | loss 1.4396 | lr 3.00e-04 | grad 3.84 | tok/s 15845
step    920 | loss 1.5278 | lr 3.00e-04 | grad 3.53 | tok/s 16470
step    930 | loss 1.6132 | lr 3.00e-04 | grad 3.44 | tok/s 15732
step    940 | loss 1.3875 | lr 3.00e-04 | grad 1.84 | tok/s 16569
step    950 | loss 1.4944 | lr 3.00e-04 | grad 2.59 | tok/s 16640
step    960 | loss 1.3251 | lr 3.00e-04 | grad 2.48 | tok/s 16657
step    970 | loss 1.7626 | lr 3.00e-04 | grad 3.55 | tok/s 15674
step    980 | loss 1.6623 | lr 3.00e-04 | grad 2.34 | tok/s 16103
step    990 | loss 1.4594 | lr 3.00e-04 | grad 2.08 | tok/s 16374
step   1000 | loss 1.8489 | lr 3.00e-04 | grad 7.22 | tok/s 15721
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8489.pt
step   1010 | loss 1.6323 | lr 3.00e-04 | grad 3.36 | tok/s 6501
step   1020 | loss 1.6617 | lr 3.00e-04 | grad 1.92 | tok/s 15406
step   1030 | loss 1.4626 | lr 3.00e-04 | grad 2.03 | tok/s 16038
step   1040 | loss 1.4868 | lr 3.00e-04 | grad 2.17 | tok/s 16555
step   1050 | loss 1.6325 | lr 3.00e-04 | grad 3.16 | tok/s 15284
step   1060 | loss 1.7392 | lr 3.00e-04 | grad 3.34 | tok/s 16531
step   1070 | loss 1.6504 | lr 3.00e-04 | grad 2.62 | tok/s 16446
step   1080 | loss 1.4087 | lr 3.00e-04 | grad 1.91 | tok/s 14952
step   1090 | loss 1.1306 | lr 3.00e-04 | grad 1.20 | tok/s 16451
step   1100 | loss 1.4555 | lr 3.00e-04 | grad 3.44 | tok/s 15964
step   1110 | loss 1.4707 | lr 3.00e-04 | grad 2.05 | tok/s 16797
step   1120 | loss 1.3423 | lr 3.00e-04 | grad 2.05 | tok/s 16792
step   1130 | loss 1.2948 | lr 3.00e-04 | grad 1.94 | tok/s 16778
step   1140 | loss 1.2876 | lr 3.00e-04 | grad 2.02 | tok/s 16799
step   1150 | loss 1.3033 | lr 3.00e-04 | grad 1.77 | tok/s 15982
step   1160 | loss 1.2164 | lr 3.00e-04 | grad 1.82 | tok/s 16890
step   1170 | loss 1.2449 | lr 3.00e-04 | grad 2.08 | tok/s 16901
step   1180 | loss 1.3427 | lr 3.00e-04 | grad 1.66 | tok/s 16886
step   1190 | loss 1.2227 | lr 3.00e-04 | grad 2.19 | tok/s 16866
step   1200 | loss 1.2180 | lr 3.00e-04 | grad 2.00 | tok/s 16864
step   1210 | loss 1.2719 | lr 3.00e-04 | grad 2.00 | tok/s 16877
step   1220 | loss 1.2941 | lr 3.00e-04 | grad 1.92 | tok/s 16868
step   1230 | loss 1.2615 | lr 3.00e-04 | grad 1.80 | tok/s 16880
step   1240 | loss 1.2259 | lr 3.00e-04 | grad 1.54 | tok/s 16864
step   1250 | loss 1.8046 | lr 3.00e-04 | grad 3.64 | tok/s 15953
step   1260 | loss 1.4045 | lr 3.00e-04 | grad 2.36 | tok/s 15815
step   1270 | loss 1.6658 | lr 3.00e-04 | grad 4.88 | tok/s 15763
step   1280 | loss 1.6384 | lr 3.00e-04 | grad 1.84 | tok/s 16230
step   1290 | loss 1.4882 | lr 3.00e-04 | grad 2.03 | tok/s 16127
step   1300 | loss 1.5358 | lr 3.00e-04 | grad 2.47 | tok/s 16227
step   1310 | loss 1.4675 | lr 3.00e-04 | grad 2.36 | tok/s 16517
step   1320 | loss 1.5969 | lr 3.00e-04 | grad 2.23 | tok/s 16566
step   1330 | loss 1.6130 | lr 3.00e-04 | grad 2.41 | tok/s 16577
step   1340 | loss 1.5012 | lr 3.00e-04 | grad 9.25 | tok/s 15826
step   1350 | loss 1.7376 | lr 3.00e-04 | grad 2.44 | tok/s 15310
step   1360 | loss 1.4942 | lr 3.00e-04 | grad 2.38 | tok/s 16275
step   1370 | loss 1.4182 | lr 3.00e-04 | grad 1.60 | tok/s 16088
step   1380 | loss 1.6681 | lr 3.00e-04 | grad 2.34 | tok/s 15450
step   1390 | loss 1.5305 | lr 3.00e-04 | grad 1.77 | tok/s 16397
step   1400 | loss 1.4185 | lr 3.00e-04 | grad 1.77 | tok/s 15818
step   1410 | loss 1.4669 | lr 3.00e-04 | grad 3.09 | tok/s 15857
step   1420 | loss 1.6867 | lr 3.00e-04 | grad 3.81 | tok/s 15898
step   1430 | loss 1.3471 | lr 3.00e-04 | grad 1.95 | tok/s 16179
step   1440 | loss 1.1518 | lr 3.00e-04 | grad 1.93 | tok/s 16691
step   1450 | loss 1.1681 | lr 3.00e-04 | grad 4.34 | tok/s 16831
step   1460 | loss 1.6768 | lr 3.00e-04 | grad 2.08 | tok/s 15858
step   1470 | loss 1.5255 | lr 3.00e-04 | grad 1.86 | tok/s 16465
step   1480 | loss 1.8189 | lr 3.00e-04 | grad 4.00 | tok/s 16558
step   1490 | loss 1.5742 | lr 3.00e-04 | grad 1.73 | tok/s 16795
step   1500 | loss 1.3467 | lr 3.00e-04 | grad 1.81 | tok/s 16857
step   1510 | loss 1.5357 | lr 3.00e-04 | grad 1.96 | tok/s 16626
step   1520 | loss 1.4368 | lr 3.00e-04 | grad 3.50 | tok/s 16291
step   1530 | loss 1.4759 | lr 3.00e-04 | grad 1.73 | tok/s 16674
step   1540 | loss 1.6236 | lr 3.00e-04 | grad 2.36 | tok/s 15703
step   1550 | loss 1.2957 | lr 3.00e-04 | grad 2.25 | tok/s 16741
step   1560 | loss 1.5938 | lr 3.00e-04 | grad 2.61 | tok/s 15857
step   1570 | loss 1.2887 | lr 3.00e-04 | grad 2.19 | tok/s 16847
step   1580 | loss 1.7200 | lr 3.00e-04 | grad 4.03 | tok/s 16461
step   1590 | loss 1.5952 | lr 3.00e-04 | grad 2.36 | tok/s 15839
step   1600 | loss 1.0053 | lr 3.00e-04 | grad 1.51 | tok/s 15735
step   1610 | loss 1.0366 | lr 3.00e-04 | grad 2.48 | tok/s 16229
step   1620 | loss 1.4224 | lr 3.00e-04 | grad 2.91 | tok/s 15243
step   1630 | loss 1.3464 | lr 3.00e-04 | grad 2.19 | tok/s 16311
step   1640 | loss 1.3392 | lr 3.00e-04 | grad 2.30 | tok/s 15929
step   1650 | loss 1.5367 | lr 3.00e-04 | grad 2.45 | tok/s 15275
step   1660 | loss 1.3913 | lr 3.00e-04 | grad 1.74 | tok/s 16313
step   1670 | loss 1.3597 | lr 3.00e-04 | grad 5.56 | tok/s 16257
step   1680 | loss 1.7415 | lr 3.00e-04 | grad 2.02 | tok/s 15606
step   1690 | loss 1.5020 | lr 3.00e-04 | grad 4.12 | tok/s 15903
step   1700 | loss 1.5225 | lr 3.00e-04 | grad 2.28 | tok/s 16240
step   1710 | loss 1.4392 | lr 3.00e-04 | grad 2.22 | tok/s 15925
step   1720 | loss 1.5314 | lr 3.00e-04 | grad 3.02 | tok/s 16629
step   1730 | loss 1.2431 | lr 3.00e-04 | grad 2.81 | tok/s 16794
step   1740 | loss 1.3422 | lr 3.00e-04 | grad 2.67 | tok/s 16391
step   1750 | loss 1.5849 | lr 3.00e-04 | grad 2.52 | tok/s 16100
step   1760 | loss 1.5512 | lr 3.00e-04 | grad 2.25 | tok/s 16152
step   1770 | loss 1.4509 | lr 3.00e-04 | grad 2.31 | tok/s 15880
step   1780 | loss 1.5198 | lr 3.00e-04 | grad 1.81 | tok/s 16504
step   1790 | loss 1.4092 | lr 3.00e-04 | grad 1.68 | tok/s 16054
step   1800 | loss 1.6275 | lr 3.00e-04 | grad 2.44 | tok/s 16196
step   1810 | loss 1.4531 | lr 3.00e-04 | grad 2.05 | tok/s 15607
step   1820 | loss 1.4889 | lr 3.00e-04 | grad 4.81 | tok/s 15822
step   1830 | loss 1.4248 | lr 3.00e-04 | grad 2.30 | tok/s 16483
step   1840 | loss 1.5652 | lr 3.00e-04 | grad 2.67 | tok/s 15791
step   1850 | loss 1.2956 | lr 3.00e-04 | grad 1.82 | tok/s 16510
step   1860 | loss 1.3338 | lr 3.00e-04 | grad 2.33 | tok/s 14910
step   1870 | loss 1.4276 | lr 3.00e-04 | grad 3.20 | tok/s 16067
step   1880 | loss 1.2448 | lr 3.00e-04 | grad 2.19 | tok/s 15766
step   1890 | loss 1.5348 | lr 3.00e-04 | grad 1.85 | tok/s 14990
step   1900 | loss 1.4071 | lr 3.00e-04 | grad 2.55 | tok/s 16209
step   1910 | loss 1.4831 | lr 3.00e-04 | grad 2.69 | tok/s 15355
step   1920 | loss 1.4028 | lr 3.00e-04 | grad 1.98 | tok/s 16816
step   1930 | loss 1.4643 | lr 3.00e-04 | grad 2.42 | tok/s 15765
step   1940 | loss 1.4494 | lr 3.00e-04 | grad 2.00 | tok/s 16399
step   1950 | loss 1.8643 | lr 3.00e-04 | grad 3.62 | tok/s 16658
step   1960 | loss 1.4544 | lr 3.00e-04 | grad 4.19 | tok/s 16817
step   1970 | loss 1.5268 | lr 3.00e-04 | grad 2.50 | tok/s 16387
step   1980 | loss 1.5554 | lr 3.00e-04 | grad 2.03 | tok/s 15649
step   1990 | loss 1.6048 | lr 3.00e-04 | grad 7.84 | tok/s 15960
step   2000 | loss 1.5020 | lr 3.00e-04 | grad 2.06 | tok/s 16181
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5020.pt
step   2010 | loss 1.0929 | lr 3.00e-04 | grad 1.73 | tok/s 7151
step   2020 | loss 1.3113 | lr 3.00e-04 | grad 2.09 | tok/s 16062
step   2030 | loss 1.1134 | lr 3.00e-04 | grad 0.91 | tok/s 15705
step   2040 | loss 1.2289 | lr 3.00e-04 | grad 2.06 | tok/s 16910
step   2050 | loss 1.1978 | lr 3.00e-04 | grad 2.28 | tok/s 16513
step   2060 | loss 1.5771 | lr 3.00e-04 | grad 2.22 | tok/s 15591
step   2070 | loss 1.6773 | lr 3.00e-04 | grad 2.34 | tok/s 16170
step   2080 | loss 2.2701 | lr 3.00e-04 | grad 4.22 | tok/s 16716
step   2090 | loss 1.7220 | lr 3.00e-04 | grad 4.28 | tok/s 16748
step   2100 | loss 1.4025 | lr 3.00e-04 | grad 2.66 | tok/s 16317
step   2110 | loss 1.5374 | lr 3.00e-04 | grad 19.88 | tok/s 16029
step   2120 | loss 0.9375 | lr 3.00e-04 | grad 1.73 | tok/s 16569
step   2130 | loss 1.0530 | lr 3.00e-04 | grad 3.17 | tok/s 16532
step   2140 | loss 1.5661 | lr 3.00e-04 | grad 1.80 | tok/s 16009
step   2150 | loss 1.2991 | lr 3.00e-04 | grad 1.95 | tok/s 16789
step   2160 | loss 1.1919 | lr 3.00e-04 | grad 1.63 | tok/s 16828
step   2170 | loss 1.2444 | lr 3.00e-04 | grad 1.59 | tok/s 16804
step   2180 | loss 1.1957 | lr 3.00e-04 | grad 1.96 | tok/s 16829
step   2190 | loss 1.2113 | lr 3.00e-04 | grad 1.55 | tok/s 16802
step   2200 | loss 1.1987 | lr 3.00e-04 | grad 1.73 | tok/s 16798
step   2210 | loss 1.1470 | lr 3.00e-04 | grad 1.54 | tok/s 16801
step   2220 | loss 1.1462 | lr 3.00e-04 | grad 1.61 | tok/s 16792
step   2230 | loss 1.3625 | lr 3.00e-04 | grad 2.03 | tok/s 16519
step   2240 | loss 1.3300 | lr 3.00e-04 | grad 1.77 | tok/s 16762
step   2250 | loss 1.5568 | lr 3.00e-04 | grad 3.23 | tok/s 16200
step   2260 | loss 1.6237 | lr 3.00e-04 | grad 2.22 | tok/s 16384
step   2270 | loss 1.9382 | lr 3.00e-04 | grad 3.00 | tok/s 16644
step   2280 | loss 1.4648 | lr 3.00e-04 | grad 2.36 | tok/s 16599
step   2290 | loss 1.2941 | lr 3.00e-04 | grad 2.73 | tok/s 16225
step   2300 | loss 1.6736 | lr 3.00e-04 | grad 4.56 | tok/s 16580
step   2310 | loss 1.3822 | lr 3.00e-04 | grad 1.68 | tok/s 15717
step   2320 | loss 1.6403 | lr 3.00e-04 | grad 4.22 | tok/s 15785
step   2330 | loss 1.8397 | lr 3.00e-04 | grad 2.05 | tok/s 16065
step   2340 | loss 1.4300 | lr 3.00e-04 | grad 5.62 | tok/s 15638
step   2350 | loss 1.3522 | lr 3.00e-04 | grad 4.56 | tok/s 16411
step   2360 | loss 1.3229 | lr 3.00e-04 | grad 2.33 | tok/s 16620
step   2370 | loss 1.4778 | lr 3.00e-04 | grad 3.61 | tok/s 16452
step   2380 | loss 1.5088 | lr 3.00e-04 | grad 2.39 | tok/s 16814

Training complete! Final step: 2380
