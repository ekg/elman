Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_18/levelE88_100m_20260126_144923
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 471,880,378 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2249 | lr 3.00e-04 | grad 16.62 | tok/s 8747
step     20 | loss 2.8160 | lr 3.00e-04 | grad 6.25 | tok/s 15342
step     30 | loss 3.1550 | lr 3.00e-04 | grad 8.81 | tok/s 16172
step     40 | loss 4.5329 | lr 3.00e-04 | grad 69.00 | tok/s 16527
step     50 | loss 5.0153 | lr 3.00e-04 | grad 26.62 | tok/s 16657
step     60 | loss 4.0314 | lr 3.00e-04 | grad 23.75 | tok/s 16597
step     70 | loss 3.1688 | lr 3.00e-04 | grad 11.69 | tok/s 16578
step     80 | loss 2.8369 | lr 3.00e-04 | grad 9.44 | tok/s 16562
step     90 | loss 2.5827 | lr 3.00e-04 | grad 7.53 | tok/s 16525
step    100 | loss 2.3882 | lr 3.00e-04 | grad 4.34 | tok/s 16511
step    110 | loss 2.3467 | lr 3.00e-04 | grad 3.81 | tok/s 16390
step    120 | loss 2.7709 | lr 3.00e-04 | grad 2.39 | tok/s 15589
step    130 | loss 2.1457 | lr 3.00e-04 | grad 6.88 | tok/s 15609
step    140 | loss 2.4011 | lr 3.00e-04 | grad 9.44 | tok/s 16040
step    150 | loss 1.5691 | lr 3.00e-04 | grad 5.53 | tok/s 16395
step    160 | loss 2.3487 | lr 3.00e-04 | grad 2.69 | tok/s 15873
step    170 | loss 2.3195 | lr 3.00e-04 | grad 2.05 | tok/s 15635
step    180 | loss 1.8741 | lr 3.00e-04 | grad 3.64 | tok/s 16010
step    190 | loss 1.9550 | lr 3.00e-04 | grad 2.56 | tok/s 15715
step    200 | loss 1.6885 | lr 3.00e-04 | grad 2.00 | tok/s 16448
step    210 | loss 1.9230 | lr 3.00e-04 | grad 5.69 | tok/s 15586
step    220 | loss 2.2358 | lr 3.00e-04 | grad 2.95 | tok/s 15756
step    230 | loss 1.9741 | lr 3.00e-04 | grad 3.20 | tok/s 15740
step    240 | loss 2.3133 | lr 3.00e-04 | grad 6.00 | tok/s 15947
step    250 | loss 1.7979 | lr 3.00e-04 | grad 1.80 | tok/s 15849
step    260 | loss 1.9272 | lr 3.00e-04 | grad 3.50 | tok/s 16290
step    270 | loss 1.8515 | lr 3.00e-04 | grad 2.08 | tok/s 15919
step    280 | loss 1.8033 | lr 3.00e-04 | grad 2.02 | tok/s 14676
step    290 | loss 1.7000 | lr 3.00e-04 | grad 2.42 | tok/s 15424
step    300 | loss 1.9961 | lr 3.00e-04 | grad 2.22 | tok/s 15573
step    310 | loss 1.6931 | lr 3.00e-04 | grad 2.02 | tok/s 15511
step    320 | loss 1.9149 | lr 3.00e-04 | grad 3.50 | tok/s 15660
step    330 | loss 1.7531 | lr 3.00e-04 | grad 1.95 | tok/s 15813
step    340 | loss 2.0842 | lr 3.00e-04 | grad 2.33 | tok/s 15790
step    350 | loss 1.7551 | lr 3.00e-04 | grad 2.08 | tok/s 16212
step    360 | loss 1.6086 | lr 3.00e-04 | grad 2.30 | tok/s 15499
step    370 | loss 1.5053 | lr 3.00e-04 | grad 1.80 | tok/s 16368
step    380 | loss 1.2425 | lr 3.00e-04 | grad 1.77 | tok/s 16465
step    390 | loss 1.1422 | lr 3.00e-04 | grad 1.62 | tok/s 16497
step    400 | loss 1.7799 | lr 3.00e-04 | grad 1.95 | tok/s 15600
step    410 | loss 1.7873 | lr 3.00e-04 | grad 2.47 | tok/s 15786
step    420 | loss 1.6380 | lr 3.00e-04 | grad 3.30 | tok/s 15877
step    430 | loss 1.6486 | lr 3.00e-04 | grad 1.98 | tok/s 16150
step    440 | loss 1.7277 | lr 3.00e-04 | grad 2.31 | tok/s 15675
step    450 | loss 1.6580 | lr 3.00e-04 | grad 1.64 | tok/s 15853
step    460 | loss 1.6267 | lr 3.00e-04 | grad 2.14 | tok/s 16080
step    470 | loss 1.5944 | lr 3.00e-04 | grad 3.92 | tok/s 15985
step    480 | loss 1.6177 | lr 3.00e-04 | grad 2.91 | tok/s 16333
step    490 | loss 1.7357 | lr 3.00e-04 | grad 2.55 | tok/s 15685
step    500 | loss 1.8412 | lr 3.00e-04 | grad 1.97 | tok/s 15943
step    510 | loss 1.7061 | lr 3.00e-04 | grad 1.61 | tok/s 15218
step    520 | loss 1.5569 | lr 3.00e-04 | grad 2.23 | tok/s 15911
step    530 | loss 1.7455 | lr 3.00e-04 | grad 2.16 | tok/s 15650
step    540 | loss 1.6185 | lr 3.00e-04 | grad 1.73 | tok/s 15285
step    550 | loss 1.3937 | lr 3.00e-04 | grad 2.77 | tok/s 15968
step    560 | loss 1.4603 | lr 3.00e-04 | grad 1.82 | tok/s 16183
step    570 | loss 1.3652 | lr 3.00e-04 | grad 1.80 | tok/s 16470
step    580 | loss 1.3239 | lr 3.00e-04 | grad 1.43 | tok/s 16455
step    590 | loss 1.3527 | lr 3.00e-04 | grad 1.43 | tok/s 16508
step    600 | loss 1.2918 | lr 3.00e-04 | grad 1.70 | tok/s 16521
step    610 | loss 1.3230 | lr 3.00e-04 | grad 1.58 | tok/s 16518
step    620 | loss 1.3141 | lr 3.00e-04 | grad 1.77 | tok/s 16456
step    630 | loss 1.6998 | lr 3.00e-04 | grad 4.91 | tok/s 15517
step    640 | loss 1.7786 | lr 3.00e-04 | grad 1.94 | tok/s 15738
step    650 | loss 1.5841 | lr 3.00e-04 | grad 1.88 | tok/s 15728
step    660 | loss 1.6238 | lr 3.00e-04 | grad 1.81 | tok/s 16337
step    670 | loss 1.6616 | lr 3.00e-04 | grad 5.22 | tok/s 15785
step    680 | loss 1.6674 | lr 3.00e-04 | grad 2.09 | tok/s 15536
step    690 | loss 1.6138 | lr 3.00e-04 | grad 1.98 | tok/s 15417
step    700 | loss 1.5096 | lr 3.00e-04 | grad 1.47 | tok/s 15754
step    710 | loss 1.6846 | lr 3.00e-04 | grad 2.84 | tok/s 14960
step    720 | loss 1.3346 | lr 3.00e-04 | grad 1.66 | tok/s 16095
step    730 | loss 1.5107 | lr 3.00e-04 | grad 1.58 | tok/s 15819
step    740 | loss 1.8154 | lr 3.00e-04 | grad 3.78 | tok/s 16291
step    750 | loss 1.5597 | lr 3.00e-04 | grad 1.56 | tok/s 16487
step    760 | loss 1.5752 | lr 3.00e-04 | grad 3.23 | tok/s 16123
step    770 | loss 1.6190 | lr 3.00e-04 | grad 2.00 | tok/s 15859
step    780 | loss 1.5124 | lr 3.00e-04 | grad 1.97 | tok/s 15956
step    790 | loss 1.6775 | lr 3.00e-04 | grad 4.97 | tok/s 16327
step    800 | loss 1.3513 | lr 3.00e-04 | grad 1.29 | tok/s 16007
step    810 | loss 1.3411 | lr 3.00e-04 | grad 2.91 | tok/s 15470
step    820 | loss 1.4463 | lr 3.00e-04 | grad 2.14 | tok/s 15805
step    830 | loss 1.5230 | lr 3.00e-04 | grad 1.45 | tok/s 15588
step    840 | loss 1.6549 | lr 3.00e-04 | grad 1.66 | tok/s 15517
step    850 | loss 1.5993 | lr 3.00e-04 | grad 1.65 | tok/s 15694
step    860 | loss 1.6223 | lr 3.00e-04 | grad 2.48 | tok/s 16070
step    870 | loss 1.4373 | lr 3.00e-04 | grad 1.94 | tok/s 16161
step    880 | loss 1.6241 | lr 3.00e-04 | grad 1.80 | tok/s 15890
step    890 | loss 1.5186 | lr 3.00e-04 | grad 1.43 | tok/s 15840
step    900 | loss 1.5685 | lr 3.00e-04 | grad 1.73 | tok/s 15779
step    910 | loss 1.5781 | lr 3.00e-04 | grad 6.16 | tok/s 15613
step    920 | loss 1.5170 | lr 3.00e-04 | grad 1.75 | tok/s 15748
step    930 | loss 1.4132 | lr 3.00e-04 | grad 1.95 | tok/s 15943
step    940 | loss 1.3850 | lr 3.00e-04 | grad 1.94 | tok/s 15568
step    950 | loss 1.5258 | lr 3.00e-04 | grad 2.30 | tok/s 15317
step    960 | loss 1.4774 | lr 3.00e-04 | grad 1.43 | tok/s 15747
step    970 | loss 1.5012 | lr 3.00e-04 | grad 1.62 | tok/s 15780
step    980 | loss 1.9515 | lr 3.00e-04 | grad 3.38 | tok/s 16424
step    990 | loss 1.6102 | lr 3.00e-04 | grad 1.76 | tok/s 15313
step   1000 | loss 1.6211 | lr 3.00e-04 | grad 1.74 | tok/s 15799
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6211.pt
step   1010 | loss 1.3925 | lr 3.00e-04 | grad 2.47 | tok/s 9655
step   1020 | loss 1.2367 | lr 3.00e-04 | grad 1.42 | tok/s 16598
step   1030 | loss 1.5738 | lr 3.00e-04 | grad 1.93 | tok/s 15749
step   1040 | loss 2.1972 | lr 3.00e-04 | grad 3.25 | tok/s 16094
step   1050 | loss 1.5274 | lr 3.00e-04 | grad 2.88 | tok/s 16235
step   1060 | loss 1.1927 | lr 3.00e-04 | grad 2.31 | tok/s 15983
step   1070 | loss 1.4749 | lr 3.00e-04 | grad 1.70 | tok/s 15952
step   1080 | loss 1.2978 | lr 3.00e-04 | grad 1.45 | tok/s 16515
step   1090 | loss 1.2505 | lr 3.00e-04 | grad 1.28 | tok/s 16506
step   1100 | loss 1.2390 | lr 3.00e-04 | grad 1.23 | tok/s 16520
step   1110 | loss 1.1780 | lr 3.00e-04 | grad 1.30 | tok/s 16519
step   1120 | loss 1.4799 | lr 3.00e-04 | grad 4.22 | tok/s 16064
step   1130 | loss 1.7156 | lr 3.00e-04 | grad 1.60 | tok/s 16205
step   1140 | loss 1.7888 | lr 3.00e-04 | grad 1.94 | tok/s 15916
step   1150 | loss 1.6262 | lr 3.00e-04 | grad 2.20 | tok/s 15897
step   1160 | loss 1.7795 | lr 3.00e-04 | grad 2.27 | tok/s 15664
step   1170 | loss 1.5437 | lr 3.00e-04 | grad 2.05 | tok/s 15466
step   1180 | loss 1.3787 | lr 3.00e-04 | grad 2.94 | tok/s 16251

Training complete! Final step: 1184
