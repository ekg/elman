Using device: cuda
Output directory: benchmark_results/cmaes_e88_10min/e88_480M_15gen_20260126_142838/eval_41/levelE88_100m_20260126_152013
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 490,731,908 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 4.3219 | lr 3.00e-04 | grad 19.50 | tok/s 6002
step     20 | loss 2.6960 | lr 3.00e-04 | grad 9.50 | tok/s 14774
step     30 | loss 2.5548 | lr 3.00e-04 | grad 4.97 | tok/s 14929
step     40 | loss 2.3884 | lr 3.00e-04 | grad 4.00 | tok/s 14320
step     50 | loss 3.0654 | lr 3.00e-04 | grad 14.44 | tok/s 14529
step     60 | loss 2.0708 | lr 3.00e-04 | grad 4.16 | tok/s 14980
step     70 | loss 1.9033 | lr 3.00e-04 | grad 5.62 | tok/s 15118
step     80 | loss 6.0576 | lr 3.00e-04 | grad 141.00 | tok/s 15171
step     90 | loss 5.9567 | lr 3.00e-04 | grad 15.81 | tok/s 15418
step    100 | loss 4.7735 | lr 3.00e-04 | grad 12.25 | tok/s 15365
step    110 | loss 4.2594 | lr 3.00e-04 | grad 22.50 | tok/s 15361
step    120 | loss 3.6997 | lr 3.00e-04 | grad 20.88 | tok/s 15291
step    130 | loss 3.3827 | lr 3.00e-04 | grad 24.62 | tok/s 14312
step    140 | loss 2.8431 | lr 3.00e-04 | grad 14.25 | tok/s 15229
step    150 | loss 3.0767 | lr 3.00e-04 | grad 19.75 | tok/s 15156
step    160 | loss 2.4471 | lr 3.00e-04 | grad 17.50 | tok/s 15064
step    170 | loss 2.5225 | lr 3.00e-04 | grad 17.12 | tok/s 15015
step    180 | loss 2.3368 | lr 3.00e-04 | grad 7.53 | tok/s 15035
step    190 | loss 2.5724 | lr 3.00e-04 | grad 14.44 | tok/s 15008
step    200 | loss 2.2464 | lr 3.00e-04 | grad 10.25 | tok/s 14947
step    210 | loss 2.1471 | lr 3.00e-04 | grad 7.53 | tok/s 14900
step    220 | loss 2.2526 | lr 3.00e-04 | grad 3.12 | tok/s 14676
step    230 | loss 2.0780 | lr 3.00e-04 | grad 3.80 | tok/s 14462
step    240 | loss 2.2958 | lr 3.00e-04 | grad 4.53 | tok/s 13727
step    250 | loss 2.1191 | lr 3.00e-04 | grad 2.44 | tok/s 14109
step    260 | loss 1.5729 | lr 3.00e-04 | grad 2.75 | tok/s 14492
step    270 | loss 2.1054 | lr 3.00e-04 | grad 2.61 | tok/s 14265
step    280 | loss 2.2824 | lr 3.00e-04 | grad 4.78 | tok/s 13220
step    290 | loss 1.4782 | lr 3.00e-04 | grad 3.45 | tok/s 14681
step    300 | loss 0.5996 | lr 3.00e-04 | grad 2.98 | tok/s 14681
step    310 | loss 2.4116 | lr 3.00e-04 | grad 3.59 | tok/s 14413
step    320 | loss 1.9511 | lr 3.00e-04 | grad 5.59 | tok/s 14070
step    330 | loss 1.9662 | lr 3.00e-04 | grad 2.73 | tok/s 13551
step    340 | loss 2.2805 | lr 3.00e-04 | grad 2.64 | tok/s 13747
step    350 | loss 1.8950 | lr 3.00e-04 | grad 4.47 | tok/s 14079
step    360 | loss 1.2253 | lr 3.00e-04 | grad 6.03 | tok/s 14357
step    370 | loss 1.8135 | lr 3.00e-04 | grad 2.47 | tok/s 12992
step    380 | loss 1.7930 | lr 3.00e-04 | grad 2.36 | tok/s 13857
step    390 | loss 1.5528 | lr 3.00e-04 | grad 1.96 | tok/s 14435
step    400 | loss 1.5081 | lr 3.00e-04 | grad 2.41 | tok/s 14294
step    410 | loss 1.2881 | lr 3.00e-04 | grad 1.95 | tok/s 13973
step    420 | loss 1.8326 | lr 3.00e-04 | grad 4.19 | tok/s 12960
step    430 | loss 2.1835 | lr 3.00e-04 | grad 2.80 | tok/s 14189
step    440 | loss 2.1680 | lr 3.00e-04 | grad 3.95 | tok/s 13390
step    450 | loss 1.9108 | lr 3.00e-04 | grad 2.61 | tok/s 13866
step    460 | loss 1.7428 | lr 3.00e-04 | grad 2.81 | tok/s 13571
step    470 | loss 1.8480 | lr 3.00e-04 | grad 2.19 | tok/s 13993
step    480 | loss 2.2620 | lr 3.00e-04 | grad 6.34 | tok/s 13979
step    490 | loss 1.8024 | lr 3.00e-04 | grad 2.33 | tok/s 13172
step    500 | loss 1.6998 | lr 3.00e-04 | grad 3.11 | tok/s 14061
step    510 | loss 1.7259 | lr 3.00e-04 | grad 2.20 | tok/s 14230
step    520 | loss 1.6733 | lr 3.00e-04 | grad 1.95 | tok/s 14217
step    530 | loss 1.9140 | lr 3.00e-04 | grad 2.36 | tok/s 13647
step    540 | loss 1.7528 | lr 3.00e-04 | grad 2.06 | tok/s 13656
step    550 | loss 1.5792 | lr 3.00e-04 | grad 2.69 | tok/s 13385
step    560 | loss 1.7446 | lr 3.00e-04 | grad 2.44 | tok/s 12593
step    570 | loss 1.6781 | lr 3.00e-04 | grad 3.42 | tok/s 13385
step    580 | loss 1.5585 | lr 3.00e-04 | grad 1.99 | tok/s 13311
step    590 | loss 1.8742 | lr 3.00e-04 | grad 2.91 | tok/s 13658
step    600 | loss 1.8350 | lr 3.00e-04 | grad 2.16 | tok/s 13194
step    610 | loss 1.6367 | lr 3.00e-04 | grad 2.19 | tok/s 13882
step    620 | loss 1.5566 | lr 3.00e-04 | grad 2.27 | tok/s 13151
step    630 | loss 1.6729 | lr 3.00e-04 | grad 4.03 | tok/s 13235
step    640 | loss 1.8244 | lr 3.00e-04 | grad 2.30 | tok/s 13598
step    650 | loss 1.6859 | lr 3.00e-04 | grad 2.39 | tok/s 13653
step    660 | loss 1.7105 | lr 3.00e-04 | grad 1.97 | tok/s 13698
step    670 | loss 1.9370 | lr 3.00e-04 | grad 3.05 | tok/s 13799
step    680 | loss 1.7403 | lr 3.00e-04 | grad 2.30 | tok/s 13515
step    690 | loss 1.8488 | lr 3.00e-04 | grad 3.03 | tok/s 13986
step    700 | loss 1.4437 | lr 3.00e-04 | grad 2.84 | tok/s 14281
step    710 | loss 1.5996 | lr 3.00e-04 | grad 2.20 | tok/s 12884
step    720 | loss 1.4849 | lr 3.00e-04 | grad 3.03 | tok/s 13128
step    730 | loss 1.3078 | lr 3.00e-04 | grad 2.56 | tok/s 14229
step    740 | loss 1.5114 | lr 3.00e-04 | grad 2.22 | tok/s 14022
step    750 | loss 1.2184 | lr 3.00e-04 | grad 2.48 | tok/s 14227
step    760 | loss 1.1234 | lr 3.00e-04 | grad 2.09 | tok/s 14238
step    770 | loss 1.0738 | lr 3.00e-04 | grad 1.91 | tok/s 14251
step    780 | loss 1.0055 | lr 3.00e-04 | grad 1.95 | tok/s 14241
step    790 | loss 1.1403 | lr 3.00e-04 | grad 3.00 | tok/s 13797
step    800 | loss 1.8287 | lr 3.00e-04 | grad 5.12 | tok/s 13753
step    810 | loss 1.7154 | lr 3.00e-04 | grad 2.00 | tok/s 13682
step    820 | loss 1.7268 | lr 3.00e-04 | grad 3.66 | tok/s 13146
step    830 | loss 1.5026 | lr 3.00e-04 | grad 2.14 | tok/s 14080
step    840 | loss 1.3757 | lr 3.00e-04 | grad 2.11 | tok/s 14246
step    850 | loss 1.6024 | lr 3.00e-04 | grad 1.96 | tok/s 13694
step    860 | loss 1.4954 | lr 3.00e-04 | grad 3.27 | tok/s 14026
step    870 | loss 1.5155 | lr 3.00e-04 | grad 2.48 | tok/s 13515
step    880 | loss 1.6848 | lr 3.00e-04 | grad 2.47 | tok/s 13565
step    890 | loss 1.6954 | lr 3.00e-04 | grad 2.77 | tok/s 13773
step    900 | loss 1.5823 | lr 3.00e-04 | grad 2.38 | tok/s 13805
step    910 | loss 1.4377 | lr 3.00e-04 | grad 3.66 | tok/s 13511
step    920 | loss 1.5308 | lr 3.00e-04 | grad 3.36 | tok/s 14038
step    930 | loss 1.6073 | lr 3.00e-04 | grad 3.25 | tok/s 13414
step    940 | loss 1.3950 | lr 3.00e-04 | grad 1.80 | tok/s 14146
step    950 | loss 1.4874 | lr 3.00e-04 | grad 2.62 | tok/s 14201
step    960 | loss 1.3270 | lr 3.00e-04 | grad 2.36 | tok/s 14207
step    970 | loss 1.7498 | lr 3.00e-04 | grad 3.36 | tok/s 13384
step    980 | loss 1.6574 | lr 3.00e-04 | grad 2.31 | tok/s 13752
step    990 | loss 1.4582 | lr 3.00e-04 | grad 2.03 | tok/s 13675
step   1000 | loss 1.8455 | lr 3.00e-04 | grad 6.97 | tok/s 13432
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8455.pt
step   1010 | loss 1.6440 | lr 3.00e-04 | grad 3.23 | tok/s 6781
step   1020 | loss 1.6599 | lr 3.00e-04 | grad 1.88 | tok/s 13152
step   1030 | loss 1.4694 | lr 3.00e-04 | grad 1.99 | tok/s 13689
step   1040 | loss 1.4881 | lr 3.00e-04 | grad 2.08 | tok/s 14129
step   1050 | loss 1.6370 | lr 3.00e-04 | grad 3.05 | tok/s 13064
step   1060 | loss 1.7422 | lr 3.00e-04 | grad 3.19 | tok/s 14087
step   1070 | loss 1.6489 | lr 3.00e-04 | grad 2.48 | tok/s 14032
step   1080 | loss 1.4093 | lr 3.00e-04 | grad 1.84 | tok/s 12749
step   1090 | loss 1.1314 | lr 3.00e-04 | grad 1.17 | tok/s 14032
step   1100 | loss 1.4506 | lr 3.00e-04 | grad 3.16 | tok/s 13614
step   1110 | loss 1.4708 | lr 3.00e-04 | grad 1.92 | tok/s 14311
step   1120 | loss 1.3422 | lr 3.00e-04 | grad 1.95 | tok/s 14308
step   1130 | loss 1.2952 | lr 3.00e-04 | grad 1.77 | tok/s 14325
step   1140 | loss 1.2832 | lr 3.00e-04 | grad 1.99 | tok/s 13692
step   1150 | loss 1.3018 | lr 3.00e-04 | grad 1.72 | tok/s 14318
step   1160 | loss 1.2147 | lr 3.00e-04 | grad 1.74 | tok/s 14305
step   1170 | loss 1.2439 | lr 3.00e-04 | grad 2.00 | tok/s 14298
step   1180 | loss 1.3437 | lr 3.00e-04 | grad 1.62 | tok/s 14312
step   1190 | loss 1.2240 | lr 3.00e-04 | grad 2.09 | tok/s 14302
step   1200 | loss 1.2212 | lr 3.00e-04 | grad 1.95 | tok/s 14292
step   1210 | loss 1.2713 | lr 3.00e-04 | grad 1.94 | tok/s 14297
step   1220 | loss 1.2951 | lr 3.00e-04 | grad 1.87 | tok/s 14274
step   1230 | loss 1.2588 | lr 3.00e-04 | grad 1.69 | tok/s 14288
step   1240 | loss 1.2229 | lr 3.00e-04 | grad 1.49 | tok/s 14287
step   1250 | loss 1.8067 | lr 3.00e-04 | grad 3.47 | tok/s 13501
step   1260 | loss 1.4006 | lr 3.00e-04 | grad 2.31 | tok/s 13382
step   1270 | loss 1.6624 | lr 3.00e-04 | grad 4.62 | tok/s 13351
step   1280 | loss 1.6309 | lr 3.00e-04 | grad 1.70 | tok/s 13438
step   1290 | loss 1.4831 | lr 3.00e-04 | grad 1.97 | tok/s 13634
step   1300 | loss 1.5291 | lr 3.00e-04 | grad 2.41 | tok/s 13752
step   1310 | loss 1.4662 | lr 3.00e-04 | grad 2.22 | tok/s 14001
step   1320 | loss 1.5960 | lr 3.00e-04 | grad 2.17 | tok/s 14033
step   1330 | loss 1.6187 | lr 3.00e-04 | grad 2.33 | tok/s 14058
step   1340 | loss 1.5013 | lr 3.00e-04 | grad 8.88 | tok/s 13408
step   1350 | loss 1.7347 | lr 3.00e-04 | grad 2.30 | tok/s 12930
step   1360 | loss 1.5011 | lr 3.00e-04 | grad 2.31 | tok/s 13753
step   1370 | loss 1.4187 | lr 3.00e-04 | grad 1.54 | tok/s 13571
step   1380 | loss 1.6603 | lr 3.00e-04 | grad 2.25 | tok/s 13037
step   1390 | loss 1.5281 | lr 3.00e-04 | grad 1.63 | tok/s 13841
step   1400 | loss 1.4158 | lr 3.00e-04 | grad 1.69 | tok/s 13355
step   1410 | loss 1.4680 | lr 3.00e-04 | grad 2.97 | tok/s 13401
step   1420 | loss 1.6908 | lr 3.00e-04 | grad 3.81 | tok/s 12978
step   1430 | loss 1.3568 | lr 3.00e-04 | grad 1.91 | tok/s 13674
step   1440 | loss 1.1519 | lr 3.00e-04 | grad 1.81 | tok/s 14126
step   1450 | loss 1.1723 | lr 3.00e-04 | grad 4.06 | tok/s 14216
step   1460 | loss 1.6810 | lr 3.00e-04 | grad 1.99 | tok/s 13405
step   1470 | loss 1.5314 | lr 3.00e-04 | grad 1.81 | tok/s 13887
step   1480 | loss 1.8248 | lr 3.00e-04 | grad 3.77 | tok/s 13985
step   1490 | loss 1.5678 | lr 3.00e-04 | grad 1.63 | tok/s 14202
step   1500 | loss 1.3462 | lr 3.00e-04 | grad 1.73 | tok/s 14262
step   1510 | loss 1.5145 | lr 3.00e-04 | grad 1.89 | tok/s 14070
step   1520 | loss 1.4397 | lr 3.00e-04 | grad 3.34 | tok/s 13791
step   1530 | loss 1.4578 | lr 3.00e-04 | grad 1.63 | tok/s 14115
step   1540 | loss 1.6306 | lr 3.00e-04 | grad 2.33 | tok/s 13275
step   1550 | loss 1.2966 | lr 3.00e-04 | grad 2.23 | tok/s 14170
step   1560 | loss 1.5879 | lr 3.00e-04 | grad 2.55 | tok/s 13147
step   1570 | loss 1.2876 | lr 3.00e-04 | grad 2.14 | tok/s 14286
step   1580 | loss 1.7234 | lr 3.00e-04 | grad 3.83 | tok/s 13948
step   1590 | loss 1.6015 | lr 3.00e-04 | grad 2.34 | tok/s 13397
step   1600 | loss 1.0106 | lr 3.00e-04 | grad 1.42 | tok/s 14297
step   1610 | loss 1.0348 | lr 3.00e-04 | grad 2.39 | tok/s 13826
step   1620 | loss 1.4183 | lr 3.00e-04 | grad 2.73 | tok/s 12955
step   1630 | loss 1.3448 | lr 3.00e-04 | grad 2.03 | tok/s 13863
step   1640 | loss 1.3488 | lr 3.00e-04 | grad 2.27 | tok/s 13531
step   1650 | loss 1.5337 | lr 3.00e-04 | grad 2.39 | tok/s 12966
step   1660 | loss 1.3998 | lr 3.00e-04 | grad 1.67 | tok/s 13845
step   1670 | loss 1.3701 | lr 3.00e-04 | grad 6.16 | tok/s 13798
step   1680 | loss 1.7348 | lr 3.00e-04 | grad 1.87 | tok/s 13254
step   1690 | loss 1.5023 | lr 3.00e-04 | grad 3.97 | tok/s 13506
step   1700 | loss 1.5141 | lr 3.00e-04 | grad 2.20 | tok/s 13788
step   1710 | loss 1.4315 | lr 3.00e-04 | grad 2.11 | tok/s 13074
step   1720 | loss 1.5313 | lr 3.00e-04 | grad 2.75 | tok/s 14125
step   1730 | loss 1.2308 | lr 3.00e-04 | grad 2.73 | tok/s 14256
step   1740 | loss 1.3437 | lr 3.00e-04 | grad 2.61 | tok/s 13902
step   1750 | loss 1.5872 | lr 3.00e-04 | grad 2.41 | tok/s 13650
step   1760 | loss 1.5518 | lr 3.00e-04 | grad 2.22 | tok/s 13693
step   1770 | loss 1.4380 | lr 3.00e-04 | grad 2.22 | tok/s 13442
step   1780 | loss 1.5137 | lr 3.00e-04 | grad 1.74 | tok/s 13988
step   1790 | loss 1.4107 | lr 3.00e-04 | grad 1.63 | tok/s 13612
step   1800 | loss 1.6279 | lr 3.00e-04 | grad 2.62 | tok/s 13723
step   1810 | loss 1.4596 | lr 3.00e-04 | grad 1.98 | tok/s 13236
step   1820 | loss 1.4814 | lr 3.00e-04 | grad 4.88 | tok/s 13430
step   1830 | loss 1.4206 | lr 3.00e-04 | grad 2.31 | tok/s 13970
step   1840 | loss 1.5479 | lr 3.00e-04 | grad 2.58 | tok/s 13367
step   1850 | loss 1.2983 | lr 3.00e-04 | grad 1.68 | tok/s 13691
step   1860 | loss 1.3325 | lr 3.00e-04 | grad 2.17 | tok/s 13538
step   1870 | loss 1.4233 | lr 3.00e-04 | grad 2.88 | tok/s 13588
step   1880 | loss 1.2382 | lr 3.00e-04 | grad 2.12 | tok/s 13301
step   1890 | loss 1.5226 | lr 3.00e-04 | grad 1.76 | tok/s 12671
step   1900 | loss 1.4025 | lr 3.00e-04 | grad 2.42 | tok/s 13690
step   1910 | loss 1.4801 | lr 3.00e-04 | grad 2.75 | tok/s 12976
step   1920 | loss 1.4002 | lr 3.00e-04 | grad 1.91 | tok/s 14220
step   1930 | loss 1.4688 | lr 3.00e-04 | grad 2.42 | tok/s 13350
step   1940 | loss 1.4486 | lr 3.00e-04 | grad 1.97 | tok/s 13897
step   1950 | loss 1.8622 | lr 3.00e-04 | grad 3.55 | tok/s 14088
step   1960 | loss 1.4746 | lr 3.00e-04 | grad 4.12 | tok/s 14242
step   1970 | loss 1.5335 | lr 3.00e-04 | grad 2.31 | tok/s 13894
step   1980 | loss 1.5545 | lr 3.00e-04 | grad 2.03 | tok/s 13275
step   1990 | loss 1.6124 | lr 3.00e-04 | grad 8.75 | tok/s 13071
step   2000 | loss 1.4940 | lr 3.00e-04 | grad 2.00 | tok/s 13731
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4940.pt
step   2010 | loss 1.0981 | lr 3.00e-04 | grad 1.66 | tok/s 7342
step   2020 | loss 1.3127 | lr 3.00e-04 | grad 2.00 | tok/s 13673
step   2030 | loss 1.1053 | lr 3.00e-04 | grad 0.86 | tok/s 14375
step   2040 | loss 1.2397 | lr 3.00e-04 | grad 2.06 | tok/s 14331

Training complete! Final step: 2047
