Loading data from data/fineweb_100mb.txt...

Creating 1 model with ~50m parameters...
Created Level 1 model: dim=512, depth=21, params=49,714,944

============================================================
Training: 1 (steps=100)
Parameters: 49.71M
Vocab size: 256
============================================================
[1] step    1 | loss 5.6562 | ppl 286.1 | grad 8.25 | 5171 tok/s | 6.3s | 6335ms/step
[1] step   20 | loss 3.1250 | ppl 22.8 | grad 2.98 | 6125 tok/s | 107.0s | 5227ms/step
[1] step   40 | loss 2.2812 | ppl 9.8 | grad 2.66 | 6142 tok/s | 213.4s | 5229ms/step
[1] step   60 | loss 1.8984 | ppl 6.7 | grad 2.50 | 6151 tok/s | 319.6s | 5327ms/step
[1] step   80 | loss 1.8516 | ppl 6.4 | grad 3.11 | 6156 tok/s | 425.8s | 5324ms/step
[1] step  100 | loss 1.7266 | ppl 5.6 | grad 3.12 | 6160 tok/s | 532.0s | 5334ms/step

1 Final: loss=2.7565, grad=3.77, steps=100, tokens=3,276,800, time=532.0s

==========================================================================================
BENCHMARK SUMMARY
==========================================================================================
Model           Params       Loss       Steps    Tokens       tok/s      Time    
------------------------------------------------------------------------------------------
1               49.71M       2.7565     100      3,276,800    6160       532.0   s

Results saved to: benchmark_results
