Using device: cuda
Output directory: benchmark_results/e75mh_100m_20260119_150335/E75h2/levelE75h2_100m_20260119_150342
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75h2, 58,064,256 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.8752 | lr 2.70e-06 | grad 372.00 | tok/s 19788
step     20 | loss 5.8210 | lr 5.70e-06 | grad 66.50 | tok/s 46466
step     30 | loss 5.8597 | lr 8.70e-06 | grad 82.50 | tok/s 49213
step     40 | loss 5.8506 | lr 1.17e-05 | grad 73.00 | tok/s 49174
step     50 | loss 5.8277 | lr 1.47e-05 | grad 94.00 | tok/s 49178
step     60 | loss 5.7950 | lr 1.77e-05 | grad 258.00 | tok/s 48172
step     70 | loss 5.7276 | lr 2.07e-05 | grad 112.00 | tok/s 46452
step     80 | loss 5.4759 | lr 2.37e-05 | grad 60.00 | tok/s 48145
step     90 | loss 5.4650 | lr 2.67e-05 | grad 113.50 | tok/s 46469
step    100 | loss 5.3147 | lr 2.97e-05 | grad 60.00 | tok/s 46925
step    110 | loss 5.0536 | lr 3.27e-05 | grad 344.00 | tok/s 46234
step    120 | loss 4.9070 | lr 3.57e-05 | grad 358.00 | tok/s 45619
step    130 | loss 4.6823 | lr 3.87e-05 | grad 42.00 | tok/s 46635
step    140 | loss 4.1244 | lr 4.17e-05 | grad 174.00 | tok/s 46712
step    150 | loss 3.8491 | lr 4.47e-05 | grad 56.00 | tok/s 44552
step    160 | loss 3.6180 | lr 4.77e-05 | grad 17.88 | tok/s 44949
step    170 | loss 3.5496 | lr 5.07e-05 | grad 25.00 | tok/s 46495
step    180 | loss 3.5183 | lr 5.37e-05 | grad 7.34 | tok/s 46627
step    190 | loss 3.3287 | lr 5.67e-05 | grad 7.16 | tok/s 47374
step    200 | loss 3.2102 | lr 5.97e-05 | grad 6.78 | tok/s 48378
step    210 | loss 3.1618 | lr 6.27e-05 | grad 10.69 | tok/s 46246
step    220 | loss 3.1811 | lr 6.57e-05 | grad 4.28 | tok/s 47724
step    230 | loss 2.9083 | lr 6.87e-05 | grad 5.53 | tok/s 46125
step    240 | loss 2.9911 | lr 7.17e-05 | grad 4.75 | tok/s 47046
step    250 | loss 2.7773 | lr 7.47e-05 | grad 3.42 | tok/s 46344
step    260 | loss 2.7458 | lr 7.77e-05 | grad 3.55 | tok/s 44496
step    270 | loss 2.6147 | lr 8.07e-05 | grad 5.09 | tok/s 46144
step    280 | loss 2.7435 | lr 8.37e-05 | grad 5.97 | tok/s 46102
step    290 | loss 2.4325 | lr 8.67e-05 | grad 2.95 | tok/s 48586
step    300 | loss 2.2855 | lr 8.97e-05 | grad 3.03 | tok/s 48577
step    310 | loss 2.1854 | lr 9.27e-05 | grad 2.56 | tok/s 48581
step    320 | loss 2.2983 | lr 9.57e-05 | grad 4.94 | tok/s 46772
step    330 | loss 2.4392 | lr 9.87e-05 | grad 4.97 | tok/s 45637
step    340 | loss 2.3918 | lr 1.02e-04 | grad 5.31 | tok/s 46589
step    350 | loss 2.3767 | lr 1.05e-04 | grad 3.30 | tok/s 45262
step    360 | loss 2.3537 | lr 1.08e-04 | grad 6.41 | tok/s 45825
step    370 | loss 2.2065 | lr 1.11e-04 | grad 2.97 | tok/s 46662
step    380 | loss 2.6904 | lr 1.14e-04 | grad 3.55 | tok/s 47847
step    390 | loss 2.2845 | lr 1.17e-04 | grad 2.50 | tok/s 46037
step    400 | loss 2.3478 | lr 1.20e-04 | grad 6.12 | tok/s 47282
step    410 | loss 2.1117 | lr 1.23e-04 | grad 4.16 | tok/s 45804
step    420 | loss 2.2230 | lr 1.26e-04 | grad 2.83 | tok/s 45545
step    430 | loss 2.3467 | lr 1.29e-04 | grad 3.31 | tok/s 45378
step    440 | loss 2.4590 | lr 1.32e-04 | grad 2.89 | tok/s 47176
step    450 | loss 2.1500 | lr 1.35e-04 | grad 2.47 | tok/s 45872
step    460 | loss 2.1511 | lr 1.38e-04 | grad 2.34 | tok/s 46022
step    470 | loss 2.1348 | lr 1.41e-04 | grad 2.86 | tok/s 46596
step    480 | loss 2.0386 | lr 1.44e-04 | grad 1.83 | tok/s 44955
step    490 | loss 1.9378 | lr 1.47e-04 | grad 1.71 | tok/s 45801
step    500 | loss 2.8671 | lr 1.50e-04 | grad 3.17 | tok/s 47174
step    510 | loss 2.0480 | lr 1.53e-04 | grad 2.98 | tok/s 46114
step    520 | loss 2.0334 | lr 1.56e-04 | grad 2.25 | tok/s 47592
step    530 | loss 2.5654 | lr 1.59e-04 | grad 4.81 | tok/s 46641
step    540 | loss 2.0578 | lr 1.62e-04 | grad 8.38 | tok/s 46525
step    550 | loss 1.8967 | lr 1.65e-04 | grad 1.75 | tok/s 47838
step    560 | loss 1.6668 | lr 1.68e-04 | grad 1.97 | tok/s 48481
step    570 | loss 2.0303 | lr 1.71e-04 | grad 3.66 | tok/s 47394
step    580 | loss 2.3728 | lr 1.74e-04 | grad 2.41 | tok/s 46730
step    590 | loss 2.5949 | lr 1.77e-04 | grad 3.61 | tok/s 45796
step    600 | loss 2.1161 | lr 1.80e-04 | grad 2.61 | tok/s 45930
step    610 | loss 2.1386 | lr 1.83e-04 | grad 1.64 | tok/s 48129
step    620 | loss 1.9725 | lr 1.86e-04 | grad 1.97 | tok/s 45669
step    630 | loss 1.9236 | lr 1.89e-04 | grad 1.84 | tok/s 47097
step    640 | loss 2.3097 | lr 1.92e-04 | grad 1.59 | tok/s 47149
step    650 | loss 1.9679 | lr 1.95e-04 | grad 2.50 | tok/s 46267
step    660 | loss 2.2595 | lr 1.98e-04 | grad 6.59 | tok/s 45641
step    670 | loss 2.1402 | lr 2.01e-04 | grad 3.03 | tok/s 47245
step    680 | loss 2.0960 | lr 2.04e-04 | grad 2.75 | tok/s 45663
step    690 | loss 2.0977 | lr 2.07e-04 | grad 2.08 | tok/s 45938
step    700 | loss 2.1615 | lr 2.10e-04 | grad 1.80 | tok/s 46145
step    710 | loss 2.0581 | lr 2.13e-04 | grad 1.77 | tok/s 46481
step    720 | loss 2.0217 | lr 2.16e-04 | grad 5.25 | tok/s 46496
step    730 | loss 2.1993 | lr 2.19e-04 | grad 2.02 | tok/s 46922
step    740 | loss 2.1133 | lr 2.22e-04 | grad 2.77 | tok/s 46456
step    750 | loss 1.8755 | lr 2.25e-04 | grad 1.98 | tok/s 45988
step    760 | loss 2.2706 | lr 2.28e-04 | grad 1.38 | tok/s 46515
step    770 | loss 1.8646 | lr 2.31e-04 | grad 1.81 | tok/s 46066
step    780 | loss 1.9319 | lr 2.34e-04 | grad 1.79 | tok/s 46539
step    790 | loss 1.8659 | lr 2.37e-04 | grad 1.16 | tok/s 46937
step    800 | loss 1.8153 | lr 2.40e-04 | grad 1.52 | tok/s 46967
step    810 | loss 1.9109 | lr 2.43e-04 | grad 3.34 | tok/s 46463
step    820 | loss 2.5801 | lr 2.46e-04 | grad 2.19 | tok/s 47719
step    830 | loss 2.0201 | lr 2.49e-04 | grad 1.02 | tok/s 48470
step    840 | loss 1.6935 | lr 2.52e-04 | grad 0.91 | tok/s 48459
step    850 | loss 2.2546 | lr 2.55e-04 | grad 1.60 | tok/s 46139
step    860 | loss 1.9826 | lr 2.58e-04 | grad 1.34 | tok/s 45162
step    870 | loss 1.8645 | lr 2.61e-04 | grad 1.13 | tok/s 46502
step    880 | loss 1.9381 | lr 2.64e-04 | grad 1.55 | tok/s 46296
step    890 | loss 1.8063 | lr 2.67e-04 | grad 1.16 | tok/s 46233
step    900 | loss 2.2862 | lr 2.70e-04 | grad 1.66 | tok/s 45062
step    910 | loss 1.8358 | lr 2.73e-04 | grad 1.12 | tok/s 45884
step    920 | loss 1.8098 | lr 2.76e-04 | grad 1.12 | tok/s 45746
step    930 | loss 1.8913 | lr 2.79e-04 | grad 1.66 | tok/s 45639
step    940 | loss 1.8019 | lr 2.82e-04 | grad 1.84 | tok/s 45169
step    950 | loss 1.9137 | lr 2.85e-04 | grad 1.50 | tok/s 46216
step    960 | loss 1.5744 | lr 2.88e-04 | grad 0.84 | tok/s 48461
step    970 | loss 1.4103 | lr 2.91e-04 | grad 0.61 | tok/s 48461
step    980 | loss 1.6022 | lr 2.94e-04 | grad 2.70 | tok/s 47093
step    990 | loss 2.0094 | lr 2.97e-04 | grad 1.02 | tok/s 45831
step   1000 | loss 1.8279 | lr 3.00e-04 | grad 0.78 | tok/s 44695
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8279.pt
step   1010 | loss 2.1407 | lr 1.06e-06 | grad 4.59 | tok/s 39499
step   1020 | loss 1.7757 | lr 1.27e-06 | grad 0.92 | tok/s 44146
step   1030 | loss 2.0947 | lr 1.62e-06 | grad 1.16 | tok/s 43428
step   1040 | loss 1.7453 | lr 2.12e-06 | grad 1.61 | tok/s 44377
step   1050 | loss 1.7932 | lr 2.77e-06 | grad 0.91 | tok/s 44478
step   1060 | loss 2.1505 | lr 3.56e-06 | grad 2.50 | tok/s 44649
step   1070 | loss 2.2450 | lr 4.50e-06 | grad 1.09 | tok/s 44732
step   1080 | loss 2.5021 | lr 5.58e-06 | grad 1.41 | tok/s 44184
step   1090 | loss 2.1906 | lr 6.81e-06 | grad 1.20 | tok/s 44534
step   1100 | loss 1.8245 | lr 8.17e-06 | grad 0.98 | tok/s 44230
step   1110 | loss 1.9245 | lr 9.68e-06 | grad 0.98 | tok/s 44982
step   1120 | loss 2.2754 | lr 1.13e-05 | grad 1.04 | tok/s 45501
step   1130 | loss 1.8465 | lr 1.31e-05 | grad 0.92 | tok/s 43278
step   1140 | loss 1.7350 | lr 1.50e-05 | grad 0.95 | tok/s 44372
step   1150 | loss 2.0116 | lr 1.71e-05 | grad 1.39 | tok/s 44276
step   1160 | loss 1.6637 | lr 1.93e-05 | grad 0.66 | tok/s 43790
step   1170 | loss 2.0687 | lr 2.16e-05 | grad 0.88 | tok/s 44301
step   1180 | loss 1.7776 | lr 2.40e-05 | grad 0.96 | tok/s 46535
step   1190 | loss 1.7380 | lr 2.66e-05 | grad 0.68 | tok/s 46576
step   1200 | loss 1.6851 | lr 2.93e-05 | grad 0.64 | tok/s 46593
step   1210 | loss 1.6650 | lr 3.21e-05 | grad 0.66 | tok/s 46569
step   1220 | loss 1.6643 | lr 3.50e-05 | grad 0.82 | tok/s 46201
step   1230 | loss 1.5960 | lr 3.80e-05 | grad 0.75 | tok/s 44533
step   1240 | loss 1.7226 | lr 4.12e-05 | grad 0.79 | tok/s 43815
step   1250 | loss 1.8673 | lr 4.45e-05 | grad 3.23 | tok/s 45159
step   1260 | loss 1.8627 | lr 4.78e-05 | grad 2.67 | tok/s 45115
step   1270 | loss 1.9376 | lr 5.13e-05 | grad 1.39 | tok/s 44539
step   1280 | loss 1.8161 | lr 5.48e-05 | grad 0.91 | tok/s 43987
step   1290 | loss 1.7243 | lr 5.85e-05 | grad 0.99 | tok/s 43817
step   1300 | loss 1.7716 | lr 6.22e-05 | grad 0.86 | tok/s 43581
step   1310 | loss 1.8527 | lr 6.61e-05 | grad 0.88 | tok/s 43548
step   1320 | loss 1.7956 | lr 7.00e-05 | grad 1.27 | tok/s 44329
step   1330 | loss 1.7139 | lr 7.40e-05 | grad 0.77 | tok/s 44439
step   1340 | loss 1.6474 | lr 7.81e-05 | grad 1.03 | tok/s 44466
step   1350 | loss 1.7335 | lr 8.22e-05 | grad 2.08 | tok/s 45576
step   1360 | loss 1.6168 | lr 8.64e-05 | grad 0.75 | tok/s 43310
step   1370 | loss 1.7662 | lr 9.07e-05 | grad 0.71 | tok/s 43925
step   1380 | loss 1.8095 | lr 9.50e-05 | grad 1.05 | tok/s 44306
step   1390 | loss 1.7092 | lr 9.94e-05 | grad 1.66 | tok/s 43374
step   1400 | loss 1.7300 | lr 1.04e-04 | grad 10.31 | tok/s 45017
step   1410 | loss 1.7590 | lr 1.08e-04 | grad 1.07 | tok/s 45408
step   1420 | loss 1.7801 | lr 1.13e-04 | grad 0.96 | tok/s 43215
step   1430 | loss 1.5903 | lr 1.17e-04 | grad 0.90 | tok/s 42241
step   1440 | loss 1.5393 | lr 1.22e-04 | grad 0.87 | tok/s 44558
step   1450 | loss 1.6126 | lr 1.27e-04 | grad 2.16 | tok/s 45689
step   1460 | loss 1.6545 | lr 1.31e-04 | grad 0.71 | tok/s 42685
step   1470 | loss 1.7654 | lr 1.36e-04 | grad 2.02 | tok/s 44218
step   1480 | loss 1.6427 | lr 1.41e-04 | grad 2.25 | tok/s 44672
step   1490 | loss 1.8109 | lr 1.45e-04 | grad 2.73 | tok/s 44667
step   1500 | loss 1.9223 | lr 1.50e-04 | grad 2.33 | tok/s 43457
step   1510 | loss 1.7777 | lr 1.55e-04 | grad 1.36 | tok/s 45728
step   1520 | loss 1.7339 | lr 1.59e-04 | grad 1.19 | tok/s 45381
step   1530 | loss 1.6950 | lr 1.64e-04 | grad 0.89 | tok/s 45003
step   1540 | loss 1.6638 | lr 1.69e-04 | grad 0.82 | tok/s 44015
step   1550 | loss 1.6618 | lr 1.73e-04 | grad 2.78 | tok/s 45668
step   1560 | loss 2.2780 | lr 1.78e-04 | grad 1.66 | tok/s 44431
step   1570 | loss 1.6940 | lr 1.83e-04 | grad 1.31 | tok/s 43643
step   1580 | loss 1.8393 | lr 1.87e-04 | grad 1.21 | tok/s 45079
step   1590 | loss 1.6045 | lr 1.92e-04 | grad 0.89 | tok/s 43991
step   1600 | loss 1.7061 | lr 1.96e-04 | grad 1.08 | tok/s 43143
step   1610 | loss 1.5688 | lr 2.01e-04 | grad 1.15 | tok/s 45646
step   1620 | loss 1.7093 | lr 2.05e-04 | grad 0.80 | tok/s 44975
step   1630 | loss 1.7241 | lr 2.09e-04 | grad 0.91 | tok/s 45334
step   1640 | loss 1.6391 | lr 2.14e-04 | grad 0.77 | tok/s 43818
step   1650 | loss 1.6632 | lr 2.18e-04 | grad 1.77 | tok/s 43086
step   1660 | loss 1.6702 | lr 2.22e-04 | grad 1.01 | tok/s 43399
step   1670 | loss 1.7332 | lr 2.26e-04 | grad 2.20 | tok/s 45308
step   1680 | loss 2.1163 | lr 2.30e-04 | grad 0.82 | tok/s 45309
step   1690 | loss 1.6578 | lr 2.34e-04 | grad 1.18 | tok/s 44188
step   1700 | loss 2.0188 | lr 2.38e-04 | grad 1.23 | tok/s 45127
step   1710 | loss 1.8014 | lr 2.42e-04 | grad 1.11 | tok/s 43846
step   1720 | loss 1.7470 | lr 2.45e-04 | grad 1.18 | tok/s 43971
step   1730 | loss 1.9071 | lr 2.49e-04 | grad 1.20 | tok/s 44034

Training complete! Final step: 1732
