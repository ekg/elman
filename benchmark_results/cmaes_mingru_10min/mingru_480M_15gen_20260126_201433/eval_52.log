Using device: cuda
Output directory: benchmark_results/cmaes_mingru_10min/mingru_480M_15gen_20260126_201433/eval_52/levelmingru_100m_20260126_211601
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level mingru, 236,682,240 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.0346 | lr 3.00e-04 | grad 11.75 | tok/s 20206
step     20 | loss 2.9770 | lr 3.00e-04 | grad 3.36 | tok/s 22181
step     30 | loss 3.1269 | lr 3.00e-04 | grad 5.59 | tok/s 22362
step     40 | loss 3.3781 | lr 3.00e-04 | grad 15.38 | tok/s 23099
step     50 | loss 4.4043 | lr 3.00e-04 | grad 4.62 | tok/s 23677
step     60 | loss 3.4686 | lr 3.00e-04 | grad 9.12 | tok/s 23686
step     70 | loss 3.2348 | lr 3.00e-04 | grad 3.36 | tok/s 23671
step     80 | loss 3.0027 | lr 3.00e-04 | grad 2.39 | tok/s 23648
step     90 | loss 2.8219 | lr 3.00e-04 | grad 3.12 | tok/s 23616
step    100 | loss 2.7729 | lr 3.00e-04 | grad 2.66 | tok/s 23605
step    110 | loss 2.5609 | lr 3.00e-04 | grad 3.39 | tok/s 23441
step    120 | loss 2.9439 | lr 3.00e-04 | grad 2.38 | tok/s 22677
step    130 | loss 2.5637 | lr 3.00e-04 | grad 4.91 | tok/s 22813
step    140 | loss 2.5268 | lr 3.00e-04 | grad 3.39 | tok/s 22787
step    150 | loss 2.3884 | lr 3.00e-04 | grad 6.34 | tok/s 23286
step    160 | loss 2.6741 | lr 3.00e-04 | grad 4.44 | tok/s 23385
step    170 | loss 2.5639 | lr 3.00e-04 | grad 2.72 | tok/s 21948
step    180 | loss 2.6134 | lr 3.00e-04 | grad 3.11 | tok/s 22963
step    190 | loss 2.3415 | lr 3.00e-04 | grad 2.08 | tok/s 21890
step    200 | loss 2.1746 | lr 3.00e-04 | grad 4.06 | tok/s 23334
step    210 | loss 2.1688 | lr 3.00e-04 | grad 2.06 | tok/s 22533
step    220 | loss 2.5424 | lr 3.00e-04 | grad 3.11 | tok/s 22703
step    230 | loss 2.5303 | lr 3.00e-04 | grad 5.12 | tok/s 22336
step    240 | loss 2.3355 | lr 3.00e-04 | grad 2.56 | tok/s 22677
step    250 | loss 2.4472 | lr 3.00e-04 | grad 3.84 | tok/s 22457
step    260 | loss 2.2906 | lr 3.00e-04 | grad 2.44 | tok/s 23285
step    270 | loss 2.3018 | lr 3.00e-04 | grad 2.44 | tok/s 22791
step    280 | loss 2.0541 | lr 3.00e-04 | grad 2.98 | tok/s 21516
step    290 | loss 2.1135 | lr 3.00e-04 | grad 2.39 | tok/s 22270
step    300 | loss 2.2322 | lr 3.00e-04 | grad 2.72 | tok/s 22057
step    310 | loss 2.0625 | lr 3.00e-04 | grad 3.33 | tok/s 22268
step    320 | loss 2.1642 | lr 3.00e-04 | grad 3.08 | tok/s 22011
step    330 | loss 2.1552 | lr 3.00e-04 | grad 1.98 | tok/s 22673
step    340 | loss 2.2379 | lr 3.00e-04 | grad 2.70 | tok/s 22739
step    350 | loss 2.2817 | lr 3.00e-04 | grad 2.72 | tok/s 22780
step    360 | loss 1.9638 | lr 3.00e-04 | grad 1.95 | tok/s 21809
step    370 | loss 2.0285 | lr 3.00e-04 | grad 2.30 | tok/s 23329
step    380 | loss 1.8603 | lr 3.00e-04 | grad 3.12 | tok/s 23528
step    390 | loss 1.7010 | lr 3.00e-04 | grad 3.30 | tok/s 23526
step    400 | loss 1.8153 | lr 3.00e-04 | grad 4.12 | tok/s 22759
step    410 | loss 2.1479 | lr 3.00e-04 | grad 2.92 | tok/s 22452
step    420 | loss 2.0922 | lr 3.00e-04 | grad 1.52 | tok/s 23129
step    430 | loss 2.0896 | lr 3.00e-04 | grad 1.97 | tok/s 23373
step    440 | loss 1.9779 | lr 3.00e-04 | grad 2.53 | tok/s 22400
step    450 | loss 2.0391 | lr 3.00e-04 | grad 1.65 | tok/s 22792
step    460 | loss 1.8850 | lr 3.00e-04 | grad 2.06 | tok/s 22725
step    470 | loss 1.9550 | lr 3.00e-04 | grad 2.36 | tok/s 22525
step    480 | loss 1.9332 | lr 3.00e-04 | grad 3.12 | tok/s 23463
step    490 | loss 2.0217 | lr 3.00e-04 | grad 2.48 | tok/s 22570
step    500 | loss 1.9202 | lr 3.00e-04 | grad 1.80 | tok/s 22443
step    510 | loss 2.1579 | lr 3.00e-04 | grad 1.99 | tok/s 22277
step    520 | loss 1.8488 | lr 3.00e-04 | grad 1.73 | tok/s 22106
step    530 | loss 1.8972 | lr 3.00e-04 | grad 2.16 | tok/s 22393
step    540 | loss 2.0956 | lr 3.00e-04 | grad 2.02 | tok/s 22614
step    550 | loss 1.5435 | lr 3.00e-04 | grad 1.55 | tok/s 22202
step    560 | loss 1.8350 | lr 3.00e-04 | grad 2.20 | tok/s 23196
step    570 | loss 1.7217 | lr 3.00e-04 | grad 2.84 | tok/s 23520
step    580 | loss 1.6561 | lr 3.00e-04 | grad 1.98 | tok/s 23508
step    590 | loss 1.6005 | lr 3.00e-04 | grad 1.63 | tok/s 23514
step    600 | loss 1.6420 | lr 3.00e-04 | grad 2.59 | tok/s 23515
step    610 | loss 1.6118 | lr 3.00e-04 | grad 2.27 | tok/s 23517
step    620 | loss 1.5721 | lr 3.00e-04 | grad 1.86 | tok/s 23520
step    630 | loss 1.9002 | lr 3.00e-04 | grad 1.91 | tok/s 22624
step    640 | loss 1.8167 | lr 3.00e-04 | grad 1.77 | tok/s 21938
step    650 | loss 1.8889 | lr 3.00e-04 | grad 2.09 | tok/s 22904
step    660 | loss 1.8097 | lr 3.00e-04 | grad 2.23 | tok/s 22772
step    670 | loss 1.8817 | lr 3.00e-04 | grad 2.33 | tok/s 23058
step    680 | loss 1.9504 | lr 3.00e-04 | grad 2.22 | tok/s 21688
step    690 | loss 1.8512 | lr 3.00e-04 | grad 1.70 | tok/s 22614
step    700 | loss 1.7569 | lr 3.00e-04 | grad 1.85 | tok/s 21833
step    710 | loss 1.8477 | lr 3.00e-04 | grad 1.98 | tok/s 22460
step    720 | loss 1.7729 | lr 3.00e-04 | grad 2.11 | tok/s 22293
step    730 | loss 1.5879 | lr 3.00e-04 | grad 4.69 | tok/s 23375
step    740 | loss 1.8314 | lr 3.00e-04 | grad 1.79 | tok/s 22499
step    750 | loss 2.2489 | lr 3.00e-04 | grad 2.48 | tok/s 23365
step    760 | loss 1.7379 | lr 3.00e-04 | grad 2.22 | tok/s 23334
step    770 | loss 1.7025 | lr 3.00e-04 | grad 2.25 | tok/s 22677
step    780 | loss 1.7536 | lr 3.00e-04 | grad 2.08 | tok/s 22880
step    790 | loss 1.8104 | lr 3.00e-04 | grad 2.67 | tok/s 22744
step    800 | loss 2.1032 | lr 3.00e-04 | grad 1.77 | tok/s 22615
step    810 | loss 1.2820 | lr 3.00e-04 | grad 1.80 | tok/s 22671
step    820 | loss 1.7793 | lr 3.00e-04 | grad 1.77 | tok/s 22632
step    830 | loss 1.7727 | lr 3.00e-04 | grad 1.76 | tok/s 21560
step    840 | loss 1.8425 | lr 3.00e-04 | grad 2.86 | tok/s 22793
step    850 | loss 1.7974 | lr 3.00e-04 | grad 2.69 | tok/s 22420
step    860 | loss 1.8292 | lr 3.00e-04 | grad 2.03 | tok/s 22428
step    870 | loss 1.9636 | lr 3.00e-04 | grad 2.19 | tok/s 23511
step    880 | loss 1.7692 | lr 3.00e-04 | grad 2.02 | tok/s 22661
step    890 | loss 1.7371 | lr 3.00e-04 | grad 1.58 | tok/s 22395
step    900 | loss 1.7161 | lr 3.00e-04 | grad 1.76 | tok/s 22677
step    910 | loss 1.7836 | lr 3.00e-04 | grad 2.20 | tok/s 22197
step    920 | loss 1.7538 | lr 3.00e-04 | grad 1.42 | tok/s 22822
step    930 | loss 1.7020 | lr 3.00e-04 | grad 1.40 | tok/s 22685
step    940 | loss 1.5981 | lr 3.00e-04 | grad 1.11 | tok/s 22531
step    950 | loss 1.7027 | lr 3.00e-04 | grad 2.23 | tok/s 21496
step    960 | loss 1.6436 | lr 3.00e-04 | grad 1.32 | tok/s 22042
step    970 | loss 1.6478 | lr 3.00e-04 | grad 1.49 | tok/s 22573
step    980 | loss 2.1285 | lr 3.00e-04 | grad 3.56 | tok/s 23323
step    990 | loss 2.0583 | lr 3.00e-04 | grad 2.27 | tok/s 23019
step   1000 | loss 1.7901 | lr 3.00e-04 | grad 1.22 | tok/s 22231
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7901.pt
step   1010 | loss 1.4704 | lr 3.00e-04 | grad 2.20 | tok/s 14823
step   1020 | loss 1.4493 | lr 3.00e-04 | grad 2.75 | tok/s 23287
step   1030 | loss 1.6917 | lr 3.00e-04 | grad 1.80 | tok/s 23078
step   1040 | loss 2.0458 | lr 3.00e-04 | grad 3.83 | tok/s 22290
step   1050 | loss 2.1671 | lr 3.00e-04 | grad 2.34 | tok/s 23213
step   1060 | loss 1.7146 | lr 3.00e-04 | grad 1.30 | tok/s 22623
step   1070 | loss 1.3492 | lr 3.00e-04 | grad 1.56 | tok/s 22964
step   1080 | loss 1.5769 | lr 3.00e-04 | grad 1.83 | tok/s 23392
step   1090 | loss 1.4568 | lr 3.00e-04 | grad 1.91 | tok/s 23536
step   1100 | loss 1.4563 | lr 3.00e-04 | grad 1.68 | tok/s 23526
step   1110 | loss 1.3997 | lr 3.00e-04 | grad 1.77 | tok/s 23544
step   1120 | loss 1.5137 | lr 3.00e-04 | grad 1.75 | tok/s 23311
step   1130 | loss 1.8606 | lr 3.00e-04 | grad 3.67 | tok/s 23115
step   1140 | loss 2.0856 | lr 3.00e-04 | grad 2.55 | tok/s 23001
step   1150 | loss 1.7605 | lr 3.00e-04 | grad 1.40 | tok/s 22974
step   1160 | loss 2.0362 | lr 3.00e-04 | grad 2.03 | tok/s 22630
step   1170 | loss 1.8820 | lr 3.00e-04 | grad 1.56 | tok/s 22022
step   1180 | loss 1.7006 | lr 3.00e-04 | grad 2.12 | tok/s 22731
step   1190 | loss 1.7442 | lr 3.00e-04 | grad 2.64 | tok/s 23280
step   1200 | loss 1.6771 | lr 3.00e-04 | grad 2.39 | tok/s 23506
step   1210 | loss 1.3841 | lr 3.00e-04 | grad 1.60 | tok/s 23013
step   1220 | loss 1.6105 | lr 3.00e-04 | grad 1.57 | tok/s 22402
step   1230 | loss 1.6950 | lr 3.00e-04 | grad 1.82 | tok/s 22617
step   1240 | loss 1.4548 | lr 3.00e-04 | grad 1.62 | tok/s 23384
step   1250 | loss 1.6059 | lr 3.00e-04 | grad 1.94 | tok/s 22806
step   1260 | loss 1.7492 | lr 3.00e-04 | grad 2.16 | tok/s 23388
step   1270 | loss 1.6927 | lr 3.00e-04 | grad 2.06 | tok/s 22826
step   1280 | loss 1.5407 | lr 3.00e-04 | grad 1.73 | tok/s 23067
step   1290 | loss 1.6905 | lr 3.00e-04 | grad 1.57 | tok/s 21828
step   1300 | loss 1.6325 | lr 3.00e-04 | grad 1.16 | tok/s 22196
step   1310 | loss 1.9842 | lr 3.00e-04 | grad 1.84 | tok/s 22861
step   1320 | loss 1.6902 | lr 3.00e-04 | grad 2.05 | tok/s 23212
step   1330 | loss 1.8262 | lr 3.00e-04 | grad 1.66 | tok/s 22856
step   1340 | loss 1.6033 | lr 3.00e-04 | grad 1.94 | tok/s 22311
step   1350 | loss 1.7893 | lr 3.00e-04 | grad 2.25 | tok/s 22996
step   1360 | loss 1.7744 | lr 3.00e-04 | grad 2.66 | tok/s 22185
step   1370 | loss 1.5062 | lr 3.00e-04 | grad 1.91 | tok/s 22584
step   1380 | loss 1.9733 | lr 3.00e-04 | grad 1.73 | tok/s 22987
step   1390 | loss 1.6051 | lr 3.00e-04 | grad 1.78 | tok/s 22216
step   1400 | loss 1.7885 | lr 3.00e-04 | grad 1.50 | tok/s 22240
step   1410 | loss 1.4691 | lr 3.00e-04 | grad 1.78 | tok/s 22636
step   1420 | loss 1.5981 | lr 3.00e-04 | grad 2.06 | tok/s 22803
step   1430 | loss 1.7885 | lr 3.00e-04 | grad 3.34 | tok/s 22814
step   1440 | loss 1.7277 | lr 3.00e-04 | grad 1.93 | tok/s 22561
step   1450 | loss 1.6907 | lr 3.00e-04 | grad 1.62 | tok/s 23150
step   1460 | loss 1.6577 | lr 3.00e-04 | grad 1.80 | tok/s 22541
step   1470 | loss 1.7552 | lr 3.00e-04 | grad 1.34 | tok/s 22452
step   1480 | loss 1.5399 | lr 3.00e-04 | grad 1.58 | tok/s 22262
step   1490 | loss 1.6964 | lr 3.00e-04 | grad 3.75 | tok/s 22649
step   1500 | loss 2.0415 | lr 3.00e-04 | grad 1.52 | tok/s 23049
step   1510 | loss 1.5018 | lr 3.00e-04 | grad 10.31 | tok/s 22827
step   1520 | loss 1.6109 | lr 3.00e-04 | grad 2.61 | tok/s 22289
step   1530 | loss 1.6618 | lr 3.00e-04 | grad 1.50 | tok/s 22728
step   1540 | loss 1.6870 | lr 3.00e-04 | grad 1.65 | tok/s 23298
step   1550 | loss 1.6017 | lr 3.00e-04 | grad 1.39 | tok/s 22909
step   1560 | loss 1.6143 | lr 3.00e-04 | grad 1.64 | tok/s 22919
step   1570 | loss 1.4141 | lr 3.00e-04 | grad 2.03 | tok/s 22863
step   1580 | loss 1.4098 | lr 3.00e-04 | grad 1.35 | tok/s 23524
step   1590 | loss 1.6918 | lr 3.00e-04 | grad 1.74 | tok/s 22147
step   1600 | loss 1.4732 | lr 3.00e-04 | grad 3.64 | tok/s 22796
step   1610 | loss 1.7460 | lr 3.00e-04 | grad 3.02 | tok/s 23129
step   1620 | loss 2.3521 | lr 3.00e-04 | grad 2.75 | tok/s 23211
step   1630 | loss 2.0607 | lr 3.00e-04 | grad 2.30 | tok/s 23529
step   1640 | loss 1.9154 | lr 3.00e-04 | grad 2.62 | tok/s 23524
step   1650 | loss 1.7974 | lr 3.00e-04 | grad 2.45 | tok/s 23525
step   1660 | loss 1.7313 | lr 3.00e-04 | grad 2.39 | tok/s 23533
step   1670 | loss 1.7296 | lr 3.00e-04 | grad 2.64 | tok/s 23436
step   1680 | loss 1.7780 | lr 3.00e-04 | grad 2.23 | tok/s 22549
step   1690 | loss 1.6192 | lr 3.00e-04 | grad 2.69 | tok/s 22093
step   1700 | loss 1.6284 | lr 3.00e-04 | grad 2.05 | tok/s 22344

Training complete! Final step: 1708
