Using device: cuda
Output directory: benchmark_results/cmaes_mingru_10min/mingru_480M_15gen_20260126_201433/eval_29/levelmingru_100m_20260126_204519
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level mingru, 179,490,816 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.5196 | lr 3.00e-04 | grad 2.89 | tok/s 20522
step     20 | loss 3.1499 | lr 3.00e-04 | grad 1.51 | tok/s 22185
step     30 | loss 3.5927 | lr 3.00e-04 | grad 3.27 | tok/s 22379
step     40 | loss 3.7726 | lr 3.00e-04 | grad 6.56 | tok/s 23123
step     50 | loss 5.0189 | lr 3.00e-04 | grad 3.28 | tok/s 23729
step     60 | loss 4.0058 | lr 3.00e-04 | grad 4.25 | tok/s 23735
step     70 | loss 3.7724 | lr 3.00e-04 | grad 3.12 | tok/s 23721
step     80 | loss 3.6754 | lr 3.00e-04 | grad 4.53 | tok/s 23716
step     90 | loss 3.4449 | lr 3.00e-04 | grad 2.47 | tok/s 23711
step    100 | loss 3.4001 | lr 3.00e-04 | grad 1.88 | tok/s 23695
step    110 | loss 3.2127 | lr 3.00e-04 | grad 4.78 | tok/s 23546
step    120 | loss 3.4610 | lr 3.00e-04 | grad 1.63 | tok/s 22755
step    130 | loss 2.7929 | lr 3.00e-04 | grad 2.16 | tok/s 22864
step    140 | loss 2.8105 | lr 3.00e-04 | grad 2.55 | tok/s 22860
step    150 | loss 2.9580 | lr 3.00e-04 | grad 2.75 | tok/s 23352
step    160 | loss 3.0066 | lr 3.00e-04 | grad 2.52 | tok/s 23467
step    170 | loss 2.8226 | lr 3.00e-04 | grad 1.87 | tok/s 22010
step    180 | loss 2.9986 | lr 3.00e-04 | grad 2.72 | tok/s 23023
step    190 | loss 2.6881 | lr 3.00e-04 | grad 1.78 | tok/s 21935
step    200 | loss 2.4528 | lr 3.00e-04 | grad 1.48 | tok/s 23404
step    210 | loss 2.4361 | lr 3.00e-04 | grad 1.08 | tok/s 22599
step    220 | loss 2.7935 | lr 3.00e-04 | grad 1.34 | tok/s 22757
step    230 | loss 2.8664 | lr 3.00e-04 | grad 3.81 | tok/s 22367
step    240 | loss 2.6146 | lr 3.00e-04 | grad 2.17 | tok/s 22741
step    250 | loss 2.7408 | lr 3.00e-04 | grad 2.31 | tok/s 22522
step    260 | loss 2.5580 | lr 3.00e-04 | grad 2.28 | tok/s 23366
step    270 | loss 2.5667 | lr 3.00e-04 | grad 2.31 | tok/s 22844
step    280 | loss 2.3121 | lr 3.00e-04 | grad 2.23 | tok/s 21571
step    290 | loss 2.3683 | lr 3.00e-04 | grad 2.25 | tok/s 22333
step    300 | loss 2.5239 | lr 3.00e-04 | grad 1.67 | tok/s 22112
step    310 | loss 2.3077 | lr 3.00e-04 | grad 2.14 | tok/s 22321
step    320 | loss 2.4266 | lr 3.00e-04 | grad 2.77 | tok/s 22057
step    330 | loss 2.4381 | lr 3.00e-04 | grad 1.88 | tok/s 22712
step    340 | loss 2.5683 | lr 3.00e-04 | grad 1.38 | tok/s 22789
step    350 | loss 2.6642 | lr 3.00e-04 | grad 1.61 | tok/s 22837
step    360 | loss 2.3033 | lr 3.00e-04 | grad 1.78 | tok/s 21866
step    370 | loss 2.3817 | lr 3.00e-04 | grad 1.69 | tok/s 23394
step    380 | loss 2.2722 | lr 3.00e-04 | grad 1.85 | tok/s 23603
step    390 | loss 2.2267 | lr 3.00e-04 | grad 2.02 | tok/s 23595
step    400 | loss 2.2512 | lr 3.00e-04 | grad 2.95 | tok/s 22797
step    410 | loss 2.4472 | lr 3.00e-04 | grad 2.22 | tok/s 22496
step    420 | loss 2.4597 | lr 3.00e-04 | grad 2.03 | tok/s 23175
step    430 | loss 2.4380 | lr 3.00e-04 | grad 1.44 | tok/s 23417
step    440 | loss 2.3237 | lr 3.00e-04 | grad 1.70 | tok/s 22462
step    450 | loss 2.3545 | lr 3.00e-04 | grad 1.25 | tok/s 22835
step    460 | loss 2.2885 | lr 3.00e-04 | grad 1.67 | tok/s 22774
step    470 | loss 2.3120 | lr 3.00e-04 | grad 1.75 | tok/s 22571
step    480 | loss 2.4139 | lr 3.00e-04 | grad 2.36 | tok/s 23530
step    490 | loss 2.3715 | lr 3.00e-04 | grad 2.27 | tok/s 22623
step    500 | loss 2.2316 | lr 3.00e-04 | grad 1.49 | tok/s 22510
step    510 | loss 2.4702 | lr 3.00e-04 | grad 1.20 | tok/s 22341
step    520 | loss 2.1905 | lr 3.00e-04 | grad 1.64 | tok/s 22168
step    530 | loss 2.2153 | lr 3.00e-04 | grad 1.41 | tok/s 22431
step    540 | loss 2.4759 | lr 3.00e-04 | grad 1.46 | tok/s 22674
step    550 | loss 1.9014 | lr 3.00e-04 | grad 1.80 | tok/s 22263
step    560 | loss 2.2283 | lr 3.00e-04 | grad 2.02 | tok/s 23265
step    570 | loss 2.1669 | lr 3.00e-04 | grad 1.77 | tok/s 23589
step    580 | loss 2.1105 | lr 3.00e-04 | grad 1.82 | tok/s 23585
step    590 | loss 2.0727 | lr 3.00e-04 | grad 1.22 | tok/s 23585
step    600 | loss 2.1495 | lr 3.00e-04 | grad 1.73 | tok/s 23584
step    610 | loss 2.0926 | lr 3.00e-04 | grad 1.55 | tok/s 23586
step    620 | loss 2.0430 | lr 3.00e-04 | grad 1.58 | tok/s 23582
step    630 | loss 2.2735 | lr 3.00e-04 | grad 1.56 | tok/s 22670
step    640 | loss 2.0997 | lr 3.00e-04 | grad 1.48 | tok/s 21997
step    650 | loss 2.2670 | lr 3.00e-04 | grad 1.80 | tok/s 22971
step    660 | loss 2.1420 | lr 3.00e-04 | grad 1.85 | tok/s 22835
step    670 | loss 2.3081 | lr 3.00e-04 | grad 1.42 | tok/s 23125
step    680 | loss 2.2737 | lr 3.00e-04 | grad 1.88 | tok/s 21741
step    690 | loss 2.2551 | lr 3.00e-04 | grad 1.78 | tok/s 22682
step    700 | loss 2.0901 | lr 3.00e-04 | grad 1.59 | tok/s 21891
step    710 | loss 2.2889 | lr 3.00e-04 | grad 1.88 | tok/s 22523
step    720 | loss 2.1686 | lr 3.00e-04 | grad 2.02 | tok/s 22360
step    730 | loss 2.1367 | lr 3.00e-04 | grad 4.12 | tok/s 23430
step    740 | loss 2.1972 | lr 3.00e-04 | grad 1.39 | tok/s 22560
step    750 | loss 2.7396 | lr 3.00e-04 | grad 1.81 | tok/s 23448
step    760 | loss 2.2018 | lr 3.00e-04 | grad 2.25 | tok/s 23408
step    770 | loss 2.0785 | lr 3.00e-04 | grad 1.32 | tok/s 22747
step    780 | loss 2.1686 | lr 3.00e-04 | grad 1.78 | tok/s 22963
step    790 | loss 2.1759 | lr 3.00e-04 | grad 1.70 | tok/s 22824
step    800 | loss 2.5450 | lr 3.00e-04 | grad 1.54 | tok/s 22697
step    810 | loss 1.6598 | lr 3.00e-04 | grad 1.88 | tok/s 22726
step    820 | loss 2.2234 | lr 3.00e-04 | grad 1.12 | tok/s 22689
step    830 | loss 2.1117 | lr 3.00e-04 | grad 1.29 | tok/s 21595
step    840 | loss 2.2794 | lr 3.00e-04 | grad 2.12 | tok/s 22850
step    850 | loss 2.2157 | lr 3.00e-04 | grad 2.06 | tok/s 22490
step    860 | loss 2.2166 | lr 3.00e-04 | grad 1.70 | tok/s 22486
step    870 | loss 2.4767 | lr 3.00e-04 | grad 1.45 | tok/s 23587
step    880 | loss 2.1780 | lr 3.00e-04 | grad 1.80 | tok/s 22727
step    890 | loss 2.0973 | lr 3.00e-04 | grad 1.55 | tok/s 22465
step    900 | loss 2.0822 | lr 3.00e-04 | grad 1.32 | tok/s 22747
step    910 | loss 2.1850 | lr 3.00e-04 | grad 2.00 | tok/s 22249
step    920 | loss 2.1345 | lr 3.00e-04 | grad 1.43 | tok/s 22878
step    930 | loss 2.1701 | lr 3.00e-04 | grad 1.61 | tok/s 22739
step    940 | loss 2.0148 | lr 3.00e-04 | grad 1.22 | tok/s 22591
step    950 | loss 2.0934 | lr 3.00e-04 | grad 1.62 | tok/s 21542
step    960 | loss 2.0142 | lr 3.00e-04 | grad 1.46 | tok/s 22108
step    970 | loss 2.0289 | lr 3.00e-04 | grad 1.88 | tok/s 22637
step    980 | loss 2.5947 | lr 3.00e-04 | grad 2.33 | tok/s 23382
step    990 | loss 2.5569 | lr 3.00e-04 | grad 2.16 | tok/s 23087
step   1000 | loss 2.1947 | lr 3.00e-04 | grad 0.86 | tok/s 22292
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1947.pt
step   1010 | loss 1.7985 | lr 3.00e-04 | grad 1.76 | tok/s 16873
step   1020 | loss 1.8123 | lr 3.00e-04 | grad 1.19 | tok/s 23321
step   1030 | loss 2.2414 | lr 3.00e-04 | grad 1.61 | tok/s 23184
step   1040 | loss 2.2816 | lr 3.00e-04 | grad 4.44 | tok/s 22276
step   1050 | loss 2.5020 | lr 3.00e-04 | grad 1.59 | tok/s 23361
step   1060 | loss 2.2604 | lr 3.00e-04 | grad 1.76 | tok/s 22559
step   1070 | loss 1.7009 | lr 3.00e-04 | grad 1.38 | tok/s 23165
step   1080 | loss 2.0044 | lr 3.00e-04 | grad 1.59 | tok/s 23213
step   1090 | loss 1.9256 | lr 3.00e-04 | grad 2.06 | tok/s 23574
step   1100 | loss 1.9023 | lr 3.00e-04 | grad 2.02 | tok/s 23596
step   1110 | loss 1.8752 | lr 3.00e-04 | grad 2.00 | tok/s 23594
step   1120 | loss 1.9266 | lr 3.00e-04 | grad 1.63 | tok/s 23364
step   1130 | loss 2.3067 | lr 3.00e-04 | grad 4.59 | tok/s 23179
step   1140 | loss 2.4657 | lr 3.00e-04 | grad 1.97 | tok/s 23057
step   1150 | loss 2.2097 | lr 3.00e-04 | grad 4.78 | tok/s 23162
step   1160 | loss 2.4503 | lr 3.00e-04 | grad 1.64 | tok/s 22680
step   1170 | loss 2.2295 | lr 3.00e-04 | grad 2.08 | tok/s 22234
step   1180 | loss 2.0701 | lr 3.00e-04 | grad 1.31 | tok/s 22506
step   1190 | loss 2.1789 | lr 3.00e-04 | grad 1.41 | tok/s 23344
step   1200 | loss 2.1827 | lr 3.00e-04 | grad 2.09 | tok/s 23582
step   1210 | loss 1.8597 | lr 3.00e-04 | grad 1.58 | tok/s 23099
step   1220 | loss 2.0292 | lr 3.00e-04 | grad 1.41 | tok/s 22413
step   1230 | loss 2.0690 | lr 3.00e-04 | grad 1.10 | tok/s 22772
step   1240 | loss 1.9909 | lr 3.00e-04 | grad 1.95 | tok/s 23354
step   1250 | loss 2.0178 | lr 3.00e-04 | grad 1.55 | tok/s 22861
step   1260 | loss 2.3761 | lr 3.00e-04 | grad 2.34 | tok/s 23442
step   1270 | loss 2.1424 | lr 3.00e-04 | grad 1.56 | tok/s 22994
step   1280 | loss 2.0088 | lr 3.00e-04 | grad 2.08 | tok/s 23114
step   1290 | loss 2.1033 | lr 3.00e-04 | grad 1.70 | tok/s 22268
step   1300 | loss 2.0776 | lr 3.00e-04 | grad 1.95 | tok/s 22150
step   1310 | loss 2.4322 | lr 3.00e-04 | grad 1.73 | tok/s 22682
step   1320 | loss 2.0574 | lr 3.00e-04 | grad 1.72 | tok/s 23068
step   1330 | loss 2.2941 | lr 3.00e-04 | grad 1.82 | tok/s 22943
step   1340 | loss 1.9721 | lr 3.00e-04 | grad 1.73 | tok/s 22296
step   1350 | loss 2.1961 | lr 3.00e-04 | grad 1.59 | tok/s 23040
step   1360 | loss 2.2222 | lr 3.00e-04 | grad 1.34 | tok/s 22372
step   1370 | loss 2.0000 | lr 3.00e-04 | grad 1.79 | tok/s 22463
step   1380 | loss 2.3473 | lr 3.00e-04 | grad 1.54 | tok/s 23074
step   1390 | loss 2.0425 | lr 3.00e-04 | grad 1.89 | tok/s 22216
step   1400 | loss 2.2849 | lr 3.00e-04 | grad 1.87 | tok/s 22644
step   1410 | loss 1.9403 | lr 3.00e-04 | grad 1.73 | tok/s 22460
step   1420 | loss 2.0976 | lr 3.00e-04 | grad 2.36 | tok/s 22693
step   1430 | loss 2.2533 | lr 3.00e-04 | grad 1.30 | tok/s 22978
step   1440 | loss 2.1187 | lr 3.00e-04 | grad 1.27 | tok/s 22600
step   1450 | loss 2.2021 | lr 3.00e-04 | grad 1.85 | tok/s 23318
step   1460 | loss 2.0164 | lr 3.00e-04 | grad 1.77 | tok/s 22607
step   1470 | loss 2.0958 | lr 3.00e-04 | grad 2.08 | tok/s 22450
step   1480 | loss 1.9450 | lr 3.00e-04 | grad 1.79 | tok/s 22124
step   1490 | loss 1.9840 | lr 3.00e-04 | grad 1.43 | tok/s 22659
step   1500 | loss 2.4734 | lr 3.00e-04 | grad 1.58 | tok/s 23106
step   1510 | loss 1.9651 | lr 3.00e-04 | grad 3.92 | tok/s 22952
step   1520 | loss 1.9605 | lr 3.00e-04 | grad 1.91 | tok/s 22235
step   1530 | loss 1.9896 | lr 3.00e-04 | grad 1.35 | tok/s 22936
step   1540 | loss 2.0890 | lr 3.00e-04 | grad 1.88 | tok/s 23198
step   1550 | loss 1.9987 | lr 3.00e-04 | grad 1.07 | tok/s 23131
step   1560 | loss 2.0719 | lr 3.00e-04 | grad 1.80 | tok/s 22798
step   1570 | loss 1.9571 | lr 3.00e-04 | grad 2.75 | tok/s 22920
step   1580 | loss 2.0142 | lr 3.00e-04 | grad 2.08 | tok/s 23572
step   1590 | loss 2.0461 | lr 3.00e-04 | grad 1.70 | tok/s 22333
step   1600 | loss 1.8601 | lr 3.00e-04 | grad 1.85 | tok/s 22688
step   1610 | loss 2.0331 | lr 3.00e-04 | grad 1.63 | tok/s 23176
step   1620 | loss 2.6028 | lr 3.00e-04 | grad 1.65 | tok/s 23246
step   1630 | loss 2.3577 | lr 3.00e-04 | grad 1.63 | tok/s 23575
step   1640 | loss 2.2764 | lr 3.00e-04 | grad 2.06 | tok/s 23576
step   1650 | loss 2.2113 | lr 3.00e-04 | grad 1.83 | tok/s 23581
step   1660 | loss 2.1777 | lr 3.00e-04 | grad 1.94 | tok/s 23576
step   1670 | loss 2.1610 | lr 3.00e-04 | grad 1.90 | tok/s 23581
step   1680 | loss 2.1505 | lr 3.00e-04 | grad 1.85 | tok/s 22587
step   1690 | loss 1.9505 | lr 3.00e-04 | grad 2.47 | tok/s 22096
step   1700 | loss 2.0382 | lr 3.00e-04 | grad 1.59 | tok/s 22357
step   1710 | loss 1.8719 | lr 3.00e-04 | grad 1.44 | tok/s 23554

Training complete! Final step: 1716
