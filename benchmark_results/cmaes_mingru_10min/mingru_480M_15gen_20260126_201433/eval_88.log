Using device: cuda
Output directory: benchmark_results/cmaes_mingru_10min/mingru_480M_15gen_20260126_201433/eval_88/levelmingru_100m_20260126_215700
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level mingru, 226,937,856 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.4114 | lr 3.00e-04 | grad 5.16 | tok/s 19043
step     20 | loss 3.1987 | lr 3.00e-04 | grad 3.91 | tok/s 22160
step     30 | loss 3.5896 | lr 3.00e-04 | grad 2.11 | tok/s 22456
step     40 | loss 4.2542 | lr 3.00e-04 | grad 5.91 | tok/s 23093
step     50 | loss 4.9153 | lr 3.00e-04 | grad 3.25 | tok/s 23685
step     60 | loss 4.0657 | lr 3.00e-04 | grad 4.41 | tok/s 23676
step     70 | loss 3.6991 | lr 3.00e-04 | grad 3.39 | tok/s 23655
step     80 | loss 3.6495 | lr 3.00e-04 | grad 4.88 | tok/s 23647
step     90 | loss 3.4225 | lr 3.00e-04 | grad 4.91 | tok/s 23631
step    100 | loss 3.2659 | lr 3.00e-04 | grad 6.66 | tok/s 23606
step    110 | loss 3.1463 | lr 3.00e-04 | grad 3.80 | tok/s 23434
step    120 | loss 3.4591 | lr 3.00e-04 | grad 2.62 | tok/s 22477
step    130 | loss 2.7405 | lr 3.00e-04 | grad 2.27 | tok/s 22952
step    140 | loss 2.8943 | lr 3.00e-04 | grad 3.78 | tok/s 22763
step    150 | loss 2.8961 | lr 3.00e-04 | grad 2.95 | tok/s 23262
step    160 | loss 3.0627 | lr 3.00e-04 | grad 2.02 | tok/s 23296
step    170 | loss 2.8281 | lr 3.00e-04 | grad 3.48 | tok/s 21887
step    180 | loss 2.9785 | lr 3.00e-04 | grad 3.44 | tok/s 23033
step    190 | loss 2.6050 | lr 3.00e-04 | grad 2.12 | tok/s 21741
step    200 | loss 2.4550 | lr 3.00e-04 | grad 2.08 | tok/s 23405
step    210 | loss 2.4376 | lr 3.00e-04 | grad 1.89 | tok/s 22419
step    220 | loss 2.7992 | lr 3.00e-04 | grad 1.90 | tok/s 22492
step    230 | loss 2.8274 | lr 3.00e-04 | grad 2.09 | tok/s 22292
step    240 | loss 2.7550 | lr 3.00e-04 | grad 4.44 | tok/s 22856
step    250 | loss 2.6277 | lr 3.00e-04 | grad 2.08 | tok/s 22398
step    260 | loss 2.5291 | lr 3.00e-04 | grad 1.37 | tok/s 23309
step    270 | loss 2.5361 | lr 3.00e-04 | grad 1.38 | tok/s 22502
step    280 | loss 2.3288 | lr 3.00e-04 | grad 3.52 | tok/s 21687
step    290 | loss 2.3458 | lr 3.00e-04 | grad 2.05 | tok/s 22080
step    300 | loss 2.5226 | lr 3.00e-04 | grad 1.81 | tok/s 21921
step    310 | loss 2.2889 | lr 3.00e-04 | grad 2.72 | tok/s 22378
step    320 | loss 2.4568 | lr 3.00e-04 | grad 2.81 | tok/s 22109
step    330 | loss 2.4007 | lr 3.00e-04 | grad 1.80 | tok/s 22504
step    340 | loss 2.5450 | lr 3.00e-04 | grad 3.19 | tok/s 22532
step    350 | loss 2.6926 | lr 3.00e-04 | grad 2.33 | tok/s 23134
step    360 | loss 2.2880 | lr 3.00e-04 | grad 2.33 | tok/s 21795
step    370 | loss 2.3690 | lr 3.00e-04 | grad 1.69 | tok/s 23318
step    380 | loss 2.2407 | lr 3.00e-04 | grad 1.70 | tok/s 23515
step    390 | loss 2.2028 | lr 3.00e-04 | grad 1.56 | tok/s 23523
step    400 | loss 2.3122 | lr 3.00e-04 | grad 2.25 | tok/s 22737
step    410 | loss 2.3843 | lr 3.00e-04 | grad 1.84 | tok/s 22231
step    420 | loss 2.4770 | lr 3.00e-04 | grad 2.30 | tok/s 23279
step    430 | loss 2.4354 | lr 3.00e-04 | grad 2.97 | tok/s 23331
step    440 | loss 2.3558 | lr 3.00e-04 | grad 2.20 | tok/s 22370
step    450 | loss 2.2743 | lr 3.00e-04 | grad 2.53 | tok/s 22694
step    460 | loss 2.3042 | lr 3.00e-04 | grad 1.98 | tok/s 22793
step    470 | loss 2.3057 | lr 3.00e-04 | grad 1.38 | tok/s 22481
step    480 | loss 2.4371 | lr 3.00e-04 | grad 2.41 | tok/s 23449
step    490 | loss 2.3389 | lr 3.00e-04 | grad 1.65 | tok/s 22505
step    500 | loss 2.2430 | lr 3.00e-04 | grad 2.03 | tok/s 22366
step    510 | loss 2.4870 | lr 3.00e-04 | grad 2.78 | tok/s 22194
step    520 | loss 2.1416 | lr 3.00e-04 | grad 1.59 | tok/s 22297
step    530 | loss 2.2321 | lr 3.00e-04 | grad 2.56 | tok/s 22269
step    540 | loss 2.4343 | lr 3.00e-04 | grad 2.05 | tok/s 22600
step    550 | loss 1.9103 | lr 3.00e-04 | grad 2.09 | tok/s 22310
step    560 | loss 2.2217 | lr 3.00e-04 | grad 2.08 | tok/s 23193
step    570 | loss 2.1528 | lr 3.00e-04 | grad 2.78 | tok/s 23516
step    580 | loss 2.0934 | lr 3.00e-04 | grad 2.19 | tok/s 23518
step    590 | loss 2.0924 | lr 3.00e-04 | grad 2.88 | tok/s 23504
step    600 | loss 2.1237 | lr 3.00e-04 | grad 2.58 | tok/s 23507
step    610 | loss 2.0841 | lr 3.00e-04 | grad 2.30 | tok/s 23501
step    620 | loss 2.0381 | lr 3.00e-04 | grad 1.91 | tok/s 23516
step    630 | loss 2.2755 | lr 3.00e-04 | grad 1.83 | tok/s 22328
step    640 | loss 2.1344 | lr 3.00e-04 | grad 2.45 | tok/s 22072
step    650 | loss 2.2223 | lr 3.00e-04 | grad 2.00 | tok/s 23000
step    660 | loss 2.1264 | lr 3.00e-04 | grad 1.79 | tok/s 22646
step    670 | loss 2.2733 | lr 3.00e-04 | grad 1.49 | tok/s 22913
step    680 | loss 2.3212 | lr 3.00e-04 | grad 1.81 | tok/s 21952
step    690 | loss 2.2083 | lr 3.00e-04 | grad 1.19 | tok/s 22504
step    700 | loss 2.0681 | lr 3.00e-04 | grad 1.91 | tok/s 21784
step    710 | loss 2.2569 | lr 3.00e-04 | grad 1.40 | tok/s 22182
step    720 | loss 2.1808 | lr 3.00e-04 | grad 1.96 | tok/s 22683
step    730 | loss 2.1311 | lr 3.00e-04 | grad 2.33 | tok/s 23173
step    740 | loss 2.2386 | lr 3.00e-04 | grad 2.25 | tok/s 22584
step    750 | loss 2.6702 | lr 3.00e-04 | grad 1.57 | tok/s 23461
step    760 | loss 2.1836 | lr 3.00e-04 | grad 1.52 | tok/s 23352
step    770 | loss 2.1430 | lr 3.00e-04 | grad 2.53 | tok/s 22604
step    780 | loss 2.1065 | lr 3.00e-04 | grad 2.45 | tok/s 22896
step    790 | loss 2.1614 | lr 3.00e-04 | grad 1.64 | tok/s 22619
step    800 | loss 2.4968 | lr 3.00e-04 | grad 1.95 | tok/s 22797
step    810 | loss 1.6377 | lr 3.00e-04 | grad 1.92 | tok/s 22254
step    820 | loss 2.2372 | lr 3.00e-04 | grad 2.23 | tok/s 22960
step    830 | loss 2.1066 | lr 3.00e-04 | grad 1.81 | tok/s 21566
step    840 | loss 2.2875 | lr 3.00e-04 | grad 2.45 | tok/s 22482
step    850 | loss 2.2156 | lr 3.00e-04 | grad 2.30 | tok/s 22620
step    860 | loss 2.1608 | lr 3.00e-04 | grad 2.03 | tok/s 22498
step    870 | loss 2.4650 | lr 3.00e-04 | grad 2.02 | tok/s 23500
step    880 | loss 2.1360 | lr 3.00e-04 | grad 1.98 | tok/s 22527
step    890 | loss 2.1144 | lr 3.00e-04 | grad 1.91 | tok/s 22474
step    900 | loss 2.0539 | lr 3.00e-04 | grad 2.23 | tok/s 22668
step    910 | loss 2.1659 | lr 3.00e-04 | grad 2.61 | tok/s 22067
step    920 | loss 2.1532 | lr 3.00e-04 | grad 4.12 | tok/s 22825
step    930 | loss 2.1050 | lr 3.00e-04 | grad 2.05 | tok/s 22609
step    940 | loss 2.0009 | lr 3.00e-04 | grad 2.00 | tok/s 22380
step    950 | loss 2.0979 | lr 3.00e-04 | grad 1.79 | tok/s 21777
step    960 | loss 2.0076 | lr 3.00e-04 | grad 2.39 | tok/s 22030
step    970 | loss 2.0197 | lr 3.00e-04 | grad 2.25 | tok/s 22566
step    980 | loss 2.6515 | lr 3.00e-04 | grad 3.92 | tok/s 23308
step    990 | loss 2.4500 | lr 3.00e-04 | grad 1.29 | tok/s 22860
step   1000 | loss 2.1459 | lr 3.00e-04 | grad 1.66 | tok/s 22372
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1459.pt
step   1010 | loss 1.7736 | lr 3.00e-04 | grad 2.05 | tok/s 15173
step   1020 | loss 1.8587 | lr 3.00e-04 | grad 1.53 | tok/s 23404
step   1030 | loss 2.2015 | lr 3.00e-04 | grad 1.92 | tok/s 22830
step   1040 | loss 2.3870 | lr 3.00e-04 | grad 3.33 | tok/s 22486
step   1050 | loss 2.3889 | lr 3.00e-04 | grad 1.73 | tok/s 23088
step   1060 | loss 2.1568 | lr 3.00e-04 | grad 2.64 | tok/s 22714
step   1070 | loss 1.7450 | lr 3.00e-04 | grad 2.25 | tok/s 22774
step   1080 | loss 1.9463 | lr 3.00e-04 | grad 2.09 | tok/s 23515
step   1090 | loss 1.9150 | lr 3.00e-04 | grad 2.28 | tok/s 23521
step   1100 | loss 1.8957 | lr 3.00e-04 | grad 2.11 | tok/s 23519
step   1110 | loss 1.8577 | lr 3.00e-04 | grad 2.55 | tok/s 23512
step   1120 | loss 1.9626 | lr 3.00e-04 | grad 2.27 | tok/s 23284
step   1130 | loss 2.3735 | lr 3.00e-04 | grad 2.38 | tok/s 23093
step   1140 | loss 2.3671 | lr 3.00e-04 | grad 1.48 | tok/s 22980
step   1150 | loss 2.2872 | lr 3.00e-04 | grad 4.03 | tok/s 22911
step   1160 | loss 2.4114 | lr 3.00e-04 | grad 2.50 | tok/s 22659
step   1170 | loss 2.1204 | lr 3.00e-04 | grad 1.80 | tok/s 21867
step   1180 | loss 2.0834 | lr 3.00e-04 | grad 1.78 | tok/s 22827
step   1190 | loss 2.2070 | lr 3.00e-04 | grad 2.80 | tok/s 23266
step   1200 | loss 2.0838 | lr 3.00e-04 | grad 1.44 | tok/s 23493
step   1210 | loss 1.8359 | lr 3.00e-04 | grad 2.06 | tok/s 22663
step   1220 | loss 2.0220 | lr 3.00e-04 | grad 2.75 | tok/s 22703
step   1230 | loss 2.0855 | lr 3.00e-04 | grad 2.12 | tok/s 22591
step   1240 | loss 1.9321 | lr 3.00e-04 | grad 1.84 | tok/s 23365
step   1250 | loss 2.0333 | lr 3.00e-04 | grad 1.95 | tok/s 22620
step   1260 | loss 2.3491 | lr 3.00e-04 | grad 3.06 | tok/s 23504
step   1270 | loss 2.0447 | lr 3.00e-04 | grad 2.19 | tok/s 22746
step   1280 | loss 1.9596 | lr 3.00e-04 | grad 1.99 | tok/s 22780
step   1290 | loss 2.0855 | lr 3.00e-04 | grad 1.73 | tok/s 21785
step   1300 | loss 2.1064 | lr 3.00e-04 | grad 2.12 | tok/s 22294
step   1310 | loss 2.4263 | lr 3.00e-04 | grad 2.08 | tok/s 22830
step   1320 | loss 2.0695 | lr 3.00e-04 | grad 1.64 | tok/s 23174
step   1330 | loss 2.2211 | lr 3.00e-04 | grad 2.56 | tok/s 22890
step   1340 | loss 1.9545 | lr 3.00e-04 | grad 1.99 | tok/s 22120
step   1350 | loss 2.2073 | lr 3.00e-04 | grad 2.34 | tok/s 23223
step   1360 | loss 2.1593 | lr 3.00e-04 | grad 2.16 | tok/s 21890
step   1370 | loss 2.0083 | lr 3.00e-04 | grad 2.22 | tok/s 22596
step   1380 | loss 2.2612 | lr 3.00e-04 | grad 1.34 | tok/s 22983
step   1390 | loss 2.0682 | lr 3.00e-04 | grad 1.58 | tok/s 22353
step   1400 | loss 2.2617 | lr 3.00e-04 | grad 1.80 | tok/s 22197
step   1410 | loss 1.9144 | lr 3.00e-04 | grad 2.33 | tok/s 22443
step   1420 | loss 2.0873 | lr 3.00e-04 | grad 2.28 | tok/s 22905
step   1430 | loss 2.2798 | lr 3.00e-04 | grad 1.52 | tok/s 22704
step   1440 | loss 2.0343 | lr 3.00e-04 | grad 1.89 | tok/s 22616
step   1450 | loss 2.1280 | lr 3.00e-04 | grad 1.80 | tok/s 22932
step   1460 | loss 2.0312 | lr 3.00e-04 | grad 2.02 | tok/s 22709
step   1470 | loss 2.0261 | lr 3.00e-04 | grad 2.02 | tok/s 22290
step   1480 | loss 1.9622 | lr 3.00e-04 | grad 1.40 | tok/s 22241
step   1490 | loss 2.0996 | lr 3.00e-04 | grad 1.49 | tok/s 22652
step   1500 | loss 2.3175 | lr 3.00e-04 | grad 1.62 | tok/s 23118
step   1510 | loss 1.8674 | lr 3.00e-04 | grad 2.30 | tok/s 22537
step   1520 | loss 1.9874 | lr 3.00e-04 | grad 1.77 | tok/s 22508
step   1530 | loss 2.0123 | lr 3.00e-04 | grad 2.53 | tok/s 22714
step   1540 | loss 2.0536 | lr 3.00e-04 | grad 2.02 | tok/s 23283
step   1550 | loss 1.9636 | lr 3.00e-04 | grad 1.52 | tok/s 22777
step   1560 | loss 2.0584 | lr 3.00e-04 | grad 1.60 | tok/s 23005
step   1570 | loss 1.9613 | lr 3.00e-04 | grad 2.41 | tok/s 22855
step   1580 | loss 1.9472 | lr 3.00e-04 | grad 1.93 | tok/s 23392
step   1590 | loss 2.0223 | lr 3.00e-04 | grad 2.14 | tok/s 22243
step   1600 | loss 1.7758 | lr 3.00e-04 | grad 2.41 | tok/s 22771
step   1610 | loss 2.2177 | lr 3.00e-04 | grad 2.11 | tok/s 22900
step   1620 | loss 2.5689 | lr 3.00e-04 | grad 1.85 | tok/s 23352
step   1630 | loss 2.3412 | lr 3.00e-04 | grad 2.27 | tok/s 23501
step   1640 | loss 2.2359 | lr 3.00e-04 | grad 1.66 | tok/s 23509
step   1650 | loss 2.1837 | lr 3.00e-04 | grad 1.60 | tok/s 23507
step   1660 | loss 2.1581 | lr 3.00e-04 | grad 1.39 | tok/s 23490
step   1670 | loss 2.2046 | lr 3.00e-04 | grad 3.06 | tok/s 23315
step   1680 | loss 2.0579 | lr 3.00e-04 | grad 1.73 | tok/s 22519
step   1690 | loss 1.9550 | lr 3.00e-04 | grad 1.73 | tok/s 21747
step   1700 | loss 1.9873 | lr 3.00e-04 | grad 1.88 | tok/s 22688

Training complete! Final step: 1707
