Using device: cuda
Output directory: benchmark_results/cmaes_mingru_10min/mingru_480M_15gen_20260126_201433/eval_17/levelmingru_100m_20260126_203505
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level mingru, 179,490,816 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.5196 | lr 3.00e-04 | grad 2.89 | tok/s 20319
step     20 | loss 3.1499 | lr 3.00e-04 | grad 1.51 | tok/s 22117
step     30 | loss 3.5927 | lr 3.00e-04 | grad 3.27 | tok/s 22298
step     40 | loss 3.7726 | lr 3.00e-04 | grad 6.56 | tok/s 22998
step     50 | loss 5.0189 | lr 3.00e-04 | grad 3.28 | tok/s 23633
step     60 | loss 4.0058 | lr 3.00e-04 | grad 4.25 | tok/s 23609
step     70 | loss 3.7724 | lr 3.00e-04 | grad 3.12 | tok/s 23612
step     80 | loss 3.6754 | lr 3.00e-04 | grad 4.53 | tok/s 23604
step     90 | loss 3.4449 | lr 3.00e-04 | grad 2.47 | tok/s 23597
step    100 | loss 3.4001 | lr 3.00e-04 | grad 1.88 | tok/s 23590
step    110 | loss 3.2127 | lr 3.00e-04 | grad 4.78 | tok/s 23406
step    120 | loss 3.4610 | lr 3.00e-04 | grad 1.63 | tok/s 22641
step    130 | loss 2.7929 | lr 3.00e-04 | grad 2.16 | tok/s 22760
step    140 | loss 2.8105 | lr 3.00e-04 | grad 2.55 | tok/s 22754
step    150 | loss 2.9580 | lr 3.00e-04 | grad 2.75 | tok/s 23247
step    160 | loss 3.0066 | lr 3.00e-04 | grad 2.52 | tok/s 23358
step    170 | loss 2.8226 | lr 3.00e-04 | grad 1.87 | tok/s 21923
step    180 | loss 2.9986 | lr 3.00e-04 | grad 2.72 | tok/s 22929
step    190 | loss 2.6881 | lr 3.00e-04 | grad 1.78 | tok/s 21851
step    200 | loss 2.4528 | lr 3.00e-04 | grad 1.48 | tok/s 23310
step    210 | loss 2.4361 | lr 3.00e-04 | grad 1.08 | tok/s 22502
step    220 | loss 2.7935 | lr 3.00e-04 | grad 1.34 | tok/s 22669
step    230 | loss 2.8664 | lr 3.00e-04 | grad 3.81 | tok/s 22291
step    240 | loss 2.6146 | lr 3.00e-04 | grad 2.17 | tok/s 22652
step    250 | loss 2.7408 | lr 3.00e-04 | grad 2.31 | tok/s 22437
step    260 | loss 2.5580 | lr 3.00e-04 | grad 2.28 | tok/s 23278
step    270 | loss 2.5667 | lr 3.00e-04 | grad 2.31 | tok/s 22760
step    280 | loss 2.3121 | lr 3.00e-04 | grad 2.23 | tok/s 21490
step    290 | loss 2.3683 | lr 3.00e-04 | grad 2.25 | tok/s 22243
step    300 | loss 2.5239 | lr 3.00e-04 | grad 1.67 | tok/s 22034
step    310 | loss 2.3077 | lr 3.00e-04 | grad 2.14 | tok/s 22248
step    320 | loss 2.4266 | lr 3.00e-04 | grad 2.77 | tok/s 21982
step    330 | loss 2.4381 | lr 3.00e-04 | grad 1.88 | tok/s 22632
step    340 | loss 2.5683 | lr 3.00e-04 | grad 1.38 | tok/s 22722
step    350 | loss 2.6642 | lr 3.00e-04 | grad 1.61 | tok/s 22751
step    360 | loss 2.3033 | lr 3.00e-04 | grad 1.78 | tok/s 21797
step    370 | loss 2.3817 | lr 3.00e-04 | grad 1.69 | tok/s 23316
step    380 | loss 2.2722 | lr 3.00e-04 | grad 1.85 | tok/s 23519
step    390 | loss 2.2267 | lr 3.00e-04 | grad 2.02 | tok/s 23516
step    400 | loss 2.2512 | lr 3.00e-04 | grad 2.95 | tok/s 22740
step    410 | loss 2.4472 | lr 3.00e-04 | grad 2.22 | tok/s 22429
step    420 | loss 2.4597 | lr 3.00e-04 | grad 2.03 | tok/s 23105
step    430 | loss 2.4380 | lr 3.00e-04 | grad 1.44 | tok/s 23342
step    440 | loss 2.3237 | lr 3.00e-04 | grad 1.70 | tok/s 22400
step    450 | loss 2.3545 | lr 3.00e-04 | grad 1.25 | tok/s 22771
step    460 | loss 2.2885 | lr 3.00e-04 | grad 1.67 | tok/s 22700
step    470 | loss 2.3120 | lr 3.00e-04 | grad 1.75 | tok/s 22486
step    480 | loss 2.4139 | lr 3.00e-04 | grad 2.36 | tok/s 23450
step    490 | loss 2.3715 | lr 3.00e-04 | grad 2.27 | tok/s 22547
step    500 | loss 2.2316 | lr 3.00e-04 | grad 1.49 | tok/s 22436
step    510 | loss 2.4702 | lr 3.00e-04 | grad 1.20 | tok/s 22269
step    520 | loss 2.1905 | lr 3.00e-04 | grad 1.64 | tok/s 22107
step    530 | loss 2.2153 | lr 3.00e-04 | grad 1.41 | tok/s 22365
step    540 | loss 2.4759 | lr 3.00e-04 | grad 1.46 | tok/s 22607
step    550 | loss 1.9014 | lr 3.00e-04 | grad 1.80 | tok/s 22195
step    560 | loss 2.2283 | lr 3.00e-04 | grad 2.02 | tok/s 23193
step    570 | loss 2.1669 | lr 3.00e-04 | grad 1.77 | tok/s 23510
step    580 | loss 2.1105 | lr 3.00e-04 | grad 1.82 | tok/s 23516
step    590 | loss 2.0727 | lr 3.00e-04 | grad 1.22 | tok/s 23524
step    600 | loss 2.1495 | lr 3.00e-04 | grad 1.73 | tok/s 23511
step    610 | loss 2.0926 | lr 3.00e-04 | grad 1.55 | tok/s 23519
step    620 | loss 2.0430 | lr 3.00e-04 | grad 1.58 | tok/s 23520
step    630 | loss 2.2735 | lr 3.00e-04 | grad 1.56 | tok/s 22615
step    640 | loss 2.0997 | lr 3.00e-04 | grad 1.48 | tok/s 21930
step    650 | loss 2.2670 | lr 3.00e-04 | grad 1.80 | tok/s 22912
step    660 | loss 2.1420 | lr 3.00e-04 | grad 1.85 | tok/s 22775
step    670 | loss 2.3081 | lr 3.00e-04 | grad 1.42 | tok/s 23077
step    680 | loss 2.2737 | lr 3.00e-04 | grad 1.88 | tok/s 21688
step    690 | loss 2.2551 | lr 3.00e-04 | grad 1.78 | tok/s 22615
step    700 | loss 2.0901 | lr 3.00e-04 | grad 1.59 | tok/s 21820
step    710 | loss 2.2889 | lr 3.00e-04 | grad 1.88 | tok/s 22455
step    720 | loss 2.1686 | lr 3.00e-04 | grad 2.02 | tok/s 22291
step    730 | loss 2.1367 | lr 3.00e-04 | grad 4.12 | tok/s 23362
step    740 | loss 2.1972 | lr 3.00e-04 | grad 1.39 | tok/s 22504
step    750 | loss 2.7396 | lr 3.00e-04 | grad 1.81 | tok/s 23382
step    760 | loss 2.2018 | lr 3.00e-04 | grad 2.25 | tok/s 23356
step    770 | loss 2.0785 | lr 3.00e-04 | grad 1.32 | tok/s 22678
step    780 | loss 2.1686 | lr 3.00e-04 | grad 1.78 | tok/s 22892
step    790 | loss 2.1759 | lr 3.00e-04 | grad 1.70 | tok/s 22751
step    800 | loss 2.5450 | lr 3.00e-04 | grad 1.54 | tok/s 22630
step    810 | loss 1.6598 | lr 3.00e-04 | grad 1.88 | tok/s 22654
step    820 | loss 2.2234 | lr 3.00e-04 | grad 1.12 | tok/s 22628
step    830 | loss 2.1117 | lr 3.00e-04 | grad 1.29 | tok/s 21532
step    840 | loss 2.2794 | lr 3.00e-04 | grad 2.12 | tok/s 22790
step    850 | loss 2.2157 | lr 3.00e-04 | grad 2.06 | tok/s 22424
step    860 | loss 2.2166 | lr 3.00e-04 | grad 1.70 | tok/s 22427
step    870 | loss 2.4767 | lr 3.00e-04 | grad 1.45 | tok/s 23527
step    880 | loss 2.1780 | lr 3.00e-04 | grad 1.80 | tok/s 22671
step    890 | loss 2.0973 | lr 3.00e-04 | grad 1.55 | tok/s 22401
step    900 | loss 2.0822 | lr 3.00e-04 | grad 1.32 | tok/s 22681
step    910 | loss 2.1850 | lr 3.00e-04 | grad 2.00 | tok/s 22163
step    920 | loss 2.1345 | lr 3.00e-04 | grad 1.43 | tok/s 22823
step    930 | loss 2.1701 | lr 3.00e-04 | grad 1.61 | tok/s 22686
step    940 | loss 2.0148 | lr 3.00e-04 | grad 1.22 | tok/s 22526
step    950 | loss 2.0934 | lr 3.00e-04 | grad 1.62 | tok/s 21484
step    960 | loss 2.0142 | lr 3.00e-04 | grad 1.46 | tok/s 22057
step    970 | loss 2.0289 | lr 3.00e-04 | grad 1.88 | tok/s 22585
step    980 | loss 2.5947 | lr 3.00e-04 | grad 2.33 | tok/s 23322
step    990 | loss 2.5569 | lr 3.00e-04 | grad 2.16 | tok/s 23024
step   1000 | loss 2.1947 | lr 3.00e-04 | grad 0.86 | tok/s 22247
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1947.pt
step   1010 | loss 1.7985 | lr 3.00e-04 | grad 1.76 | tok/s 17099
step   1020 | loss 1.8123 | lr 3.00e-04 | grad 1.19 | tok/s 23262
step   1030 | loss 2.2414 | lr 3.00e-04 | grad 1.61 | tok/s 23108
step   1040 | loss 2.2816 | lr 3.00e-04 | grad 4.44 | tok/s 22213
step   1050 | loss 2.5020 | lr 3.00e-04 | grad 1.59 | tok/s 23304
step   1060 | loss 2.2604 | lr 3.00e-04 | grad 1.76 | tok/s 22521
step   1070 | loss 1.7009 | lr 3.00e-04 | grad 1.38 | tok/s 23135
step   1080 | loss 2.0044 | lr 3.00e-04 | grad 1.59 | tok/s 23189
step   1090 | loss 1.9256 | lr 3.00e-04 | grad 2.06 | tok/s 23527
step   1100 | loss 1.9023 | lr 3.00e-04 | grad 2.02 | tok/s 23520
step   1110 | loss 1.8752 | lr 3.00e-04 | grad 2.00 | tok/s 23531
step   1120 | loss 1.9266 | lr 3.00e-04 | grad 1.63 | tok/s 23300
step   1130 | loss 2.3067 | lr 3.00e-04 | grad 4.59 | tok/s 23109
step   1140 | loss 2.4657 | lr 3.00e-04 | grad 1.97 | tok/s 22991
step   1150 | loss 2.2097 | lr 3.00e-04 | grad 4.78 | tok/s 23104
step   1160 | loss 2.4503 | lr 3.00e-04 | grad 1.64 | tok/s 22613
step   1170 | loss 2.2295 | lr 3.00e-04 | grad 2.08 | tok/s 22174
step   1180 | loss 2.0701 | lr 3.00e-04 | grad 1.31 | tok/s 22453
step   1190 | loss 2.1789 | lr 3.00e-04 | grad 1.41 | tok/s 23282
step   1200 | loss 2.1827 | lr 3.00e-04 | grad 2.09 | tok/s 23509
step   1210 | loss 1.8597 | lr 3.00e-04 | grad 1.58 | tok/s 23034
step   1220 | loss 2.0292 | lr 3.00e-04 | grad 1.41 | tok/s 22350
step   1230 | loss 2.0690 | lr 3.00e-04 | grad 1.10 | tok/s 22715
step   1240 | loss 1.9909 | lr 3.00e-04 | grad 1.95 | tok/s 23281
step   1250 | loss 2.0178 | lr 3.00e-04 | grad 1.55 | tok/s 22794
step   1260 | loss 2.3761 | lr 3.00e-04 | grad 2.34 | tok/s 23380
step   1270 | loss 2.1424 | lr 3.00e-04 | grad 1.56 | tok/s 22942
step   1280 | loss 2.0088 | lr 3.00e-04 | grad 2.08 | tok/s 23066
step   1290 | loss 2.1033 | lr 3.00e-04 | grad 1.70 | tok/s 22218
step   1300 | loss 2.0776 | lr 3.00e-04 | grad 1.95 | tok/s 22082
step   1310 | loss 2.4322 | lr 3.00e-04 | grad 1.73 | tok/s 22610
step   1320 | loss 2.0574 | lr 3.00e-04 | grad 1.72 | tok/s 23010
step   1330 | loss 2.2941 | lr 3.00e-04 | grad 1.82 | tok/s 22883
step   1340 | loss 1.9721 | lr 3.00e-04 | grad 1.73 | tok/s 22239
step   1350 | loss 2.1961 | lr 3.00e-04 | grad 1.59 | tok/s 22989
step   1360 | loss 2.2222 | lr 3.00e-04 | grad 1.34 | tok/s 22317
step   1370 | loss 2.0000 | lr 3.00e-04 | grad 1.79 | tok/s 22401
step   1380 | loss 2.3473 | lr 3.00e-04 | grad 1.54 | tok/s 23019
step   1390 | loss 2.0425 | lr 3.00e-04 | grad 1.89 | tok/s 22159
step   1400 | loss 2.2849 | lr 3.00e-04 | grad 1.87 | tok/s 22563
step   1410 | loss 1.9403 | lr 3.00e-04 | grad 1.73 | tok/s 22389
step   1420 | loss 2.0976 | lr 3.00e-04 | grad 2.36 | tok/s 22638
step   1430 | loss 2.2533 | lr 3.00e-04 | grad 1.30 | tok/s 22925
step   1440 | loss 2.1187 | lr 3.00e-04 | grad 1.27 | tok/s 22543
step   1450 | loss 2.2021 | lr 3.00e-04 | grad 1.85 | tok/s 23266
step   1460 | loss 2.0164 | lr 3.00e-04 | grad 1.77 | tok/s 22556
step   1470 | loss 2.0958 | lr 3.00e-04 | grad 2.08 | tok/s 22405
step   1480 | loss 1.9450 | lr 3.00e-04 | grad 1.79 | tok/s 22051
step   1490 | loss 1.9840 | lr 3.00e-04 | grad 1.43 | tok/s 22597
step   1500 | loss 2.4734 | lr 3.00e-04 | grad 1.58 | tok/s 23050
step   1510 | loss 1.9651 | lr 3.00e-04 | grad 3.92 | tok/s 22887
step   1520 | loss 1.9605 | lr 3.00e-04 | grad 1.91 | tok/s 22176
step   1530 | loss 1.9896 | lr 3.00e-04 | grad 1.35 | tok/s 22880
step   1540 | loss 2.0890 | lr 3.00e-04 | grad 1.88 | tok/s 23138
step   1550 | loss 1.9987 | lr 3.00e-04 | grad 1.07 | tok/s 23075
step   1560 | loss 2.0719 | lr 3.00e-04 | grad 1.80 | tok/s 22734
step   1570 | loss 1.9571 | lr 3.00e-04 | grad 2.75 | tok/s 22867
step   1580 | loss 2.0142 | lr 3.00e-04 | grad 2.08 | tok/s 23515
step   1590 | loss 2.0461 | lr 3.00e-04 | grad 1.70 | tok/s 22284
step   1600 | loss 1.8601 | lr 3.00e-04 | grad 1.85 | tok/s 22630
step   1610 | loss 2.0331 | lr 3.00e-04 | grad 1.63 | tok/s 23125
step   1620 | loss 2.6028 | lr 3.00e-04 | grad 1.65 | tok/s 23196
step   1630 | loss 2.3577 | lr 3.00e-04 | grad 1.63 | tok/s 23518
step   1640 | loss 2.2764 | lr 3.00e-04 | grad 2.06 | tok/s 23522
step   1650 | loss 2.2113 | lr 3.00e-04 | grad 1.83 | tok/s 23517
step   1660 | loss 2.1777 | lr 3.00e-04 | grad 1.94 | tok/s 23522
step   1670 | loss 2.1610 | lr 3.00e-04 | grad 1.90 | tok/s 23523
step   1680 | loss 2.1505 | lr 3.00e-04 | grad 1.85 | tok/s 22533
step   1690 | loss 1.9505 | lr 3.00e-04 | grad 2.47 | tok/s 22041
step   1700 | loss 2.0382 | lr 3.00e-04 | grad 1.59 | tok/s 22299
step   1710 | loss 1.8719 | lr 3.00e-04 | grad 1.44 | tok/s 23484

Training complete! Final step: 1711
