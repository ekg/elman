Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_51/levelE90_100m_20260129_142952
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 493,593,723 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.7743 | lr 3.00e-04 | grad 16.00 | tok/s 5908
step     20 | loss 3.2276 | lr 3.00e-04 | grad 6.84 | tok/s 14627
step     30 | loss 2.6462 | lr 3.00e-04 | grad 4.41 | tok/s 14774
step     40 | loss 2.3556 | lr 3.00e-04 | grad 4.19 | tok/s 14119
step     50 | loss 3.0152 | lr 3.00e-04 | grad 14.75 | tok/s 14307
step     60 | loss 2.0673 | lr 3.00e-04 | grad 3.50 | tok/s 14743
step     70 | loss 1.8628 | lr 3.00e-04 | grad 4.78 | tok/s 14867
step     80 | loss 5.3630 | lr 3.00e-04 | grad 49.00 | tok/s 14957
step     90 | loss 4.8227 | lr 3.00e-04 | grad 7.25 | tok/s 15186
step    100 | loss 3.9608 | lr 3.00e-04 | grad 6.00 | tok/s 15200
step    110 | loss 3.2429 | lr 3.00e-04 | grad 12.06 | tok/s 15186
step    120 | loss 3.0154 | lr 3.00e-04 | grad 9.69 | tok/s 15198
step    130 | loss 2.8378 | lr 3.00e-04 | grad 9.25 | tok/s 15189
step    140 | loss 2.5815 | lr 3.00e-04 | grad 8.25 | tok/s 15114
step    150 | loss 2.6609 | lr 3.00e-04 | grad 7.53 | tok/s 15088
step    160 | loss 2.2814 | lr 3.00e-04 | grad 7.62 | tok/s 15100
step    170 | loss 2.2917 | lr 3.00e-04 | grad 13.44 | tok/s 15106
step    180 | loss 2.1994 | lr 3.00e-04 | grad 6.66 | tok/s 15094
step    190 | loss 2.3098 | lr 3.00e-04 | grad 8.25 | tok/s 15102
step    200 | loss 2.0488 | lr 3.00e-04 | grad 5.03 | tok/s 15100
step    210 | loss 2.0725 | lr 3.00e-04 | grad 8.38 | tok/s 15104
step    220 | loss 2.1236 | lr 3.00e-04 | grad 3.31 | tok/s 14893
step    230 | loss 2.0544 | lr 3.00e-04 | grad 3.50 | tok/s 14718
step    240 | loss 2.2822 | lr 3.00e-04 | grad 4.75 | tok/s 14000
step    250 | loss 2.0950 | lr 3.00e-04 | grad 2.62 | tok/s 14340
step    260 | loss 1.5543 | lr 3.00e-04 | grad 3.05 | tok/s 14777
step    270 | loss 2.0757 | lr 3.00e-04 | grad 2.98 | tok/s 14609
step    280 | loss 2.2552 | lr 3.00e-04 | grad 5.78 | tok/s 14271
step    290 | loss 1.4556 | lr 3.00e-04 | grad 5.75 | tok/s 15076
step    300 | loss 0.6179 | lr 3.00e-04 | grad 2.27 | tok/s 15053
step    310 | loss 2.4209 | lr 3.00e-04 | grad 3.84 | tok/s 14772
step    320 | loss 1.9353 | lr 3.00e-04 | grad 5.66 | tok/s 14455
step    330 | loss 1.9574 | lr 3.00e-04 | grad 3.22 | tok/s 13960
step    340 | loss 2.2834 | lr 3.00e-04 | grad 2.84 | tok/s 14165
step    350 | loss 1.8721 | lr 3.00e-04 | grad 4.41 | tok/s 14515
step    360 | loss 1.2503 | lr 3.00e-04 | grad 13.06 | tok/s 14819
step    370 | loss 1.8233 | lr 3.00e-04 | grad 3.02 | tok/s 13541
step    380 | loss 1.7597 | lr 3.00e-04 | grad 2.89 | tok/s 14317
step    390 | loss 1.5344 | lr 3.00e-04 | grad 2.34 | tok/s 14931
step    400 | loss 1.4940 | lr 3.00e-04 | grad 2.75 | tok/s 14821
step    410 | loss 1.2859 | lr 3.00e-04 | grad 2.17 | tok/s 14529
step    420 | loss 1.8258 | lr 3.00e-04 | grad 4.94 | tok/s 13879

Training complete! Final step: 423
