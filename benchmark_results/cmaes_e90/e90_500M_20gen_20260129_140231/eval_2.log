Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_2/levelE90_100m_20260129_140237
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 508,031,070 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.1552 | lr 3.00e-04 | grad 15.44 | tok/s 5900
step     20 | loss 2.8211 | lr 3.00e-04 | grad 6.47 | tok/s 14345
step     30 | loss 2.5518 | lr 3.00e-04 | grad 3.48 | tok/s 14502
step     40 | loss 2.4075 | lr 3.00e-04 | grad 3.70 | tok/s 13881
step     50 | loss 2.9848 | lr 3.00e-04 | grad 13.00 | tok/s 14078
step     60 | loss 2.0661 | lr 3.00e-04 | grad 5.47 | tok/s 14519
step     70 | loss 1.9511 | lr 3.00e-04 | grad 5.38 | tok/s 14674
step     80 | loss 5.7527 | lr 3.00e-04 | grad 95.50 | tok/s 14745
step     90 | loss 5.1828 | lr 3.00e-04 | grad 11.31 | tok/s 14953
step    100 | loss 4.1489 | lr 3.00e-04 | grad 9.94 | tok/s 14877
step    110 | loss 3.5450 | lr 3.00e-04 | grad 13.31 | tok/s 14871
step    120 | loss 3.1863 | lr 3.00e-04 | grad 15.88 | tok/s 14825
step    130 | loss 2.9044 | lr 3.00e-04 | grad 18.62 | tok/s 14813
step    140 | loss 2.7313 | lr 3.00e-04 | grad 12.50 | tok/s 14730
step    150 | loss 2.7331 | lr 3.00e-04 | grad 10.75 | tok/s 14723
step    160 | loss 2.3409 | lr 3.00e-04 | grad 11.06 | tok/s 14689
step    170 | loss 2.3907 | lr 3.00e-04 | grad 12.19 | tok/s 14656
step    180 | loss 2.2242 | lr 3.00e-04 | grad 6.94 | tok/s 14636
step    190 | loss 2.3780 | lr 3.00e-04 | grad 6.56 | tok/s 14602
step    200 | loss 2.0745 | lr 3.00e-04 | grad 5.16 | tok/s 14577
step    210 | loss 2.1235 | lr 3.00e-04 | grad 7.75 | tok/s 14559
step    220 | loss 2.1833 | lr 3.00e-04 | grad 3.45 | tok/s 14335
step    230 | loss 2.1283 | lr 3.00e-04 | grad 3.98 | tok/s 14145
step    240 | loss 2.3201 | lr 3.00e-04 | grad 5.06 | tok/s 13427
step    250 | loss 2.1364 | lr 3.00e-04 | grad 2.78 | tok/s 13804
step    260 | loss 1.6163 | lr 3.00e-04 | grad 3.17 | tok/s 14222
step    270 | loss 2.1165 | lr 3.00e-04 | grad 2.97 | tok/s 14034
step    280 | loss 2.2938 | lr 3.00e-04 | grad 6.69 | tok/s 13746
step    290 | loss 1.5598 | lr 3.00e-04 | grad 3.84 | tok/s 14449
step    300 | loss 0.6300 | lr 3.00e-04 | grad 2.45 | tok/s 14429
step    310 | loss 2.4356 | lr 3.00e-04 | grad 3.83 | tok/s 14192
step    320 | loss 1.9984 | lr 3.00e-04 | grad 5.69 | tok/s 13875
step    330 | loss 1.9899 | lr 3.00e-04 | grad 3.25 | tok/s 13394
step    340 | loss 2.3372 | lr 3.00e-04 | grad 2.95 | tok/s 13595
step    350 | loss 1.9434 | lr 3.00e-04 | grad 4.53 | tok/s 13906
step    360 | loss 1.3455 | lr 3.00e-04 | grad 11.25 | tok/s 14219
step    370 | loss 1.8663 | lr 3.00e-04 | grad 3.00 | tok/s 12887
step    380 | loss 1.8075 | lr 3.00e-04 | grad 2.67 | tok/s 13732
step    390 | loss 1.5674 | lr 3.00e-04 | grad 2.28 | tok/s 14318
step    400 | loss 1.5396 | lr 3.00e-04 | grad 2.83 | tok/s 14193
step    410 | loss 1.3300 | lr 3.00e-04 | grad 2.34 | tok/s 13897

Training complete! Final step: 410
