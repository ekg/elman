Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_19/levelE90_100m_20260129_141142
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 496,229,664 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 7.4535 | lr 3.00e-04 | grad 24.12 | tok/s 5979
step     20 | loss 3.4012 | lr 3.00e-04 | grad 8.25 | tok/s 16917
step     30 | loss 2.6592 | lr 3.00e-04 | grad 7.22 | tok/s 17161
step     40 | loss 2.5137 | lr 3.00e-04 | grad 5.28 | tok/s 16401
step     50 | loss 3.2147 | lr 3.00e-04 | grad 17.62 | tok/s 16680
step     60 | loss 2.2055 | lr 3.00e-04 | grad 14.62 | tok/s 17162
step     70 | loss 2.0429 | lr 3.00e-04 | grad 6.03 | tok/s 17376
step     80 | loss 7.0252 | lr 3.00e-04 | grad 78.00 | tok/s 17501
step     90 | loss 5.3953 | lr 3.00e-04 | grad 6.38 | tok/s 17795
step    100 | loss 4.2278 | lr 3.00e-04 | grad 6.88 | tok/s 17696
step    110 | loss 3.2729 | lr 3.00e-04 | grad 14.44 | tok/s 17714
step    120 | loss 2.8443 | lr 3.00e-04 | grad 12.12 | tok/s 17673
step    130 | loss 2.6548 | lr 3.00e-04 | grad 11.56 | tok/s 17671
step    140 | loss 2.5710 | lr 3.00e-04 | grad 10.62 | tok/s 17633
step    150 | loss 2.3609 | lr 3.00e-04 | grad 8.25 | tok/s 17632
step    160 | loss 2.1995 | lr 3.00e-04 | grad 14.88 | tok/s 17609
step    170 | loss 2.2458 | lr 3.00e-04 | grad 24.00 | tok/s 17588
step    180 | loss 2.0924 | lr 3.00e-04 | grad 7.41 | tok/s 17567
step    190 | loss 2.2371 | lr 3.00e-04 | grad 8.12 | tok/s 17601
step    200 | loss 1.9825 | lr 3.00e-04 | grad 5.62 | tok/s 17558
step    210 | loss 2.0478 | lr 3.00e-04 | grad 11.62 | tok/s 17546
step    220 | loss 2.3124 | lr 3.00e-04 | grad 4.94 | tok/s 17367
step    230 | loss 2.3596 | lr 3.00e-04 | grad 3.59 | tok/s 17135
step    240 | loss 2.4154 | lr 3.00e-04 | grad 5.47 | tok/s 16266
step    250 | loss 2.1916 | lr 3.00e-04 | grad 3.34 | tok/s 16721
step    260 | loss 1.6922 | lr 3.00e-04 | grad 3.94 | tok/s 17208
step    270 | loss 2.1913 | lr 3.00e-04 | grad 3.59 | tok/s 17030
step    280 | loss 2.3413 | lr 3.00e-04 | grad 7.94 | tok/s 16712
step    290 | loss 1.6931 | lr 3.00e-04 | grad 2.53 | tok/s 17622
step    300 | loss 0.6704 | lr 3.00e-04 | grad 4.19 | tok/s 17556
step    310 | loss 2.5360 | lr 3.00e-04 | grad 5.03 | tok/s 17227
step    320 | loss 2.0820 | lr 3.00e-04 | grad 6.09 | tok/s 16851
step    330 | loss 2.0368 | lr 3.00e-04 | grad 4.16 | tok/s 16302
step    340 | loss 2.3811 | lr 3.00e-04 | grad 3.44 | tok/s 16556
step    350 | loss 1.9816 | lr 3.00e-04 | grad 4.06 | tok/s 16948
step    360 | loss 1.3472 | lr 3.00e-04 | grad 13.12 | tok/s 17311
step    370 | loss 1.9166 | lr 3.00e-04 | grad 3.81 | tok/s 15722
step    380 | loss 1.8293 | lr 3.00e-04 | grad 4.03 | tok/s 16747
step    390 | loss 1.6096 | lr 3.00e-04 | grad 3.41 | tok/s 17478
step    400 | loss 1.5749 | lr 3.00e-04 | grad 3.38 | tok/s 17302
step    410 | loss 1.3600 | lr 3.00e-04 | grad 2.75 | tok/s 16942
step    420 | loss 1.8995 | lr 3.00e-04 | grad 5.78 | tok/s 16174
step    430 | loss 2.2334 | lr 3.00e-04 | grad 4.22 | tok/s 17216
step    440 | loss 2.2226 | lr 3.00e-04 | grad 4.94 | tok/s 16266
step    450 | loss 2.0451 | lr 3.00e-04 | grad 3.28 | tok/s 16821
step    460 | loss 1.7911 | lr 3.00e-04 | grad 4.25 | tok/s 16483
step    470 | loss 1.8929 | lr 3.00e-04 | grad 3.47 | tok/s 16963
step    480 | loss 2.2824 | lr 3.00e-04 | grad 7.53 | tok/s 16993
step    490 | loss 1.8682 | lr 3.00e-04 | grad 3.56 | tok/s 16067

Training complete! Final step: 490
