Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_15/levelE90_100m_20260129_140927
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 496,922,733 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.4741 | lr 3.00e-04 | grad 11.75 | tok/s 5928
step     20 | loss 2.8292 | lr 3.00e-04 | grad 5.97 | tok/s 14454
step     30 | loss 2.4476 | lr 3.00e-04 | grad 3.45 | tok/s 14604
step     40 | loss 2.3155 | lr 3.00e-04 | grad 3.72 | tok/s 13944
step     50 | loss 2.8543 | lr 3.00e-04 | grad 14.44 | tok/s 14136
step     60 | loss 2.0440 | lr 3.00e-04 | grad 3.56 | tok/s 14626
step     70 | loss 1.8573 | lr 3.00e-04 | grad 4.88 | tok/s 14786
step     80 | loss 5.2745 | lr 3.00e-04 | grad 51.00 | tok/s 14859
step     90 | loss 4.6486 | lr 3.00e-04 | grad 7.41 | tok/s 15096
step    100 | loss 3.7623 | lr 3.00e-04 | grad 6.31 | tok/s 15046
step    110 | loss 3.0811 | lr 3.00e-04 | grad 9.12 | tok/s 14974
step    120 | loss 3.0414 | lr 3.00e-04 | grad 22.50 | tok/s 14867
step    130 | loss 2.7868 | lr 3.00e-04 | grad 8.69 | tok/s 14828
step    140 | loss 2.5367 | lr 3.00e-04 | grad 7.22 | tok/s 14830
step    150 | loss 2.5621 | lr 3.00e-04 | grad 6.94 | tok/s 14833
step    160 | loss 2.2555 | lr 3.00e-04 | grad 7.47 | tok/s 14833
step    170 | loss 2.2877 | lr 3.00e-04 | grad 9.31 | tok/s 14833
step    180 | loss 2.1084 | lr 3.00e-04 | grad 5.12 | tok/s 14832
step    190 | loss 2.2572 | lr 3.00e-04 | grad 12.56 | tok/s 14832
step    200 | loss 1.9811 | lr 3.00e-04 | grad 4.25 | tok/s 14832
step    210 | loss 2.0295 | lr 3.00e-04 | grad 5.66 | tok/s 14829
step    220 | loss 2.1152 | lr 3.00e-04 | grad 3.23 | tok/s 14649
step    230 | loss 2.1041 | lr 3.00e-04 | grad 3.59 | tok/s 14478
step    240 | loss 2.2799 | lr 3.00e-04 | grad 4.69 | tok/s 13741
step    250 | loss 2.1010 | lr 3.00e-04 | grad 2.78 | tok/s 14130
step    260 | loss 1.5623 | lr 3.00e-04 | grad 3.06 | tok/s 14592
step    270 | loss 2.0729 | lr 3.00e-04 | grad 3.00 | tok/s 14384
step    280 | loss 2.2601 | lr 3.00e-04 | grad 6.03 | tok/s 14080
step    290 | loss 1.4947 | lr 3.00e-04 | grad 26.75 | tok/s 14834
step    300 | loss 0.6093 | lr 3.00e-04 | grad 6.28 | tok/s 14831
step    310 | loss 2.4102 | lr 3.00e-04 | grad 3.52 | tok/s 14598
step    320 | loss 1.9435 | lr 3.00e-04 | grad 5.62 | tok/s 14261
step    330 | loss 1.9633 | lr 3.00e-04 | grad 3.25 | tok/s 13802
step    340 | loss 2.2791 | lr 3.00e-04 | grad 2.81 | tok/s 13991
step    350 | loss 1.8965 | lr 3.00e-04 | grad 4.16 | tok/s 14374
step    360 | loss 1.2672 | lr 3.00e-04 | grad 10.31 | tok/s 14630
step    370 | loss 1.8411 | lr 3.00e-04 | grad 3.00 | tok/s 13288
step    380 | loss 1.7735 | lr 3.00e-04 | grad 2.83 | tok/s 14141
step    390 | loss 1.5525 | lr 3.00e-04 | grad 2.33 | tok/s 14725
step    400 | loss 1.5076 | lr 3.00e-04 | grad 2.77 | tok/s 14595
step    410 | loss 1.3010 | lr 3.00e-04 | grad 2.25 | tok/s 14274

Training complete! Final step: 417
