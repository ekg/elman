Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_72/levelE90_100m_20260129_144114
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 503,546,058 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.3828 | lr 3.00e-04 | grad 12.81 | tok/s 5984
step     20 | loss 2.9600 | lr 3.00e-04 | grad 7.31 | tok/s 14372
step     30 | loss 2.4996 | lr 3.00e-04 | grad 3.94 | tok/s 14494
step     40 | loss 2.3131 | lr 3.00e-04 | grad 3.80 | tok/s 13912
step     50 | loss 2.9637 | lr 3.00e-04 | grad 11.75 | tok/s 14148
step     60 | loss 2.0657 | lr 3.00e-04 | grad 6.97 | tok/s 14549
step     70 | loss 1.8597 | lr 3.00e-04 | grad 4.91 | tok/s 14705
step     80 | loss 5.1457 | lr 3.00e-04 | grad 59.75 | tok/s 14785
step     90 | loss 4.7330 | lr 3.00e-04 | grad 7.47 | tok/s 15004
step    100 | loss 3.9796 | lr 3.00e-04 | grad 7.03 | tok/s 14978
step    110 | loss 3.2411 | lr 3.00e-04 | grad 12.06 | tok/s 14914
step    120 | loss 3.0987 | lr 3.00e-04 | grad 9.56 | tok/s 14856
step    130 | loss 2.8945 | lr 3.00e-04 | grad 11.62 | tok/s 14834
step    140 | loss 2.7084 | lr 3.00e-04 | grad 9.69 | tok/s 14832
step    150 | loss 2.7849 | lr 3.00e-04 | grad 20.38 | tok/s 14837
step    160 | loss 2.3300 | lr 3.00e-04 | grad 10.62 | tok/s 14835
step    170 | loss 2.3949 | lr 3.00e-04 | grad 11.06 | tok/s 14835
step    180 | loss 2.2281 | lr 3.00e-04 | grad 5.75 | tok/s 14832
step    190 | loss 2.3005 | lr 3.00e-04 | grad 6.50 | tok/s 14832
step    200 | loss 2.0257 | lr 3.00e-04 | grad 3.98 | tok/s 14839
step    210 | loss 2.1324 | lr 3.00e-04 | grad 5.09 | tok/s 14832
step    220 | loss 2.1280 | lr 3.00e-04 | grad 3.17 | tok/s 14648
step    230 | loss 2.0665 | lr 3.00e-04 | grad 3.39 | tok/s 14478
step    240 | loss 2.2655 | lr 3.00e-04 | grad 4.66 | tok/s 13747
step    250 | loss 2.0829 | lr 3.00e-04 | grad 2.66 | tok/s 14136
step    260 | loss 1.5371 | lr 3.00e-04 | grad 3.08 | tok/s 14592
step    270 | loss 2.0527 | lr 3.00e-04 | grad 2.92 | tok/s 14388
step    280 | loss 2.2392 | lr 3.00e-04 | grad 6.16 | tok/s 14117
step    290 | loss 1.4276 | lr 3.00e-04 | grad 3.53 | tok/s 14833
step    300 | loss 0.5877 | lr 3.00e-04 | grad 2.83 | tok/s 14837
step    310 | loss 2.3895 | lr 3.00e-04 | grad 3.22 | tok/s 14601
step    320 | loss 1.9011 | lr 3.00e-04 | grad 5.34 | tok/s 14297
step    330 | loss 1.9495 | lr 3.00e-04 | grad 3.11 | tok/s 13802
step    340 | loss 2.2678 | lr 3.00e-04 | grad 2.86 | tok/s 14026
step    350 | loss 1.8911 | lr 3.00e-04 | grad 4.56 | tok/s 14379
step    360 | loss 1.2309 | lr 3.00e-04 | grad 9.38 | tok/s 14707
step    370 | loss 1.8167 | lr 3.00e-04 | grad 2.91 | tok/s 13312
step    380 | loss 1.7521 | lr 3.00e-04 | grad 2.70 | tok/s 14194
step    390 | loss 1.5330 | lr 3.00e-04 | grad 2.22 | tok/s 14800
step    400 | loss 1.4926 | lr 3.00e-04 | grad 2.64 | tok/s 14699
step    410 | loss 1.2840 | lr 3.00e-04 | grad 2.16 | tok/s 14382

Training complete! Final step: 417
