Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_24/levelE90_100m_20260129_141358
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 504,803,020 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 5.0267 | lr 3.00e-04 | grad 14.31 | tok/s 6136
step     20 | loss 3.0963 | lr 3.00e-04 | grad 5.69 | tok/s 16149
step     30 | loss 2.5292 | lr 3.00e-04 | grad 3.72 | tok/s 16317
step     40 | loss 2.3575 | lr 3.00e-04 | grad 3.73 | tok/s 15550
step     50 | loss 3.0222 | lr 3.00e-04 | grad 14.69 | tok/s 15822
step     60 | loss 2.0887 | lr 3.00e-04 | grad 2.92 | tok/s 16298
step     70 | loss 1.9293 | lr 3.00e-04 | grad 4.47 | tok/s 16487
step     80 | loss 5.4955 | lr 3.00e-04 | grad 48.25 | tok/s 16566
step     90 | loss 4.7774 | lr 3.00e-04 | grad 5.75 | tok/s 16807
step    100 | loss 3.9721 | lr 3.00e-04 | grad 5.50 | tok/s 16782
step    110 | loss 3.3144 | lr 3.00e-04 | grad 10.88 | tok/s 16774
step    120 | loss 3.1069 | lr 3.00e-04 | grad 9.31 | tok/s 16722
step    130 | loss 2.7896 | lr 3.00e-04 | grad 8.12 | tok/s 16707
step    140 | loss 2.6511 | lr 3.00e-04 | grad 8.12 | tok/s 16702
step    150 | loss 2.6212 | lr 3.00e-04 | grad 6.59 | tok/s 16638
step    160 | loss 2.2892 | lr 3.00e-04 | grad 10.38 | tok/s 16636
step    170 | loss 2.3021 | lr 3.00e-04 | grad 7.81 | tok/s 16629
step    180 | loss 2.1340 | lr 3.00e-04 | grad 6.06 | tok/s 16643
step    190 | loss 2.2946 | lr 3.00e-04 | grad 5.62 | tok/s 16629
step    200 | loss 1.9835 | lr 3.00e-04 | grad 4.81 | tok/s 16629
step    210 | loss 2.0486 | lr 3.00e-04 | grad 6.03 | tok/s 16567
step    220 | loss 2.1580 | lr 3.00e-04 | grad 3.53 | tok/s 16380
step    230 | loss 2.1717 | lr 3.00e-04 | grad 3.52 | tok/s 16167
step    240 | loss 2.3169 | lr 3.00e-04 | grad 4.22 | tok/s 15364
step    250 | loss 2.1270 | lr 3.00e-04 | grad 2.61 | tok/s 15726
step    260 | loss 1.6176 | lr 3.00e-04 | grad 3.08 | tok/s 16157
step    270 | loss 2.1208 | lr 3.00e-04 | grad 2.95 | tok/s 15961
step    280 | loss 2.2932 | lr 3.00e-04 | grad 5.25 | tok/s 15644
step    290 | loss 1.4977 | lr 3.00e-04 | grad 2.70 | tok/s 16527
step    300 | loss 0.6129 | lr 3.00e-04 | grad 5.00 | tok/s 16465
step    310 | loss 2.4586 | lr 3.00e-04 | grad 3.75 | tok/s 16159
step    320 | loss 1.9835 | lr 3.00e-04 | grad 5.66 | tok/s 15835
step    330 | loss 1.9840 | lr 3.00e-04 | grad 3.30 | tok/s 15309
step    340 | loss 2.3119 | lr 3.00e-04 | grad 2.77 | tok/s 15522
step    350 | loss 1.9370 | lr 3.00e-04 | grad 3.64 | tok/s 15918
step    360 | loss 1.3103 | lr 3.00e-04 | grad 9.62 | tok/s 16273
step    370 | loss 1.8632 | lr 3.00e-04 | grad 2.95 | tok/s 14814
step    380 | loss 1.7936 | lr 3.00e-04 | grad 2.95 | tok/s 15712
step    390 | loss 1.5724 | lr 3.00e-04 | grad 2.41 | tok/s 16418
step    400 | loss 1.5237 | lr 3.00e-04 | grad 2.69 | tok/s 16272
step    410 | loss 1.3192 | lr 3.00e-04 | grad 2.25 | tok/s 15928
step    420 | loss 1.8499 | lr 3.00e-04 | grad 4.72 | tok/s 15203
step    430 | loss 2.1999 | lr 3.00e-04 | grad 3.39 | tok/s 16161
step    440 | loss 2.1836 | lr 3.00e-04 | grad 4.16 | tok/s 15267
step    450 | loss 1.9629 | lr 3.00e-04 | grad 2.92 | tok/s 15807
step    460 | loss 1.7529 | lr 3.00e-04 | grad 3.42 | tok/s 15471

Training complete! Final step: 464
