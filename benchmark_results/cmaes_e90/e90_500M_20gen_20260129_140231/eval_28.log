Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_28/levelE90_100m_20260129_141615
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 506,439,332 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 5.1387 | lr 3.00e-04 | grad 16.75 | tok/s 6170
step     20 | loss 3.2222 | lr 3.00e-04 | grad 5.38 | tok/s 16726
step     30 | loss 2.5626 | lr 3.00e-04 | grad 4.09 | tok/s 16899
step     40 | loss 2.3637 | lr 3.00e-04 | grad 4.09 | tok/s 16159
step     50 | loss 3.0126 | lr 3.00e-04 | grad 12.44 | tok/s 16412
step     60 | loss 2.0820 | lr 3.00e-04 | grad 4.03 | tok/s 16949
step     70 | loss 1.9338 | lr 3.00e-04 | grad 4.88 | tok/s 17087
step     80 | loss 5.5783 | lr 3.00e-04 | grad 54.25 | tok/s 17178
step     90 | loss 4.8449 | lr 3.00e-04 | grad 5.81 | tok/s 17467
step    100 | loss 4.0142 | lr 3.00e-04 | grad 5.41 | tok/s 17444
step    110 | loss 3.1739 | lr 3.00e-04 | grad 10.19 | tok/s 17377
step    120 | loss 3.1485 | lr 3.00e-04 | grad 9.00 | tok/s 17331
step    130 | loss 2.7780 | lr 3.00e-04 | grad 9.00 | tok/s 17333
step    140 | loss 2.5715 | lr 3.00e-04 | grad 8.69 | tok/s 17323
step    150 | loss 2.7274 | lr 3.00e-04 | grad 17.62 | tok/s 17331
step    160 | loss 2.3402 | lr 3.00e-04 | grad 9.50 | tok/s 17334
step    170 | loss 2.3279 | lr 3.00e-04 | grad 8.50 | tok/s 17306
step    180 | loss 2.2523 | lr 3.00e-04 | grad 8.62 | tok/s 17247
step    190 | loss 2.3275 | lr 3.00e-04 | grad 5.28 | tok/s 17290
step    200 | loss 2.1007 | lr 3.00e-04 | grad 4.28 | tok/s 17213
step    210 | loss 2.0670 | lr 3.00e-04 | grad 6.00 | tok/s 17189
step    220 | loss 2.1639 | lr 3.00e-04 | grad 3.47 | tok/s 16980
step    230 | loss 2.1301 | lr 3.00e-04 | grad 2.89 | tok/s 16787
step    240 | loss 2.3073 | lr 3.00e-04 | grad 4.47 | tok/s 15941
step    250 | loss 2.1172 | lr 3.00e-04 | grad 2.55 | tok/s 16366
step    260 | loss 1.6145 | lr 3.00e-04 | grad 2.98 | tok/s 16868
step    270 | loss 2.1279 | lr 3.00e-04 | grad 2.86 | tok/s 16668
step    280 | loss 2.2970 | lr 3.00e-04 | grad 6.31 | tok/s 16346
step    290 | loss 1.5104 | lr 3.00e-04 | grad 9.00 | tok/s 17211
step    300 | loss 0.6483 | lr 3.00e-04 | grad 21.00 | tok/s 17179
step    310 | loss 2.4518 | lr 3.00e-04 | grad 4.22 | tok/s 16872
step    320 | loss 1.9781 | lr 3.00e-04 | grad 5.47 | tok/s 16502
step    330 | loss 1.9869 | lr 3.00e-04 | grad 3.17 | tok/s 15947
step    340 | loss 2.3152 | lr 3.00e-04 | grad 2.83 | tok/s 16205
step    350 | loss 1.9259 | lr 3.00e-04 | grad 3.52 | tok/s 16595
step    360 | loss 1.2548 | lr 3.00e-04 | grad 11.81 | tok/s 16952
step    370 | loss 1.8647 | lr 3.00e-04 | grad 2.97 | tok/s 15424
step    380 | loss 1.8212 | lr 3.00e-04 | grad 3.08 | tok/s 16387
step    390 | loss 1.5745 | lr 3.00e-04 | grad 2.33 | tok/s 17109
step    400 | loss 1.5346 | lr 3.00e-04 | grad 2.75 | tok/s 16925
step    410 | loss 1.3218 | lr 3.00e-04 | grad 2.23 | tok/s 16590
step    420 | loss 1.8566 | lr 3.00e-04 | grad 4.97 | tok/s 15838
step    430 | loss 2.1986 | lr 3.00e-04 | grad 3.44 | tok/s 16819
step    440 | loss 2.1902 | lr 3.00e-04 | grad 4.38 | tok/s 15954
step    450 | loss 2.0118 | lr 3.00e-04 | grad 2.92 | tok/s 16495
step    460 | loss 1.7634 | lr 3.00e-04 | grad 3.55 | tok/s 16132
step    470 | loss 1.8627 | lr 3.00e-04 | grad 2.61 | tok/s 16633
step    480 | loss 2.2441 | lr 3.00e-04 | grad 6.47 | tok/s 16607

Training complete! Final step: 481
