Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_66/levelE90_100m_20260129_143857
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 507,536,421 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.3718 | lr 3.00e-04 | grad 15.00 | tok/s 5891
step     20 | loss 3.0433 | lr 3.00e-04 | grad 7.25 | tok/s 14467
step     30 | loss 2.5339 | lr 3.00e-04 | grad 3.98 | tok/s 14593
step     40 | loss 2.3308 | lr 3.00e-04 | grad 4.25 | tok/s 13972
step     50 | loss 2.9519 | lr 3.00e-04 | grad 15.19 | tok/s 14053
step     60 | loss 2.0626 | lr 3.00e-04 | grad 3.94 | tok/s 14501
step     70 | loss 1.8436 | lr 3.00e-04 | grad 5.03 | tok/s 14686
step     80 | loss 5.2678 | lr 3.00e-04 | grad 52.00 | tok/s 14755
step     90 | loss 4.6913 | lr 3.00e-04 | grad 8.06 | tok/s 14988
step    100 | loss 3.8341 | lr 3.00e-04 | grad 7.28 | tok/s 14960
step    110 | loss 3.1756 | lr 3.00e-04 | grad 13.69 | tok/s 14922
step    120 | loss 3.0880 | lr 3.00e-04 | grad 10.06 | tok/s 14874
step    130 | loss 2.9114 | lr 3.00e-04 | grad 13.69 | tok/s 14885
step    140 | loss 2.7743 | lr 3.00e-04 | grad 10.75 | tok/s 14848
step    150 | loss 2.6831 | lr 3.00e-04 | grad 7.00 | tok/s 14837
step    160 | loss 2.3315 | lr 3.00e-04 | grad 10.25 | tok/s 14835
step    170 | loss 2.4474 | lr 3.00e-04 | grad 11.44 | tok/s 14838
step    180 | loss 2.2547 | lr 3.00e-04 | grad 6.09 | tok/s 14833
step    190 | loss 2.3310 | lr 3.00e-04 | grad 5.09 | tok/s 14836
step    200 | loss 2.0468 | lr 3.00e-04 | grad 4.03 | tok/s 14836
step    210 | loss 2.0884 | lr 3.00e-04 | grad 7.25 | tok/s 14836
step    220 | loss 2.1374 | lr 3.00e-04 | grad 3.41 | tok/s 14651
step    230 | loss 2.0357 | lr 3.00e-04 | grad 3.44 | tok/s 14478
step    240 | loss 2.2760 | lr 3.00e-04 | grad 4.97 | tok/s 13746
step    250 | loss 2.0857 | lr 3.00e-04 | grad 2.70 | tok/s 14135
step    260 | loss 1.5451 | lr 3.00e-04 | grad 3.16 | tok/s 14594
step    270 | loss 2.0696 | lr 3.00e-04 | grad 3.00 | tok/s 14388
step    280 | loss 2.2452 | lr 3.00e-04 | grad 5.62 | tok/s 14122
step    290 | loss 1.3884 | lr 3.00e-04 | grad 4.34 | tok/s 14832
step    300 | loss 0.6301 | lr 3.00e-04 | grad 3.69 | tok/s 14836
step    310 | loss 2.3852 | lr 3.00e-04 | grad 3.73 | tok/s 14605
step    320 | loss 1.9170 | lr 3.00e-04 | grad 5.66 | tok/s 14295
step    330 | loss 1.9488 | lr 3.00e-04 | grad 3.22 | tok/s 13809
step    340 | loss 2.2834 | lr 3.00e-04 | grad 2.95 | tok/s 14027
step    350 | loss 1.8819 | lr 3.00e-04 | grad 4.22 | tok/s 14383
step    360 | loss 1.2449 | lr 3.00e-04 | grad 7.66 | tok/s 14704
step    370 | loss 1.8190 | lr 3.00e-04 | grad 2.88 | tok/s 13314
step    380 | loss 1.7660 | lr 3.00e-04 | grad 2.73 | tok/s 14202
step    390 | loss 1.5382 | lr 3.00e-04 | grad 2.20 | tok/s 14835
step    400 | loss 1.4905 | lr 3.00e-04 | grad 2.70 | tok/s 14705
step    410 | loss 1.2812 | lr 3.00e-04 | grad 2.25 | tok/s 14384

Training complete! Final step: 417
