Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_140231/eval_39/levelE90_100m_20260129_142304
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 504,123,588 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 5.5327 | lr 3.00e-04 | grad 15.38 | tok/s 6176
step     20 | loss 3.4204 | lr 3.00e-04 | grad 6.94 | tok/s 17018
step     30 | loss 2.5961 | lr 3.00e-04 | grad 4.47 | tok/s 17188
step     40 | loss 2.3945 | lr 3.00e-04 | grad 4.66 | tok/s 16433
step     50 | loss 3.1199 | lr 3.00e-04 | grad 14.25 | tok/s 16697
step     60 | loss 2.1034 | lr 3.00e-04 | grad 4.94 | tok/s 17216
step     70 | loss 1.9442 | lr 3.00e-04 | grad 5.09 | tok/s 17361
step     80 | loss 5.6529 | lr 3.00e-04 | grad 53.50 | tok/s 17485
step     90 | loss 4.7891 | lr 3.00e-04 | grad 5.81 | tok/s 17751
step    100 | loss 4.2359 | lr 3.00e-04 | grad 5.34 | tok/s 17700
step    110 | loss 3.1930 | lr 3.00e-04 | grad 9.81 | tok/s 17712
step    120 | loss 3.1109 | lr 3.00e-04 | grad 10.44 | tok/s 17648
step    130 | loss 2.7667 | lr 3.00e-04 | grad 8.00 | tok/s 17648
step    140 | loss 2.7041 | lr 3.00e-04 | grad 8.44 | tok/s 17624
step    150 | loss 2.6690 | lr 3.00e-04 | grad 7.41 | tok/s 17590
step    160 | loss 2.3312 | lr 3.00e-04 | grad 9.12 | tok/s 17552
step    170 | loss 2.3980 | lr 3.00e-04 | grad 11.62 | tok/s 17551
step    180 | loss 2.1617 | lr 3.00e-04 | grad 6.03 | tok/s 17539
step    190 | loss 2.3105 | lr 3.00e-04 | grad 7.69 | tok/s 17499
step    200 | loss 2.0830 | lr 3.00e-04 | grad 5.00 | tok/s 17492
step    210 | loss 2.0715 | lr 3.00e-04 | grad 6.03 | tok/s 17465
step    220 | loss 2.1889 | lr 3.00e-04 | grad 3.72 | tok/s 17242
step    230 | loss 2.1682 | lr 3.00e-04 | grad 3.30 | tok/s 17025
step    240 | loss 2.3315 | lr 3.00e-04 | grad 4.59 | tok/s 16184
step    250 | loss 2.1354 | lr 3.00e-04 | grad 2.77 | tok/s 16614
step    260 | loss 1.6365 | lr 3.00e-04 | grad 3.09 | tok/s 17124
step    270 | loss 2.1453 | lr 3.00e-04 | grad 3.03 | tok/s 16937
step    280 | loss 2.3093 | lr 3.00e-04 | grad 7.47 | tok/s 16527
step    290 | loss 1.5140 | lr 3.00e-04 | grad 2.97 | tok/s 17477
step    300 | loss 0.6307 | lr 3.00e-04 | grad 3.34 | tok/s 17428
step    310 | loss 2.4986 | lr 3.00e-04 | grad 3.61 | tok/s 17129
step    320 | loss 2.0091 | lr 3.00e-04 | grad 5.28 | tok/s 16782
step    330 | loss 1.9967 | lr 3.00e-04 | grad 3.41 | tok/s 16204
step    340 | loss 2.3286 | lr 3.00e-04 | grad 2.73 | tok/s 16401
step    350 | loss 1.9531 | lr 3.00e-04 | grad 3.44 | tok/s 16804
step    360 | loss 1.2942 | lr 3.00e-04 | grad 8.75 | tok/s 17193
step    370 | loss 1.8785 | lr 3.00e-04 | grad 3.08 | tok/s 15643
step    380 | loss 1.7981 | lr 3.00e-04 | grad 3.33 | tok/s 16644
step    390 | loss 1.5802 | lr 3.00e-04 | grad 2.83 | tok/s 17333
step    400 | loss 1.5368 | lr 3.00e-04 | grad 2.72 | tok/s 17178
step    410 | loss 1.3235 | lr 3.00e-04 | grad 2.22 | tok/s 16815
step    420 | loss 1.8664 | lr 3.00e-04 | grad 5.03 | tok/s 16038
step    430 | loss 2.2218 | lr 3.00e-04 | grad 3.48 | tok/s 17064
step    440 | loss 2.1924 | lr 3.00e-04 | grad 4.38 | tok/s 16134
step    450 | loss 2.0221 | lr 3.00e-04 | grad 2.98 | tok/s 16680
step    460 | loss 1.7701 | lr 3.00e-04 | grad 3.39 | tok/s 16344
step    470 | loss 1.8868 | lr 3.00e-04 | grad 2.56 | tok/s 16841
step    480 | loss 2.2543 | lr 3.00e-04 | grad 6.78 | tok/s 16869

Training complete! Final step: 488
