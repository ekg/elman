Using device: cuda
Output directory: benchmark_results/cmaes_e90/e90_500M_20gen_20260129_135747/eval_4/levelE90_100m_20260129_135754
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E90, 513,234,416 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 2.0 minutes
step     10 | loss 4.1591 | lr 3.00e-04 | grad 21.50 | tok/s 5704
step     20 | loss 2.7849 | lr 3.00e-04 | grad 7.19 | tok/s 12826
step     30 | loss 2.5617 | lr 3.00e-04 | grad 3.70 | tok/s 12945
step     40 | loss 2.4231 | lr 3.00e-04 | grad 3.64 | tok/s 12391
step     50 | loss 3.1051 | lr 3.00e-04 | grad 15.81 | tok/s 12534
step     60 | loss 2.1148 | lr 3.00e-04 | grad 4.47 | tok/s 12894
step     70 | loss 1.9595 | lr 3.00e-04 | grad 5.25 | tok/s 13033
step     80 | loss 5.6482 | lr 3.00e-04 | grad 114.50 | tok/s 13082
step     90 | loss 5.3797 | lr 3.00e-04 | grad 12.38 | tok/s 13288
step    100 | loss 4.2264 | lr 3.00e-04 | grad 12.38 | tok/s 13242
step    110 | loss 3.7042 | lr 3.00e-04 | grad 18.88 | tok/s 13195
step    120 | loss 3.3163 | lr 3.00e-04 | grad 27.12 | tok/s 13172
step    130 | loss 3.0032 | lr 3.00e-04 | grad 26.75 | tok/s 13133
step    140 | loss 2.7881 | lr 3.00e-04 | grad 16.25 | tok/s 13116
step    150 | loss 2.7041 | lr 3.00e-04 | grad 12.81 | tok/s 13085
step    160 | loss 2.3619 | lr 3.00e-04 | grad 15.19 | tok/s 13050
step    170 | loss 2.5223 | lr 3.00e-04 | grad 12.81 | tok/s 13011
step    180 | loss 2.3010 | lr 3.00e-04 | grad 7.94 | tok/s 13019
step    190 | loss 2.4197 | lr 3.00e-04 | grad 10.81 | tok/s 12959
step    200 | loss 2.2333 | lr 3.00e-04 | grad 6.62 | tok/s 12942
step    210 | loss 2.1635 | lr 3.00e-04 | grad 7.81 | tok/s 12959
step    220 | loss 2.2464 | lr 3.00e-04 | grad 3.19 | tok/s 12792
step    230 | loss 2.1588 | lr 3.00e-04 | grad 3.66 | tok/s 12623
step    240 | loss 2.3147 | lr 3.00e-04 | grad 4.88 | tok/s 11981
step    250 | loss 2.1534 | lr 3.00e-04 | grad 2.53 | tok/s 12293
step    260 | loss 1.6537 | lr 3.00e-04 | grad 2.89 | tok/s 12668
step    270 | loss 2.1328 | lr 3.00e-04 | grad 2.77 | tok/s 12505
step    280 | loss 2.3123 | lr 3.00e-04 | grad 6.22 | tok/s 12265
step    290 | loss 1.5713 | lr 3.00e-04 | grad 4.00 | tok/s 12897
step    300 | loss 0.6671 | lr 3.00e-04 | grad 3.61 | tok/s 12893
step    310 | loss 2.4514 | lr 3.00e-04 | grad 3.77 | tok/s 12651
step    320 | loss 2.0251 | lr 3.00e-04 | grad 5.47 | tok/s 12383
step    330 | loss 2.0057 | lr 3.00e-04 | grad 3.08 | tok/s 11969
step    340 | loss 2.3354 | lr 3.00e-04 | grad 2.77 | tok/s 12125
step    350 | loss 1.9763 | lr 3.00e-04 | grad 5.47 | tok/s 12430
step    360 | loss 1.3773 | lr 3.00e-04 | grad 9.44 | tok/s 12691

Training complete! Final step: 367
