Using device: cuda
Output directory: output/aligned_33_768x23/level33_100m_20260113_225511
Auto r_h_mode: spectral_norm (level 33 has full W_h)
Model: Level 33, 101,986,176 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5846 | lr 9.00e-07 | grad 21.48 | tok/s 6806
step     20 | loss 4.8407 | lr 1.90e-06 | grad 13.58 | tok/s 9617
step     30 | loss 4.2231 | lr 2.90e-06 | grad 11.49 | tok/s 9262
step     40 | loss 3.8625 | lr 3.90e-06 | grad 15.95 | tok/s 9697
step     50 | loss 5.0715 | lr 4.90e-06 | grad 13.84 | tok/s 11213
step     60 | loss 4.5021 | lr 5.90e-06 | grad 9.93 | tok/s 13060
step     70 | loss 4.1161 | lr 6.90e-06 | grad 14.70 | tok/s 12375
step     80 | loss 3.8418 | lr 7.90e-06 | grad 8.21 | tok/s 12040
step     90 | loss 3.3290 | lr 8.90e-06 | grad 7.25 | tok/s 12340
step    100 | loss 3.0710 | lr 9.90e-06 | grad 7.43 | tok/s 16600
step    110 | loss 2.7930 | lr 1.09e-05 | grad 9.98 | tok/s 15116
step    120 | loss 3.4775 | lr 1.19e-05 | grad 7.85 | tok/s 11242
step    130 | loss 2.7815 | lr 1.29e-05 | grad 5.41 | tok/s 10521
step    140 | loss 2.5812 | lr 1.39e-05 | grad 8.99 | tok/s 10300
step    150 | loss 2.9314 | lr 1.49e-05 | grad 8.33 | tok/s 10672
step    160 | loss 2.8336 | lr 1.59e-05 | grad 5.67 | tok/s 10551
step    170 | loss 2.9322 | lr 1.69e-05 | grad 8.86 | tok/s 10197
step    180 | loss 2.8561 | lr 1.79e-05 | grad 9.68 | tok/s 10625
step    190 | loss 2.6536 | lr 1.89e-05 | grad 5.62 | tok/s 10566
step    200 | loss 2.2574 | lr 1.99e-05 | grad 3.39 | tok/s 11008
step    210 | loss 2.1728 | lr 2.09e-05 | grad 5.29 | tok/s 10959
step    220 | loss 2.5386 | lr 2.19e-05 | grad 6.71 | tok/s 10916
step    230 | loss 2.7297 | lr 2.29e-05 | grad 5.20 | tok/s 10730
step    240 | loss 2.3684 | lr 2.39e-05 | grad 4.70 | tok/s 11217
step    250 | loss 2.5690 | lr 2.49e-05 | grad 4.99 | tok/s 11798
step    260 | loss 2.1597 | lr 2.59e-05 | grad 3.46 | tok/s 11568
step    270 | loss 2.3146 | lr 2.69e-05 | grad 4.97 | tok/s 11511
step    280 | loss 2.0136 | lr 2.79e-05 | grad 4.05 | tok/s 11704
step    290 | loss 2.0069 | lr 2.89e-05 | grad 7.22 | tok/s 10995
step    300 | loss 2.0670 | lr 2.99e-05 | grad 5.28 | tok/s 14543
step    310 | loss 2.0643 | lr 3.09e-05 | grad 3.40 | tok/s 15314
step    320 | loss 1.8653 | lr 3.19e-05 | grad 6.25 | tok/s 14850
step    330 | loss 2.1207 | lr 3.29e-05 | grad 3.46 | tok/s 15709
step    340 | loss 2.1675 | lr 3.39e-05 | grad 10.93 | tok/s 16935
step    350 | loss 2.1588 | lr 3.49e-05 | grad 5.10 | tok/s 16620
step    360 | loss 2.0365 | lr 3.59e-05 | grad 4.76 | tok/s 15360
step    370 | loss 1.8216 | lr 3.69e-05 | grad 3.52 | tok/s 16566
step    380 | loss 1.8330 | lr 3.79e-05 | grad 4.27 | tok/s 15920
step    390 | loss 1.4648 | lr 3.89e-05 | grad 3.38 | tok/s 11716
step    400 | loss 1.3581 | lr 3.99e-05 | grad 4.05 | tok/s 11812
step    410 | loss 2.2217 | lr 4.09e-05 | grad 3.96 | tok/s 11334
step    420 | loss 2.0759 | lr 4.19e-05 | grad 4.79 | tok/s 12802
step    430 | loss 1.9866 | lr 4.29e-05 | grad 4.87 | tok/s 13658
step    440 | loss 1.8990 | lr 4.39e-05 | grad 5.08 | tok/s 14415
step    450 | loss 2.0139 | lr 4.49e-05 | grad 3.34 | tok/s 14899
step    460 | loss 1.7802 | lr 4.59e-05 | grad 8.05 | tok/s 14838
step    470 | loss 1.8803 | lr 4.69e-05 | grad 4.14 | tok/s 15135
step    480 | loss 1.8831 | lr 4.79e-05 | grad 4.59 | tok/s 15976
step    490 | loss 1.8401 | lr 4.89e-05 | grad 3.28 | tok/s 15363
step    500 | loss 1.9057 | lr 4.99e-05 | grad 3.30 | tok/s 15429
step    510 | loss 2.1243 | lr 5.09e-05 | grad 11.48 | tok/s 15183
step    520 | loss 1.8836 | lr 5.19e-05 | grad 3.37 | tok/s 15544
step    530 | loss 1.7375 | lr 5.29e-05 | grad 3.32 | tok/s 14559
step    540 | loss 1.9635 | lr 5.39e-05 | grad 3.46 | tok/s 14724
step    550 | loss 1.8693 | lr 5.49e-05 | grad 3.24 | tok/s 14524
step    560 | loss 1.5942 | lr 5.59e-05 | grad 4.00 | tok/s 15178
step    570 | loss 1.6251 | lr 5.69e-05 | grad 3.11 | tok/s 15808
step    580 | loss 1.4602 | lr 5.79e-05 | grad 2.58 | tok/s 16704
step    590 | loss 1.4132 | lr 5.89e-05 | grad 2.48 | tok/s 17573
step    600 | loss 1.5151 | lr 5.99e-05 | grad 3.07 | tok/s 17231
step    610 | loss 1.4104 | lr 6.09e-05 | grad 2.44 | tok/s 15673
step    620 | loss 1.4369 | lr 6.19e-05 | grad 1.85 | tok/s 16200
step    630 | loss 1.5377 | lr 6.29e-05 | grad 10.03 | tok/s 15115
step    640 | loss 1.9604 | lr 6.39e-05 | grad 4.82 | tok/s 14297
step    650 | loss 1.9134 | lr 6.49e-05 | grad 3.57 | tok/s 14521
step    660 | loss 1.7815 | lr 6.59e-05 | grad 4.15 | tok/s 14619
step    670 | loss 1.8243 | lr 6.69e-05 | grad 2.96 | tok/s 15365
step    680 | loss 1.9150 | lr 6.79e-05 | grad 3.91 | tok/s 14629
step    690 | loss 1.8881 | lr 6.89e-05 | grad 3.33 | tok/s 13445
step    700 | loss 1.8700 | lr 6.99e-05 | grad 3.28 | tok/s 13210
step    710 | loss 1.7281 | lr 7.09e-05 | grad 2.83 | tok/s 12926
step    720 | loss 1.9379 | lr 7.19e-05 | grad 4.85 | tok/s 12610
step    730 | loss 1.5718 | lr 7.29e-05 | grad 2.46 | tok/s 12788
step    740 | loss 1.6617 | lr 7.39e-05 | grad 2.23 | tok/s 13740
step    750 | loss 2.2274 | lr 7.49e-05 | grad 5.92 | tok/s 14401
step    760 | loss 1.9201 | lr 7.59e-05 | grad 2.50 | tok/s 13748
step    770 | loss 1.7858 | lr 7.69e-05 | grad 3.04 | tok/s 13812
step    780 | loss 1.7911 | lr 7.79e-05 | grad 2.54 | tok/s 13729
step    790 | loss 1.7519 | lr 7.89e-05 | grad 2.55 | tok/s 14254
step    800 | loss 1.9867 | lr 7.99e-05 | grad 5.94 | tok/s 14849
step    810 | loss 1.7620 | lr 8.09e-05 | grad 2.21 | tok/s 14210
step    820 | loss 1.5135 | lr 8.19e-05 | grad 5.29 | tok/s 13845
step    830 | loss 1.7199 | lr 8.29e-05 | grad 2.95 | tok/s 14071
step    840 | loss 1.7962 | lr 8.39e-05 | grad 2.15 | tok/s 13790
step    850 | loss 1.9332 | lr 8.49e-05 | grad 2.69 | tok/s 13887
step    860 | loss 1.9054 | lr 8.59e-05 | grad 2.24 | tok/s 13747
step    870 | loss 1.8530 | lr 8.69e-05 | grad 4.43 | tok/s 13970
step    880 | loss 1.7783 | lr 8.79e-05 | grad 3.08 | tok/s 14184
step    890 | loss 1.8651 | lr 8.89e-05 | grad 2.65 | tok/s 13495
step    900 | loss 1.7072 | lr 8.99e-05 | grad 1.96 | tok/s 13152
step    910 | loss 1.6812 | lr 9.09e-05 | grad 2.19 | tok/s 13148
step    920 | loss 1.7808 | lr 9.19e-05 | grad 2.04 | tok/s 13196
step    930 | loss 1.7830 | lr 9.29e-05 | grad 1.91 | tok/s 13392
step    940 | loss 1.6611 | lr 9.39e-05 | grad 2.39 | tok/s 14201
step    950 | loss 1.5994 | lr 9.49e-05 | grad 1.52 | tok/s 11743
step    960 | loss 1.7067 | lr 9.59e-05 | grad 1.56 | tok/s 11572
step    970 | loss 1.6216 | lr 9.69e-05 | grad 1.56 | tok/s 12639
step    980 | loss 1.6111 | lr 9.79e-05 | grad 1.61 | tok/s 13819
step    990 | loss 2.3045 | lr 9.89e-05 | grad 2.91 | tok/s 15271
step   1000 | loss 1.9985 | lr 9.99e-05 | grad 2.02 | tok/s 14839
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9985.pt

Training complete! Final step: 1000
