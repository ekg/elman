Using device: cuda
Output directory: output/aligned_mamba2_768x27/levelmamba2_100m_20260113_230632
Model: Level mamba2, 101,882,520 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.6744 | lr 9.00e-07 | grad 11.64 | tok/s 5566
step     20 | loss 5.3967 | lr 1.90e-06 | grad 7.33 | tok/s 26882
step     30 | loss 5.1299 | lr 2.90e-06 | grad 6.14 | tok/s 26627
step     40 | loss 4.9832 | lr 3.90e-06 | grad 7.54 | tok/s 27321
step     50 | loss 5.3591 | lr 4.90e-06 | grad 7.46 | tok/s 28347
step     60 | loss 5.0257 | lr 5.90e-06 | grad 3.98 | tok/s 28229
step     70 | loss 4.6017 | lr 6.90e-06 | grad 7.36 | tok/s 28091
step     80 | loss 4.3770 | lr 7.90e-06 | grad 3.64 | tok/s 27938
step     90 | loss 3.8938 | lr 8.90e-06 | grad 3.70 | tok/s 27758
step    100 | loss 3.6348 | lr 9.90e-06 | grad 3.95 | tok/s 27706
step    110 | loss 3.3023 | lr 1.09e-05 | grad 3.63 | tok/s 27315
step    120 | loss 3.6549 | lr 1.19e-05 | grad 2.33 | tok/s 26383
step    130 | loss 2.9947 | lr 1.29e-05 | grad 2.14 | tok/s 25833
step    140 | loss 2.7838 | lr 1.39e-05 | grad 3.16 | tok/s 25808
step    150 | loss 3.1867 | lr 1.49e-05 | grad 4.91 | tok/s 26621
step    160 | loss 3.0594 | lr 1.59e-05 | grad 3.89 | tok/s 26626
step    170 | loss 2.9319 | lr 1.69e-05 | grad 4.22 | tok/s 25176
step    180 | loss 2.9559 | lr 1.79e-05 | grad 4.94 | tok/s 25938
step    190 | loss 2.7564 | lr 1.89e-05 | grad 2.26 | tok/s 24850
step    200 | loss 2.3994 | lr 1.99e-05 | grad 2.06 | tok/s 26505
step    210 | loss 2.2763 | lr 2.09e-05 | grad 2.39 | tok/s 25665
step    220 | loss 2.5879 | lr 2.19e-05 | grad 3.65 | tok/s 24721
step    230 | loss 2.7163 | lr 2.29e-05 | grad 1.67 | tok/s 24729
step    240 | loss 2.3496 | lr 2.39e-05 | grad 2.47 | tok/s 24816
step    250 | loss 2.5815 | lr 2.49e-05 | grad 2.27 | tok/s 24822
step    260 | loss 2.1727 | lr 2.59e-05 | grad 1.98 | tok/s 25684
step    270 | loss 2.3251 | lr 2.69e-05 | grad 2.19 | tok/s 25611
step    280 | loss 2.0041 | lr 2.79e-05 | grad 2.72 | tok/s 24851
step    290 | loss 2.0004 | lr 2.89e-05 | grad 4.03 | tok/s 23919
step    300 | loss 2.0812 | lr 2.99e-05 | grad 3.46 | tok/s 24225
step    310 | loss 2.0561 | lr 3.09e-05 | grad 2.17 | tok/s 24775
step    320 | loss 1.8502 | lr 3.19e-05 | grad 3.21 | tok/s 23724
step    330 | loss 2.1137 | lr 3.29e-05 | grad 1.86 | tok/s 24793
step    340 | loss 2.1453 | lr 3.39e-05 | grad 8.15 | tok/s 25274
step    350 | loss 2.1583 | lr 3.49e-05 | grad 3.11 | tok/s 24797
step    360 | loss 2.1063 | lr 3.59e-05 | grad 2.67 | tok/s 25395
step    370 | loss 1.7850 | lr 3.69e-05 | grad 2.04 | tok/s 24931
step    380 | loss 1.8466 | lr 3.79e-05 | grad 2.41 | tok/s 26009
step    390 | loss 1.5024 | lr 3.89e-05 | grad 1.98 | tok/s 26193
step    400 | loss 1.3671 | lr 3.99e-05 | grad 1.98 | tok/s 25819
step    410 | loss 2.1779 | lr 4.09e-05 | grad 2.39 | tok/s 24959
step    420 | loss 2.0285 | lr 4.19e-05 | grad 2.74 | tok/s 24922
step    430 | loss 1.9967 | lr 4.29e-05 | grad 3.17 | tok/s 26136
step    440 | loss 1.8487 | lr 4.39e-05 | grad 2.48 | tok/s 25319
step    450 | loss 1.9715 | lr 4.49e-05 | grad 1.67 | tok/s 24943
step    460 | loss 1.7256 | lr 4.59e-05 | grad 4.18 | tok/s 24763
step    470 | loss 1.8371 | lr 4.69e-05 | grad 2.08 | tok/s 24785
step    480 | loss 1.8129 | lr 4.79e-05 | grad 2.73 | tok/s 25951
step    490 | loss 1.8460 | lr 4.89e-05 | grad 2.12 | tok/s 25116
step    500 | loss 1.8521 | lr 4.99e-05 | grad 2.03 | tok/s 24898
step    510 | loss 2.0678 | lr 5.09e-05 | grad 6.74 | tok/s 24485
step    520 | loss 1.7891 | lr 5.19e-05 | grad 2.08 | tok/s 23473
step    530 | loss 1.6780 | lr 5.29e-05 | grad 2.18 | tok/s 24904
step    540 | loss 1.9048 | lr 5.39e-05 | grad 1.91 | tok/s 24842
step    550 | loss 1.8013 | lr 5.49e-05 | grad 1.86 | tok/s 24281
step    560 | loss 1.5419 | lr 5.59e-05 | grad 2.38 | tok/s 25420
step    570 | loss 1.6015 | lr 5.69e-05 | grad 1.85 | tok/s 26169
step    580 | loss 1.4449 | lr 5.79e-05 | grad 1.51 | tok/s 26137
step    590 | loss 1.3885 | lr 5.89e-05 | grad 1.26 | tok/s 26163
step    600 | loss 1.4836 | lr 5.99e-05 | grad 1.84 | tok/s 26167
step    610 | loss 1.3879 | lr 6.09e-05 | grad 1.37 | tok/s 26149
step    620 | loss 1.3981 | lr 6.19e-05 | grad 1.13 | tok/s 26106
step    630 | loss 1.4646 | lr 6.29e-05 | grad 3.92 | tok/s 25790
step    640 | loss 1.8808 | lr 6.39e-05 | grad 2.80 | tok/s 24580
step    650 | loss 1.8690 | lr 6.49e-05 | grad 2.75 | tok/s 24447
step    660 | loss 1.6923 | lr 6.59e-05 | grad 2.27 | tok/s 24727
step    670 | loss 1.7608 | lr 6.69e-05 | grad 1.95 | tok/s 25531
step    680 | loss 1.8251 | lr 6.79e-05 | grad 2.16 | tok/s 24634
step    690 | loss 1.8110 | lr 6.89e-05 | grad 2.16 | tok/s 24510
step    700 | loss 1.7721 | lr 6.99e-05 | grad 1.85 | tok/s 24309
step    710 | loss 1.6434 | lr 7.09e-05 | grad 1.88 | tok/s 24992
step    720 | loss 1.8732 | lr 7.19e-05 | grad 3.20 | tok/s 24414
step    730 | loss 1.4695 | lr 7.29e-05 | grad 1.48 | tok/s 25584
step    740 | loss 1.5965 | lr 7.39e-05 | grad 1.49 | tok/s 24887
step    750 | loss 2.1807 | lr 7.49e-05 | grad 3.39 | tok/s 25848
step    760 | loss 1.9039 | lr 7.59e-05 | grad 1.44 | tok/s 25838
step    770 | loss 1.6986 | lr 7.69e-05 | grad 2.23 | tok/s 25280
step    780 | loss 1.7504 | lr 7.79e-05 | grad 1.91 | tok/s 24532
step    790 | loss 1.6952 | lr 7.89e-05 | grad 1.68 | tok/s 25146
step    800 | loss 1.9163 | lr 7.99e-05 | grad 3.81 | tok/s 25890
step    810 | loss 1.6909 | lr 8.09e-05 | grad 1.88 | tok/s 25077
step    820 | loss 1.4426 | lr 8.19e-05 | grad 4.01 | tok/s 24485
step    830 | loss 1.6816 | lr 8.29e-05 | grad 1.93 | tok/s 24766
step    840 | loss 1.7170 | lr 8.39e-05 | grad 1.29 | tok/s 24342
step    850 | loss 1.7965 | lr 8.49e-05 | grad 1.39 | tok/s 24418
step    860 | loss 1.8075 | lr 8.59e-05 | grad 1.51 | tok/s 24562
step    870 | loss 1.7362 | lr 8.69e-05 | grad 3.01 | tok/s 24873
step    880 | loss 1.7828 | lr 8.79e-05 | grad 1.89 | tok/s 26105
step    890 | loss 1.7741 | lr 8.89e-05 | grad 1.77 | tok/s 24889
step    900 | loss 1.6344 | lr 8.99e-05 | grad 1.27 | tok/s 24783
step    910 | loss 1.5949 | lr 9.09e-05 | grad 1.53 | tok/s 25038
step    920 | loss 1.7018 | lr 9.19e-05 | grad 1.21 | tok/s 24725
step    930 | loss 1.6742 | lr 9.29e-05 | grad 1.29 | tok/s 24783
step    940 | loss 1.5755 | lr 9.39e-05 | grad 1.52 | tok/s 25570
step    950 | loss 1.5075 | lr 9.49e-05 | grad 1.14 | tok/s 24482
step    960 | loss 1.6073 | lr 9.59e-05 | grad 1.01 | tok/s 24138
step    970 | loss 1.5314 | lr 9.69e-05 | grad 1.02 | tok/s 24388
step    980 | loss 1.5329 | lr 9.79e-05 | grad 1.19 | tok/s 25083
step    990 | loss 2.2383 | lr 9.89e-05 | grad 1.82 | tok/s 26075
step   1000 | loss 1.8885 | lr 9.99e-05 | grad 1.29 | tok/s 24976
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8885.pt
step   1010 | loss 1.7917 | lr 1.02e-06 | grad 1.63 | tok/s 13759
step   1020 | loss 1.4793 | lr 1.09e-06 | grad 1.51 | tok/s 24062
step   1030 | loss 1.4672 | lr 1.21e-06 | grad 2.53 | tok/s 24358
step   1040 | loss 1.9013 | lr 1.37e-06 | grad 1.34 | tok/s 24479
step   1050 | loss 1.9566 | lr 1.59e-06 | grad 4.13 | tok/s 23430
step   1060 | loss 2.5253 | lr 1.85e-06 | grad 1.06 | tok/s 24329
step   1070 | loss 1.8064 | lr 2.16e-06 | grad 6.22 | tok/s 23247
step   1080 | loss 1.3163 | lr 2.52e-06 | grad 1.68 | tok/s 23977
step   1090 | loss 1.5428 | lr 2.92e-06 | grad 1.39 | tok/s 24069
step   1100 | loss 1.4609 | lr 3.37e-06 | grad 1.04 | tok/s 25219
step   1110 | loss 1.4221 | lr 3.87e-06 | grad 0.91 | tok/s 24649
step   1120 | loss 1.3924 | lr 4.42e-06 | grad 1.03 | tok/s 24463
step   1130 | loss 1.4036 | lr 5.01e-06 | grad 1.21 | tok/s 24207
step   1140 | loss 1.9064 | lr 5.65e-06 | grad 3.28 | tok/s 24272
step   1150 | loss 2.1290 | lr 6.32e-06 | grad 2.35 | tok/s 23811
step   1160 | loss 1.6627 | lr 7.05e-06 | grad 1.51 | tok/s 23555
step   1170 | loss 2.2048 | lr 7.81e-06 | grad 2.25 | tok/s 22992
step   1180 | loss 1.9084 | lr 8.62e-06 | grad 1.28 | tok/s 23267
step   1190 | loss 1.5915 | lr 9.47e-06 | grad 1.07 | tok/s 23041
step   1200 | loss 1.6214 | lr 1.04e-05 | grad 2.11 | tok/s 24252
step   1210 | loss 2.0484 | lr 1.13e-05 | grad 1.25 | tok/s 25024
step   1220 | loss 1.3487 | lr 1.23e-05 | grad 1.18 | tok/s 24819
step   1230 | loss 1.5439 | lr 1.33e-05 | grad 1.72 | tok/s 23312
step   1240 | loss 1.4842 | lr 1.43e-05 | grad 1.29 | tok/s 23961
step   1250 | loss 1.5290 | lr 1.54e-05 | grad 1.01 | tok/s 23940
step   1260 | loss 1.4527 | lr 1.65e-05 | grad 1.12 | tok/s 24043
step   1270 | loss 1.8295 | lr 1.76e-05 | grad 3.12 | tok/s 24351
step   1280 | loss 1.6987 | lr 1.88e-05 | grad 1.08 | tok/s 24392
step   1290 | loss 1.4669 | lr 2.00e-05 | grad 1.22 | tok/s 24061
step   1300 | loss 1.5639 | lr 2.13e-05 | grad 1.55 | tok/s 23833
step   1310 | loss 1.5281 | lr 2.25e-05 | grad 1.11 | tok/s 23711
step   1320 | loss 2.0392 | lr 2.38e-05 | grad 7.18 | tok/s 23696
step   1330 | loss 1.5940 | lr 2.52e-05 | grad 1.39 | tok/s 24353
step   1340 | loss 1.7088 | lr 2.65e-05 | grad 1.83 | tok/s 24626
step   1350 | loss 1.5338 | lr 2.79e-05 | grad 1.24 | tok/s 23759
step   1360 | loss 1.6729 | lr 2.93e-05 | grad 1.44 | tok/s 23637
step   1370 | loss 1.6430 | lr 3.07e-05 | grad 3.41 | tok/s 24269
step   1380 | loss 1.6216 | lr 3.21e-05 | grad 1.87 | tok/s 23764
step   1390 | loss 1.9768 | lr 3.36e-05 | grad 1.85 | tok/s 24734
step   1400 | loss 1.4949 | lr 3.51e-05 | grad 3.34 | tok/s 22900
step   1410 | loss 1.5585 | lr 3.65e-05 | grad 1.06 | tok/s 24133
step   1420 | loss 1.6819 | lr 3.80e-05 | grad 1.41 | tok/s 23565
step   1430 | loss 1.5088 | lr 3.96e-05 | grad 2.80 | tok/s 22754
step   1440 | loss 1.6103 | lr 4.11e-05 | grad 5.72 | tok/s 24840
step   1450 | loss 1.8245 | lr 4.26e-05 | grad 1.41 | tok/s 23347
step   1460 | loss 1.6594 | lr 4.41e-05 | grad 1.87 | tok/s 23995
step   1470 | loss 1.6332 | lr 4.57e-05 | grad 2.03 | tok/s 23907
step   1480 | loss 1.6518 | lr 4.72e-05 | grad 3.14 | tok/s 23212
step   1490 | loss 1.4126 | lr 4.88e-05 | grad 1.05 | tok/s 22619
step   1500 | loss 1.5388 | lr 5.03e-05 | grad 1.48 | tok/s 22987
step   1510 | loss 2.0052 | lr 5.19e-05 | grad 5.67 | tok/s 23213
step   1520 | loss 1.5612 | lr 5.35e-05 | grad 1.24 | tok/s 23441
step   1530 | loss 1.4304 | lr 5.50e-05 | grad 1.06 | tok/s 23192
step   1540 | loss 1.5819 | lr 5.65e-05 | grad 1.30 | tok/s 23708
step   1550 | loss 1.4748 | lr 5.81e-05 | grad 1.17 | tok/s 23652
step   1560 | loss 1.6507 | lr 5.96e-05 | grad 0.96 | tok/s 23662
step   1570 | loss 1.5734 | lr 6.11e-05 | grad 2.34 | tok/s 20253
step   1580 | loss 1.3583 | lr 6.27e-05 | grad 0.84 | tok/s 15896
step   1590 | loss 1.5606 | lr 6.42e-05 | grad 1.33 | tok/s 23395
step   1600 | loss 1.3707 | lr 6.56e-05 | grad 1.51 | tok/s 23920
step   1610 | loss 1.5738 | lr 6.71e-05 | grad 2.75 | tok/s 22684
step   1620 | loss 1.4974 | lr 6.86e-05 | grad 3.30 | tok/s 24149
step   1630 | loss 2.1102 | lr 7.00e-05 | grad 2.98 | tok/s 23730
step   1640 | loss 2.3969 | lr 7.14e-05 | grad 1.42 | tok/s 24535
step   1650 | loss 1.8218 | lr 7.28e-05 | grad 1.14 | tok/s 23407
step   1660 | loss 1.5499 | lr 7.42e-05 | grad 1.30 | tok/s 19088
step   1670 | loss 1.4032 | lr 7.56e-05 | grad 0.90 | tok/s 19603
step   1680 | loss 1.3483 | lr 7.69e-05 | grad 1.16 | tok/s 24764
step   1690 | loss 1.7305 | lr 7.82e-05 | grad 3.18 | tok/s 23631
step   1700 | loss 1.7086 | lr 7.95e-05 | grad 1.20 | tok/s 23576
step   1710 | loss 1.5730 | lr 8.07e-05 | grad 1.65 | tok/s 22452
step   1720 | loss 1.4904 | lr 8.19e-05 | grad 1.16 | tok/s 23301
step   1730 | loss 1.3574 | lr 8.31e-05 | grad 2.01 | tok/s 23545
step   1740 | loss 1.6014 | lr 8.43e-05 | grad 1.52 | tok/s 19744
step   1750 | loss 1.5681 | lr 8.54e-05 | grad 1.31 | tok/s 14526
step   1760 | loss 1.5555 | lr 8.65e-05 | grad 1.30 | tok/s 13606
step   1770 | loss 1.4919 | lr 8.75e-05 | grad 1.23 | tok/s 14377
step   1780 | loss 1.4697 | lr 8.85e-05 | grad 1.36 | tok/s 15834
step   1790 | loss 1.8822 | lr 8.95e-05 | grad 1.34 | tok/s 23656

Training complete! Final step: 1795
