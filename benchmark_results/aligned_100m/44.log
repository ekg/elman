Using device: cuda
Output directory: output/aligned_44_768x56/level44_100m_20260113_225511
Auto r_h_mode: none (level 44 has bounded/no W_h)
Model: Level 44, 99,459,840 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.6366 | lr 9.00e-07 | grad 16.32 | tok/s 8125
step     20 | loss 5.3464 | lr 1.90e-06 | grad 12.44 | tok/s 15141
step     30 | loss 4.9969 | lr 2.90e-06 | grad 8.27 | tok/s 16010
step     40 | loss 4.6231 | lr 3.90e-06 | grad 11.24 | tok/s 17294
step     50 | loss 5.1644 | lr 4.90e-06 | grad 11.87 | tok/s 18820
step     60 | loss 4.8150 | lr 5.90e-06 | grad 5.41 | tok/s 18102
step     70 | loss 4.4691 | lr 6.90e-06 | grad 8.23 | tok/s 17896
step     80 | loss 4.2237 | lr 7.90e-06 | grad 3.53 | tok/s 17205
step     90 | loss 3.9040 | lr 8.90e-06 | grad 2.66 | tok/s 17930
step    100 | loss 3.7451 | lr 9.90e-06 | grad 2.20 | tok/s 17146
step    110 | loss 3.5859 | lr 1.09e-05 | grad 6.88 | tok/s 17477
step    120 | loss 3.8337 | lr 1.19e-05 | grad 2.23 | tok/s 15972
step    130 | loss 3.3378 | lr 1.29e-05 | grad 1.94 | tok/s 17483
step    140 | loss 3.2443 | lr 1.39e-05 | grad 1.70 | tok/s 19136
step    150 | loss 3.5534 | lr 1.49e-05 | grad 5.36 | tok/s 20522
step    160 | loss 3.3366 | lr 1.59e-05 | grad 3.07 | tok/s 20739
step    170 | loss 3.3545 | lr 1.69e-05 | grad 4.42 | tok/s 15354
step    180 | loss 3.4439 | lr 1.79e-05 | grad 3.83 | tok/s 14728
step    190 | loss 3.2834 | lr 1.89e-05 | grad 1.85 | tok/s 13681
step    200 | loss 3.1377 | lr 1.99e-05 | grad 1.35 | tok/s 15534
step    210 | loss 3.0537 | lr 2.09e-05 | grad 3.99 | tok/s 15172
step    220 | loss 3.1376 | lr 2.19e-05 | grad 2.63 | tok/s 14663
step    230 | loss 3.2489 | lr 2.29e-05 | grad 1.35 | tok/s 14839
step    240 | loss 3.0576 | lr 2.39e-05 | grad 1.35 | tok/s 14922
step    250 | loss 3.2381 | lr 2.49e-05 | grad 3.48 | tok/s 15187
step    260 | loss 3.0147 | lr 2.59e-05 | grad 1.65 | tok/s 15732
step    270 | loss 3.0381 | lr 2.69e-05 | grad 2.75 | tok/s 15761
step    280 | loss 2.7960 | lr 2.79e-05 | grad 1.93 | tok/s 15569
step    290 | loss 2.7179 | lr 2.89e-05 | grad 3.19 | tok/s 15109
step    300 | loss 2.7873 | lr 2.99e-05 | grad 4.06 | tok/s 15332
step    310 | loss 2.7697 | lr 3.09e-05 | grad 1.80 | tok/s 15645
step    320 | loss 2.5470 | lr 3.19e-05 | grad 4.42 | tok/s 15287
step    330 | loss 2.7945 | lr 3.29e-05 | grad 2.04 | tok/s 15609
step    340 | loss 2.8068 | lr 3.39e-05 | grad 8.73 | tok/s 16450
step    350 | loss 2.7979 | lr 3.49e-05 | grad 2.49 | tok/s 15695
step    360 | loss 2.8113 | lr 3.59e-05 | grad 3.32 | tok/s 16531
step    370 | loss 2.5357 | lr 3.69e-05 | grad 2.39 | tok/s 16355
step    380 | loss 2.5823 | lr 3.79e-05 | grad 2.42 | tok/s 16639
step    390 | loss 2.3808 | lr 3.89e-05 | grad 2.74 | tok/s 17277
step    400 | loss 2.2141 | lr 3.99e-05 | grad 1.72 | tok/s 17130
step    410 | loss 2.6854 | lr 4.09e-05 | grad 2.46 | tok/s 15133
step    420 | loss 2.5710 | lr 4.19e-05 | grad 2.67 | tok/s 17198
step    430 | loss 2.6578 | lr 4.29e-05 | grad 3.47 | tok/s 18444
step    440 | loss 2.4460 | lr 4.39e-05 | grad 2.21 | tok/s 18399
step    450 | loss 2.5304 | lr 4.49e-05 | grad 1.58 | tok/s 18509
step    460 | loss 2.2576 | lr 4.59e-05 | grad 3.44 | tok/s 18845
step    470 | loss 2.3808 | lr 4.69e-05 | grad 1.98 | tok/s 19889
step    480 | loss 2.4575 | lr 4.79e-05 | grad 2.59 | tok/s 20412
step    490 | loss 2.3893 | lr 4.89e-05 | grad 1.60 | tok/s 20239
step    500 | loss 2.3371 | lr 4.99e-05 | grad 2.09 | tok/s 19447
step    510 | loss 2.5216 | lr 5.09e-05 | grad 5.50 | tok/s 20042
step    520 | loss 2.2359 | lr 5.19e-05 | grad 1.63 | tok/s 19504
step    530 | loss 2.1431 | lr 5.29e-05 | grad 1.59 | tok/s 18064
step    540 | loss 2.3047 | lr 5.39e-05 | grad 1.93 | tok/s 15763
step    550 | loss 2.2477 | lr 5.49e-05 | grad 1.99 | tok/s 17433
step    560 | loss 1.9697 | lr 5.59e-05 | grad 2.37 | tok/s 18451
step    570 | loss 2.0729 | lr 5.69e-05 | grad 1.55 | tok/s 19592
step    580 | loss 1.8890 | lr 5.79e-05 | grad 1.46 | tok/s 20500
step    590 | loss 1.7747 | lr 5.89e-05 | grad 1.49 | tok/s 19913
step    600 | loss 1.8461 | lr 5.99e-05 | grad 1.86 | tok/s 19749
step    610 | loss 1.7396 | lr 6.09e-05 | grad 1.64 | tok/s 19051
step    620 | loss 1.6991 | lr 6.19e-05 | grad 1.43 | tok/s 21087
step    630 | loss 1.7947 | lr 6.29e-05 | grad 7.08 | tok/s 21166
step    640 | loss 2.2098 | lr 6.39e-05 | grad 2.80 | tok/s 19928
step    650 | loss 2.1920 | lr 6.49e-05 | grad 2.60 | tok/s 20197
step    660 | loss 2.0429 | lr 6.59e-05 | grad 2.70 | tok/s 20618
step    670 | loss 2.0890 | lr 6.69e-05 | grad 2.14 | tok/s 21317
step    680 | loss 2.1553 | lr 6.79e-05 | grad 2.36 | tok/s 20710
step    690 | loss 2.1272 | lr 6.89e-05 | grad 2.15 | tok/s 20572
step    700 | loss 2.0764 | lr 6.99e-05 | grad 1.88 | tok/s 20381
step    710 | loss 1.9484 | lr 7.09e-05 | grad 2.32 | tok/s 21030
step    720 | loss 2.1428 | lr 7.19e-05 | grad 3.28 | tok/s 20625
step    730 | loss 1.8005 | lr 7.29e-05 | grad 1.74 | tok/s 21302
step    740 | loss 1.8803 | lr 7.39e-05 | grad 1.35 | tok/s 19201
step    750 | loss 2.4397 | lr 7.49e-05 | grad 2.85 | tok/s 19699
step    760 | loss 2.2095 | lr 7.59e-05 | grad 1.63 | tok/s 18586
step    770 | loss 1.9561 | lr 7.69e-05 | grad 1.98 | tok/s 17028
step    780 | loss 1.9762 | lr 7.79e-05 | grad 1.75 | tok/s 15874
step    790 | loss 1.9241 | lr 7.89e-05 | grad 1.87 | tok/s 17660
step    800 | loss 2.1937 | lr 7.99e-05 | grad 3.10 | tok/s 19599
step    810 | loss 1.9931 | lr 8.09e-05 | grad 8.63 | tok/s 20167
step    820 | loss 1.7069 | lr 8.19e-05 | grad 3.44 | tok/s 18871
step    830 | loss 1.9602 | lr 8.29e-05 | grad 1.81 | tok/s 17417
step    840 | loss 1.9731 | lr 8.39e-05 | grad 1.54 | tok/s 19414
step    850 | loss 2.0968 | lr 8.49e-05 | grad 1.84 | tok/s 19377
step    860 | loss 2.1038 | lr 8.59e-05 | grad 1.67 | tok/s 18102
step    870 | loss 1.9828 | lr 8.69e-05 | grad 2.63 | tok/s 18878
step    880 | loss 2.1481 | lr 8.79e-05 | grad 1.86 | tok/s 20341
step    890 | loss 1.9891 | lr 8.89e-05 | grad 1.52 | tok/s 19111
step    900 | loss 1.8431 | lr 8.99e-05 | grad 1.58 | tok/s 19086
step    910 | loss 1.8137 | lr 9.09e-05 | grad 1.21 | tok/s 19872
step    920 | loss 1.9352 | lr 9.19e-05 | grad 1.31 | tok/s 19554
step    930 | loss 1.9216 | lr 9.29e-05 | grad 1.50 | tok/s 19561
step    940 | loss 1.8094 | lr 9.39e-05 | grad 1.63 | tok/s 17834
step    950 | loss 1.7382 | lr 9.49e-05 | grad 1.16 | tok/s 16073
step    960 | loss 1.8327 | lr 9.59e-05 | grad 1.13 | tok/s 14791
step    970 | loss 1.7289 | lr 9.69e-05 | grad 1.14 | tok/s 16238
step    980 | loss 1.7066 | lr 9.79e-05 | grad 1.15 | tok/s 14986
step    990 | loss 2.5775 | lr 9.89e-05 | grad 1.87 | tok/s 15710
step   1000 | loss 2.2328 | lr 9.99e-05 | grad 1.64 | tok/s 15829
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2328.pt
step   1010 | loss 2.0345 | lr 1.02e-06 | grad 1.75 | tok/s 9899
step   1020 | loss 1.7148 | lr 1.09e-06 | grad 1.39 | tok/s 15386
step   1030 | loss 1.6588 | lr 1.21e-06 | grad 3.02 | tok/s 16572
step   1040 | loss 2.0667 | lr 1.37e-06 | grad 1.16 | tok/s 17102
step   1050 | loss 2.1228 | lr 1.59e-06 | grad 4.75 | tok/s 15929
step   1060 | loss 2.6587 | lr 1.85e-06 | grad 0.95 | tok/s 16445
step   1070 | loss 2.0144 | lr 2.16e-06 | grad 4.09 | tok/s 16244
step   1080 | loss 1.5448 | lr 2.52e-06 | grad 1.46 | tok/s 15968
step   1090 | loss 1.7200 | lr 2.92e-06 | grad 1.35 | tok/s 16530
step   1100 | loss 1.6397 | lr 3.37e-06 | grad 1.01 | tok/s 17245
step   1110 | loss 1.5866 | lr 3.87e-06 | grad 0.82 | tok/s 17871
step   1120 | loss 1.5559 | lr 4.42e-06 | grad 0.96 | tok/s 17188
step   1130 | loss 1.5572 | lr 5.01e-06 | grad 1.10 | tok/s 17632
step   1140 | loss 2.1354 | lr 5.65e-06 | grad 2.85 | tok/s 18071
step   1150 | loss 2.3490 | lr 6.32e-06 | grad 2.77 | tok/s 17446
step   1160 | loss 1.8648 | lr 7.05e-06 | grad 1.29 | tok/s 16844
step   1170 | loss 2.4714 | lr 7.81e-06 | grad 2.09 | tok/s 15553
step   1180 | loss 2.0807 | lr 8.62e-06 | grad 1.43 | tok/s 15852
step   1190 | loss 1.7844 | lr 9.47e-06 | grad 1.02 | tok/s 15797
step   1200 | loss 1.8286 | lr 1.04e-05 | grad 2.18 | tok/s 17425
step   1210 | loss 2.2626 | lr 1.13e-05 | grad 1.67 | tok/s 18468
step   1220 | loss 1.5586 | lr 1.23e-05 | grad 1.19 | tok/s 17636
step   1230 | loss 1.7315 | lr 1.33e-05 | grad 1.44 | tok/s 17184
step   1240 | loss 1.6846 | lr 1.43e-05 | grad 1.22 | tok/s 15445
step   1250 | loss 1.7071 | lr 1.54e-05 | grad 0.98 | tok/s 15032
step   1260 | loss 1.6368 | lr 1.65e-05 | grad 1.03 | tok/s 15263
step   1270 | loss 2.0872 | lr 1.76e-05 | grad 2.74 | tok/s 16078
step   1280 | loss 1.9663 | lr 1.88e-05 | grad 1.25 | tok/s 16702
step   1290 | loss 1.6653 | lr 2.00e-05 | grad 1.36 | tok/s 17029
step   1300 | loss 1.7766 | lr 2.13e-05 | grad 1.63 | tok/s 16293

Training complete! Final step: 1307
