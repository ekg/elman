Using device: cuda
Output directory: output/aligned_60_768x23/level60_100m_20260113_225511
Auto r_h_mode: spectral_norm (level 60 has full W_h)
Model: Level 60, 101,986,199 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.6625 | lr 9.00e-07 | grad 368.32 | tok/s 6871
step     20 | loss 5.6254 | lr 1.90e-06 | grad 481.18 | tok/s 10397
step     30 | loss 5.5437 | lr 2.90e-06 | grad 213.90 | tok/s 10097
step     40 | loss 5.4570 | lr 3.90e-06 | grad 314.11 | tok/s 9885
step     50 | loss 5.5566 | lr 4.90e-06 | grad 307.06 | tok/s 10724
step     60 | loss 5.4641 | lr 5.90e-06 | grad 260.81 | tok/s 10540
step     70 | loss 5.3919 | lr 6.90e-06 | grad 303.98 | tok/s 10386
step     80 | loss 5.2699 | lr 7.90e-06 | grad 237.92 | tok/s 9883
step     90 | loss 5.2122 | lr 8.90e-06 | grad 234.44 | tok/s 10235
step    100 | loss 5.1123 | lr 9.90e-06 | grad 210.60 | tok/s 10267
step    110 | loss 5.0415 | lr 1.09e-05 | grad 250.24 | tok/s 10228
step    120 | loss 4.7804 | lr 1.19e-05 | grad 343.34 | tok/s 9862
step    130 | loss 4.4834 | lr 1.29e-05 | grad 88.81 | tok/s 9688
step    140 | loss 4.2172 | lr 1.39e-05 | grad 111.74 | tok/s 9729
step    150 | loss 4.1752 | lr 1.49e-05 | grad 72.63 | tok/s 10049
step    160 | loss 4.0413 | lr 1.59e-05 | grad 80.60 | tok/s 10166
step    170 | loss 4.0414 | lr 1.69e-05 | grad 151.02 | tok/s 9488
step    180 | loss 4.0181 | lr 1.79e-05 | grad 44.94 | tok/s 9731
step    190 | loss 3.9902 | lr 1.89e-05 | grad 65.78 | tok/s 9634
step    200 | loss 3.5776 | lr 1.99e-05 | grad 22.34 | tok/s 10895
step    210 | loss 3.5842 | lr 2.09e-05 | grad 83.22 | tok/s 10662
step    220 | loss 3.7548 | lr 2.19e-05 | grad 50.64 | tok/s 10252
step    230 | loss 3.9942 | lr 2.29e-05 | grad 98.79 | tok/s 10125
step    240 | loss 3.6126 | lr 2.39e-05 | grad 47.84 | tok/s 10440
step    250 | loss 3.8430 | lr 2.49e-05 | grad 15.46 | tok/s 9428
step    260 | loss 3.5047 | lr 2.59e-05 | grad 9.03 | tok/s 10648
step    270 | loss 3.5712 | lr 2.69e-05 | grad 23.93 | tok/s 10362
step    280 | loss 3.4058 | lr 2.79e-05 | grad 14.61 | tok/s 10268
step    290 | loss 3.4480 | lr 2.89e-05 | grad 10.92 | tok/s 9735
step    300 | loss 3.5017 | lr 2.99e-05 | grad 14.80 | tok/s 10245
step    310 | loss 3.4633 | lr 3.09e-05 | grad 7.27 | tok/s 10534
step    320 | loss 3.3356 | lr 3.19e-05 | grad 9.93 | tok/s 10094
step    330 | loss 3.4645 | lr 3.29e-05 | grad 13.29 | tok/s 10558
step    340 | loss 3.4957 | lr 3.39e-05 | grad 9.82 | tok/s 10754
step    350 | loss 3.5854 | lr 3.49e-05 | grad 4.30 | tok/s 10560
step    360 | loss 3.6223 | lr 3.59e-05 | grad 3.56 | tok/s 10749
step    370 | loss 3.4873 | lr 3.69e-05 | grad 2.31 | tok/s 10522
step    380 | loss 3.3627 | lr 3.79e-05 | grad 1.82 | tok/s 11012
step    390 | loss 3.2656 | lr 3.89e-05 | grad 1.45 | tok/s 11128
step    400 | loss 3.3269 | lr 3.99e-05 | grad 6.10 | tok/s 10984
step    410 | loss 3.6848 | lr 4.09e-05 | grad 1.42 | tok/s 10612
step    420 | loss 3.4686 | lr 4.19e-05 | grad 3.22 | tok/s 10610
step    430 | loss 3.5642 | lr 4.29e-05 | grad 2.09 | tok/s 11086
step    440 | loss 3.4607 | lr 4.39e-05 | grad 1.60 | tok/s 10748
step    450 | loss 3.4944 | lr 4.49e-05 | grad 1.95 | tok/s 10601
step    460 | loss 3.3001 | lr 4.59e-05 | grad 2.58 | tok/s 10510
step    470 | loss 3.5162 | lr 4.69e-05 | grad 4.87 | tok/s 10351
step    480 | loss 3.5187 | lr 4.79e-05 | grad 1.29 | tok/s 10659
step    490 | loss 3.4660 | lr 4.89e-05 | grad 3.97 | tok/s 10308
step    500 | loss 3.4051 | lr 4.99e-05 | grad 1.92 | tok/s 10103
step    510 | loss 3.5660 | lr 5.09e-05 | grad 2.95 | tok/s 10080
step    520 | loss 3.3702 | lr 5.19e-05 | grad 1.32 | tok/s 9748
step    530 | loss 3.3262 | lr 5.29e-05 | grad 2.90 | tok/s 10174
step    540 | loss 3.4711 | lr 5.39e-05 | grad 2.19 | tok/s 10331
step    550 | loss 3.5415 | lr 5.49e-05 | grad 0.78 | tok/s 10117
step    560 | loss 3.3084 | lr 5.59e-05 | grad 3.83 | tok/s 10491
step    570 | loss 3.2525 | lr 5.69e-05 | grad 1.16 | tok/s 10735
step    580 | loss 3.1408 | lr 5.79e-05 | grad 0.76 | tok/s 10880
step    590 | loss 3.1076 | lr 5.89e-05 | grad 0.81 | tok/s 10977
step    600 | loss 3.1773 | lr 5.99e-05 | grad 0.67 | tok/s 11063
step    610 | loss 3.1973 | lr 6.09e-05 | grad 0.71 | tok/s 11081
step    620 | loss 3.1015 | lr 6.19e-05 | grad 0.70 | tok/s 11042
step    630 | loss 3.3582 | lr 6.29e-05 | grad 3.98 | tok/s 10898
step    640 | loss 3.6685 | lr 6.39e-05 | grad 2.45 | tok/s 10371
step    650 | loss 3.5450 | lr 6.49e-05 | grad 2.34 | tok/s 10417
step    660 | loss 3.4037 | lr 6.59e-05 | grad 0.99 | tok/s 10546
step    670 | loss 3.3897 | lr 6.69e-05 | grad 1.38 | tok/s 10803
step    680 | loss 3.4492 | lr 6.79e-05 | grad 3.76 | tok/s 10178
step    690 | loss 3.4672 | lr 6.89e-05 | grad 3.13 | tok/s 10117
step    700 | loss 3.4671 | lr 6.99e-05 | grad 1.46 | tok/s 10115
step    710 | loss 3.3926 | lr 7.09e-05 | grad 2.07 | tok/s 10511
step    720 | loss 3.5004 | lr 7.19e-05 | grad 2.57 | tok/s 10167
step    730 | loss 3.3122 | lr 7.29e-05 | grad 1.76 | tok/s 10638
step    740 | loss 3.4638 | lr 7.39e-05 | grad 5.53 | tok/s 10361
step    750 | loss 3.7392 | lr 7.49e-05 | grad 2.74 | tok/s 10887
step    760 | loss 3.6521 | lr 7.59e-05 | grad 1.63 | tok/s 10926
step    770 | loss 3.3449 | lr 7.69e-05 | grad 3.34 | tok/s 10685
step    780 | loss 3.3965 | lr 7.79e-05 | grad 1.34 | tok/s 10364

Training complete! Final step: 787
