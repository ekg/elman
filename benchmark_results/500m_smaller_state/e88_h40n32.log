Using device: cuda
Output directory: benchmark_results/500m_smaller_state/e88_h40n32/levelE88_h40n32_100m_20260121_144704
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h40n32, 264,903,360 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8486 | lr 9.00e-07 | grad 78.50 | tok/s 9420
step     20 | loss 5.7585 | lr 1.90e-06 | grad 51.25 | tok/s 20058
step     30 | loss 5.7148 | lr 2.90e-06 | grad 20.50 | tok/s 19928
step     40 | loss 5.8275 | lr 3.90e-06 | grad 21.25 | tok/s 20607
step     50 | loss 5.9994 | lr 4.90e-06 | grad 20.88 | tok/s 21317
step     60 | loss 5.9507 | lr 5.90e-06 | grad 18.00 | tok/s 21216
step     70 | loss 5.9029 | lr 6.90e-06 | grad 24.00 | tok/s 21176
step     80 | loss 5.8464 | lr 7.90e-06 | grad 19.38 | tok/s 21218
step     90 | loss 5.7612 | lr 8.90e-06 | grad 17.50 | tok/s 20999
step    100 | loss 5.6635 | lr 9.90e-06 | grad 18.50 | tok/s 20920
step    110 | loss 5.5245 | lr 1.09e-05 | grad 61.50 | tok/s 19937
step    120 | loss 5.2314 | lr 1.19e-05 | grad 83.50 | tok/s 19615
step    130 | loss 4.8929 | lr 1.29e-05 | grad 17.25 | tok/s 19607
step    140 | loss 4.5815 | lr 1.39e-05 | grad 61.50 | tok/s 19574
step    150 | loss 4.2676 | lr 1.49e-05 | grad 32.50 | tok/s 20164
step    160 | loss 4.0689 | lr 1.59e-05 | grad 15.00 | tok/s 20214
step    170 | loss 4.1136 | lr 1.69e-05 | grad 24.50 | tok/s 19076
step    180 | loss 4.1257 | lr 1.79e-05 | grad 21.62 | tok/s 19677
step    190 | loss 3.8507 | lr 1.89e-05 | grad 16.38 | tok/s 18782
step    200 | loss 3.5464 | lr 1.99e-05 | grad 10.56 | tok/s 19990
step    210 | loss 3.3242 | lr 2.09e-05 | grad 13.81 | tok/s 19349
step    220 | loss 3.4480 | lr 2.19e-05 | grad 13.25 | tok/s 18632
step    230 | loss 3.7584 | lr 2.29e-05 | grad 8.06 | tok/s 18549
step    240 | loss 3.2270 | lr 2.39e-05 | grad 14.25 | tok/s 18714
step    250 | loss 3.4883 | lr 2.49e-05 | grad 8.69 | tok/s 18686
step    260 | loss 3.0776 | lr 2.59e-05 | grad 6.19 | tok/s 19192
step    270 | loss 3.1393 | lr 2.69e-05 | grad 6.38 | tok/s 19195
step    280 | loss 2.7695 | lr 2.79e-05 | grad 4.97 | tok/s 18586
step    290 | loss 2.7516 | lr 2.89e-05 | grad 7.56 | tok/s 17856
step    300 | loss 2.8099 | lr 2.99e-05 | grad 5.88 | tok/s 18110
step    310 | loss 2.7725 | lr 3.09e-05 | grad 4.59 | tok/s 18462
step    320 | loss 2.4942 | lr 3.19e-05 | grad 4.94 | tok/s 17605
step    330 | loss 2.7591 | lr 3.29e-05 | grad 3.56 | tok/s 18446
step    340 | loss 2.7856 | lr 3.39e-05 | grad 32.50 | tok/s 18814
step    350 | loss 2.8052 | lr 3.49e-05 | grad 7.19 | tok/s 18445
step    360 | loss 2.8398 | lr 3.59e-05 | grad 4.28 | tok/s 18919
step    370 | loss 2.4814 | lr 3.69e-05 | grad 3.84 | tok/s 18487
step    380 | loss 2.5361 | lr 3.79e-05 | grad 4.22 | tok/s 19314
step    390 | loss 2.2928 | lr 3.89e-05 | grad 3.95 | tok/s 19420
step    400 | loss 2.1148 | lr 3.99e-05 | grad 3.53 | tok/s 19114
step    410 | loss 2.6619 | lr 4.09e-05 | grad 4.53 | tok/s 18467
step    420 | loss 2.4913 | lr 4.19e-05 | grad 4.84 | tok/s 18461
step    430 | loss 2.6182 | lr 4.29e-05 | grad 7.81 | tok/s 19355
step    440 | loss 2.3647 | lr 4.39e-05 | grad 4.00 | tok/s 18725
step    450 | loss 2.4305 | lr 4.49e-05 | grad 2.95 | tok/s 18477
step    460 | loss 2.1659 | lr 4.59e-05 | grad 6.97 | tok/s 18303
step    470 | loss 2.3213 | lr 4.69e-05 | grad 3.94 | tok/s 18290
step    480 | loss 2.3676 | lr 4.79e-05 | grad 4.81 | tok/s 19184
step    490 | loss 2.2975 | lr 4.89e-05 | grad 3.30 | tok/s 18544
step    500 | loss 2.2731 | lr 4.99e-05 | grad 3.67 | tok/s 18430
step    510 | loss 2.4406 | lr 5.09e-05 | grad 10.75 | tok/s 18148
step    520 | loss 2.1586 | lr 5.19e-05 | grad 2.56 | tok/s 17384
step    530 | loss 2.0593 | lr 5.29e-05 | grad 3.00 | tok/s 18455
step    540 | loss 2.2458 | lr 5.39e-05 | grad 3.47 | tok/s 18360
step    550 | loss 2.1991 | lr 5.49e-05 | grad 3.70 | tok/s 17943
step    560 | loss 1.9450 | lr 5.59e-05 | grad 4.88 | tok/s 18853
step    570 | loss 2.0387 | lr 5.69e-05 | grad 3.78 | tok/s 19366
step    580 | loss 1.8532 | lr 5.79e-05 | grad 3.08 | tok/s 19326
step    590 | loss 1.7348 | lr 5.89e-05 | grad 2.61 | tok/s 19332
step    600 | loss 1.8165 | lr 5.99e-05 | grad 3.45 | tok/s 19307
step    610 | loss 1.7154 | lr 6.09e-05 | grad 3.09 | tok/s 19314
step    620 | loss 1.6694 | lr 6.19e-05 | grad 2.47 | tok/s 19325
step    630 | loss 1.7983 | lr 6.29e-05 | grad 6.38 | tok/s 19063
step    640 | loss 2.1309 | lr 6.39e-05 | grad 5.31 | tok/s 18172
step    650 | loss 2.1535 | lr 6.49e-05 | grad 4.03 | tok/s 18073
step    660 | loss 2.0164 | lr 6.59e-05 | grad 4.84 | tok/s 18234
step    670 | loss 2.0585 | lr 6.69e-05 | grad 3.78 | tok/s 18876
step    680 | loss 2.1195 | lr 6.79e-05 | grad 4.50 | tok/s 18251
step    690 | loss 2.0900 | lr 6.89e-05 | grad 3.36 | tok/s 18070
step    700 | loss 2.0641 | lr 6.99e-05 | grad 3.70 | tok/s 17917
step    710 | loss 1.9073 | lr 7.09e-05 | grad 4.41 | tok/s 18413
step    720 | loss 2.1188 | lr 7.19e-05 | grad 6.00 | tok/s 17931
step    730 | loss 1.7647 | lr 7.29e-05 | grad 3.47 | tok/s 18851
step    740 | loss 1.8302 | lr 7.39e-05 | grad 2.80 | tok/s 18298
step    750 | loss 2.4396 | lr 7.49e-05 | grad 6.56 | tok/s 19048
step    760 | loss 2.2087 | lr 7.59e-05 | grad 3.31 | tok/s 19004
step    770 | loss 1.9454 | lr 7.69e-05 | grad 4.19 | tok/s 18628
step    780 | loss 2.0209 | lr 7.79e-05 | grad 3.36 | tok/s 18041
step    790 | loss 1.9245 | lr 7.89e-05 | grad 3.64 | tok/s 18557
step    800 | loss 2.1919 | lr 7.99e-05 | grad 7.44 | tok/s 19089
step    810 | loss 1.9442 | lr 8.09e-05 | grad 4.34 | tok/s 18453
step    820 | loss 1.6362 | lr 8.19e-05 | grad 6.38 | tok/s 18005
step    830 | loss 1.8766 | lr 8.29e-05 | grad 3.59 | tok/s 18352
step    840 | loss 1.9259 | lr 8.39e-05 | grad 3.23 | tok/s 17947
step    850 | loss 2.0781 | lr 8.49e-05 | grad 3.12 | tok/s 18016
step    860 | loss 2.0287 | lr 8.59e-05 | grad 3.05 | tok/s 18170
step    870 | loss 1.9544 | lr 8.69e-05 | grad 5.72 | tok/s 18331
step    880 | loss 1.9878 | lr 8.79e-05 | grad 3.14 | tok/s 19181
step    890 | loss 1.9706 | lr 8.89e-05 | grad 3.53 | tok/s 18329
step    900 | loss 1.8376 | lr 8.99e-05 | grad 3.02 | tok/s 18227
step    910 | loss 1.7874 | lr 9.09e-05 | grad 2.67 | tok/s 18448
step    920 | loss 1.9343 | lr 9.19e-05 | grad 3.00 | tok/s 18194
step    930 | loss 1.8932 | lr 9.29e-05 | grad 2.81 | tok/s 18267
step    940 | loss 1.7821 | lr 9.39e-05 | grad 3.02 | tok/s 18815
step    950 | loss 1.6889 | lr 9.49e-05 | grad 2.20 | tok/s 18029
step    960 | loss 1.8125 | lr 9.59e-05 | grad 2.09 | tok/s 17783
step    970 | loss 1.7105 | lr 9.69e-05 | grad 2.67 | tok/s 17977
step    980 | loss 1.6932 | lr 9.79e-05 | grad 2.25 | tok/s 18461
step    990 | loss 2.4253 | lr 9.89e-05 | grad 3.69 | tok/s 18571
step   1000 | loss 2.2305 | lr 9.99e-05 | grad 3.20 | tok/s 18418
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2305.pt
step   1010 | loss 2.1233 | lr 1.02e-06 | grad 3.23 | tok/s 13251
step   1020 | loss 1.6843 | lr 1.09e-06 | grad 3.28 | tok/s 17934
step   1030 | loss 1.7053 | lr 1.21e-06 | grad 3.61 | tok/s 18296
step   1040 | loss 2.1571 | lr 1.37e-06 | grad 2.77 | tok/s 18312
step   1050 | loss 2.2221 | lr 1.59e-06 | grad 9.69 | tok/s 17484
step   1060 | loss 2.8127 | lr 1.85e-06 | grad 2.62 | tok/s 18370
step   1070 | loss 2.1128 | lr 2.16e-06 | grad 14.69 | tok/s 17731
step   1080 | loss 1.5766 | lr 2.52e-06 | grad 3.77 | tok/s 18060
step   1090 | loss 1.8556 | lr 2.92e-06 | grad 3.39 | tok/s 18147
step   1100 | loss 1.8019 | lr 3.37e-06 | grad 2.72 | tok/s 18671
step   1110 | loss 1.7736 | lr 3.87e-06 | grad 2.64 | tok/s 18664
step   1120 | loss 1.7606 | lr 4.42e-06 | grad 2.59 | tok/s 18660
step   1130 | loss 1.7632 | lr 5.01e-06 | grad 2.41 | tok/s 18130
step   1140 | loss 2.2674 | lr 5.65e-06 | grad 5.34 | tok/s 18315
step   1150 | loss 2.4426 | lr 6.32e-06 | grad 5.19 | tok/s 18007
step   1160 | loss 1.9690 | lr 7.05e-06 | grad 3.06 | tok/s 17961
step   1170 | loss 2.6731 | lr 7.81e-06 | grad 4.31 | tok/s 17558
step   1180 | loss 2.1807 | lr 8.62e-06 | grad 2.81 | tok/s 17686
step   1190 | loss 1.8944 | lr 9.47e-06 | grad 2.03 | tok/s 17094
step   1200 | loss 1.9419 | lr 1.04e-05 | grad 4.38 | tok/s 18237
step   1210 | loss 2.4582 | lr 1.13e-05 | grad 2.61 | tok/s 18635
step   1220 | loss 1.6362 | lr 1.23e-05 | grad 2.22 | tok/s 18409
step   1230 | loss 1.8209 | lr 1.33e-05 | grad 2.72 | tok/s 17336
step   1240 | loss 1.7596 | lr 1.43e-05 | grad 2.25 | tok/s 17789
step   1250 | loss 1.8138 | lr 1.54e-05 | grad 1.87 | tok/s 18203
step   1260 | loss 1.7280 | lr 1.65e-05 | grad 1.98 | tok/s 18034
step   1270 | loss 2.2539 | lr 1.76e-05 | grad 8.06 | tok/s 18235
step   1280 | loss 2.1374 | lr 1.88e-05 | grad 2.25 | tok/s 18051
step   1290 | loss 1.6865 | lr 2.00e-05 | grad 2.59 | tok/s 17961
step   1300 | loss 1.8119 | lr 2.13e-05 | grad 2.77 | tok/s 17564
step   1310 | loss 1.7665 | lr 2.25e-05 | grad 2.05 | tok/s 17522
step   1320 | loss 2.3284 | lr 2.38e-05 | grad 12.00 | tok/s 17606
step   1330 | loss 1.8050 | lr 2.52e-05 | grad 2.36 | tok/s 18055
step   1340 | loss 1.9693 | lr 2.65e-05 | grad 3.95 | tok/s 18310
step   1350 | loss 1.7568 | lr 2.79e-05 | grad 2.30 | tok/s 17629
step   1360 | loss 1.9101 | lr 2.93e-05 | grad 2.69 | tok/s 17437
step   1370 | loss 1.8770 | lr 3.07e-05 | grad 6.09 | tok/s 17845
step   1380 | loss 1.8554 | lr 3.21e-05 | grad 3.55 | tok/s 17488
step   1390 | loss 2.2342 | lr 3.36e-05 | grad 3.67 | tok/s 18369
step   1400 | loss 1.6715 | lr 3.51e-05 | grad 5.06 | tok/s 17196

Training complete! Final step: 1401
