Using device: cuda
Output directory: benchmark_results/500m_smaller_state/e88_h80n32/levelE88_h80n32_100m_20260121_144704
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h80n32, 529,096,960 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.9926 | lr 9.00e-07 | grad 79.00 | tok/s 6549
step     20 | loss 5.9334 | lr 1.90e-06 | grad 70.00 | tok/s 9934
step     30 | loss 5.8817 | lr 2.90e-06 | grad 24.12 | tok/s 9833
step     40 | loss 5.9686 | lr 3.90e-06 | grad 25.38 | tok/s 10158
step     50 | loss 6.0606 | lr 4.90e-06 | grad 25.25 | tok/s 10480
step     60 | loss 6.0195 | lr 5.90e-06 | grad 22.50 | tok/s 10348
step     70 | loss 5.9462 | lr 6.90e-06 | grad 28.12 | tok/s 10426
step     80 | loss 5.8776 | lr 7.90e-06 | grad 23.75 | tok/s 10390
step     90 | loss 5.7646 | lr 8.90e-06 | grad 21.88 | tok/s 10265
step    100 | loss 5.6381 | lr 9.90e-06 | grad 22.75 | tok/s 10181
step    110 | loss 5.4463 | lr 1.09e-05 | grad 76.00 | tok/s 10087
step    120 | loss 5.1357 | lr 1.19e-05 | grad 75.50 | tok/s 9787
step    130 | loss 4.7552 | lr 1.29e-05 | grad 19.88 | tok/s 9534
step    140 | loss 4.3712 | lr 1.39e-05 | grad 36.50 | tok/s 9570
step    150 | loss 4.0360 | lr 1.49e-05 | grad 36.50 | tok/s 9855
step    160 | loss 3.7918 | lr 1.59e-05 | grad 17.00 | tok/s 9903
step    170 | loss 3.9238 | lr 1.69e-05 | grad 24.88 | tok/s 9386
step    180 | loss 3.9764 | lr 1.79e-05 | grad 26.00 | tok/s 9700
step    190 | loss 3.6958 | lr 1.89e-05 | grad 16.12 | tok/s 9298
step    200 | loss 3.4269 | lr 1.99e-05 | grad 13.00 | tok/s 9954
step    210 | loss 3.2259 | lr 2.09e-05 | grad 15.62 | tok/s 9657
step    220 | loss 3.4025 | lr 2.19e-05 | grad 14.06 | tok/s 9292
step    230 | loss 3.6902 | lr 2.29e-05 | grad 11.00 | tok/s 9319
step    240 | loss 3.1814 | lr 2.39e-05 | grad 18.75 | tok/s 9389
step    250 | loss 3.4391 | lr 2.49e-05 | grad 10.69 | tok/s 9414
step    260 | loss 3.0397 | lr 2.59e-05 | grad 8.12 | tok/s 9736
step    270 | loss 3.0939 | lr 2.69e-05 | grad 8.25 | tok/s 9737
step    280 | loss 2.7325 | lr 2.79e-05 | grad 6.31 | tok/s 9455
step    290 | loss 2.7265 | lr 2.89e-05 | grad 9.38 | tok/s 9090
step    300 | loss 2.7691 | lr 2.99e-05 | grad 7.53 | tok/s 9223
step    310 | loss 2.7475 | lr 3.09e-05 | grad 5.03 | tok/s 9454
step    320 | loss 2.4601 | lr 3.19e-05 | grad 5.72 | tok/s 9053
step    330 | loss 2.7315 | lr 3.29e-05 | grad 3.89 | tok/s 9480
step    340 | loss 2.7592 | lr 3.39e-05 | grad 30.38 | tok/s 9669
step    350 | loss 2.7620 | lr 3.49e-05 | grad 7.62 | tok/s 9470
step    360 | loss 2.7761 | lr 3.59e-05 | grad 4.09 | tok/s 9756
step    370 | loss 2.4548 | lr 3.69e-05 | grad 4.09 | tok/s 9583
step    380 | loss 2.4706 | lr 3.79e-05 | grad 4.06 | tok/s 10014
step    390 | loss 2.2070 | lr 3.89e-05 | grad 3.81 | tok/s 10074
step    400 | loss 2.0176 | lr 3.99e-05 | grad 3.36 | tok/s 9919
step    410 | loss 2.6195 | lr 4.09e-05 | grad 4.84 | tok/s 9571
step    420 | loss 2.4511 | lr 4.19e-05 | grad 5.19 | tok/s 9574
step    430 | loss 2.5664 | lr 4.29e-05 | grad 8.12 | tok/s 10050
step    440 | loss 2.3269 | lr 4.39e-05 | grad 5.00 | tok/s 9721
step    450 | loss 2.4041 | lr 4.49e-05 | grad 3.41 | tok/s 9573
step    460 | loss 2.1282 | lr 4.59e-05 | grad 7.12 | tok/s 9485
step    470 | loss 2.2781 | lr 4.69e-05 | grad 4.38 | tok/s 9520
step    480 | loss 2.3156 | lr 4.79e-05 | grad 4.59 | tok/s 9986
step    490 | loss 2.2341 | lr 4.89e-05 | grad 3.80 | tok/s 9648
step    500 | loss 2.2591 | lr 4.99e-05 | grad 3.89 | tok/s 9572
step    510 | loss 2.4252 | lr 5.09e-05 | grad 10.56 | tok/s 9438
step    520 | loss 2.1359 | lr 5.19e-05 | grad 2.92 | tok/s 9012
step    530 | loss 2.0116 | lr 5.29e-05 | grad 3.39 | tok/s 9584
step    540 | loss 2.2130 | lr 5.39e-05 | grad 3.77 | tok/s 9533
step    550 | loss 2.1547 | lr 5.49e-05 | grad 3.28 | tok/s 9317
step    560 | loss 1.8767 | lr 5.59e-05 | grad 3.98 | tok/s 9772
step    570 | loss 1.9723 | lr 5.69e-05 | grad 3.00 | tok/s 10067
step    580 | loss 1.7822 | lr 5.79e-05 | grad 3.02 | tok/s 10077
step    590 | loss 1.6750 | lr 5.89e-05 | grad 3.00 | tok/s 10081
step    600 | loss 1.7478 | lr 5.99e-05 | grad 3.34 | tok/s 10060
step    610 | loss 1.6436 | lr 6.09e-05 | grad 2.94 | tok/s 10071
step    620 | loss 1.6211 | lr 6.19e-05 | grad 2.61 | tok/s 10090
step    630 | loss 1.7546 | lr 6.29e-05 | grad 6.19 | tok/s 9952
step    640 | loss 2.0879 | lr 6.39e-05 | grad 6.06 | tok/s 9468
step    650 | loss 2.1360 | lr 6.49e-05 | grad 4.44 | tok/s 9428
step    660 | loss 1.9808 | lr 6.59e-05 | grad 4.38 | tok/s 9379
step    670 | loss 2.0387 | lr 6.69e-05 | grad 3.88 | tok/s 9835
step    680 | loss 2.0991 | lr 6.79e-05 | grad 4.47 | tok/s 9517
step    690 | loss 2.0573 | lr 6.89e-05 | grad 4.00 | tok/s 9453
step    700 | loss 2.0541 | lr 6.99e-05 | grad 3.50 | tok/s 9341
step    710 | loss 1.8759 | lr 7.09e-05 | grad 4.03 | tok/s 9632
step    720 | loss 2.1077 | lr 7.19e-05 | grad 5.41 | tok/s 9389
step    730 | loss 1.7386 | lr 7.29e-05 | grad 3.27 | tok/s 9835

Training complete! Final step: 733
