Using device: cuda
Output directory: benchmark_results/500m_smaller_state/e88_h20n32/levelE88_h20n32_100m_20260121_144705
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_h20n32, 132,806,560 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.1202 | lr 9.00e-07 | grad 40.50 | tok/s 11319
step     20 | loss 6.0653 | lr 1.90e-06 | grad 36.75 | tok/s 31826
step     30 | loss 5.8967 | lr 2.90e-06 | grad 19.38 | tok/s 31817
step     40 | loss 6.0934 | lr 3.90e-06 | grad 19.38 | tok/s 33077
step     50 | loss 6.1129 | lr 4.90e-06 | grad 19.25 | tok/s 34341
step     60 | loss 6.0882 | lr 5.90e-06 | grad 15.75 | tok/s 34100
step     70 | loss 6.0680 | lr 6.90e-06 | grad 21.88 | tok/s 34321
step     80 | loss 5.9981 | lr 7.90e-06 | grad 17.50 | tok/s 34327
step     90 | loss 5.9559 | lr 8.90e-06 | grad 15.44 | tok/s 34052
step    100 | loss 5.8660 | lr 9.90e-06 | grad 16.62 | tok/s 34224
step    110 | loss 5.7494 | lr 1.09e-05 | grad 41.75 | tok/s 34190
step    120 | loss 5.5166 | lr 1.19e-05 | grad 45.50 | tok/s 32969
step    130 | loss 5.2376 | lr 1.29e-05 | grad 17.12 | tok/s 32451
step    140 | loss 4.8819 | lr 1.39e-05 | grad 26.50 | tok/s 32425
step    150 | loss 4.4248 | lr 1.49e-05 | grad 31.38 | tok/s 33550
step    160 | loss 4.3926 | lr 1.59e-05 | grad 14.94 | tok/s 33673
step    170 | loss 4.2916 | lr 1.69e-05 | grad 17.12 | tok/s 30855
step    180 | loss 4.2031 | lr 1.79e-05 | grad 18.12 | tok/s 31107
step    190 | loss 3.8452 | lr 1.89e-05 | grad 11.00 | tok/s 32821
step    200 | loss 3.6464 | lr 1.99e-05 | grad 8.75 | tok/s 35209
step    210 | loss 3.3430 | lr 2.09e-05 | grad 10.19 | tok/s 33004
step    220 | loss 3.3992 | lr 2.19e-05 | grad 9.69 | tok/s 31882
step    230 | loss 3.6909 | lr 2.29e-05 | grad 5.97 | tok/s 31721
step    240 | loss 3.2027 | lr 2.39e-05 | grad 12.00 | tok/s 31901
step    250 | loss 3.4489 | lr 2.49e-05 | grad 6.38 | tok/s 31836
step    260 | loss 3.0664 | lr 2.59e-05 | grad 4.59 | tok/s 32979
step    270 | loss 3.1176 | lr 2.69e-05 | grad 4.97 | tok/s 32553
step    280 | loss 2.7608 | lr 2.79e-05 | grad 4.28 | tok/s 31452
step    290 | loss 2.7379 | lr 2.89e-05 | grad 6.44 | tok/s 30051
step    300 | loss 2.7909 | lr 2.99e-05 | grad 4.72 | tok/s 31070
step    310 | loss 2.7684 | lr 3.09e-05 | grad 3.69 | tok/s 31817
step    320 | loss 2.5002 | lr 3.19e-05 | grad 4.09 | tok/s 30307
step    330 | loss 2.7736 | lr 3.29e-05 | grad 2.97 | tok/s 31888
step    340 | loss 2.7873 | lr 3.39e-05 | grad 21.62 | tok/s 32509
step    350 | loss 2.8185 | lr 3.49e-05 | grad 5.56 | tok/s 31853
step    360 | loss 2.8599 | lr 3.59e-05 | grad 4.44 | tok/s 32527
step    370 | loss 2.5168 | lr 3.69e-05 | grad 3.55 | tok/s 31828
step    380 | loss 2.5455 | lr 3.79e-05 | grad 4.31 | tok/s 33090
step    390 | loss 2.3584 | lr 3.89e-05 | grad 3.48 | tok/s 33665
step    400 | loss 2.1862 | lr 3.99e-05 | grad 3.36 | tok/s 32713
step    410 | loss 2.6730 | lr 4.09e-05 | grad 4.09 | tok/s 31524
step    420 | loss 2.5147 | lr 4.19e-05 | grad 5.06 | tok/s 31741
step    430 | loss 2.6331 | lr 4.29e-05 | grad 7.97 | tok/s 33477
step    440 | loss 2.3937 | lr 4.39e-05 | grad 3.33 | tok/s 32455
step    450 | loss 2.4570 | lr 4.49e-05 | grad 2.64 | tok/s 31915
step    460 | loss 2.1945 | lr 4.59e-05 | grad 7.16 | tok/s 31640
step    470 | loss 2.3602 | lr 4.69e-05 | grad 3.81 | tok/s 31678
step    480 | loss 2.4095 | lr 4.79e-05 | grad 4.56 | tok/s 33251
step    490 | loss 2.3432 | lr 4.89e-05 | grad 3.16 | tok/s 31305
step    500 | loss 2.3067 | lr 4.99e-05 | grad 4.06 | tok/s 31836
step    510 | loss 2.4728 | lr 5.09e-05 | grad 10.75 | tok/s 31224
step    520 | loss 2.1977 | lr 5.19e-05 | grad 2.92 | tok/s 30070
step    530 | loss 2.0988 | lr 5.29e-05 | grad 3.75 | tok/s 31921
step    540 | loss 2.2839 | lr 5.39e-05 | grad 3.58 | tok/s 31179
step    550 | loss 2.2420 | lr 5.49e-05 | grad 3.66 | tok/s 31000
step    560 | loss 1.9466 | lr 5.59e-05 | grad 4.69 | tok/s 32373
step    570 | loss 2.0884 | lr 5.69e-05 | grad 3.25 | tok/s 33413
step    580 | loss 1.9207 | lr 5.79e-05 | grad 3.16 | tok/s 33436
step    590 | loss 1.7945 | lr 5.89e-05 | grad 3.03 | tok/s 33235
step    600 | loss 1.8768 | lr 5.99e-05 | grad 3.48 | tok/s 33363
step    610 | loss 1.7771 | lr 6.09e-05 | grad 3.30 | tok/s 33463
step    620 | loss 1.7242 | lr 6.19e-05 | grad 2.45 | tok/s 33377
step    630 | loss 1.8521 | lr 6.29e-05 | grad 6.81 | tok/s 32652
step    640 | loss 2.1320 | lr 6.39e-05 | grad 5.19 | tok/s 31353
step    650 | loss 2.1861 | lr 6.49e-05 | grad 5.44 | tok/s 30878
step    660 | loss 2.0362 | lr 6.59e-05 | grad 4.25 | tok/s 31440
step    670 | loss 2.0787 | lr 6.69e-05 | grad 3.73 | tok/s 32584
step    680 | loss 2.1376 | lr 6.79e-05 | grad 4.44 | tok/s 31368
step    690 | loss 2.1128 | lr 6.89e-05 | grad 3.73 | tok/s 31145
step    700 | loss 2.0813 | lr 6.99e-05 | grad 4.09 | tok/s 30844
step    710 | loss 1.9478 | lr 7.09e-05 | grad 4.47 | tok/s 31516
step    720 | loss 2.1576 | lr 7.19e-05 | grad 6.47 | tok/s 30151
step    730 | loss 1.8142 | lr 7.29e-05 | grad 3.33 | tok/s 32271
step    740 | loss 1.8738 | lr 7.39e-05 | grad 2.83 | tok/s 31237
step    750 | loss 2.5091 | lr 7.49e-05 | grad 7.41 | tok/s 32814
step    760 | loss 2.2683 | lr 7.59e-05 | grad 3.47 | tok/s 32845
step    770 | loss 1.9819 | lr 7.69e-05 | grad 4.34 | tok/s 32140
step    780 | loss 2.0547 | lr 7.79e-05 | grad 3.30 | tok/s 31040
step    790 | loss 1.9530 | lr 7.89e-05 | grad 3.67 | tok/s 31748
step    800 | loss 2.2266 | lr 7.99e-05 | grad 7.31 | tok/s 33285
step    810 | loss 1.9803 | lr 8.09e-05 | grad 3.66 | tok/s 32517
step    820 | loss 1.6476 | lr 8.19e-05 | grad 6.72 | tok/s 31574
step    830 | loss 1.9348 | lr 8.29e-05 | grad 3.66 | tok/s 32185
step    840 | loss 1.9553 | lr 8.39e-05 | grad 3.20 | tok/s 31464
step    850 | loss 2.1327 | lr 8.49e-05 | grad 3.16 | tok/s 31573
step    860 | loss 2.0580 | lr 8.59e-05 | grad 2.95 | tok/s 31953
step    870 | loss 1.9686 | lr 8.69e-05 | grad 5.66 | tok/s 32147
step    880 | loss 2.0715 | lr 8.79e-05 | grad 3.86 | tok/s 33699
step    890 | loss 1.9943 | lr 8.89e-05 | grad 3.55 | tok/s 32171
step    900 | loss 1.8554 | lr 8.99e-05 | grad 3.03 | tok/s 32153
step    910 | loss 1.8095 | lr 9.09e-05 | grad 2.84 | tok/s 32431
step    920 | loss 1.9709 | lr 9.19e-05 | grad 3.12 | tok/s 32078
step    930 | loss 1.9053 | lr 9.29e-05 | grad 2.72 | tok/s 32075
step    940 | loss 1.8019 | lr 9.39e-05 | grad 3.23 | tok/s 33175
step    950 | loss 1.7209 | lr 9.49e-05 | grad 2.36 | tok/s 31624
step    960 | loss 1.8409 | lr 9.59e-05 | grad 2.56 | tok/s 31174
step    970 | loss 1.7290 | lr 9.69e-05 | grad 2.56 | tok/s 31540
step    980 | loss 1.7140 | lr 9.79e-05 | grad 2.45 | tok/s 32442
step    990 | loss 2.5366 | lr 9.89e-05 | grad 4.47 | tok/s 33768
step   1000 | loss 2.2841 | lr 9.99e-05 | grad 3.20 | tok/s 32330
  >>> saved checkpoint: checkpoint_step_001000_loss_2.2841.pt
step   1010 | loss 2.1968 | lr 1.02e-06 | grad 3.59 | tok/s 22745
step   1020 | loss 1.7184 | lr 1.09e-06 | grad 3.52 | tok/s 29101
step   1030 | loss 1.7268 | lr 1.21e-06 | grad 4.69 | tok/s 29424
step   1040 | loss 2.1964 | lr 1.37e-06 | grad 2.98 | tok/s 29725
step   1050 | loss 2.2463 | lr 1.59e-06 | grad 11.00 | tok/s 28464
step   1060 | loss 2.8399 | lr 1.85e-06 | grad 2.84 | tok/s 29597
step   1070 | loss 2.1504 | lr 2.16e-06 | grad 12.94 | tok/s 28488
step   1080 | loss 1.6197 | lr 2.52e-06 | grad 3.92 | tok/s 29115
step   1090 | loss 1.8690 | lr 2.92e-06 | grad 3.70 | tok/s 29496
step   1100 | loss 1.8173 | lr 3.37e-06 | grad 3.20 | tok/s 30031
step   1110 | loss 1.7927 | lr 3.87e-06 | grad 3.05 | tok/s 30343
step   1120 | loss 1.7852 | lr 4.42e-06 | grad 3.02 | tok/s 29787
step   1130 | loss 1.7871 | lr 5.01e-06 | grad 2.73 | tok/s 29524
step   1140 | loss 2.2863 | lr 5.65e-06 | grad 5.91 | tok/s 29763
step   1150 | loss 2.4843 | lr 6.32e-06 | grad 5.88 | tok/s 29047
step   1160 | loss 2.0080 | lr 7.05e-06 | grad 3.41 | tok/s 28807
step   1170 | loss 2.7826 | lr 7.81e-06 | grad 4.94 | tok/s 28318
step   1180 | loss 2.2108 | lr 8.62e-06 | grad 3.20 | tok/s 28344
step   1190 | loss 1.9780 | lr 9.47e-06 | grad 2.27 | tok/s 27349
step   1200 | loss 1.9795 | lr 1.04e-05 | grad 4.78 | tok/s 29371
step   1210 | loss 2.5203 | lr 1.13e-05 | grad 2.98 | tok/s 30384
step   1220 | loss 1.6843 | lr 1.23e-05 | grad 2.39 | tok/s 29960
step   1230 | loss 1.8535 | lr 1.33e-05 | grad 2.92 | tok/s 28201
step   1240 | loss 1.8000 | lr 1.43e-05 | grad 2.48 | tok/s 29019
step   1250 | loss 1.8674 | lr 1.54e-05 | grad 2.08 | tok/s 29673
step   1260 | loss 1.7870 | lr 1.65e-05 | grad 2.17 | tok/s 29004
step   1270 | loss 2.3324 | lr 1.76e-05 | grad 8.81 | tok/s 29682
step   1280 | loss 2.2216 | lr 1.88e-05 | grad 2.38 | tok/s 29272
step   1290 | loss 1.7314 | lr 2.00e-05 | grad 2.84 | tok/s 29121
step   1300 | loss 1.8635 | lr 2.13e-05 | grad 3.20 | tok/s 28610
step   1310 | loss 1.8101 | lr 2.25e-05 | grad 2.19 | tok/s 28423
step   1320 | loss 2.3760 | lr 2.38e-05 | grad 13.12 | tok/s 28630
step   1330 | loss 1.8338 | lr 2.52e-05 | grad 2.59 | tok/s 29076
step   1340 | loss 2.0191 | lr 2.65e-05 | grad 4.22 | tok/s 29171
step   1350 | loss 1.7969 | lr 2.79e-05 | grad 2.44 | tok/s 28162
step   1360 | loss 1.9431 | lr 2.93e-05 | grad 2.95 | tok/s 28356
step   1370 | loss 1.9197 | lr 3.07e-05 | grad 6.75 | tok/s 28928
step   1380 | loss 1.9052 | lr 3.21e-05 | grad 3.83 | tok/s 28623
step   1390 | loss 2.3142 | lr 3.36e-05 | grad 3.97 | tok/s 30011
step   1400 | loss 1.7138 | lr 3.51e-05 | grad 5.19 | tok/s 27962
step   1410 | loss 1.8172 | lr 3.65e-05 | grad 2.39 | tok/s 29105
step   1420 | loss 1.9645 | lr 3.80e-05 | grad 3.06 | tok/s 28771
step   1430 | loss 1.7342 | lr 3.96e-05 | grad 5.84 | tok/s 27770
step   1440 | loss 2.0053 | lr 4.11e-05 | grad 11.06 | tok/s 30082
step   1450 | loss 2.0539 | lr 4.26e-05 | grad 2.88 | tok/s 28761
step   1460 | loss 1.8851 | lr 4.41e-05 | grad 3.94 | tok/s 29663
step   1470 | loss 1.8769 | lr 4.57e-05 | grad 4.72 | tok/s 29439
step   1480 | loss 1.8276 | lr 4.72e-05 | grad 5.50 | tok/s 28197
step   1490 | loss 1.6067 | lr 4.88e-05 | grad 2.34 | tok/s 27748
step   1500 | loss 1.7596 | lr 5.03e-05 | grad 3.30 | tok/s 29318
step   1510 | loss 2.6624 | lr 5.19e-05 | grad 21.00 | tok/s 28524
step   1520 | loss 1.8351 | lr 5.35e-05 | grad 2.78 | tok/s 28877
step   1530 | loss 1.6260 | lr 5.50e-05 | grad 2.19 | tok/s 28717
step   1540 | loss 1.7841 | lr 5.65e-05 | grad 3.09 | tok/s 28538
step   1550 | loss 1.6685 | lr 5.81e-05 | grad 2.39 | tok/s 29005
step   1560 | loss 1.8585 | lr 5.96e-05 | grad 2.22 | tok/s 29100
step   1570 | loss 1.7929 | lr 6.11e-05 | grad 4.38 | tok/s 28921
step   1580 | loss 1.5736 | lr 6.27e-05 | grad 2.23 | tok/s 29889
step   1590 | loss 1.7586 | lr 6.42e-05 | grad 2.89 | tok/s 29387
step   1600 | loss 1.5698 | lr 6.56e-05 | grad 2.77 | tok/s 29553
step   1610 | loss 1.8023 | lr 6.71e-05 | grad 5.03 | tok/s 27873
step   1620 | loss 1.7337 | lr 6.86e-05 | grad 7.28 | tok/s 30188
step   1630 | loss 2.3620 | lr 7.00e-05 | grad 5.75 | tok/s 28741
step   1640 | loss 2.4809 | lr 7.14e-05 | grad 3.02 | tok/s 30135
step   1650 | loss 1.9229 | lr 7.28e-05 | grad 2.11 | tok/s 30528
step   1660 | loss 1.6787 | lr 7.42e-05 | grad 2.27 | tok/s 30305
step   1670 | loss 1.5496 | lr 7.56e-05 | grad 1.85 | tok/s 30484
step   1680 | loss 1.5018 | lr 7.69e-05 | grad 2.05 | tok/s 30499
step   1690 | loss 1.9387 | lr 7.82e-05 | grad 6.75 | tok/s 28419
step   1700 | loss 1.9546 | lr 7.95e-05 | grad 2.88 | tok/s 28251
step   1710 | loss 1.7818 | lr 8.07e-05 | grad 2.97 | tok/s 26932
step   1720 | loss 1.6888 | lr 8.19e-05 | grad 2.95 | tok/s 28561
step   1730 | loss 1.5872 | lr 8.31e-05 | grad 3.89 | tok/s 29646
step   1740 | loss 1.8009 | lr 8.43e-05 | grad 3.25 | tok/s 28404
step   1750 | loss 1.8009 | lr 8.54e-05 | grad 3.08 | tok/s 29233
step   1760 | loss 1.8023 | lr 8.65e-05 | grad 2.94 | tok/s 28622
step   1770 | loss 1.6953 | lr 8.75e-05 | grad 2.97 | tok/s 29146
step   1780 | loss 1.6629 | lr 8.85e-05 | grad 3.45 | tok/s 28513
step   1790 | loss 2.1965 | lr 8.95e-05 | grad 3.19 | tok/s 27956
step   1800 | loss 2.0999 | lr 9.05e-05 | grad 2.36 | tok/s 27072
step   1810 | loss 1.7136 | lr 9.14e-05 | grad 2.83 | tok/s 28615
step   1820 | loss 1.7171 | lr 9.22e-05 | grad 2.45 | tok/s 28679
step   1830 | loss 1.7387 | lr 9.30e-05 | grad 2.27 | tok/s 28251
step   1840 | loss 1.7333 | lr 9.38e-05 | grad 3.00 | tok/s 28977
step   1850 | loss 1.7672 | lr 9.45e-05 | grad 2.86 | tok/s 28310
step   1860 | loss 1.8186 | lr 9.52e-05 | grad 3.84 | tok/s 28698
step   1870 | loss 1.6676 | lr 9.59e-05 | grad 5.69 | tok/s 28029
step   1880 | loss 1.8219 | lr 9.65e-05 | grad 4.41 | tok/s 28709
step   1890 | loss 1.8132 | lr 9.70e-05 | grad 2.53 | tok/s 27989
step   1900 | loss 1.8018 | lr 9.75e-05 | grad 3.00 | tok/s 29405
step   1910 | loss 1.5525 | lr 9.80e-05 | grad 1.92 | tok/s 29823
step   1920 | loss 1.4008 | lr 9.84e-05 | grad 1.86 | tok/s 30376
step   1930 | loss 1.3611 | lr 9.88e-05 | grad 1.96 | tok/s 30473
step   1940 | loss 1.3127 | lr 9.91e-05 | grad 1.59 | tok/s 30414
step   1950 | loss 1.2528 | lr 9.94e-05 | grad 1.65 | tok/s 30435
step   1960 | loss 1.7902 | lr 9.96e-05 | grad 6.09 | tok/s 28487
step   1970 | loss 1.8976 | lr 9.98e-05 | grad 3.28 | tok/s 27379
step   1980 | loss 1.8412 | lr 9.99e-05 | grad 2.81 | tok/s 28965
step   1990 | loss 1.8562 | lr 1.00e-04 | grad 4.22 | tok/s 28334
step   2000 | loss 1.6815 | lr 1.00e-04 | grad 2.12 | tok/s 27810
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6815.pt
step   2010 | loss 2.0480 | lr 1.00e-04 | grad 2.78 | tok/s 24275
step   2020 | loss 1.7991 | lr 9.99e-05 | grad 2.25 | tok/s 29279
step   2030 | loss 1.4037 | lr 9.98e-05 | grad 2.47 | tok/s 30182
step   2040 | loss 1.7041 | lr 9.96e-05 | grad 2.16 | tok/s 26618
step   2050 | loss 1.7740 | lr 9.94e-05 | grad 2.67 | tok/s 29000
step   2060 | loss 2.2199 | lr 9.92e-05 | grad 2.28 | tok/s 27182
step   2070 | loss 1.6501 | lr 9.88e-05 | grad 1.90 | tok/s 29334
step   2080 | loss 1.7060 | lr 9.85e-05 | grad 3.33 | tok/s 28482
step   2090 | loss 1.9580 | lr 9.81e-05 | grad 2.83 | tok/s 29128
step   2100 | loss 1.4166 | lr 9.76e-05 | grad 2.14 | tok/s 28939
step   2110 | loss 1.5515 | lr 9.71e-05 | grad 1.78 | tok/s 28419
step   2120 | loss 2.1201 | lr 9.66e-05 | grad 4.28 | tok/s 29152
step   2130 | loss 2.0917 | lr 9.60e-05 | grad 2.86 | tok/s 29823
step   2140 | loss 1.7202 | lr 9.54e-05 | grad 2.48 | tok/s 28538
step   2150 | loss 2.7658 | lr 9.47e-05 | grad 5.59 | tok/s 29444
step   2160 | loss 1.9727 | lr 9.40e-05 | grad 3.61 | tok/s 28197
step   2170 | loss 1.9445 | lr 9.32e-05 | grad 2.19 | tok/s 28672
step   2180 | loss 2.2032 | lr 9.24e-05 | grad 3.17 | tok/s 29382
step   2190 | loss 1.8165 | lr 9.15e-05 | grad 1.73 | tok/s 28638
step   2200 | loss 1.7578 | lr 9.06e-05 | grad 2.53 | tok/s 29095
step   2210 | loss 1.5658 | lr 8.97e-05 | grad 3.34 | tok/s 30288
step   2220 | loss 1.9449 | lr 8.87e-05 | grad 3.55 | tok/s 28381
step   2230 | loss 1.6174 | lr 8.77e-05 | grad 2.23 | tok/s 30332
step   2240 | loss 1.8343 | lr 8.67e-05 | grad 2.14 | tok/s 28993
step   2250 | loss 1.6995 | lr 8.56e-05 | grad 2.19 | tok/s 27931
step   2260 | loss 1.8307 | lr 8.45e-05 | grad 1.92 | tok/s 28592
step   2270 | loss 1.5587 | lr 8.34e-05 | grad 3.77 | tok/s 29312
step   2280 | loss 1.6212 | lr 8.22e-05 | grad 1.92 | tok/s 28486
step   2290 | loss 1.7954 | lr 8.10e-05 | grad 2.66 | tok/s 29652

Training complete! Final step: 2290
