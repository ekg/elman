Using device: cuda
Output directory: benchmark_results/e88_scaling/n24/levelE88_n24_100m_20260122_223747
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_n24, 540,942,400 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.4073 | lr 3.00e-04 | grad 17.50 | tok/s 8742
step     20 | loss 3.4893 | lr 3.00e-04 | grad 32.00 | tok/s 11459
step     30 | loss 4.5873 | lr 3.00e-04 | grad 14.81 | tok/s 11950
step     40 | loss 3.2712 | lr 3.00e-04 | grad 7.69 | tok/s 11804
step     50 | loss 2.6971 | lr 3.00e-04 | grad 5.47 | tok/s 11728
step     60 | loss 2.9759 | lr 3.00e-04 | grad 5.03 | tok/s 11463
step     70 | loss 2.6590 | lr 3.00e-04 | grad 3.45 | tok/s 11048
step     80 | loss 2.4940 | lr 3.00e-04 | grad 5.12 | tok/s 11480
step     90 | loss 2.5901 | lr 3.00e-04 | grad 14.75 | tok/s 11090
step    100 | loss 2.2429 | lr 3.00e-04 | grad 4.06 | tok/s 11177
step    110 | loss 2.2412 | lr 3.00e-04 | grad 2.73 | tok/s 11021
step    120 | loss 2.2701 | lr 3.00e-04 | grad 2.70 | tok/s 10882
step    130 | loss 2.2840 | lr 3.00e-04 | grad 2.36 | tok/s 10890
step    140 | loss 2.0936 | lr 3.00e-04 | grad 2.25 | tok/s 11178
step    150 | loss 1.9735 | lr 3.00e-04 | grad 2.98 | tok/s 10673
step    160 | loss 1.9164 | lr 3.00e-04 | grad 1.92 | tok/s 10753
step    170 | loss 2.0822 | lr 3.00e-04 | grad 12.88 | tok/s 11133
step    180 | loss 1.9977 | lr 3.00e-04 | grad 1.41 | tok/s 11172
step    190 | loss 1.7433 | lr 3.00e-04 | grad 2.03 | tok/s 11349
step    200 | loss 1.3792 | lr 3.00e-04 | grad 1.79 | tok/s 11615
step    210 | loss 1.9535 | lr 3.00e-04 | grad 2.08 | tok/s 11124
step    220 | loss 1.7995 | lr 3.00e-04 | grad 2.05 | tok/s 11500
step    230 | loss 1.7966 | lr 3.00e-04 | grad 3.20 | tok/s 11081
step    240 | loss 1.7508 | lr 3.00e-04 | grad 2.22 | tok/s 11316
step    250 | loss 1.7274 | lr 3.00e-04 | grad 2.02 | tok/s 11165
step    260 | loss 1.8347 | lr 3.00e-04 | grad 1.70 | tok/s 10721
step    270 | loss 1.7437 | lr 3.00e-04 | grad 1.46 | tok/s 11130
step    280 | loss 1.6182 | lr 3.00e-04 | grad 2.27 | tok/s 11095
step    290 | loss 1.5247 | lr 3.00e-04 | grad 1.45 | tok/s 11693
step    300 | loss 1.4476 | lr 3.00e-04 | grad 1.61 | tok/s 11690
step    310 | loss 1.4030 | lr 3.00e-04 | grad 1.44 | tok/s 11704
step    320 | loss 1.5962 | lr 3.00e-04 | grad 2.92 | tok/s 11257
step    330 | loss 1.7136 | lr 3.00e-04 | grad 1.40 | tok/s 10965
step    340 | loss 1.7120 | lr 3.00e-04 | grad 3.48 | tok/s 11216
step    350 | loss 1.7156 | lr 3.00e-04 | grad 1.52 | tok/s 10899
step    360 | loss 1.6702 | lr 3.00e-04 | grad 3.27 | tok/s 11041
step    370 | loss 1.4817 | lr 3.00e-04 | grad 1.33 | tok/s 11268
step    380 | loss 1.8211 | lr 3.00e-04 | grad 1.59 | tok/s 11541
step    390 | loss 1.6084 | lr 3.00e-04 | grad 1.27 | tok/s 11111
step    400 | loss 1.6557 | lr 3.00e-04 | grad 3.22 | tok/s 11395
step    410 | loss 1.4313 | lr 3.00e-04 | grad 3.17 | tok/s 11021
step    420 | loss 1.5522 | lr 3.00e-04 | grad 1.20 | tok/s 10940

Training complete! Final step: 425
