Using device: cuda
Output directory: benchmark_results/e88_scaling/dim1792/levelE88_dim1792_100m_20260122_222614
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_dim1792, 492,459,616 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.3885 | lr 3.00e-04 | grad 15.19 | tok/s 8682
step     20 | loss 3.5541 | lr 3.00e-04 | grad 30.00 | tok/s 11347
step     30 | loss 4.5965 | lr 3.00e-04 | grad 13.38 | tok/s 11882
step     40 | loss 3.3120 | lr 3.00e-04 | grad 7.03 | tok/s 11733
step     50 | loss 2.7647 | lr 3.00e-04 | grad 4.53 | tok/s 11584
step     60 | loss 2.9867 | lr 3.00e-04 | grad 4.72 | tok/s 11266
step     70 | loss 2.6474 | lr 3.00e-04 | grad 2.94 | tok/s 10802
step     80 | loss 2.5009 | lr 3.00e-04 | grad 3.77 | tok/s 11229
step     90 | loss 2.6116 | lr 3.00e-04 | grad 10.56 | tok/s 10863
step    100 | loss 2.2769 | lr 3.00e-04 | grad 3.94 | tok/s 10949
step    110 | loss 2.2864 | lr 3.00e-04 | grad 2.58 | tok/s 10762
step    120 | loss 2.3342 | lr 3.00e-04 | grad 2.38 | tok/s 10607
step    130 | loss 2.3144 | lr 3.00e-04 | grad 2.17 | tok/s 10741
step    140 | loss 2.1147 | lr 3.00e-04 | grad 1.75 | tok/s 10897
step    150 | loss 2.0034 | lr 3.00e-04 | grad 2.59 | tok/s 10417
step    160 | loss 1.9405 | lr 3.00e-04 | grad 1.77 | tok/s 10514
step    170 | loss 2.1046 | lr 3.00e-04 | grad 10.94 | tok/s 10889
step    180 | loss 2.0250 | lr 3.00e-04 | grad 1.30 | tok/s 10942
step    190 | loss 1.7823 | lr 3.00e-04 | grad 1.95 | tok/s 11111
step    200 | loss 1.4277 | lr 3.00e-04 | grad 1.84 | tok/s 11363
step    210 | loss 1.9717 | lr 3.00e-04 | grad 1.93 | tok/s 10874
step    220 | loss 1.8316 | lr 3.00e-04 | grad 1.81 | tok/s 11265
step    230 | loss 1.8101 | lr 3.00e-04 | grad 2.72 | tok/s 10893
step    240 | loss 1.7681 | lr 3.00e-04 | grad 2.12 | tok/s 11111
step    250 | loss 1.7446 | lr 3.00e-04 | grad 1.80 | tok/s 10940
step    260 | loss 1.8509 | lr 3.00e-04 | grad 1.48 | tok/s 10502
step    270 | loss 1.7533 | lr 3.00e-04 | grad 1.38 | tok/s 10783
step    280 | loss 1.6337 | lr 3.00e-04 | grad 2.14 | tok/s 10896
step    290 | loss 1.5380 | lr 3.00e-04 | grad 1.46 | tok/s 11475
step    300 | loss 1.4660 | lr 3.00e-04 | grad 1.46 | tok/s 11485
step    310 | loss 1.4167 | lr 3.00e-04 | grad 1.46 | tok/s 11487
step    320 | loss 1.5999 | lr 3.00e-04 | grad 2.78 | tok/s 11053
step    330 | loss 1.7111 | lr 3.00e-04 | grad 1.29 | tok/s 10769
step    340 | loss 1.7207 | lr 3.00e-04 | grad 3.19 | tok/s 11001
step    350 | loss 1.7234 | lr 3.00e-04 | grad 1.40 | tok/s 10692
step    360 | loss 1.6747 | lr 3.00e-04 | grad 3.34 | tok/s 10842
step    370 | loss 1.4902 | lr 3.00e-04 | grad 1.25 | tok/s 11048
step    380 | loss 1.8350 | lr 3.00e-04 | grad 1.41 | tok/s 11317
step    390 | loss 1.6135 | lr 3.00e-04 | grad 1.20 | tok/s 10872
step    400 | loss 1.6695 | lr 3.00e-04 | grad 2.97 | tok/s 11164
step    410 | loss 1.4394 | lr 3.00e-04 | grad 2.89 | tok/s 10765

Training complete! Final step: 417
