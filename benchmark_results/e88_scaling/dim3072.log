Using device: cuda
Output directory: benchmark_results/e88_scaling/dim3072/levelE88_dim3072_100m_20260122_222612
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_dim3072, 533,446,720 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6015 | lr 3.00e-04 | grad 14.31 | tok/s 9885
step     20 | loss 4.4587 | lr 3.00e-04 | grad 17.12 | tok/s 13661
step     30 | loss 3.9468 | lr 3.00e-04 | grad 8.38 | tok/s 14424
step     40 | loss 3.2498 | lr 3.00e-04 | grad 12.25 | tok/s 14193
step     50 | loss 2.7172 | lr 3.00e-04 | grad 8.81 | tok/s 13888
step     60 | loss 3.0100 | lr 3.00e-04 | grad 9.31 | tok/s 13344
step     70 | loss 2.7357 | lr 3.00e-04 | grad 4.25 | tok/s 12729
step     80 | loss 2.4053 | lr 3.00e-04 | grad 8.81 | tok/s 13150
step     90 | loss 2.5136 | lr 3.00e-04 | grad 8.81 | tok/s 12604
step    100 | loss 2.1187 | lr 3.00e-04 | grad 4.56 | tok/s 12683
step    110 | loss 2.1496 | lr 3.00e-04 | grad 3.14 | tok/s 12448
step    120 | loss 2.2387 | lr 3.00e-04 | grad 3.69 | tok/s 12252
step    130 | loss 2.1862 | lr 3.00e-04 | grad 3.28 | tok/s 12509
step    140 | loss 1.9984 | lr 3.00e-04 | grad 2.34 | tok/s 12525
step    150 | loss 1.8798 | lr 3.00e-04 | grad 3.62 | tok/s 11927
step    160 | loss 1.8391 | lr 3.00e-04 | grad 2.39 | tok/s 12011
step    170 | loss 1.9839 | lr 3.00e-04 | grad 5.94 | tok/s 12435
step    180 | loss 1.8778 | lr 3.00e-04 | grad 1.41 | tok/s 12440
step    190 | loss 1.6457 | lr 3.00e-04 | grad 2.19 | tok/s 12672
step    200 | loss 1.2979 | lr 3.00e-04 | grad 1.99 | tok/s 12967
step    210 | loss 1.8915 | lr 3.00e-04 | grad 2.34 | tok/s 12410
step    220 | loss 1.7158 | lr 3.00e-04 | grad 2.27 | tok/s 12827
step    230 | loss 1.7402 | lr 3.00e-04 | grad 3.36 | tok/s 12411
step    240 | loss 1.6832 | lr 3.00e-04 | grad 2.62 | tok/s 12650
step    250 | loss 1.6668 | lr 3.00e-04 | grad 2.20 | tok/s 12474
step    260 | loss 1.7903 | lr 3.00e-04 | grad 2.30 | tok/s 11973
step    270 | loss 1.6923 | lr 3.00e-04 | grad 1.95 | tok/s 12439
step    280 | loss 1.5838 | lr 3.00e-04 | grad 2.95 | tok/s 12407
step    290 | loss 1.4691 | lr 3.00e-04 | grad 2.08 | tok/s 13079
step    300 | loss 1.4130 | lr 3.00e-04 | grad 2.56 | tok/s 13061
step    310 | loss 1.3745 | lr 3.00e-04 | grad 2.08 | tok/s 13074
step    320 | loss 1.5698 | lr 3.00e-04 | grad 3.88 | tok/s 12607
step    330 | loss 1.6759 | lr 3.00e-04 | grad 1.73 | tok/s 12292
step    340 | loss 1.6868 | lr 3.00e-04 | grad 3.50 | tok/s 12569
step    350 | loss 1.6919 | lr 3.00e-04 | grad 1.94 | tok/s 12193
step    360 | loss 1.6409 | lr 3.00e-04 | grad 4.31 | tok/s 12363
step    370 | loss 1.4513 | lr 3.00e-04 | grad 1.65 | tok/s 12579
step    380 | loss 1.7649 | lr 3.00e-04 | grad 1.96 | tok/s 12897
step    390 | loss 1.5946 | lr 3.00e-04 | grad 1.32 | tok/s 12403
step    400 | loss 1.6253 | lr 3.00e-04 | grad 3.52 | tok/s 12741
step    410 | loss 1.4079 | lr 3.00e-04 | grad 3.66 | tok/s 12348
step    420 | loss 1.5271 | lr 3.00e-04 | grad 1.34 | tok/s 12257
step    430 | loss 1.6615 | lr 3.00e-04 | grad 2.16 | tok/s 12227
step    440 | loss 1.6269 | lr 3.00e-04 | grad 1.30 | tok/s 12713
step    450 | loss 1.5971 | lr 3.00e-04 | grad 2.19 | tok/s 12364
step    460 | loss 1.5753 | lr 3.00e-04 | grad 1.48 | tok/s 12402
step    470 | loss 1.5489 | lr 3.00e-04 | grad 2.31 | tok/s 12570

Training complete! Final step: 479
