Using device: cuda
Output directory: benchmark_results/e88_scaling/dim2560/levelE88_dim2560_100m_20260122_222613
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_dim2560, 476,253,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.0837 | lr 3.00e-04 | grad 12.56 | tok/s 9973
step     20 | loss 3.8643 | lr 3.00e-04 | grad 18.50 | tok/s 13652
step     30 | loss 4.1088 | lr 3.00e-04 | grad 9.00 | tok/s 14255
step     40 | loss 3.1471 | lr 3.00e-04 | grad 10.81 | tok/s 14083
step     50 | loss 2.6696 | lr 3.00e-04 | grad 4.81 | tok/s 13855
step     60 | loss 2.9112 | lr 3.00e-04 | grad 5.88 | tok/s 13456
step     70 | loss 2.6533 | lr 3.00e-04 | grad 4.97 | tok/s 12876
step     80 | loss 2.3433 | lr 3.00e-04 | grad 5.66 | tok/s 13303
step     90 | loss 2.4895 | lr 3.00e-04 | grad 10.19 | tok/s 12780
step    100 | loss 2.1532 | lr 3.00e-04 | grad 4.78 | tok/s 12892
step    110 | loss 2.1829 | lr 3.00e-04 | grad 2.80 | tok/s 12707
step    120 | loss 2.2828 | lr 3.00e-04 | grad 3.06 | tok/s 12478
step    130 | loss 2.2180 | lr 3.00e-04 | grad 3.14 | tok/s 12731
step    140 | loss 2.0355 | lr 3.00e-04 | grad 2.31 | tok/s 12769
step    150 | loss 1.9060 | lr 3.00e-04 | grad 3.12 | tok/s 12152
step    160 | loss 1.8662 | lr 3.00e-04 | grad 2.14 | tok/s 12228
step    170 | loss 2.0252 | lr 3.00e-04 | grad 8.38 | tok/s 12662
step    180 | loss 1.9068 | lr 3.00e-04 | grad 1.34 | tok/s 12723
step    190 | loss 1.6658 | lr 3.00e-04 | grad 2.00 | tok/s 12887
step    200 | loss 1.3086 | lr 3.00e-04 | grad 2.16 | tok/s 13210
step    210 | loss 1.9049 | lr 3.00e-04 | grad 2.14 | tok/s 12644
step    220 | loss 1.7328 | lr 3.00e-04 | grad 2.05 | tok/s 13066
step    230 | loss 1.7513 | lr 3.00e-04 | grad 3.20 | tok/s 12618
step    240 | loss 1.7003 | lr 3.00e-04 | grad 2.36 | tok/s 12892
step    250 | loss 1.6792 | lr 3.00e-04 | grad 2.20 | tok/s 12711
step    260 | loss 1.7952 | lr 3.00e-04 | grad 1.88 | tok/s 12168
step    270 | loss 1.6994 | lr 3.00e-04 | grad 1.58 | tok/s 12684
step    280 | loss 1.5869 | lr 3.00e-04 | grad 2.56 | tok/s 12658
step    290 | loss 1.4834 | lr 3.00e-04 | grad 1.79 | tok/s 13292
step    300 | loss 1.4197 | lr 3.00e-04 | grad 1.72 | tok/s 13341
step    310 | loss 1.3767 | lr 3.00e-04 | grad 1.80 | tok/s 13357
step    320 | loss 1.5566 | lr 3.00e-04 | grad 2.34 | tok/s 12853
step    330 | loss 1.6796 | lr 3.00e-04 | grad 1.52 | tok/s 12575
step    340 | loss 1.6901 | lr 3.00e-04 | grad 3.39 | tok/s 12814
step    350 | loss 1.6884 | lr 3.00e-04 | grad 1.63 | tok/s 12446
step    360 | loss 1.6397 | lr 3.00e-04 | grad 3.50 | tok/s 12588
step    370 | loss 1.4484 | lr 3.00e-04 | grad 1.44 | tok/s 12848
step    380 | loss 1.7838 | lr 3.00e-04 | grad 1.72 | tok/s 13120
step    390 | loss 1.5954 | lr 3.00e-04 | grad 1.26 | tok/s 12677
step    400 | loss 1.6317 | lr 3.00e-04 | grad 3.22 | tok/s 12965
step    410 | loss 1.4063 | lr 3.00e-04 | grad 3.30 | tok/s 12574
step    420 | loss 1.5272 | lr 3.00e-04 | grad 1.21 | tok/s 12514
step    430 | loss 1.6639 | lr 3.00e-04 | grad 2.12 | tok/s 12458
step    440 | loss 1.6288 | lr 3.00e-04 | grad 1.40 | tok/s 12960
step    450 | loss 1.5955 | lr 3.00e-04 | grad 1.79 | tok/s 12617
step    460 | loss 1.5751 | lr 3.00e-04 | grad 1.24 | tok/s 12661
step    470 | loss 1.5477 | lr 3.00e-04 | grad 1.80 | tok/s 12810
step    480 | loss 1.4675 | lr 3.00e-04 | grad 1.54 | tok/s 12341

Training complete! Final step: 487
