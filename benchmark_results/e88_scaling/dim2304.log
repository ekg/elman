Using device: cuda
Output directory: benchmark_results/e88_scaling/dim2304/levelE88_dim2304_100m_20260122_222614
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_dim2304, 471,436,832 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.6471 | lr 3.00e-04 | grad 11.00 | tok/s 9815
step     20 | loss 3.5954 | lr 3.00e-04 | grad 20.62 | tok/s 13291
step     30 | loss 4.2429 | lr 3.00e-04 | grad 8.75 | tok/s 13901
step     40 | loss 3.1281 | lr 3.00e-04 | grad 10.94 | tok/s 13719
step     50 | loss 2.6032 | lr 3.00e-04 | grad 4.88 | tok/s 13665
step     60 | loss 2.9033 | lr 3.00e-04 | grad 4.53 | tok/s 13135
step     70 | loss 2.6322 | lr 3.00e-04 | grad 3.55 | tok/s 12535
step     80 | loss 2.3748 | lr 3.00e-04 | grad 5.22 | tok/s 12981
step     90 | loss 2.5222 | lr 3.00e-04 | grad 10.44 | tok/s 12463
step    100 | loss 2.1836 | lr 3.00e-04 | grad 4.25 | tok/s 12602
step    110 | loss 2.2142 | lr 3.00e-04 | grad 2.67 | tok/s 12396
step    120 | loss 2.2709 | lr 3.00e-04 | grad 2.66 | tok/s 12204
step    130 | loss 2.2389 | lr 3.00e-04 | grad 2.47 | tok/s 12491
step    140 | loss 2.0602 | lr 3.00e-04 | grad 2.12 | tok/s 12506
step    150 | loss 1.9324 | lr 3.00e-04 | grad 3.06 | tok/s 11924
step    160 | loss 1.8899 | lr 3.00e-04 | grad 1.95 | tok/s 12052
step    170 | loss 2.0410 | lr 3.00e-04 | grad 7.38 | tok/s 12467
step    180 | loss 1.9371 | lr 3.00e-04 | grad 1.38 | tok/s 12506
step    190 | loss 1.7006 | lr 3.00e-04 | grad 2.02 | tok/s 12704
step    200 | loss 1.3297 | lr 3.00e-04 | grad 1.84 | tok/s 13004
step    210 | loss 1.9183 | lr 3.00e-04 | grad 2.12 | tok/s 12461
step    220 | loss 1.7511 | lr 3.00e-04 | grad 1.92 | tok/s 12897
step    230 | loss 1.7655 | lr 3.00e-04 | grad 3.06 | tok/s 12441
step    240 | loss 1.7152 | lr 3.00e-04 | grad 2.31 | tok/s 12736
step    250 | loss 1.6895 | lr 3.00e-04 | grad 2.17 | tok/s 12538
step    260 | loss 1.8113 | lr 3.00e-04 | grad 1.76 | tok/s 12032
step    270 | loss 1.7122 | lr 3.00e-04 | grad 1.47 | tok/s 12508
step    280 | loss 1.5941 | lr 3.00e-04 | grad 2.36 | tok/s 12495
step    290 | loss 1.4943 | lr 3.00e-04 | grad 1.69 | tok/s 13184
step    300 | loss 1.4264 | lr 3.00e-04 | grad 1.73 | tok/s 13193
step    310 | loss 1.3845 | lr 3.00e-04 | grad 1.66 | tok/s 13205
step    320 | loss 1.5594 | lr 3.00e-04 | grad 2.11 | tok/s 12713
step    330 | loss 1.6918 | lr 3.00e-04 | grad 1.48 | tok/s 12386
step    340 | loss 1.6916 | lr 3.00e-04 | grad 3.28 | tok/s 12652
step    350 | loss 1.6958 | lr 3.00e-04 | grad 1.57 | tok/s 12258
step    360 | loss 1.6510 | lr 3.00e-04 | grad 3.53 | tok/s 12453
step    370 | loss 1.4604 | lr 3.00e-04 | grad 1.41 | tok/s 12681
step    380 | loss 1.7822 | lr 3.00e-04 | grad 1.62 | tok/s 13011
step    390 | loss 1.5958 | lr 3.00e-04 | grad 1.25 | tok/s 12528
step    400 | loss 1.6397 | lr 3.00e-04 | grad 3.16 | tok/s 12850
step    410 | loss 1.4167 | lr 3.00e-04 | grad 3.19 | tok/s 12432
step    420 | loss 1.5355 | lr 3.00e-04 | grad 1.20 | tok/s 12328
step    430 | loss 1.6576 | lr 3.00e-04 | grad 2.02 | tok/s 12272
step    440 | loss 1.6508 | lr 3.00e-04 | grad 1.21 | tok/s 12814
step    450 | loss 1.5986 | lr 3.00e-04 | grad 1.64 | tok/s 12446
step    460 | loss 1.5762 | lr 3.00e-04 | grad 1.20 | tok/s 12474
step    470 | loss 1.5504 | lr 3.00e-04 | grad 1.65 | tok/s 12635
step    480 | loss 1.4734 | lr 3.00e-04 | grad 1.45 | tok/s 12171

Training complete! Final step: 480
