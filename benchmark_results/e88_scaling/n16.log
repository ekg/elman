Using device: cuda
Output directory: benchmark_results/e88_scaling/n16/levelE88_n16_100m_20260122_223747
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_n16, 545,860,096 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.1150 | lr 3.00e-04 | grad 22.50 | tok/s 8683
step     20 | loss 3.4602 | lr 3.00e-04 | grad 42.75 | tok/s 11216
step     30 | loss 4.7684 | lr 3.00e-04 | grad 23.62 | tok/s 11622
step     40 | loss 3.4972 | lr 3.00e-04 | grad 12.06 | tok/s 11373
step     50 | loss 2.7811 | lr 3.00e-04 | grad 7.94 | tok/s 11250
step     60 | loss 3.1362 | lr 3.00e-04 | grad 6.94 | tok/s 10962
step     70 | loss 2.6481 | lr 3.00e-04 | grad 4.28 | tok/s 10550
step     80 | loss 2.5493 | lr 3.00e-04 | grad 5.62 | tok/s 10929
step     90 | loss 2.6588 | lr 3.00e-04 | grad 26.12 | tok/s 10526
step    100 | loss 2.3159 | lr 3.00e-04 | grad 4.12 | tok/s 10620
step    110 | loss 2.2889 | lr 3.00e-04 | grad 3.14 | tok/s 10459
step    120 | loss 2.3301 | lr 3.00e-04 | grad 2.91 | tok/s 10321
step    130 | loss 2.3275 | lr 3.00e-04 | grad 2.61 | tok/s 10554
step    140 | loss 2.1160 | lr 3.00e-04 | grad 2.31 | tok/s 10583
step    150 | loss 1.9892 | lr 3.00e-04 | grad 3.77 | tok/s 10106
step    160 | loss 1.9280 | lr 3.00e-04 | grad 2.16 | tok/s 10193
step    170 | loss 2.1049 | lr 3.00e-04 | grad 15.69 | tok/s 10543
step    180 | loss 2.0225 | lr 3.00e-04 | grad 1.56 | tok/s 10581
step    190 | loss 1.7648 | lr 3.00e-04 | grad 2.09 | tok/s 10761
step    200 | loss 1.3997 | lr 3.00e-04 | grad 2.20 | tok/s 11003
step    210 | loss 1.9769 | lr 3.00e-04 | grad 2.41 | tok/s 10545
step    220 | loss 1.8218 | lr 3.00e-04 | grad 2.36 | tok/s 10900
step    230 | loss 1.8230 | lr 3.00e-04 | grad 3.55 | tok/s 10512
step    240 | loss 1.7772 | lr 3.00e-04 | grad 2.45 | tok/s 10626
step    250 | loss 1.7488 | lr 3.00e-04 | grad 2.06 | tok/s 10577
step    260 | loss 1.8555 | lr 3.00e-04 | grad 1.82 | tok/s 10141
step    270 | loss 1.7624 | lr 3.00e-04 | grad 1.62 | tok/s 10540
step    280 | loss 1.6408 | lr 3.00e-04 | grad 2.38 | tok/s 10509
step    290 | loss 1.5431 | lr 3.00e-04 | grad 1.78 | tok/s 11087
step    300 | loss 1.4687 | lr 3.00e-04 | grad 1.62 | tok/s 11097
step    310 | loss 1.4240 | lr 3.00e-04 | grad 1.55 | tok/s 11093
step    320 | loss 1.6261 | lr 3.00e-04 | grad 3.38 | tok/s 10623
step    330 | loss 1.7220 | lr 3.00e-04 | grad 1.49 | tok/s 10438
step    340 | loss 1.7337 | lr 3.00e-04 | grad 3.67 | tok/s 10638
step    350 | loss 1.7389 | lr 3.00e-04 | grad 1.59 | tok/s 10328
step    360 | loss 1.6985 | lr 3.00e-04 | grad 3.52 | tok/s 10469
step    370 | loss 1.5024 | lr 3.00e-04 | grad 1.37 | tok/s 10680
step    380 | loss 1.8571 | lr 3.00e-04 | grad 1.65 | tok/s 10956
step    390 | loss 1.6390 | lr 3.00e-04 | grad 1.44 | tok/s 10515
step    400 | loss 1.6853 | lr 3.00e-04 | grad 3.47 | tok/s 10748

Training complete! Final step: 404
