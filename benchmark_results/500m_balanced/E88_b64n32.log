Using device: cuda
Output directory: benchmark_results/500m_balanced/E88_b64n32/levelE88_b64n32_100m_20260122_200601
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88_b64n32, 507,808,640 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.3788 | lr 3.00e-04 | grad 14.19 | tok/s 8407
step     20 | loss 3.5519 | lr 3.00e-04 | grad 26.75 | tok/s 10829
step     30 | loss 4.4399 | lr 3.00e-04 | grad 11.19 | tok/s 11260
step     40 | loss 3.2223 | lr 3.00e-04 | grad 7.78 | tok/s 11102
step     50 | loss 2.6843 | lr 3.00e-04 | grad 5.00 | tok/s 10956
step     60 | loss 2.9209 | lr 3.00e-04 | grad 4.31 | tok/s 10638
step     70 | loss 2.6508 | lr 3.00e-04 | grad 2.92 | tok/s 10201
step     80 | loss 2.4412 | lr 3.00e-04 | grad 3.53 | tok/s 10550
step     90 | loss 2.5767 | lr 3.00e-04 | grad 10.69 | tok/s 10168
step    100 | loss 2.2338 | lr 3.00e-04 | grad 2.98 | tok/s 10228
step    110 | loss 2.2631 | lr 3.00e-04 | grad 2.59 | tok/s 10064
step    120 | loss 2.3118 | lr 3.00e-04 | grad 2.56 | tok/s 9914
step    130 | loss 2.2858 | lr 3.00e-04 | grad 2.02 | tok/s 10161
step    140 | loss 2.0906 | lr 3.00e-04 | grad 1.93 | tok/s 10181
step    150 | loss 1.9644 | lr 3.00e-04 | grad 2.75 | tok/s 9704
step    160 | loss 1.9158 | lr 3.00e-04 | grad 1.84 | tok/s 9792
step    170 | loss 2.0729 | lr 3.00e-04 | grad 9.00 | tok/s 10125
step    180 | loss 1.9844 | lr 3.00e-04 | grad 1.33 | tok/s 10159
step    190 | loss 1.7395 | lr 3.00e-04 | grad 1.92 | tok/s 10322
step    200 | loss 1.3866 | lr 3.00e-04 | grad 1.74 | tok/s 10573
step    210 | loss 1.9454 | lr 3.00e-04 | grad 1.96 | tok/s 10139
step    220 | loss 1.7955 | lr 3.00e-04 | grad 1.90 | tok/s 10455
step    230 | loss 1.7861 | lr 3.00e-04 | grad 2.80 | tok/s 10088
step    240 | loss 1.7440 | lr 3.00e-04 | grad 2.17 | tok/s 10308
step    250 | loss 1.7239 | lr 3.00e-04 | grad 1.87 | tok/s 10154
step    260 | loss 1.8270 | lr 3.00e-04 | grad 1.55 | tok/s 9755
step    270 | loss 1.7316 | lr 3.00e-04 | grad 1.37 | tok/s 10136
step    280 | loss 1.6092 | lr 3.00e-04 | grad 2.22 | tok/s 10114
step    290 | loss 1.5180 | lr 3.00e-04 | grad 1.41 | tok/s 10674
step    300 | loss 1.4447 | lr 3.00e-04 | grad 1.57 | tok/s 10696
step    310 | loss 1.3962 | lr 3.00e-04 | grad 1.42 | tok/s 10679
step    320 | loss 1.5686 | lr 3.00e-04 | grad 2.25 | tok/s 10292
step    330 | loss 1.6987 | lr 3.00e-04 | grad 1.35 | tok/s 10043
step    340 | loss 1.7052 | lr 3.00e-04 | grad 3.25 | tok/s 10268
step    350 | loss 1.7092 | lr 3.00e-04 | grad 1.45 | tok/s 9961
step    360 | loss 1.6614 | lr 3.00e-04 | grad 3.34 | tok/s 10083
step    370 | loss 1.4765 | lr 3.00e-04 | grad 1.30 | tok/s 10289
step    380 | loss 1.7992 | lr 3.00e-04 | grad 1.52 | tok/s 10543
step    390 | loss 1.6043 | lr 3.00e-04 | grad 1.20 | tok/s 10140

Training complete! Final step: 390
