Using device: cuda
Output directory: /home/erikg/elman/benchmark_results/25layer_specnorm/E44_diagonal_w/level44_100m_20260113_194402
Model: Level 44, 99,873,120 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.1258 | lr 4.90e-06 | grad 16.72 | tok/s 22940
step    100 | loss 3.9407 | lr 9.90e-06 | grad 3.58 | tok/s 30830
step    150 | loss 3.3694 | lr 1.49e-05 | grad 7.18 | tok/s 28247
step    200 | loss 3.1525 | lr 1.99e-05 | grad 1.78 | tok/s 28564
step    250 | loss 2.9417 | lr 2.49e-05 | grad 4.88 | tok/s 28171
step    300 | loss 2.6121 | lr 2.99e-05 | grad 4.28 | tok/s 28015
step    350 | loss 2.4797 | lr 3.49e-05 | grad 3.17 | tok/s 27227
step    400 | loss 2.1466 | lr 3.99e-05 | grad 1.87 | tok/s 29050
step    450 | loss 2.3664 | lr 4.49e-05 | grad 2.24 | tok/s 28504
step    500 | loss 2.1452 | lr 4.99e-05 | grad 2.07 | tok/s 28360
step    550 | loss 2.1266 | lr 5.49e-05 | grad 2.03 | tok/s 27599
step    600 | loss 1.7570 | lr 5.99e-05 | grad 1.90 | tok/s 29365
step    650 | loss 1.8185 | lr 6.49e-05 | grad 2.34 | tok/s 28712
step    700 | loss 2.0157 | lr 6.99e-05 | grad 2.04 | tok/s 27885
step    750 | loss 1.9588 | lr 7.49e-05 | grad 3.08 | tok/s 28213
step    800 | loss 1.9967 | lr 7.99e-05 | grad 3.79 | tok/s 28409
step    850 | loss 1.8963 | lr 8.49e-05 | grad 1.78 | tok/s 27748
step    900 | loss 1.9688 | lr 8.99e-05 | grad 1.55 | tok/s 28172
step    950 | loss 1.8220 | lr 9.49e-05 | grad 1.34 | tok/s 28070
step   1000 | loss 1.9897 | lr 9.99e-05 | grad 1.72 | tok/s 28120
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9897.pt
step   1050 | loss 1.9187 | lr 1.59e-06 | grad 5.14 | tok/s 25576
step   1100 | loss 1.9131 | lr 3.37e-06 | grad 1.07 | tok/s 28542
step   1150 | loss 1.8226 | lr 6.32e-06 | grad 3.07 | tok/s 28845
step   1200 | loss 1.9731 | lr 1.04e-05 | grad 2.30 | tok/s 27766
step   1250 | loss 1.7534 | lr 1.54e-05 | grad 1.04 | tok/s 28291
step   1300 | loss 1.7790 | lr 2.13e-05 | grad 1.63 | tok/s 28156
step   1350 | loss 1.8595 | lr 2.79e-05 | grad 1.33 | tok/s 27970
step   1400 | loss 1.8635 | lr 3.51e-05 | grad 2.84 | tok/s 27688
step   1450 | loss 1.8527 | lr 4.26e-05 | grad 2.22 | tok/s 27794
step   1500 | loss 1.7628 | lr 5.03e-05 | grad 1.65 | tok/s 28019
step   1550 | loss 1.8052 | lr 5.81e-05 | grad 1.28 | tok/s 27943
step   1600 | loss 1.6729 | lr 6.56e-05 | grad 1.36 | tok/s 28363
step   1650 | loss 2.0539 | lr 7.28e-05 | grad 1.42 | tok/s 28677
step   1700 | loss 1.7231 | lr 7.95e-05 | grad 1.32 | tok/s 28761
step   1750 | loss 1.7001 | lr 8.54e-05 | grad 1.76 | tok/s 27812
step   1800 | loss 1.8621 | lr 9.05e-05 | grad 1.42 | tok/s 27637
step   1850 | loss 1.7133 | lr 9.45e-05 | grad 1.48 | tok/s 27642
step   1900 | loss 1.7773 | lr 9.75e-05 | grad 1.32 | tok/s 27823
step   1950 | loss 1.3537 | lr 9.94e-05 | grad 0.83 | tok/s 29253
step   2000 | loss 1.8095 | lr 1.00e-04 | grad 1.05 | tok/s 27527
  >>> saved checkpoint: checkpoint_step_002000_loss_1.8095.pt
step   2050 | loss 1.7421 | lr 9.94e-05 | grad 1.20 | tok/s 25695
step   2100 | loss 1.8358 | lr 9.76e-05 | grad 1.62 | tok/s 27683

Training complete! Final step: 2132
