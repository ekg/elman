Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_46/levelE88_100m_20260126_050059
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 488,271,872 parameters
Using schedule-free AdamW (lr=0.00031971998954668016)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.5007 | lr 3.20e-04 | grad 25.00 | tok/s 5650
step     20 | loss 2.7240 | lr 3.20e-04 | grad 8.88 | tok/s 13035
step     30 | loss 2.6121 | lr 3.20e-04 | grad 4.84 | tok/s 13167
step     40 | loss 2.4365 | lr 3.20e-04 | grad 4.09 | tok/s 12599
step     50 | loss 3.1206 | lr 3.20e-04 | grad 15.56 | tok/s 12845
step     60 | loss 2.1341 | lr 3.20e-04 | grad 5.53 | tok/s 13241
step     70 | loss 1.9985 | lr 3.20e-04 | grad 5.81 | tok/s 12555
step     80 | loss 6.2615 | lr 3.20e-04 | grad 210.00 | tok/s 13500
step     90 | loss 6.3320 | lr 3.20e-04 | grad 16.38 | tok/s 13680
step    100 | loss 5.0998 | lr 3.20e-04 | grad 21.00 | tok/s 13695
step    110 | loss 4.6889 | lr 3.20e-04 | grad 29.12 | tok/s 13706
step    120 | loss 4.1149 | lr 3.20e-04 | grad 36.00 | tok/s 13670
step    130 | loss 3.7452 | lr 3.20e-04 | grad 43.00 | tok/s 13642
step    140 | loss 3.0456 | lr 3.20e-04 | grad 24.88 | tok/s 13029
step    150 | loss 3.4131 | lr 3.20e-04 | grad 28.62 | tok/s 13624
step    160 | loss 2.6303 | lr 3.20e-04 | grad 26.62 | tok/s 13601
step    170 | loss 2.6642 | lr 3.20e-04 | grad 22.50 | tok/s 13659
step    180 | loss 2.4969 | lr 3.20e-04 | grad 6.84 | tok/s 13650
step    190 | loss 2.7940 | lr 3.20e-04 | grad 9.19 | tok/s 13606
step    200 | loss 2.3061 | lr 3.20e-04 | grad 14.25 | tok/s 13620
step    210 | loss 2.3136 | lr 3.20e-04 | grad 10.50 | tok/s 13609
step    220 | loss 2.3427 | lr 3.20e-04 | grad 3.05 | tok/s 13420
step    230 | loss 2.1355 | lr 3.20e-04 | grad 3.25 | tok/s 13258
step    240 | loss 2.3075 | lr 3.20e-04 | grad 4.44 | tok/s 12595
step    250 | loss 2.1361 | lr 3.20e-04 | grad 2.34 | tok/s 12952
step    260 | loss 1.5954 | lr 3.20e-04 | grad 2.64 | tok/s 13398
step    270 | loss 2.1400 | lr 3.20e-04 | grad 2.42 | tok/s 13198
step    280 | loss 2.2895 | lr 3.20e-04 | grad 4.53 | tok/s 12930
step    290 | loss 1.4937 | lr 3.20e-04 | grad 4.78 | tok/s 13633
step    300 | loss 0.6044 | lr 3.20e-04 | grad 3.38 | tok/s 13642
step    310 | loss 2.4089 | lr 3.20e-04 | grad 3.27 | tok/s 13408
step    320 | loss 1.9571 | lr 3.20e-04 | grad 5.50 | tok/s 13137
step    330 | loss 1.9755 | lr 3.20e-04 | grad 2.70 | tok/s 12679
step    340 | loss 2.2943 | lr 3.20e-04 | grad 2.64 | tok/s 12863
step    350 | loss 1.9179 | lr 3.20e-04 | grad 4.91 | tok/s 13175
step    360 | loss 1.2276 | lr 3.20e-04 | grad 6.72 | tok/s 13486
step    370 | loss 1.8302 | lr 3.20e-04 | grad 2.30 | tok/s 12236
step    380 | loss 1.8019 | lr 3.20e-04 | grad 2.12 | tok/s 13005
step    390 | loss 1.5586 | lr 3.20e-04 | grad 1.76 | tok/s 13595
step    400 | loss 1.5149 | lr 3.20e-04 | grad 2.30 | tok/s 13467
step    410 | loss 1.3017 | lr 3.20e-04 | grad 1.78 | tok/s 12458
step    420 | loss 1.8398 | lr 3.20e-04 | grad 3.89 | tok/s 12575
step    430 | loss 2.1833 | lr 3.20e-04 | grad 2.55 | tok/s 13386
step    440 | loss 2.1769 | lr 3.20e-04 | grad 3.77 | tok/s 12651
step    450 | loss 1.9033 | lr 3.20e-04 | grad 2.47 | tok/s 13151
step    460 | loss 1.7402 | lr 3.20e-04 | grad 2.47 | tok/s 12858
step    470 | loss 1.8592 | lr 3.20e-04 | grad 2.03 | tok/s 13250
step    480 | loss 2.2796 | lr 3.20e-04 | grad 5.91 | tok/s 13270
step    490 | loss 1.8035 | lr 3.20e-04 | grad 2.22 | tok/s 12526
step    500 | loss 1.7040 | lr 3.20e-04 | grad 2.91 | tok/s 13350
step    510 | loss 1.7270 | lr 3.20e-04 | grad 2.06 | tok/s 13546
step    520 | loss 1.6804 | lr 3.20e-04 | grad 1.80 | tok/s 13546
step    530 | loss 1.9399 | lr 3.20e-04 | grad 2.14 | tok/s 13045
step    540 | loss 1.7533 | lr 3.20e-04 | grad 1.92 | tok/s 13030
step    550 | loss 1.5821 | lr 3.20e-04 | grad 2.67 | tok/s 12724
step    560 | loss 1.7411 | lr 3.20e-04 | grad 2.23 | tok/s 12400
step    570 | loss 1.6898 | lr 3.20e-04 | grad 3.28 | tok/s 12255

Training complete! Final step: 578
