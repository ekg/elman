Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_113/levelE88_100m_20260126_053050
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 481,586,496 parameters
Using schedule-free AdamW (lr=0.0004291756595916413)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.0972 | lr 4.29e-04 | grad 15.88 | tok/s 5933
step     20 | loss 2.9496 | lr 4.29e-04 | grad 9.06 | tok/s 15018
step     30 | loss 2.6168 | lr 4.29e-04 | grad 6.22 | tok/s 15196
step     40 | loss 2.4231 | lr 4.29e-04 | grad 3.70 | tok/s 14513
step     50 | loss 3.0636 | lr 4.29e-04 | grad 37.25 | tok/s 14775
step     60 | loss 2.0737 | lr 4.29e-04 | grad 10.69 | tok/s 15231
step     70 | loss 1.8580 | lr 4.29e-04 | grad 4.53 | tok/s 15381
step     80 | loss 6.6752 | lr 4.29e-04 | grad 63.50 | tok/s 15466
step     90 | loss 5.5289 | lr 4.29e-04 | grad 8.38 | tok/s 15741
step    100 | loss 4.1453 | lr 4.29e-04 | grad 6.28 | tok/s 15735
step    110 | loss 3.4593 | lr 4.29e-04 | grad 22.00 | tok/s 15719
step    120 | loss 3.0796 | lr 4.29e-04 | grad 10.75 | tok/s 15656
step    130 | loss 2.8936 | lr 4.29e-04 | grad 13.31 | tok/s 14376
step    140 | loss 2.6889 | lr 4.29e-04 | grad 7.81 | tok/s 15642
step    150 | loss 2.7839 | lr 4.29e-04 | grad 12.50 | tok/s 15629
step    160 | loss 2.3316 | lr 4.29e-04 | grad 7.53 | tok/s 15612
step    170 | loss 2.3085 | lr 4.29e-04 | grad 9.06 | tok/s 15621
step    180 | loss 2.1574 | lr 4.29e-04 | grad 6.97 | tok/s 15622
step    190 | loss 2.2713 | lr 4.29e-04 | grad 8.25 | tok/s 15612
step    200 | loss 2.0193 | lr 4.29e-04 | grad 3.92 | tok/s 15602
step    210 | loss 2.0056 | lr 4.29e-04 | grad 6.00 | tok/s 15670
step    220 | loss 2.0747 | lr 4.29e-04 | grad 3.73 | tok/s 15421
step    230 | loss 2.0400 | lr 4.29e-04 | grad 4.25 | tok/s 15236
step    240 | loss 2.2902 | lr 4.29e-04 | grad 4.53 | tok/s 14463
step    250 | loss 2.0764 | lr 4.29e-04 | grad 2.48 | tok/s 14853
step    260 | loss 1.4877 | lr 4.29e-04 | grad 2.80 | tok/s 15294
step    270 | loss 2.0484 | lr 4.29e-04 | grad 2.77 | tok/s 15071
step    280 | loss 2.2086 | lr 4.29e-04 | grad 4.53 | tok/s 14819
step    290 | loss 1.4403 | lr 4.29e-04 | grad 21.88 | tok/s 15578
step    300 | loss 0.5868 | lr 4.29e-04 | grad 2.97 | tok/s 15560
step    310 | loss 2.3386 | lr 4.29e-04 | grad 3.42 | tok/s 15299
step    320 | loss 1.8391 | lr 4.29e-04 | grad 5.00 | tok/s 14959
step    330 | loss 1.9200 | lr 4.29e-04 | grad 2.61 | tok/s 14457
step    340 | loss 2.2555 | lr 4.29e-04 | grad 2.81 | tok/s 14691
step    350 | loss 1.7871 | lr 4.29e-04 | grad 3.02 | tok/s 15059
step    360 | loss 1.1130 | lr 4.29e-04 | grad 6.59 | tok/s 15381
step    370 | loss 1.7675 | lr 4.29e-04 | grad 2.25 | tok/s 13955
step    380 | loss 1.7231 | lr 4.29e-04 | grad 2.56 | tok/s 14881
step    390 | loss 1.4953 | lr 4.29e-04 | grad 2.22 | tok/s 15520
step    400 | loss 1.4551 | lr 4.29e-04 | grad 2.50 | tok/s 15390
step    410 | loss 1.2284 | lr 4.29e-04 | grad 1.79 | tok/s 15049
step    420 | loss 1.7823 | lr 4.29e-04 | grad 3.84 | tok/s 14364
step    430 | loss 2.0992 | lr 4.29e-04 | grad 2.73 | tok/s 15298
step    440 | loss 2.1172 | lr 4.29e-04 | grad 3.27 | tok/s 14453
step    450 | loss 1.8673 | lr 4.29e-04 | grad 2.25 | tok/s 14975
step    460 | loss 1.6806 | lr 4.29e-04 | grad 2.78 | tok/s 14646
step    470 | loss 1.7967 | lr 4.29e-04 | grad 2.50 | tok/s 15133
step    480 | loss 2.1571 | lr 4.29e-04 | grad 5.25 | tok/s 15129
step    490 | loss 1.7486 | lr 4.29e-04 | grad 2.14 | tok/s 14273
step    500 | loss 1.6314 | lr 4.29e-04 | grad 3.14 | tok/s 15264
step    510 | loss 1.6755 | lr 4.29e-04 | grad 2.47 | tok/s 15469
step    520 | loss 1.6157 | lr 4.29e-04 | grad 1.91 | tok/s 15427
step    530 | loss 1.8492 | lr 4.29e-04 | grad 2.06 | tok/s 14829
step    540 | loss 1.7049 | lr 4.29e-04 | grad 2.20 | tok/s 14828
step    550 | loss 1.5425 | lr 4.29e-04 | grad 2.62 | tok/s 14527
step    560 | loss 1.6884 | lr 4.29e-04 | grad 2.36 | tok/s 13041
step    570 | loss 1.6258 | lr 4.29e-04 | grad 3.09 | tok/s 14413
step    580 | loss 1.5150 | lr 4.29e-04 | grad 2.08 | tok/s 14368
step    590 | loss 1.8025 | lr 4.29e-04 | grad 2.58 | tok/s 14727
step    600 | loss 1.7877 | lr 4.29e-04 | grad 1.84 | tok/s 14229
step    610 | loss 1.5815 | lr 4.29e-04 | grad 2.09 | tok/s 14959
step    620 | loss 1.5255 | lr 4.29e-04 | grad 2.05 | tok/s 14163
step    630 | loss 1.6126 | lr 4.29e-04 | grad 3.59 | tok/s 14289
step    640 | loss 1.7630 | lr 4.29e-04 | grad 2.05 | tok/s 14712
step    650 | loss 1.6443 | lr 4.29e-04 | grad 2.30 | tok/s 14769

Training complete! Final step: 659
