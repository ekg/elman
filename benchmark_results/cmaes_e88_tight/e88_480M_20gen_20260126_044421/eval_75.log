Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_75/levelE88_100m_20260126_051413
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 485,323,132 parameters
Using schedule-free AdamW (lr=0.0004782870221912218)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1202 | lr 4.78e-04 | grad 12.25 | tok/s 5766
step     20 | loss 2.7692 | lr 4.78e-04 | grad 5.72 | tok/s 13774
step     30 | loss 2.5704 | lr 4.78e-04 | grad 3.67 | tok/s 13950
step     40 | loss 2.4194 | lr 4.78e-04 | grad 3.19 | tok/s 13311
step     50 | loss 3.0139 | lr 4.78e-04 | grad 10.94 | tok/s 13491
step     60 | loss 2.0702 | lr 4.78e-04 | grad 4.97 | tok/s 13919
step     70 | loss 1.8296 | lr 4.78e-04 | grad 3.83 | tok/s 14072
step     80 | loss 6.0900 | lr 4.78e-04 | grad 68.50 | tok/s 14135
step     90 | loss 5.0280 | lr 4.78e-04 | grad 8.62 | tok/s 14364
step    100 | loss 4.0810 | lr 4.78e-04 | grad 5.25 | tok/s 14358
step    110 | loss 3.3452 | lr 4.78e-04 | grad 9.88 | tok/s 14360
step    120 | loss 3.1110 | lr 4.78e-04 | grad 6.56 | tok/s 14326
step    130 | loss 2.8238 | lr 4.78e-04 | grad 7.97 | tok/s 13505
step    140 | loss 2.6279 | lr 4.78e-04 | grad 6.81 | tok/s 14288
step    150 | loss 2.6527 | lr 4.78e-04 | grad 5.34 | tok/s 14289
step    160 | loss 2.3730 | lr 4.78e-04 | grad 8.62 | tok/s 14263
step    170 | loss 2.2987 | lr 4.78e-04 | grad 7.16 | tok/s 14223
step    180 | loss 2.1133 | lr 4.78e-04 | grad 5.19 | tok/s 14235
step    190 | loss 2.2322 | lr 4.78e-04 | grad 9.50 | tok/s 14239
step    200 | loss 1.9630 | lr 4.78e-04 | grad 3.16 | tok/s 14278
step    210 | loss 1.9857 | lr 4.78e-04 | grad 4.03 | tok/s 14230
step    220 | loss 2.0329 | lr 4.78e-04 | grad 2.75 | tok/s 14054
step    230 | loss 1.9799 | lr 4.78e-04 | grad 4.12 | tok/s 13909
step    240 | loss 2.2577 | lr 4.78e-04 | grad 3.53 | tok/s 13197
step    250 | loss 2.0517 | lr 4.78e-04 | grad 1.98 | tok/s 13558
step    260 | loss 1.4670 | lr 4.78e-04 | grad 2.27 | tok/s 13959
step    270 | loss 2.0157 | lr 4.78e-04 | grad 2.08 | tok/s 13788
step    280 | loss 2.2133 | lr 4.78e-04 | grad 6.81 | tok/s 12677
step    290 | loss 1.3577 | lr 4.78e-04 | grad 3.20 | tok/s 14232
step    300 | loss 0.5276 | lr 4.78e-04 | grad 2.84 | tok/s 14222
step    310 | loss 2.2941 | lr 4.78e-04 | grad 2.75 | tok/s 13982
step    320 | loss 1.8197 | lr 4.78e-04 | grad 3.86 | tok/s 13698
step    330 | loss 1.8911 | lr 4.78e-04 | grad 2.12 | tok/s 13238
step    340 | loss 2.2018 | lr 4.78e-04 | grad 2.08 | tok/s 13462
step    350 | loss 1.7662 | lr 4.78e-04 | grad 2.88 | tok/s 13771
step    360 | loss 1.1079 | lr 4.78e-04 | grad 4.91 | tok/s 14086
step    370 | loss 1.7299 | lr 4.78e-04 | grad 1.80 | tok/s 12761
step    380 | loss 1.6938 | lr 4.78e-04 | grad 1.92 | tok/s 13576
step    390 | loss 1.4778 | lr 4.78e-04 | grad 1.70 | tok/s 14191
step    400 | loss 1.4338 | lr 4.78e-04 | grad 1.86 | tok/s 14046
step    410 | loss 1.2146 | lr 4.78e-04 | grad 1.45 | tok/s 13740
step    420 | loss 1.7622 | lr 4.78e-04 | grad 3.09 | tok/s 12536
step    430 | loss 2.0905 | lr 4.78e-04 | grad 2.16 | tok/s 13989
step    440 | loss 2.1010 | lr 4.78e-04 | grad 2.86 | tok/s 13221
step    450 | loss 1.8433 | lr 4.78e-04 | grad 1.84 | tok/s 13670
step    460 | loss 1.6578 | lr 4.78e-04 | grad 2.09 | tok/s 13384
step    470 | loss 1.7677 | lr 4.78e-04 | grad 1.91 | tok/s 13792
step    480 | loss 2.1521 | lr 4.78e-04 | grad 4.59 | tok/s 13821
step    490 | loss 1.7310 | lr 4.78e-04 | grad 1.91 | tok/s 13051
step    500 | loss 1.6193 | lr 4.78e-04 | grad 2.44 | tok/s 13904
step    510 | loss 1.6560 | lr 4.78e-04 | grad 1.86 | tok/s 14119
step    520 | loss 1.5953 | lr 4.78e-04 | grad 1.48 | tok/s 14094
step    530 | loss 1.8165 | lr 4.78e-04 | grad 1.71 | tok/s 13552
step    540 | loss 1.6914 | lr 4.78e-04 | grad 1.64 | tok/s 13550
step    550 | loss 1.5314 | lr 4.78e-04 | grad 1.94 | tok/s 13269
step    560 | loss 1.6784 | lr 4.78e-04 | grad 1.85 | tok/s 12411
step    570 | loss 1.6053 | lr 4.78e-04 | grad 2.39 | tok/s 13274
step    580 | loss 1.5049 | lr 4.78e-04 | grad 1.62 | tok/s 13213
step    590 | loss 1.7919 | lr 4.78e-04 | grad 2.19 | tok/s 13565
step    600 | loss 1.7760 | lr 4.78e-04 | grad 1.57 | tok/s 13094

Training complete! Final step: 603
