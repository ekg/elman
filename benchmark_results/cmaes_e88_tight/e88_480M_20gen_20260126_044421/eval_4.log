Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_4/levelE88_100m_20260126_044428
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 483,306,072 parameters
Using schedule-free AdamW (lr=0.0003774361770520916)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.3040 | lr 3.77e-04 | grad 9.31 | tok/s 5017
step     20 | loss 2.5950 | lr 3.77e-04 | grad 4.16 | tok/s 9786
step     30 | loss 2.5340 | lr 3.77e-04 | grad 1.83 | tok/s 9863
step     40 | loss 2.3523 | lr 3.77e-04 | grad 2.08 | tok/s 9434
step     50 | loss 2.9419 | lr 3.77e-04 | grad 9.00 | tok/s 9571
step     60 | loss 2.0599 | lr 3.77e-04 | grad 4.62 | tok/s 9912
step     70 | loss 1.9430 | lr 3.77e-04 | grad 2.86 | tok/s 10024
step     80 | loss 5.3763 | lr 3.77e-04 | grad 84.50 | tok/s 10060
step     90 | loss 5.3135 | lr 3.77e-04 | grad 7.44 | tok/s 10245
step    100 | loss 4.3683 | lr 3.77e-04 | grad 6.72 | tok/s 10223
step    110 | loss 3.8300 | lr 3.77e-04 | grad 12.19 | tok/s 10240
step    120 | loss 3.3284 | lr 3.77e-04 | grad 10.00 | tok/s 10224
step    130 | loss 2.9337 | lr 3.77e-04 | grad 10.00 | tok/s 9741
step    140 | loss 2.5942 | lr 3.77e-04 | grad 5.47 | tok/s 10232
step    150 | loss 2.6250 | lr 3.77e-04 | grad 6.59 | tok/s 10206
step    160 | loss 2.1809 | lr 3.77e-04 | grad 4.22 | tok/s 10220
step    170 | loss 2.2767 | lr 3.77e-04 | grad 6.44 | tok/s 10211
step    180 | loss 2.0611 | lr 3.77e-04 | grad 2.52 | tok/s 10231
step    190 | loss 2.2240 | lr 3.77e-04 | grad 2.55 | tok/s 10219
step    200 | loss 1.9131 | lr 3.77e-04 | grad 2.56 | tok/s 10207
step    210 | loss 1.9387 | lr 3.77e-04 | grad 4.00 | tok/s 10217
step    220 | loss 2.0772 | lr 3.77e-04 | grad 1.89 | tok/s 10086
step    230 | loss 2.0217 | lr 3.77e-04 | grad 2.11 | tok/s 9965
step    240 | loss 2.2392 | lr 3.77e-04 | grad 2.52 | tok/s 9463
step    250 | loss 2.0646 | lr 3.77e-04 | grad 1.42 | tok/s 9744
step    260 | loss 1.5530 | lr 3.77e-04 | grad 1.67 | tok/s 10042
step    270 | loss 2.0401 | lr 3.77e-04 | grad 1.49 | tok/s 9540
step    280 | loss 2.2357 | lr 3.77e-04 | grad 3.19 | tok/s 9721
step    290 | loss 1.3429 | lr 3.77e-04 | grad 1.84 | tok/s 10231
step    300 | loss 0.5468 | lr 3.77e-04 | grad 1.52 | tok/s 10223
step    310 | loss 2.3616 | lr 3.77e-04 | grad 2.22 | tok/s 10062
step    320 | loss 1.8981 | lr 3.77e-04 | grad 3.14 | tok/s 9829
step    330 | loss 1.9005 | lr 3.77e-04 | grad 1.62 | tok/s 9512
step    340 | loss 2.2166 | lr 3.77e-04 | grad 1.48 | tok/s 9654
step    350 | loss 1.8570 | lr 3.77e-04 | grad 2.28 | tok/s 9905
step    360 | loss 1.1873 | lr 3.77e-04 | grad 3.78 | tok/s 10115
step    370 | loss 1.7746 | lr 3.77e-04 | grad 1.48 | tok/s 9171
step    380 | loss 1.7306 | lr 3.77e-04 | grad 1.45 | tok/s 9774
step    390 | loss 1.5052 | lr 3.77e-04 | grad 1.15 | tok/s 10217
step    400 | loss 1.4684 | lr 3.77e-04 | grad 1.42 | tok/s 10115
step    410 | loss 1.2633 | lr 3.77e-04 | grad 1.17 | tok/s 9513
step    420 | loss 1.7778 | lr 3.77e-04 | grad 2.56 | tok/s 9455
step    430 | loss 2.0819 | lr 3.77e-04 | grad 1.75 | tok/s 10053

Training complete! Final step: 435
