Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_61/levelE88_100m_20260126_050736
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 484,526,864 parameters
Using schedule-free AdamW (lr=0.0005097421689843857)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.5655 | lr 5.10e-04 | grad 16.50 | tok/s 6056
step     20 | loss 3.5292 | lr 5.10e-04 | grad 11.38 | tok/s 17035
step     30 | loss 3.1384 | lr 5.10e-04 | grad 9.06 | tok/s 17281
step     40 | loss 2.5717 | lr 5.10e-04 | grad 6.66 | tok/s 16494
step     50 | loss 3.0493 | lr 5.10e-04 | grad 9.06 | tok/s 16749
step     60 | loss 2.1349 | lr 5.10e-04 | grad 2.84 | tok/s 17284
step     70 | loss 1.8351 | lr 5.10e-04 | grad 4.75 | tok/s 17488
step     80 | loss 5.6101 | lr 5.10e-04 | grad 34.00 | tok/s 17576
step     90 | loss 4.2692 | lr 5.10e-04 | grad 6.25 | tok/s 17888
step    100 | loss 3.6371 | lr 5.10e-04 | grad 7.41 | tok/s 17830
step    110 | loss 3.0351 | lr 5.10e-04 | grad 11.75 | tok/s 17830
step    120 | loss 2.8141 | lr 5.10e-04 | grad 5.69 | tok/s 17761
step    130 | loss 2.7022 | lr 5.10e-04 | grad 7.59 | tok/s 17748
step    140 | loss 2.4111 | lr 5.10e-04 | grad 5.16 | tok/s 17751
step    150 | loss 2.4888 | lr 5.10e-04 | grad 11.31 | tok/s 17699
step    160 | loss 2.1970 | lr 5.10e-04 | grad 7.50 | tok/s 17699
step    170 | loss 2.1413 | lr 5.10e-04 | grad 12.06 | tok/s 17686
step    180 | loss 2.0672 | lr 5.10e-04 | grad 7.50 | tok/s 17655
step    190 | loss 2.1474 | lr 5.10e-04 | grad 8.44 | tok/s 17643
step    200 | loss 1.9139 | lr 5.10e-04 | grad 4.06 | tok/s 17656
step    210 | loss 1.9529 | lr 5.10e-04 | grad 6.53 | tok/s 17610
step    220 | loss 2.0999 | lr 5.10e-04 | grad 4.88 | tok/s 16178
step    230 | loss 2.0747 | lr 5.10e-04 | grad 5.53 | tok/s 17206
step    240 | loss 2.3015 | lr 5.10e-04 | grad 4.19 | tok/s 16342
step    250 | loss 2.0542 | lr 5.10e-04 | grad 2.88 | tok/s 16808
step    260 | loss 1.4656 | lr 5.10e-04 | grad 3.06 | tok/s 17307
step    270 | loss 2.0217 | lr 5.10e-04 | grad 3.59 | tok/s 17099
step    280 | loss 2.1975 | lr 5.10e-04 | grad 6.22 | tok/s 16735
step    290 | loss 1.3866 | lr 5.10e-04 | grad 1.60 | tok/s 17645
step    300 | loss 0.6260 | lr 5.10e-04 | grad 3.16 | tok/s 17629
step    310 | loss 2.2949 | lr 5.10e-04 | grad 4.50 | tok/s 17302
step    320 | loss 1.8275 | lr 5.10e-04 | grad 5.25 | tok/s 16954
step    330 | loss 1.8900 | lr 5.10e-04 | grad 2.44 | tok/s 16395
step    340 | loss 2.1874 | lr 5.10e-04 | grad 3.70 | tok/s 16640
step    350 | loss 1.7683 | lr 5.10e-04 | grad 3.03 | tok/s 17074
step    360 | loss 1.1605 | lr 5.10e-04 | grad 8.69 | tok/s 17444
step    370 | loss 1.7519 | lr 5.10e-04 | grad 2.67 | tok/s 15772
step    380 | loss 1.7058 | lr 5.10e-04 | grad 2.89 | tok/s 16799
step    390 | loss 1.4704 | lr 5.10e-04 | grad 2.69 | tok/s 17551
step    400 | loss 1.4321 | lr 5.10e-04 | grad 2.89 | tok/s 17423
step    410 | loss 1.2195 | lr 5.10e-04 | grad 2.00 | tok/s 17004
step    420 | loss 1.7511 | lr 5.10e-04 | grad 3.88 | tok/s 16261
step    430 | loss 2.0490 | lr 5.10e-04 | grad 3.12 | tok/s 17292
step    440 | loss 2.0903 | lr 5.10e-04 | grad 3.39 | tok/s 16344
step    450 | loss 1.9064 | lr 5.10e-04 | grad 2.39 | tok/s 16921
step    460 | loss 1.6593 | lr 5.10e-04 | grad 3.17 | tok/s 16549
step    470 | loss 1.7591 | lr 5.10e-04 | grad 3.06 | tok/s 17051
step    480 | loss 2.0949 | lr 5.10e-04 | grad 4.97 | tok/s 17114
step    490 | loss 1.7325 | lr 5.10e-04 | grad 2.52 | tok/s 16130
step    500 | loss 1.6205 | lr 5.10e-04 | grad 3.70 | tok/s 17251
step    510 | loss 1.6499 | lr 5.10e-04 | grad 2.67 | tok/s 17494
step    520 | loss 1.5916 | lr 5.10e-04 | grad 2.39 | tok/s 17465
step    530 | loss 1.8215 | lr 5.10e-04 | grad 2.50 | tok/s 16812
step    540 | loss 1.6829 | lr 5.10e-04 | grad 3.14 | tok/s 16763
step    550 | loss 1.5299 | lr 5.10e-04 | grad 2.11 | tok/s 16470
step    560 | loss 1.6615 | lr 5.10e-04 | grad 2.78 | tok/s 16058
step    570 | loss 1.6045 | lr 5.10e-04 | grad 2.81 | tok/s 15614
step    580 | loss 1.5043 | lr 5.10e-04 | grad 2.70 | tok/s 16420
step    590 | loss 1.7564 | lr 5.10e-04 | grad 2.53 | tok/s 16846
step    600 | loss 1.7781 | lr 5.10e-04 | grad 2.52 | tok/s 16287
step    610 | loss 1.5704 | lr 5.10e-04 | grad 2.83 | tok/s 17115
step    620 | loss 1.5091 | lr 5.10e-04 | grad 2.36 | tok/s 16203
step    630 | loss 1.5993 | lr 5.10e-04 | grad 3.92 | tok/s 16348
step    640 | loss 1.7445 | lr 5.10e-04 | grad 2.36 | tok/s 16774
step    650 | loss 1.6320 | lr 5.10e-04 | grad 3.03 | tok/s 16888
step    660 | loss 1.6369 | lr 5.10e-04 | grad 2.06 | tok/s 16965
step    670 | loss 1.8549 | lr 5.10e-04 | grad 2.73 | tok/s 17022
step    680 | loss 1.6725 | lr 5.10e-04 | grad 2.33 | tok/s 16651
step    690 | loss 1.7730 | lr 5.10e-04 | grad 3.02 | tok/s 17255
step    700 | loss 1.3007 | lr 5.10e-04 | grad 2.20 | tok/s 17601
step    710 | loss 1.5437 | lr 5.10e-04 | grad 2.50 | tok/s 16415
step    720 | loss 1.4271 | lr 5.10e-04 | grad 3.89 | tok/s 16206
step    730 | loss 1.2421 | lr 5.10e-04 | grad 2.58 | tok/s 17553
step    740 | loss 1.4295 | lr 5.10e-04 | grad 2.09 | tok/s 17348

Training complete! Final step: 746
