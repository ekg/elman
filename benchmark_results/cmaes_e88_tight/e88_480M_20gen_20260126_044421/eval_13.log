Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_13/levelE88_100m_20260126_044746
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,603,590 parameters
Using schedule-free AdamW (lr=0.0004953527273182898)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.1947 | lr 4.95e-04 | grad 6.53 | tok/s 6490
step     20 | loss 2.7165 | lr 4.95e-04 | grad 1.91 | tok/s 8925
step     30 | loss 3.0711 | lr 4.95e-04 | grad 3.27 | tok/s 9405
step     40 | loss 4.2611 | lr 4.95e-04 | grad 27.75 | tok/s 9550
step     50 | loss 4.3199 | lr 4.95e-04 | grad 7.75 | tok/s 9637
step     60 | loss 3.3075 | lr 4.95e-04 | grad 3.89 | tok/s 9602
step     70 | loss 2.6427 | lr 4.95e-04 | grad 2.08 | tok/s 9569
step     80 | loss 2.2649 | lr 4.95e-04 | grad 2.09 | tok/s 9560
step     90 | loss 2.1715 | lr 4.95e-04 | grad 1.56 | tok/s 9553
step    100 | loss 1.9865 | lr 4.95e-04 | grad 1.26 | tok/s 9558
step    110 | loss 2.1091 | lr 4.95e-04 | grad 1.80 | tok/s 9479
step    120 | loss 2.6330 | lr 4.95e-04 | grad 0.98 | tok/s 9037
step    130 | loss 2.0663 | lr 4.95e-04 | grad 2.95 | tok/s 9236
step    140 | loss 2.3007 | lr 4.95e-04 | grad 4.22 | tok/s 9263
step    150 | loss 1.3548 | lr 4.95e-04 | grad 3.27 | tok/s 9352
step    160 | loss 2.2719 | lr 4.95e-04 | grad 1.12 | tok/s 9184
step    170 | loss 2.2128 | lr 4.95e-04 | grad 0.89 | tok/s 9065
step    180 | loss 1.7708 | lr 4.95e-04 | grad 1.60 | tok/s 9269
step    190 | loss 1.8422 | lr 4.95e-04 | grad 1.20 | tok/s 9109
step    200 | loss 1.5831 | lr 4.95e-04 | grad 0.95 | tok/s 9541

Training complete! Final step: 206
