Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_111/levelE88_100m_20260126_052730
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,594,432 parameters
Using schedule-free AdamW (lr=0.0004254166831924528)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1412 | lr 4.25e-04 | grad 9.25 | tok/s 5719
step     20 | loss 2.6763 | lr 4.25e-04 | grad 3.97 | tok/s 12738
step     30 | loss 2.4945 | lr 4.25e-04 | grad 3.03 | tok/s 12850
step     40 | loss 2.3658 | lr 4.25e-04 | grad 2.78 | tok/s 12259
step     50 | loss 2.9517 | lr 4.25e-04 | grad 7.72 | tok/s 12425
step     60 | loss 2.0405 | lr 4.25e-04 | grad 2.81 | tok/s 12803
step     70 | loss 1.8620 | lr 4.25e-04 | grad 3.17 | tok/s 12928
step     80 | loss 5.7016 | lr 4.25e-04 | grad 42.00 | tok/s 13000
step     90 | loss 4.6877 | lr 4.25e-04 | grad 6.22 | tok/s 13190
step    100 | loss 3.8248 | lr 4.25e-04 | grad 4.22 | tok/s 13167
step    110 | loss 3.1873 | lr 4.25e-04 | grad 7.41 | tok/s 13150
step    120 | loss 2.9101 | lr 4.25e-04 | grad 6.09 | tok/s 13106
step    130 | loss 2.6607 | lr 4.25e-04 | grad 6.56 | tok/s 13095
step    140 | loss 2.4562 | lr 4.25e-04 | grad 3.83 | tok/s 13104
step    150 | loss 2.4261 | lr 4.25e-04 | grad 7.03 | tok/s 13113
step    160 | loss 2.1134 | lr 4.25e-04 | grad 5.59 | tok/s 13091
step    170 | loss 2.1057 | lr 4.25e-04 | grad 4.91 | tok/s 13064
step    180 | loss 2.0094 | lr 4.25e-04 | grad 3.53 | tok/s 13049
step    190 | loss 2.1114 | lr 4.25e-04 | grad 5.88 | tok/s 13044
step    200 | loss 1.8979 | lr 4.25e-04 | grad 2.20 | tok/s 13043
step    210 | loss 1.9127 | lr 4.25e-04 | grad 3.56 | tok/s 13047
step    220 | loss 2.0088 | lr 4.25e-04 | grad 2.25 | tok/s 12886
step    230 | loss 1.9879 | lr 4.25e-04 | grad 3.36 | tok/s 12767
step    240 | loss 2.2374 | lr 4.25e-04 | grad 3.06 | tok/s 12098
step    250 | loss 2.0366 | lr 4.25e-04 | grad 1.63 | tok/s 12433
step    260 | loss 1.4793 | lr 4.25e-04 | grad 1.91 | tok/s 12832
step    270 | loss 2.0024 | lr 4.25e-04 | grad 1.84 | tok/s 12661
step    280 | loss 2.1935 | lr 4.25e-04 | grad 4.12 | tok/s 12420
step    290 | loss 1.3347 | lr 4.25e-04 | grad 2.77 | tok/s 13070
step    300 | loss 0.5392 | lr 4.25e-04 | grad 1.39 | tok/s 13044
step    310 | loss 2.3182 | lr 4.25e-04 | grad 2.28 | tok/s 12805
step    320 | loss 1.8431 | lr 4.25e-04 | grad 3.41 | tok/s 12546
step    330 | loss 1.8844 | lr 4.25e-04 | grad 1.88 | tok/s 12093
step    340 | loss 2.2097 | lr 4.25e-04 | grad 1.76 | tok/s 12319
step    350 | loss 1.7879 | lr 4.25e-04 | grad 2.28 | tok/s 12609
step    360 | loss 1.1195 | lr 4.25e-04 | grad 4.28 | tok/s 12884
step    370 | loss 1.7320 | lr 4.25e-04 | grad 1.58 | tok/s 11667
step    380 | loss 1.6935 | lr 4.25e-04 | grad 1.66 | tok/s 11696
step    390 | loss 1.4816 | lr 4.25e-04 | grad 1.36 | tok/s 12984
step    400 | loss 1.4420 | lr 4.25e-04 | grad 1.64 | tok/s 12845
step    410 | loss 1.2259 | lr 4.25e-04 | grad 1.29 | tok/s 12596
step    420 | loss 1.7520 | lr 4.25e-04 | grad 2.69 | tok/s 12042
step    430 | loss 2.0763 | lr 4.25e-04 | grad 1.85 | tok/s 12822
step    440 | loss 2.0887 | lr 4.25e-04 | grad 2.52 | tok/s 12114
step    450 | loss 1.8339 | lr 4.25e-04 | grad 1.65 | tok/s 12514
step    460 | loss 1.6580 | lr 4.25e-04 | grad 2.12 | tok/s 12235
step    470 | loss 1.7605 | lr 4.25e-04 | grad 1.55 | tok/s 12646
step    480 | loss 2.1185 | lr 4.25e-04 | grad 3.92 | tok/s 12674
step    490 | loss 1.7227 | lr 4.25e-04 | grad 1.61 | tok/s 11962
step    500 | loss 1.6159 | lr 4.25e-04 | grad 2.16 | tok/s 12779
step    510 | loss 1.6482 | lr 4.25e-04 | grad 1.48 | tok/s 12952
step    520 | loss 1.5980 | lr 4.25e-04 | grad 1.33 | tok/s 12925
step    530 | loss 1.8257 | lr 4.25e-04 | grad 1.55 | tok/s 12395
step    540 | loss 1.6783 | lr 4.25e-04 | grad 1.41 | tok/s 12412
step    550 | loss 1.5281 | lr 4.25e-04 | grad 1.84 | tok/s 12154

Training complete! Final step: 556
