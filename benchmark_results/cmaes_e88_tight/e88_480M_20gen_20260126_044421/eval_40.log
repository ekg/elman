Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_40/levelE88_100m_20260126_045740
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,107,964 parameters
Using schedule-free AdamW (lr=0.0005283742234665033)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.3046 | lr 5.28e-04 | grad 15.62 | tok/s 6092
step     20 | loss 3.3813 | lr 5.28e-04 | grad 10.94 | tok/s 17005
step     30 | loss 2.7778 | lr 5.28e-04 | grad 6.81 | tok/s 17183
step     40 | loss 2.4770 | lr 5.28e-04 | grad 4.00 | tok/s 16436
step     50 | loss 3.0717 | lr 5.28e-04 | grad 11.31 | tok/s 16698
step     60 | loss 2.1368 | lr 5.28e-04 | grad 6.84 | tok/s 17177
step     70 | loss 1.8587 | lr 5.28e-04 | grad 4.16 | tok/s 17373
step     80 | loss 6.3415 | lr 5.28e-04 | grad 39.25 | tok/s 17457
step     90 | loss 4.8174 | lr 5.28e-04 | grad 6.00 | tok/s 17742
step    100 | loss 4.1017 | lr 5.28e-04 | grad 4.34 | tok/s 17689
step    110 | loss 3.5503 | lr 5.28e-04 | grad 16.12 | tok/s 17650
step    120 | loss 2.9753 | lr 5.28e-04 | grad 7.41 | tok/s 17612
step    130 | loss 2.8237 | lr 5.28e-04 | grad 7.03 | tok/s 17574
step    140 | loss 2.6242 | lr 5.28e-04 | grad 4.38 | tok/s 17551
step    150 | loss 2.5432 | lr 5.28e-04 | grad 7.25 | tok/s 17500
step    160 | loss 2.1919 | lr 5.28e-04 | grad 6.66 | tok/s 16313
step    170 | loss 2.1915 | lr 5.28e-04 | grad 6.88 | tok/s 17437
step    180 | loss 2.0798 | lr 5.28e-04 | grad 7.22 | tok/s 17431
step    190 | loss 2.1363 | lr 5.28e-04 | grad 3.91 | tok/s 17348
step    200 | loss 1.8974 | lr 5.28e-04 | grad 3.52 | tok/s 17320
step    210 | loss 1.9890 | lr 5.28e-04 | grad 5.84 | tok/s 17279
step    220 | loss 2.0847 | lr 5.28e-04 | grad 3.94 | tok/s 17077
step    230 | loss 2.1012 | lr 5.28e-04 | grad 2.98 | tok/s 16797
step    240 | loss 2.2885 | lr 5.28e-04 | grad 3.84 | tok/s 15962
step    250 | loss 2.0516 | lr 5.28e-04 | grad 2.52 | tok/s 16372
step    260 | loss 1.4760 | lr 5.28e-04 | grad 2.70 | tok/s 16857
step    270 | loss 2.0355 | lr 5.28e-04 | grad 2.92 | tok/s 16632
step    280 | loss 2.2121 | lr 5.28e-04 | grad 5.12 | tok/s 16263
step    290 | loss 1.3946 | lr 5.28e-04 | grad 2.39 | tok/s 17127
step    300 | loss 0.5738 | lr 5.28e-04 | grad 3.92 | tok/s 17070
step    310 | loss 2.2831 | lr 5.28e-04 | grad 3.52 | tok/s 15683
step    320 | loss 1.8344 | lr 5.28e-04 | grad 4.78 | tok/s 16424
step    330 | loss 1.8919 | lr 5.28e-04 | grad 2.36 | tok/s 15834
step    340 | loss 2.1847 | lr 5.28e-04 | grad 3.00 | tok/s 16049
step    350 | loss 1.7739 | lr 5.28e-04 | grad 2.73 | tok/s 16438
step    360 | loss 1.1358 | lr 5.28e-04 | grad 5.94 | tok/s 16792
step    370 | loss 1.7411 | lr 5.28e-04 | grad 2.42 | tok/s 15216
step    380 | loss 1.6968 | lr 5.28e-04 | grad 2.34 | tok/s 16181
step    390 | loss 1.4755 | lr 5.28e-04 | grad 2.17 | tok/s 16877
step    400 | loss 1.4375 | lr 5.28e-04 | grad 2.30 | tok/s 16704
step    410 | loss 1.2244 | lr 5.28e-04 | grad 1.82 | tok/s 16325
step    420 | loss 1.7647 | lr 5.28e-04 | grad 3.47 | tok/s 15586
step    430 | loss 2.0615 | lr 5.28e-04 | grad 2.62 | tok/s 16576
step    440 | loss 2.0854 | lr 5.28e-04 | grad 2.89 | tok/s 15632
step    450 | loss 1.9343 | lr 5.28e-04 | grad 2.09 | tok/s 16169
step    460 | loss 1.6664 | lr 5.28e-04 | grad 2.69 | tok/s 15827
step    470 | loss 1.7674 | lr 5.28e-04 | grad 2.50 | tok/s 16307
step    480 | loss 2.1093 | lr 5.28e-04 | grad 5.16 | tok/s 16310
step    490 | loss 1.7275 | lr 5.28e-04 | grad 2.08 | tok/s 15387
step    500 | loss 1.6183 | lr 5.28e-04 | grad 3.11 | tok/s 16417
step    510 | loss 1.6474 | lr 5.28e-04 | grad 2.28 | tok/s 16627
step    520 | loss 1.5982 | lr 5.28e-04 | grad 1.82 | tok/s 16590
step    530 | loss 1.8060 | lr 5.28e-04 | grad 2.00 | tok/s 15930
step    540 | loss 1.6820 | lr 5.28e-04 | grad 2.38 | tok/s 15921
step    550 | loss 1.5342 | lr 5.28e-04 | grad 2.16 | tok/s 15581
step    560 | loss 1.6789 | lr 5.28e-04 | grad 2.28 | tok/s 15187
step    570 | loss 1.6071 | lr 5.28e-04 | grad 2.56 | tok/s 15585
step    580 | loss 1.5075 | lr 5.28e-04 | grad 2.20 | tok/s 15545
step    590 | loss 1.7665 | lr 5.28e-04 | grad 2.23 | tok/s 15897
step    600 | loss 1.7801 | lr 5.28e-04 | grad 1.77 | tok/s 15362
step    610 | loss 1.5732 | lr 5.28e-04 | grad 2.27 | tok/s 16151
step    620 | loss 1.5161 | lr 5.28e-04 | grad 1.98 | tok/s 15297
step    630 | loss 1.6017 | lr 5.28e-04 | grad 3.28 | tok/s 15399
step    640 | loss 1.7383 | lr 5.28e-04 | grad 1.92 | tok/s 15821
step    650 | loss 1.6229 | lr 5.28e-04 | grad 2.36 | tok/s 15898
step    660 | loss 1.6441 | lr 5.28e-04 | grad 1.76 | tok/s 15962
step    670 | loss 1.8344 | lr 5.28e-04 | grad 2.58 | tok/s 16045
step    680 | loss 1.6766 | lr 5.28e-04 | grad 1.92 | tok/s 15730
step    690 | loss 1.7529 | lr 5.28e-04 | grad 2.47 | tok/s 16269
step    700 | loss 1.3033 | lr 5.28e-04 | grad 2.08 | tok/s 16569
step    710 | loss 1.5459 | lr 5.28e-04 | grad 2.08 | tok/s 15492
step    720 | loss 1.4309 | lr 5.28e-04 | grad 2.91 | tok/s 14326

Training complete! Final step: 720
