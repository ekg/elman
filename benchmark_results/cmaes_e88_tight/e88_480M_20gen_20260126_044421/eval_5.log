Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_5/levelE88_100m_20260126_044428
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 462,794,298 parameters
Using schedule-free AdamW (lr=0.00024587359497894607)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.8036 | lr 2.46e-04 | grad 8.44 | tok/s 5729
step     20 | loss 2.9142 | lr 2.46e-04 | grad 22.75 | tok/s 7533
step     30 | loss 3.8282 | lr 2.46e-04 | grad 49.75 | tok/s 8038
step     40 | loss 5.0218 | lr 2.46e-04 | grad 40.50 | tok/s 8150
step     50 | loss 4.4566 | lr 2.46e-04 | grad 24.12 | tok/s 8133
step     60 | loss 4.1500 | lr 2.46e-04 | grad 18.50 | tok/s 8107
step     70 | loss 3.4209 | lr 2.46e-04 | grad 6.59 | tok/s 8014
step     80 | loss 2.4206 | lr 2.46e-04 | grad 1.85 | tok/s 7639
step     90 | loss 2.6564 | lr 2.46e-04 | grad 2.80 | tok/s 8051
step    100 | loss 2.6295 | lr 2.46e-04 | grad 1.92 | tok/s 7688
step    110 | loss 2.5681 | lr 2.46e-04 | grad 1.42 | tok/s 7757
step    120 | loss 2.2015 | lr 2.46e-04 | grad 7.09 | tok/s 7750
step    130 | loss 2.5286 | lr 2.46e-04 | grad 3.08 | tok/s 7727
step    140 | loss 2.3848 | lr 2.46e-04 | grad 2.17 | tok/s 7648
step    150 | loss 2.2302 | lr 2.46e-04 | grad 2.16 | tok/s 7933
step    160 | loss 2.1213 | lr 2.46e-04 | grad 1.62 | tok/s 7636
step    170 | loss 2.2665 | lr 2.46e-04 | grad 1.40 | tok/s 7676

Training complete! Final step: 174
