Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_44/levelE88_100m_20260126_050059
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,257,664 parameters
Using schedule-free AdamW (lr=0.00040927905429887943)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.0354 | lr 4.09e-04 | grad 18.38 | tok/s 8916
step     20 | loss 3.1238 | lr 4.09e-04 | grad 6.66 | tok/s 16246
step     30 | loss 3.1455 | lr 4.09e-04 | grad 8.56 | tok/s 17130
step     40 | loss 5.0477 | lr 4.09e-04 | grad 64.50 | tok/s 17440
step     50 | loss 4.7524 | lr 4.09e-04 | grad 19.50 | tok/s 17603
step     60 | loss 3.5336 | lr 4.09e-04 | grad 10.94 | tok/s 17527
step     70 | loss 2.8636 | lr 4.09e-04 | grad 6.19 | tok/s 17475
step     80 | loss 2.6535 | lr 4.09e-04 | grad 8.69 | tok/s 17438
step     90 | loss 2.4700 | lr 4.09e-04 | grad 4.50 | tok/s 17428
step    100 | loss 2.3142 | lr 4.09e-04 | grad 3.12 | tok/s 17403
step    110 | loss 2.2670 | lr 4.09e-04 | grad 4.03 | tok/s 17272
step    120 | loss 2.7501 | lr 4.09e-04 | grad 2.58 | tok/s 16428
step    130 | loss 2.1047 | lr 4.09e-04 | grad 6.25 | tok/s 16193
step    140 | loss 2.3464 | lr 4.09e-04 | grad 7.41 | tok/s 16834
step    150 | loss 1.3784 | lr 4.09e-04 | grad 5.50 | tok/s 17257
step    160 | loss 2.2812 | lr 4.09e-04 | grad 2.45 | tok/s 16676
step    170 | loss 2.3021 | lr 4.09e-04 | grad 2.28 | tok/s 16447
step    180 | loss 1.7330 | lr 4.09e-04 | grad 3.27 | tok/s 16822
step    190 | loss 1.8893 | lr 4.09e-04 | grad 3.22 | tok/s 16528
step    200 | loss 1.6100 | lr 4.09e-04 | grad 2.11 | tok/s 17290
step    210 | loss 1.8737 | lr 4.09e-04 | grad 6.62 | tok/s 16388
step    220 | loss 2.1740 | lr 4.09e-04 | grad 3.41 | tok/s 16565
step    230 | loss 1.9486 | lr 4.09e-04 | grad 2.81 | tok/s 16526
step    240 | loss 2.2491 | lr 4.09e-04 | grad 5.91 | tok/s 16746
step    250 | loss 1.7462 | lr 4.09e-04 | grad 1.88 | tok/s 16651
step    260 | loss 1.8704 | lr 4.09e-04 | grad 3.20 | tok/s 17122
step    270 | loss 1.7987 | lr 4.09e-04 | grad 2.27 | tok/s 16456
step    280 | loss 1.7606 | lr 4.09e-04 | grad 1.88 | tok/s 15842
step    290 | loss 1.6586 | lr 4.09e-04 | grad 2.31 | tok/s 16309
step    300 | loss 1.9602 | lr 4.09e-04 | grad 2.11 | tok/s 16393
step    310 | loss 1.6531 | lr 4.09e-04 | grad 1.86 | tok/s 16305
step    320 | loss 1.8634 | lr 4.09e-04 | grad 3.47 | tok/s 16509
step    330 | loss 1.7090 | lr 4.09e-04 | grad 2.06 | tok/s 16679
step    340 | loss 2.0353 | lr 4.09e-04 | grad 2.25 | tok/s 16607
step    350 | loss 1.6750 | lr 4.09e-04 | grad 2.05 | tok/s 17102
step    360 | loss 1.5648 | lr 4.09e-04 | grad 1.84 | tok/s 16383
step    370 | loss 1.4492 | lr 4.09e-04 | grad 1.78 | tok/s 17226

Training complete! Final step: 371
