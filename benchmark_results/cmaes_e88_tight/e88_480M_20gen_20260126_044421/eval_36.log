Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_36/levelE88_100m_20260126_045740
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 468,763,024 parameters
Using schedule-free AdamW (lr=0.0005331870585338503)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.0969 | lr 5.33e-04 | grad 14.06 | tok/s 8597
step     20 | loss 3.1791 | lr 5.33e-04 | grad 5.78 | tok/s 15255
step     30 | loss 3.1671 | lr 5.33e-04 | grad 6.16 | tok/s 16081
step     40 | loss 4.9278 | lr 5.33e-04 | grad 39.00 | tok/s 16348
step     50 | loss 4.4171 | lr 5.33e-04 | grad 13.56 | tok/s 16518
step     60 | loss 3.2918 | lr 5.33e-04 | grad 6.00 | tok/s 16408
step     70 | loss 2.8340 | lr 5.33e-04 | grad 6.12 | tok/s 16433
step     80 | loss 2.5425 | lr 5.33e-04 | grad 3.92 | tok/s 16400
step     90 | loss 2.3391 | lr 5.33e-04 | grad 3.53 | tok/s 16374
step    100 | loss 2.1490 | lr 5.33e-04 | grad 2.75 | tok/s 16391
step    110 | loss 2.1798 | lr 5.33e-04 | grad 3.67 | tok/s 16208
step    120 | loss 2.6802 | lr 5.33e-04 | grad 2.39 | tok/s 15441
step    130 | loss 2.0458 | lr 5.33e-04 | grad 4.94 | tok/s 15359
step    140 | loss 2.3111 | lr 5.33e-04 | grad 5.62 | tok/s 15795
step    150 | loss 1.3175 | lr 5.33e-04 | grad 4.72 | tok/s 16196
step    160 | loss 2.2252 | lr 5.33e-04 | grad 2.20 | tok/s 15673
step    170 | loss 2.2756 | lr 5.33e-04 | grad 1.95 | tok/s 15425
step    180 | loss 1.6939 | lr 5.33e-04 | grad 2.86 | tok/s 15781
step    190 | loss 1.8585 | lr 5.33e-04 | grad 2.75 | tok/s 15513
step    200 | loss 1.5709 | lr 5.33e-04 | grad 1.73 | tok/s 16225
step    210 | loss 1.8291 | lr 5.33e-04 | grad 5.75 | tok/s 15389
step    220 | loss 2.1182 | lr 5.33e-04 | grad 3.36 | tok/s 15555
step    230 | loss 1.9566 | lr 5.33e-04 | grad 2.23 | tok/s 15541
step    240 | loss 2.1943 | lr 5.33e-04 | grad 4.50 | tok/s 15711
step    250 | loss 1.7177 | lr 5.33e-04 | grad 1.48 | tok/s 15640
step    260 | loss 1.8294 | lr 5.33e-04 | grad 2.72 | tok/s 16074
step    270 | loss 1.7692 | lr 5.33e-04 | grad 1.95 | tok/s 15749
step    280 | loss 1.7340 | lr 5.33e-04 | grad 1.51 | tok/s 14202
step    290 | loss 1.6261 | lr 5.33e-04 | grad 1.90 | tok/s 15255
step    300 | loss 1.9245 | lr 5.33e-04 | grad 1.88 | tok/s 15414
step    310 | loss 1.6281 | lr 5.33e-04 | grad 1.66 | tok/s 15362
step    320 | loss 1.8421 | lr 5.33e-04 | grad 4.41 | tok/s 15541
step    330 | loss 1.6791 | lr 5.33e-04 | grad 1.82 | tok/s 15702
step    340 | loss 1.9949 | lr 5.33e-04 | grad 1.65 | tok/s 15601

Training complete! Final step: 348
