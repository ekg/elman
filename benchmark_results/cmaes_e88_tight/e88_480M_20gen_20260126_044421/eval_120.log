Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_120/levelE88_100m_20260126_053050
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 488,597,282 parameters
Using schedule-free AdamW (lr=0.0005387427712504956)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.4374 | lr 5.39e-04 | grad 16.50 | tok/s 6098
step     20 | loss 3.3544 | lr 5.39e-04 | grad 9.31 | tok/s 16694
step     30 | loss 2.7387 | lr 5.39e-04 | grad 7.88 | tok/s 16859
step     40 | loss 2.5118 | lr 5.39e-04 | grad 6.38 | tok/s 16120
step     50 | loss 2.9696 | lr 5.39e-04 | grad 11.56 | tok/s 16364
step     60 | loss 2.0952 | lr 5.39e-04 | grad 3.39 | tok/s 16908
step     70 | loss 1.8429 | lr 5.39e-04 | grad 4.88 | tok/s 17053
step     80 | loss 5.9394 | lr 5.39e-04 | grad 23.50 | tok/s 17145
step     90 | loss 4.1547 | lr 5.39e-04 | grad 6.53 | tok/s 17406
step    100 | loss 3.7503 | lr 5.39e-04 | grad 6.19 | tok/s 17358
step    110 | loss 3.0952 | lr 5.39e-04 | grad 11.12 | tok/s 17301
step    120 | loss 2.8876 | lr 5.39e-04 | grad 11.75 | tok/s 17324
step    130 | loss 2.6520 | lr 5.39e-04 | grad 7.41 | tok/s 17312
step    140 | loss 2.4703 | lr 5.39e-04 | grad 7.19 | tok/s 17315
step    150 | loss 2.4440 | lr 5.39e-04 | grad 5.44 | tok/s 17330
step    160 | loss 2.1424 | lr 5.39e-04 | grad 5.47 | tok/s 17319
step    170 | loss 2.1717 | lr 5.39e-04 | grad 8.69 | tok/s 17323
step    180 | loss 2.0656 | lr 5.39e-04 | grad 4.62 | tok/s 17296
step    190 | loss 2.1571 | lr 5.39e-04 | grad 4.28 | tok/s 17240
step    200 | loss 1.9244 | lr 5.39e-04 | grad 3.72 | tok/s 17292
step    210 | loss 1.9440 | lr 5.39e-04 | grad 6.41 | tok/s 17231
step    220 | loss 2.0710 | lr 5.39e-04 | grad 4.06 | tok/s 17064
step    230 | loss 2.1449 | lr 5.39e-04 | grad 3.00 | tok/s 14923
step    240 | loss 2.2917 | lr 5.39e-04 | grad 4.22 | tok/s 16035
step    250 | loss 2.0666 | lr 5.39e-04 | grad 2.55 | tok/s 16484
step    260 | loss 1.4637 | lr 5.39e-04 | grad 2.84 | tok/s 16927
step    270 | loss 2.0248 | lr 5.39e-04 | grad 3.27 | tok/s 16744
step    280 | loss 2.1887 | lr 5.39e-04 | grad 5.22 | tok/s 16421
step    290 | loss 1.3746 | lr 5.39e-04 | grad 5.25 | tok/s 17321
step    300 | loss 0.5406 | lr 5.39e-04 | grad 3.12 | tok/s 17313
step    310 | loss 2.3220 | lr 5.39e-04 | grad 4.53 | tok/s 17073
step    320 | loss 1.8264 | lr 5.39e-04 | grad 5.28 | tok/s 16509
step    330 | loss 1.8958 | lr 5.39e-04 | grad 2.58 | tok/s 15965
step    340 | loss 2.1822 | lr 5.39e-04 | grad 3.38 | tok/s 16272
step    350 | loss 1.7457 | lr 5.39e-04 | grad 2.44 | tok/s 16658
step    360 | loss 1.1004 | lr 5.39e-04 | grad 6.31 | tok/s 16987
step    370 | loss 1.7379 | lr 5.39e-04 | grad 2.55 | tok/s 15493
step    380 | loss 1.6924 | lr 5.39e-04 | grad 2.77 | tok/s 16469
step    390 | loss 1.4728 | lr 5.39e-04 | grad 2.80 | tok/s 17267
step    400 | loss 1.4301 | lr 5.39e-04 | grad 2.52 | tok/s 17094
step    410 | loss 1.2128 | lr 5.39e-04 | grad 1.97 | tok/s 15681
step    420 | loss 1.7552 | lr 5.39e-04 | grad 3.59 | tok/s 15950
step    430 | loss 2.0443 | lr 5.39e-04 | grad 2.89 | tok/s 17016
step    440 | loss 2.0981 | lr 5.39e-04 | grad 3.11 | tok/s 16060
step    450 | loss 1.8913 | lr 5.39e-04 | grad 2.47 | tok/s 16641
step    460 | loss 1.6395 | lr 5.39e-04 | grad 3.02 | tok/s 16315
step    470 | loss 1.7651 | lr 5.39e-04 | grad 2.86 | tok/s 16801
step    480 | loss 2.0859 | lr 5.39e-04 | grad 4.97 | tok/s 16771
step    490 | loss 1.7298 | lr 5.39e-04 | grad 2.31 | tok/s 15901
step    500 | loss 1.6229 | lr 5.39e-04 | grad 3.58 | tok/s 16973
step    510 | loss 1.6580 | lr 5.39e-04 | grad 2.58 | tok/s 17127
step    520 | loss 1.5863 | lr 5.39e-04 | grad 2.08 | tok/s 17109
step    530 | loss 1.8113 | lr 5.39e-04 | grad 2.25 | tok/s 16449
step    540 | loss 1.6859 | lr 5.39e-04 | grad 3.03 | tok/s 16501
step    550 | loss 1.5326 | lr 5.39e-04 | grad 2.28 | tok/s 15252
step    560 | loss 1.6718 | lr 5.39e-04 | grad 2.45 | tok/s 15722
step    570 | loss 1.6027 | lr 5.39e-04 | grad 2.86 | tok/s 16168
step    580 | loss 1.4953 | lr 5.39e-04 | grad 2.41 | tok/s 16102
step    590 | loss 1.7727 | lr 5.39e-04 | grad 2.44 | tok/s 16488
step    600 | loss 1.7711 | lr 5.39e-04 | grad 1.95 | tok/s 15960
step    610 | loss 1.5590 | lr 5.39e-04 | grad 2.42 | tok/s 16731
step    620 | loss 1.5099 | lr 5.39e-04 | grad 2.00 | tok/s 15877
step    630 | loss 1.5930 | lr 5.39e-04 | grad 3.64 | tok/s 15965
step    640 | loss 1.7329 | lr 5.39e-04 | grad 2.22 | tok/s 16441
step    650 | loss 1.6349 | lr 5.39e-04 | grad 2.36 | tok/s 16481
step    660 | loss 1.6283 | lr 5.39e-04 | grad 1.84 | tok/s 16560
step    670 | loss 1.8460 | lr 5.39e-04 | grad 4.75 | tok/s 16671
step    680 | loss 1.6722 | lr 5.39e-04 | grad 2.17 | tok/s 16342
step    690 | loss 1.7412 | lr 5.39e-04 | grad 2.92 | tok/s 16950
step    700 | loss 1.2913 | lr 5.39e-04 | grad 2.12 | tok/s 17227
step    710 | loss 1.5348 | lr 5.39e-04 | grad 2.39 | tok/s 16144
step    720 | loss 1.4296 | lr 5.39e-04 | grad 3.48 | tok/s 15885
step    730 | loss 1.2284 | lr 5.39e-04 | grad 2.61 | tok/s 17232

Training complete! Final step: 730
