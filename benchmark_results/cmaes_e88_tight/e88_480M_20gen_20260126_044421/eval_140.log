Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_140/levelE88_100m_20260126_054048
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,164,496 parameters
Using schedule-free AdamW (lr=0.00046554937197735983)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1863 | lr 4.66e-04 | grad 15.19 | tok/s 5982
step     20 | loss 3.1254 | lr 4.66e-04 | grad 7.50 | tok/s 16469
step     30 | loss 2.5881 | lr 4.66e-04 | grad 6.31 | tok/s 16672
step     40 | loss 2.4061 | lr 4.66e-04 | grad 4.94 | tok/s 15974
step     50 | loss 3.1122 | lr 4.66e-04 | grad 16.00 | tok/s 16179
step     60 | loss 2.0707 | lr 4.66e-04 | grad 3.61 | tok/s 16739
step     70 | loss 1.8054 | lr 4.66e-04 | grad 4.69 | tok/s 16916
step     80 | loss 6.6397 | lr 4.66e-04 | grad 45.75 | tok/s 17026
step     90 | loss 4.8275 | lr 4.66e-04 | grad 7.50 | tok/s 17368
step    100 | loss 4.2417 | lr 4.66e-04 | grad 4.94 | tok/s 17327
step    110 | loss 3.5141 | lr 4.66e-04 | grad 47.00 | tok/s 17341
step    120 | loss 3.0089 | lr 4.66e-04 | grad 7.72 | tok/s 17275
step    130 | loss 3.0004 | lr 4.66e-04 | grad 10.75 | tok/s 16028
step    140 | loss 2.8847 | lr 4.66e-04 | grad 6.56 | tok/s 17306
step    150 | loss 2.8026 | lr 4.66e-04 | grad 11.25 | tok/s 17308
step    160 | loss 2.4031 | lr 4.66e-04 | grad 9.31 | tok/s 17285
step    170 | loss 2.4448 | lr 4.66e-04 | grad 9.94 | tok/s 17251
step    180 | loss 2.2604 | lr 4.66e-04 | grad 6.75 | tok/s 17287
step    190 | loss 2.3544 | lr 4.66e-04 | grad 4.47 | tok/s 17268
step    200 | loss 2.0250 | lr 4.66e-04 | grad 3.72 | tok/s 17241
step    210 | loss 2.1088 | lr 4.66e-04 | grad 5.03 | tok/s 17231
step    220 | loss 2.0972 | lr 4.66e-04 | grad 4.38 | tok/s 17053
step    230 | loss 2.0452 | lr 4.66e-04 | grad 3.84 | tok/s 16811
step    240 | loss 2.2707 | lr 4.66e-04 | grad 4.59 | tok/s 16022
step    250 | loss 2.0563 | lr 4.66e-04 | grad 2.62 | tok/s 16433
step    260 | loss 1.4740 | lr 4.66e-04 | grad 2.98 | tok/s 16995
step    270 | loss 2.0248 | lr 4.66e-04 | grad 3.00 | tok/s 16763
step    280 | loss 2.1926 | lr 4.66e-04 | grad 6.31 | tok/s 16404
step    290 | loss 1.4213 | lr 4.66e-04 | grad 16.25 | tok/s 17303
step    300 | loss 0.5568 | lr 4.66e-04 | grad 2.75 | tok/s 17263
step    310 | loss 2.3371 | lr 4.66e-04 | grad 3.80 | tok/s 16967
step    320 | loss 1.8391 | lr 4.66e-04 | grad 5.03 | tok/s 16612
step    330 | loss 1.8942 | lr 4.66e-04 | grad 2.61 | tok/s 16025
step    340 | loss 2.2139 | lr 4.66e-04 | grad 3.03 | tok/s 16308
step    350 | loss 1.7409 | lr 4.66e-04 | grad 2.69 | tok/s 16706
step    360 | loss 1.1161 | lr 4.66e-04 | grad 6.66 | tok/s 17045
step    370 | loss 1.7410 | lr 4.66e-04 | grad 2.48 | tok/s 15509
step    380 | loss 1.7065 | lr 4.66e-04 | grad 2.61 | tok/s 16508
step    390 | loss 1.4719 | lr 4.66e-04 | grad 2.62 | tok/s 17238
step    400 | loss 1.4443 | lr 4.66e-04 | grad 2.61 | tok/s 17090
step    410 | loss 1.2237 | lr 4.66e-04 | grad 1.90 | tok/s 16720
step    420 | loss 1.7607 | lr 4.66e-04 | grad 3.88 | tok/s 15951
step    430 | loss 2.0767 | lr 4.66e-04 | grad 2.98 | tok/s 16971
step    440 | loss 2.1085 | lr 4.66e-04 | grad 3.20 | tok/s 16039
step    450 | loss 1.9706 | lr 4.66e-04 | grad 2.34 | tok/s 16603
step    460 | loss 1.6585 | lr 4.66e-04 | grad 2.95 | tok/s 16259
step    470 | loss 1.7710 | lr 4.66e-04 | grad 2.72 | tok/s 16736
step    480 | loss 2.1260 | lr 4.66e-04 | grad 5.38 | tok/s 16797
step    490 | loss 1.7418 | lr 4.66e-04 | grad 2.20 | tok/s 15830
step    500 | loss 1.6253 | lr 4.66e-04 | grad 3.72 | tok/s 16930
step    510 | loss 1.6572 | lr 4.66e-04 | grad 2.66 | tok/s 17134
step    520 | loss 1.5964 | lr 4.66e-04 | grad 2.02 | tok/s 17096
step    530 | loss 1.8265 | lr 4.66e-04 | grad 2.22 | tok/s 16469
step    540 | loss 1.6908 | lr 4.66e-04 | grad 2.56 | tok/s 16476
step    550 | loss 1.5362 | lr 4.66e-04 | grad 2.38 | tok/s 16133
step    560 | loss 1.6808 | lr 4.66e-04 | grad 2.44 | tok/s 15717
step    570 | loss 1.6122 | lr 4.66e-04 | grad 2.81 | tok/s 15105
step    580 | loss 1.5057 | lr 4.66e-04 | grad 2.22 | tok/s 16016
step    590 | loss 1.7815 | lr 4.66e-04 | grad 2.55 | tok/s 16439
step    600 | loss 1.7887 | lr 4.66e-04 | grad 1.84 | tok/s 15856
step    610 | loss 1.5700 | lr 4.66e-04 | grad 2.42 | tok/s 16663
step    620 | loss 1.5193 | lr 4.66e-04 | grad 2.12 | tok/s 15832
step    630 | loss 1.6051 | lr 4.66e-04 | grad 3.81 | tok/s 15947
step    640 | loss 1.7538 | lr 4.66e-04 | grad 2.14 | tok/s 16405
step    650 | loss 1.6405 | lr 4.66e-04 | grad 2.56 | tok/s 16441
step    660 | loss 1.6457 | lr 4.66e-04 | grad 1.84 | tok/s 16498
step    670 | loss 1.8484 | lr 4.66e-04 | grad 2.70 | tok/s 16642
step    680 | loss 1.6838 | lr 4.66e-04 | grad 2.14 | tok/s 16292
step    690 | loss 1.7450 | lr 4.66e-04 | grad 3.20 | tok/s 16740
step    700 | loss 1.3176 | lr 4.66e-04 | grad 2.30 | tok/s 17146
step    710 | loss 1.5483 | lr 4.66e-04 | grad 2.34 | tok/s 16042
step    720 | loss 1.4360 | lr 4.66e-04 | grad 3.48 | tok/s 15797

Training complete! Final step: 729
