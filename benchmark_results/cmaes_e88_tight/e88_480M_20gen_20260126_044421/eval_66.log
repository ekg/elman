Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_66/levelE88_100m_20260126_051055
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 468,877,516 parameters
Using schedule-free AdamW (lr=0.0004669715834000477)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.1660 | lr 4.67e-04 | grad 13.94 | tok/s 9004
step     20 | loss 3.0165 | lr 4.67e-04 | grad 4.47 | tok/s 16592
step     30 | loss 3.1417 | lr 4.67e-04 | grad 7.44 | tok/s 17533
step     40 | loss 4.5867 | lr 4.67e-04 | grad 29.62 | tok/s 17789
step     50 | loss 4.2637 | lr 4.67e-04 | grad 12.19 | tok/s 17990
step     60 | loss 3.2890 | lr 4.67e-04 | grad 5.69 | tok/s 17958
step     70 | loss 2.8271 | lr 4.67e-04 | grad 5.22 | tok/s 17927
step     80 | loss 2.5259 | lr 4.67e-04 | grad 3.97 | tok/s 17868
step     90 | loss 2.4503 | lr 4.67e-04 | grad 4.16 | tok/s 17857
step    100 | loss 2.1776 | lr 4.67e-04 | grad 2.59 | tok/s 17838
step    110 | loss 2.1633 | lr 4.67e-04 | grad 3.36 | tok/s 17702
step    120 | loss 2.6573 | lr 4.67e-04 | grad 2.55 | tok/s 16849
step    130 | loss 2.0307 | lr 4.67e-04 | grad 5.16 | tok/s 17237
step    140 | loss 2.3093 | lr 4.67e-04 | grad 6.22 | tok/s 17306
step    150 | loss 1.3347 | lr 4.67e-04 | grad 5.19 | tok/s 17692
step    160 | loss 2.2253 | lr 4.67e-04 | grad 2.33 | tok/s 17131
step    170 | loss 2.2568 | lr 4.67e-04 | grad 2.03 | tok/s 16866
step    180 | loss 1.7075 | lr 4.67e-04 | grad 3.11 | tok/s 17268
step    190 | loss 1.8430 | lr 4.67e-04 | grad 2.61 | tok/s 17002
step    200 | loss 1.5673 | lr 4.67e-04 | grad 1.85 | tok/s 17732
step    210 | loss 1.8201 | lr 4.67e-04 | grad 7.03 | tok/s 16823
step    220 | loss 2.1433 | lr 4.67e-04 | grad 3.62 | tok/s 17011
step    230 | loss 1.9429 | lr 4.67e-04 | grad 2.36 | tok/s 16473
step    240 | loss 2.1936 | lr 4.67e-04 | grad 4.72 | tok/s 17268
step    250 | loss 1.7108 | lr 4.67e-04 | grad 1.62 | tok/s 17123
step    260 | loss 1.8297 | lr 4.67e-04 | grad 2.80 | tok/s 17582
step    270 | loss 1.7675 | lr 4.67e-04 | grad 2.11 | tok/s 17175
step    280 | loss 1.7288 | lr 4.67e-04 | grad 1.64 | tok/s 16148
step    290 | loss 1.6225 | lr 4.67e-04 | grad 2.05 | tok/s 16691
step    300 | loss 1.9165 | lr 4.67e-04 | grad 2.00 | tok/s 16819
step    310 | loss 1.6261 | lr 4.67e-04 | grad 1.69 | tok/s 16744
step    320 | loss 1.8373 | lr 4.67e-04 | grad 2.86 | tok/s 16915
step    330 | loss 1.6764 | lr 4.67e-04 | grad 1.92 | tok/s 17098
step    340 | loss 1.9857 | lr 4.67e-04 | grad 1.84 | tok/s 17046
step    350 | loss 1.6357 | lr 4.67e-04 | grad 1.80 | tok/s 17518
step    360 | loss 1.5404 | lr 4.67e-04 | grad 1.67 | tok/s 16783
step    370 | loss 1.4204 | lr 4.67e-04 | grad 1.61 | tok/s 17669
step    380 | loss 1.1341 | lr 4.67e-04 | grad 1.45 | tok/s 17855

Training complete! Final step: 380
