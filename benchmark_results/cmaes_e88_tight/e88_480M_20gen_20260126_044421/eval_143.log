Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_143/levelE88_100m_20260126_054048
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 488,597,282 parameters
Using schedule-free AdamW (lr=0.00048418083155057716)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.3375 | lr 4.84e-04 | grad 15.50 | tok/s 6100
step     20 | loss 3.1869 | lr 4.84e-04 | grad 8.69 | tok/s 16134
step     30 | loss 2.7126 | lr 4.84e-04 | grad 6.97 | tok/s 16271
step     40 | loss 2.4497 | lr 4.84e-04 | grad 3.92 | tok/s 15543
step     50 | loss 2.9837 | lr 4.84e-04 | grad 10.94 | tok/s 15780
step     60 | loss 2.0809 | lr 4.84e-04 | grad 3.91 | tok/s 16254
step     70 | loss 1.8267 | lr 4.84e-04 | grad 4.72 | tok/s 16430
step     80 | loss 5.9240 | lr 4.84e-04 | grad 24.88 | tok/s 16507
step     90 | loss 4.2359 | lr 4.84e-04 | grad 7.19 | tok/s 16809
step    100 | loss 3.5746 | lr 4.84e-04 | grad 5.56 | tok/s 16766
step    110 | loss 3.1722 | lr 4.84e-04 | grad 16.50 | tok/s 16750
step    120 | loss 2.9646 | lr 4.84e-04 | grad 7.12 | tok/s 16721
step    130 | loss 2.6742 | lr 4.84e-04 | grad 8.81 | tok/s 15213
step    140 | loss 2.4867 | lr 4.84e-04 | grad 5.91 | tok/s 16719
step    150 | loss 2.5014 | lr 4.84e-04 | grad 5.69 | tok/s 16685
step    160 | loss 2.2161 | lr 4.84e-04 | grad 7.00 | tok/s 16639
step    170 | loss 2.2057 | lr 4.84e-04 | grad 11.62 | tok/s 16628
step    180 | loss 2.0401 | lr 4.84e-04 | grad 7.66 | tok/s 16615
step    190 | loss 2.2337 | lr 4.84e-04 | grad 9.50 | tok/s 16625
step    200 | loss 1.9328 | lr 4.84e-04 | grad 3.67 | tok/s 16627
step    210 | loss 1.9665 | lr 4.84e-04 | grad 5.34 | tok/s 16587
step    220 | loss 2.0567 | lr 4.84e-04 | grad 4.44 | tok/s 16389
step    230 | loss 2.0958 | lr 4.84e-04 | grad 5.62 | tok/s 16194
step    240 | loss 2.2967 | lr 4.84e-04 | grad 4.50 | tok/s 15372
step    250 | loss 2.0617 | lr 4.84e-04 | grad 2.70 | tok/s 15815
step    260 | loss 1.4759 | lr 4.84e-04 | grad 3.00 | tok/s 16317
step    270 | loss 2.0380 | lr 4.84e-04 | grad 3.31 | tok/s 16091
step    280 | loss 2.2013 | lr 4.84e-04 | grad 5.03 | tok/s 15790
step    290 | loss 1.4462 | lr 4.84e-04 | grad 17.38 | tok/s 16601
step    300 | loss 0.5535 | lr 4.84e-04 | grad 3.28 | tok/s 16573
step    310 | loss 2.3061 | lr 4.84e-04 | grad 3.77 | tok/s 16310
step    320 | loss 1.8257 | lr 4.84e-04 | grad 5.03 | tok/s 15999
step    330 | loss 1.9018 | lr 4.84e-04 | grad 2.59 | tok/s 15428
step    340 | loss 2.2097 | lr 4.84e-04 | grad 3.19 | tok/s 15679
step    350 | loss 1.7605 | lr 4.84e-04 | grad 3.55 | tok/s 16061
step    360 | loss 1.1107 | lr 4.84e-04 | grad 6.22 | tok/s 16420
step    370 | loss 1.7422 | lr 4.84e-04 | grad 2.88 | tok/s 14899
step    380 | loss 1.6992 | lr 4.84e-04 | grad 2.59 | tok/s 15864
step    390 | loss 1.4757 | lr 4.84e-04 | grad 2.72 | tok/s 16526
step    400 | loss 1.4369 | lr 4.84e-04 | grad 2.55 | tok/s 16391
step    410 | loss 1.2254 | lr 4.84e-04 | grad 2.02 | tok/s 16035
step    420 | loss 1.7652 | lr 4.84e-04 | grad 3.69 | tok/s 15321
step    430 | loss 2.0623 | lr 4.84e-04 | grad 2.91 | tok/s 16315
step    440 | loss 2.1066 | lr 4.84e-04 | grad 3.09 | tok/s 15426
step    450 | loss 2.0124 | lr 4.84e-04 | grad 2.39 | tok/s 15958
step    460 | loss 1.6632 | lr 4.84e-04 | grad 3.06 | tok/s 15634
step    470 | loss 1.7631 | lr 4.84e-04 | grad 2.78 | tok/s 16125
step    480 | loss 2.1119 | lr 4.84e-04 | grad 4.88 | tok/s 16132
step    490 | loss 1.7358 | lr 4.84e-04 | grad 2.36 | tok/s 15245
step    500 | loss 1.6235 | lr 4.84e-04 | grad 3.48 | tok/s 16280
step    510 | loss 1.6588 | lr 4.84e-04 | grad 2.72 | tok/s 16506
step    520 | loss 1.5918 | lr 4.84e-04 | grad 1.92 | tok/s 16447
step    530 | loss 1.8234 | lr 4.84e-04 | grad 2.17 | tok/s 15827
step    540 | loss 1.6891 | lr 4.84e-04 | grad 2.72 | tok/s 15848
step    550 | loss 1.5370 | lr 4.84e-04 | grad 2.17 | tok/s 15486
step    560 | loss 1.6713 | lr 4.84e-04 | grad 2.53 | tok/s 14229
step    570 | loss 1.6040 | lr 4.84e-04 | grad 2.91 | tok/s 15507
step    580 | loss 1.5025 | lr 4.84e-04 | grad 2.27 | tok/s 15448
step    590 | loss 1.7724 | lr 4.84e-04 | grad 2.64 | tok/s 15869
step    600 | loss 1.7835 | lr 4.84e-04 | grad 1.88 | tok/s 15321
step    610 | loss 1.5733 | lr 4.84e-04 | grad 2.34 | tok/s 16107
step    620 | loss 1.5183 | lr 4.84e-04 | grad 2.09 | tok/s 15268
step    630 | loss 1.5960 | lr 4.84e-04 | grad 3.55 | tok/s 15375
step    640 | loss 1.7549 | lr 4.84e-04 | grad 2.20 | tok/s 15798
step    650 | loss 1.6348 | lr 4.84e-04 | grad 2.66 | tok/s 15886
step    660 | loss 1.6454 | lr 4.84e-04 | grad 1.88 | tok/s 15941
step    670 | loss 1.8566 | lr 4.84e-04 | grad 10.12 | tok/s 16067
step    680 | loss 1.6821 | lr 4.84e-04 | grad 2.55 | tok/s 15701
step    690 | loss 1.7447 | lr 4.84e-04 | grad 2.80 | tok/s 16290
step    700 | loss 1.2953 | lr 4.84e-04 | grad 2.23 | tok/s 16613

Training complete! Final step: 703
