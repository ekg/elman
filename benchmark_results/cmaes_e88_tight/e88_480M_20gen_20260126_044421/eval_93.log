Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_93/levelE88_100m_20260126_052051
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,728,688 parameters
Using schedule-free AdamW (lr=0.0005522589249051848)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.3211 | lr 5.52e-04 | grad 11.12 | tok/s 5891
step     20 | loss 2.9008 | lr 5.52e-04 | grad 4.81 | tok/s 15005
step     30 | loss 2.5383 | lr 5.52e-04 | grad 5.28 | tok/s 15215
step     40 | loss 2.3807 | lr 5.52e-04 | grad 5.09 | tok/s 14527
step     50 | loss 3.1595 | lr 5.52e-04 | grad 10.62 | tok/s 14747
step     60 | loss 2.0349 | lr 5.52e-04 | grad 2.52 | tok/s 15219
step     70 | loss 1.7736 | lr 5.52e-04 | grad 3.92 | tok/s 15362
step     80 | loss 5.9925 | lr 5.52e-04 | grad 38.25 | tok/s 15471
step     90 | loss 4.4851 | lr 5.52e-04 | grad 5.22 | tok/s 15723
step    100 | loss 4.0526 | lr 5.52e-04 | grad 4.62 | tok/s 15663
step    110 | loss 3.1661 | lr 5.52e-04 | grad 7.59 | tok/s 15712
step    120 | loss 3.0219 | lr 5.52e-04 | grad 5.66 | tok/s 15620
step    130 | loss 2.7705 | lr 5.52e-04 | grad 6.84 | tok/s 15614
step    140 | loss 2.5112 | lr 5.52e-04 | grad 5.22 | tok/s 15597
step    150 | loss 2.5419 | lr 5.52e-04 | grad 5.75 | tok/s 15622
step    160 | loss 2.1863 | lr 5.52e-04 | grad 8.31 | tok/s 15603
step    170 | loss 2.1609 | lr 5.52e-04 | grad 7.44 | tok/s 15564
step    180 | loss 2.0167 | lr 5.52e-04 | grad 5.72 | tok/s 15578
step    190 | loss 2.1703 | lr 5.52e-04 | grad 4.06 | tok/s 15563
step    200 | loss 1.8807 | lr 5.52e-04 | grad 3.06 | tok/s 15548
step    210 | loss 1.9351 | lr 5.52e-04 | grad 4.38 | tok/s 15501
step    220 | loss 2.0437 | lr 5.52e-04 | grad 3.33 | tok/s 15318
step    230 | loss 2.0476 | lr 5.52e-04 | grad 3.27 | tok/s 14202
step    240 | loss 2.2307 | lr 5.52e-04 | grad 3.94 | tok/s 14390
step    250 | loss 2.0180 | lr 5.52e-04 | grad 2.19 | tok/s 14803
step    260 | loss 1.4453 | lr 5.52e-04 | grad 2.45 | tok/s 15251
step    270 | loss 1.9964 | lr 5.52e-04 | grad 2.44 | tok/s 15030
step    280 | loss 2.1572 | lr 5.52e-04 | grad 4.34 | tok/s 14761
step    290 | loss 1.5168 | lr 5.52e-04 | grad 1.86 | tok/s 15468
step    300 | loss 0.5421 | lr 5.52e-04 | grad 2.83 | tok/s 15466
step    310 | loss 2.2910 | lr 5.52e-04 | grad 3.42 | tok/s 15249
step    320 | loss 1.7929 | lr 5.52e-04 | grad 4.53 | tok/s 14932
step    330 | loss 1.8647 | lr 5.52e-04 | grad 2.25 | tok/s 14447
step    340 | loss 2.1497 | lr 5.52e-04 | grad 2.53 | tok/s 14642
step    350 | loss 1.7224 | lr 5.52e-04 | grad 2.36 | tok/s 15026
step    360 | loss 1.0752 | lr 5.52e-04 | grad 5.72 | tok/s 15358
step    370 | loss 1.7175 | lr 5.52e-04 | grad 1.92 | tok/s 13920
step    380 | loss 1.6721 | lr 5.52e-04 | grad 2.09 | tok/s 14833
step    390 | loss 1.4510 | lr 5.52e-04 | grad 1.98 | tok/s 15497
step    400 | loss 1.4178 | lr 5.52e-04 | grad 2.06 | tok/s 15351
step    410 | loss 1.2024 | lr 5.52e-04 | grad 1.64 | tok/s 15001
step    420 | loss 1.7415 | lr 5.52e-04 | grad 3.34 | tok/s 14317
step    430 | loss 2.0470 | lr 5.52e-04 | grad 2.41 | tok/s 15233
step    440 | loss 2.0788 | lr 5.52e-04 | grad 2.64 | tok/s 14418
step    450 | loss 1.8815 | lr 5.52e-04 | grad 1.95 | tok/s 14894
step    460 | loss 1.6342 | lr 5.52e-04 | grad 2.55 | tok/s 14564
step    470 | loss 1.7368 | lr 5.52e-04 | grad 2.19 | tok/s 15007
step    480 | loss 2.0916 | lr 5.52e-04 | grad 4.47 | tok/s 15044
step    490 | loss 1.7188 | lr 5.52e-04 | grad 1.84 | tok/s 13468
step    500 | loss 1.5989 | lr 5.52e-04 | grad 2.94 | tok/s 15143
step    510 | loss 1.6285 | lr 5.52e-04 | grad 2.00 | tok/s 15366
step    520 | loss 1.5676 | lr 5.52e-04 | grad 1.59 | tok/s 15326
step    530 | loss 1.7912 | lr 5.52e-04 | grad 1.79 | tok/s 14760
step    540 | loss 1.6633 | lr 5.52e-04 | grad 2.14 | tok/s 14775
step    550 | loss 1.5180 | lr 5.52e-04 | grad 2.11 | tok/s 14449
step    560 | loss 1.6549 | lr 5.52e-04 | grad 1.98 | tok/s 14080
step    570 | loss 1.5914 | lr 5.52e-04 | grad 2.28 | tok/s 14468
step    580 | loss 1.4866 | lr 5.52e-04 | grad 1.92 | tok/s 14415
step    590 | loss 1.7506 | lr 5.52e-04 | grad 2.16 | tok/s 14768
step    600 | loss 1.7640 | lr 5.52e-04 | grad 1.52 | tok/s 14279
step    610 | loss 1.5481 | lr 5.52e-04 | grad 1.88 | tok/s 14997
step    620 | loss 1.4969 | lr 5.52e-04 | grad 1.71 | tok/s 14214
step    630 | loss 1.5897 | lr 5.52e-04 | grad 2.97 | tok/s 14341
step    640 | loss 1.7283 | lr 5.52e-04 | grad 1.74 | tok/s 14703
step    650 | loss 1.5997 | lr 5.52e-04 | grad 2.11 | tok/s 14784

Training complete! Final step: 658
