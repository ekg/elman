Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_48/levelE88_100m_20260126_050059
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 468,817,302 parameters
Using schedule-free AdamW (lr=0.0005846810812927496)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.2532 | lr 5.85e-04 | grad 8.56 | tok/s 7487
step     20 | loss 2.8315 | lr 5.85e-04 | grad 3.05 | tok/s 11461
step     30 | loss 3.0863 | lr 5.85e-04 | grad 4.75 | tok/s 12108
step     40 | loss 4.8183 | lr 5.85e-04 | grad 70.50 | tok/s 12305
step     50 | loss 4.7758 | lr 5.85e-04 | grad 12.06 | tok/s 12147
step     60 | loss 3.4781 | lr 5.85e-04 | grad 5.84 | tok/s 12371
step     70 | loss 2.7489 | lr 5.85e-04 | grad 3.61 | tok/s 12336
step     80 | loss 2.4278 | lr 5.85e-04 | grad 2.38 | tok/s 12336
step     90 | loss 2.2679 | lr 5.85e-04 | grad 1.98 | tok/s 12308
step    100 | loss 2.0084 | lr 5.85e-04 | grad 1.47 | tok/s 12133
step    110 | loss 2.1124 | lr 5.85e-04 | grad 2.50 | tok/s 12214
step    120 | loss 2.6537 | lr 5.85e-04 | grad 1.36 | tok/s 11630
step    130 | loss 2.0164 | lr 5.85e-04 | grad 4.12 | tok/s 11892
step    140 | loss 2.2895 | lr 5.85e-04 | grad 5.03 | tok/s 11942
step    150 | loss 1.4102 | lr 5.85e-04 | grad 3.48 | tok/s 12206
step    160 | loss 2.1698 | lr 5.85e-04 | grad 1.52 | tok/s 11817
step    170 | loss 2.2236 | lr 5.85e-04 | grad 1.23 | tok/s 11627
step    180 | loss 1.6953 | lr 5.85e-04 | grad 1.99 | tok/s 11904
step    190 | loss 1.8207 | lr 5.85e-04 | grad 1.73 | tok/s 11590
step    200 | loss 1.5532 | lr 5.85e-04 | grad 1.09 | tok/s 12239
step    210 | loss 1.7977 | lr 5.85e-04 | grad 3.39 | tok/s 11591
step    220 | loss 2.1034 | lr 5.85e-04 | grad 1.71 | tok/s 11736
step    230 | loss 1.8522 | lr 5.85e-04 | grad 1.72 | tok/s 11697
step    240 | loss 2.1552 | lr 5.85e-04 | grad 3.06 | tok/s 11855
step    250 | loss 1.6929 | lr 5.85e-04 | grad 1.02 | tok/s 11803
step    260 | loss 1.8064 | lr 5.85e-04 | grad 1.91 | tok/s 12055

Training complete! Final step: 263
