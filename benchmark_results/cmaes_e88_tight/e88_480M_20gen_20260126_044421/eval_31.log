Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_31/levelE88_100m_20260126_045422
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 487,125,214 parameters
Using schedule-free AdamW (lr=0.0005238933311511003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1148 | lr 5.24e-04 | grad 4.28 | tok/s 4537
step     20 | loss 2.5665 | lr 5.24e-04 | grad 1.90 | tok/s 7942
step     30 | loss 2.4889 | lr 5.24e-04 | grad 1.12 | tok/s 8008
step     40 | loss 2.3038 | lr 5.24e-04 | grad 1.84 | tok/s 7642
step     50 | loss 2.7581 | lr 5.24e-04 | grad 5.31 | tok/s 7736
step     60 | loss 2.0502 | lr 5.24e-04 | grad 6.97 | tok/s 7947
step     70 | loss 1.9233 | lr 5.24e-04 | grad 1.86 | tok/s 7887
step     80 | loss 4.9012 | lr 5.24e-04 | grad 22.50 | tok/s 8060
step     90 | loss 4.3789 | lr 5.24e-04 | grad 2.77 | tok/s 8194
step    100 | loss 3.5876 | lr 5.24e-04 | grad 2.91 | tok/s 8174
step    110 | loss 3.0922 | lr 5.24e-04 | grad 4.69 | tok/s 8172
step    120 | loss 2.6858 | lr 5.24e-04 | grad 3.73 | tok/s 8153
step    130 | loss 2.4063 | lr 5.24e-04 | grad 2.89 | tok/s 8156
step    140 | loss 2.2918 | lr 5.24e-04 | grad 2.52 | tok/s 8171
step    150 | loss 2.2007 | lr 5.24e-04 | grad 1.69 | tok/s 8010
step    160 | loss 2.0430 | lr 5.24e-04 | grad 4.19 | tok/s 8140
step    170 | loss 2.0317 | lr 5.24e-04 | grad 3.44 | tok/s 8144
step    180 | loss 1.9597 | lr 5.24e-04 | grad 1.88 | tok/s 8138
step    190 | loss 2.0340 | lr 5.24e-04 | grad 2.28 | tok/s 8144
step    200 | loss 1.8157 | lr 5.24e-04 | grad 1.65 | tok/s 8132
step    210 | loss 1.8366 | lr 5.24e-04 | grad 1.52 | tok/s 8129
step    220 | loss 2.0162 | lr 5.24e-04 | grad 1.40 | tok/s 8027
step    230 | loss 1.9990 | lr 5.24e-04 | grad 2.02 | tok/s 7811
step    240 | loss 2.1978 | lr 5.24e-04 | grad 1.89 | tok/s 7548
step    250 | loss 2.0145 | lr 5.24e-04 | grad 1.01 | tok/s 7746
step    260 | loss 1.5190 | lr 5.24e-04 | grad 1.20 | tok/s 8002
step    270 | loss 1.9998 | lr 5.24e-04 | grad 1.12 | tok/s 7891
step    280 | loss 2.1741 | lr 5.24e-04 | grad 2.64 | tok/s 7730
step    290 | loss 1.4109 | lr 5.24e-04 | grad 1.21 | tok/s 8152
step    300 | loss 0.5329 | lr 5.24e-04 | grad 1.90 | tok/s 8149
step    310 | loss 2.3234 | lr 5.24e-04 | grad 1.89 | tok/s 7881
step    320 | loss 1.8514 | lr 5.24e-04 | grad 2.30 | tok/s 7836
step    330 | loss 1.8464 | lr 5.24e-04 | grad 1.21 | tok/s 7568
step    340 | loss 2.1178 | lr 5.24e-04 | grad 0.98 | tok/s 7688

Training complete! Final step: 349
