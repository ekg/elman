Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_123/levelE88_100m_20260126_053409
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,426,100 parameters
Using schedule-free AdamW (lr=0.0004793244905145044)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1985 | lr 4.79e-04 | grad 17.75 | tok/s 5950
step     20 | loss 3.2275 | lr 4.79e-04 | grad 13.19 | tok/s 15794
step     30 | loss 2.7175 | lr 4.79e-04 | grad 7.31 | tok/s 15945
step     40 | loss 2.4313 | lr 4.79e-04 | grad 4.16 | tok/s 15256
step     50 | loss 3.1196 | lr 4.79e-04 | grad 11.88 | tok/s 15516
step     60 | loss 2.1157 | lr 4.79e-04 | grad 7.12 | tok/s 16008
step     70 | loss 1.8177 | lr 4.79e-04 | grad 4.56 | tok/s 16193
step     80 | loss 5.9611 | lr 4.79e-04 | grad 34.25 | tok/s 16286
step     90 | loss 4.6549 | lr 4.79e-04 | grad 6.81 | tok/s 16555
step    100 | loss 4.0784 | lr 4.79e-04 | grad 5.41 | tok/s 16533
step    110 | loss 3.2478 | lr 4.79e-04 | grad 19.38 | tok/s 16516
step    120 | loss 2.9755 | lr 4.79e-04 | grad 7.50 | tok/s 16508
step    130 | loss 2.7747 | lr 4.79e-04 | grad 7.84 | tok/s 15231
step    140 | loss 2.5049 | lr 4.79e-04 | grad 5.12 | tok/s 16378
step    150 | loss 2.4097 | lr 4.79e-04 | grad 5.06 | tok/s 16391
step    160 | loss 2.1660 | lr 4.79e-04 | grad 5.72 | tok/s 16393
step    170 | loss 2.1952 | lr 4.79e-04 | grad 10.00 | tok/s 16360
step    180 | loss 2.0158 | lr 4.79e-04 | grad 4.66 | tok/s 16363
step    190 | loss 2.1714 | lr 4.79e-04 | grad 4.66 | tok/s 16385
step    200 | loss 1.9049 | lr 4.79e-04 | grad 3.48 | tok/s 16386
step    210 | loss 1.9360 | lr 4.79e-04 | grad 4.75 | tok/s 16380
step    220 | loss 2.0642 | lr 4.79e-04 | grad 3.53 | tok/s 16190
step    230 | loss 2.0562 | lr 4.79e-04 | grad 3.08 | tok/s 15978
step    240 | loss 2.2969 | lr 4.79e-04 | grad 4.16 | tok/s 15190
step    250 | loss 2.0630 | lr 4.79e-04 | grad 2.55 | tok/s 15618
step    260 | loss 1.4760 | lr 4.79e-04 | grad 2.77 | tok/s 16082
step    270 | loss 2.0266 | lr 4.79e-04 | grad 3.09 | tok/s 15824
step    280 | loss 2.2247 | lr 4.79e-04 | grad 5.34 | tok/s 15561
step    290 | loss 1.3819 | lr 4.79e-04 | grad 2.11 | tok/s 16401
step    300 | loss 0.5572 | lr 4.79e-04 | grad 6.84 | tok/s 16361
step    310 | loss 2.3268 | lr 4.79e-04 | grad 3.64 | tok/s 16085
step    320 | loss 1.8520 | lr 4.79e-04 | grad 4.50 | tok/s 15755
step    330 | loss 1.8916 | lr 4.79e-04 | grad 2.44 | tok/s 15185
step    340 | loss 2.2175 | lr 4.79e-04 | grad 2.97 | tok/s 15407
step    350 | loss 1.7813 | lr 4.79e-04 | grad 2.64 | tok/s 15821
step    360 | loss 1.1529 | lr 4.79e-04 | grad 7.28 | tok/s 16200
step    370 | loss 1.7521 | lr 4.79e-04 | grad 2.41 | tok/s 14670
step    380 | loss 1.6956 | lr 4.79e-04 | grad 2.55 | tok/s 15625
step    390 | loss 1.4818 | lr 4.79e-04 | grad 2.14 | tok/s 16326
step    400 | loss 1.4415 | lr 4.79e-04 | grad 2.41 | tok/s 16178
step    410 | loss 1.2229 | lr 4.79e-04 | grad 1.88 | tok/s 15800
step    420 | loss 1.7608 | lr 4.79e-04 | grad 3.50 | tok/s 15118
step    430 | loss 2.0750 | lr 4.79e-04 | grad 2.61 | tok/s 16093
step    440 | loss 2.1026 | lr 4.79e-04 | grad 3.03 | tok/s 15182
step    450 | loss 1.9205 | lr 4.79e-04 | grad 2.20 | tok/s 15747
step    460 | loss 1.6664 | lr 4.79e-04 | grad 2.83 | tok/s 15386
step    470 | loss 1.7844 | lr 4.79e-04 | grad 2.62 | tok/s 15894
step    480 | loss 2.1617 | lr 4.79e-04 | grad 5.00 | tok/s 15916
step    490 | loss 1.7394 | lr 4.79e-04 | grad 2.22 | tok/s 15015
step    500 | loss 1.6174 | lr 4.79e-04 | grad 3.11 | tok/s 16042
step    510 | loss 1.6554 | lr 4.79e-04 | grad 2.23 | tok/s 16251
step    520 | loss 1.6071 | lr 4.79e-04 | grad 1.82 | tok/s 16214
step    530 | loss 1.8228 | lr 4.79e-04 | grad 2.09 | tok/s 15608
step    540 | loss 1.6912 | lr 4.79e-04 | grad 2.38 | tok/s 15571
step    550 | loss 1.5410 | lr 4.79e-04 | grad 2.14 | tok/s 15263
step    560 | loss 1.6784 | lr 4.79e-04 | grad 2.50 | tok/s 14846
step    570 | loss 1.6115 | lr 4.79e-04 | grad 2.78 | tok/s 14077
step    580 | loss 1.5091 | lr 4.79e-04 | grad 2.14 | tok/s 15192
step    590 | loss 1.7900 | lr 4.79e-04 | grad 2.27 | tok/s 15620
step    600 | loss 1.7738 | lr 4.79e-04 | grad 1.88 | tok/s 15052
step    610 | loss 1.5742 | lr 4.79e-04 | grad 2.14 | tok/s 15833
step    620 | loss 1.5162 | lr 4.79e-04 | grad 1.98 | tok/s 15008
step    630 | loss 1.6077 | lr 4.79e-04 | grad 3.59 | tok/s 15134
step    640 | loss 1.7531 | lr 4.79e-04 | grad 2.14 | tok/s 15532
step    650 | loss 1.6269 | lr 4.79e-04 | grad 2.45 | tok/s 15587
step    660 | loss 1.6436 | lr 4.79e-04 | grad 2.00 | tok/s 15682
step    670 | loss 1.8312 | lr 4.79e-04 | grad 2.48 | tok/s 15799
step    680 | loss 1.6769 | lr 4.79e-04 | grad 1.84 | tok/s 15435
step    690 | loss 1.7655 | lr 4.79e-04 | grad 2.73 | tok/s 16009

Training complete! Final step: 693
