Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_63/levelE88_100m_20260126_050737
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 475,733,312 parameters
Using schedule-free AdamW (lr=0.0005389953366339401)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.1860 | lr 5.39e-04 | grad 4.84 | tok/s 6037
step     20 | loss 2.7561 | lr 5.39e-04 | grad 1.77 | tok/s 8049
step     30 | loss 3.1881 | lr 5.39e-04 | grad 2.98 | tok/s 8455
step     40 | loss 4.2028 | lr 5.39e-04 | grad 21.88 | tok/s 8569
step     50 | loss 4.2125 | lr 5.39e-04 | grad 7.19 | tok/s 8646
step     60 | loss 3.2102 | lr 5.39e-04 | grad 3.17 | tok/s 8622
step     70 | loss 2.6183 | lr 5.39e-04 | grad 1.73 | tok/s 8587
step     80 | loss 2.2631 | lr 5.39e-04 | grad 1.39 | tok/s 8594
step     90 | loss 2.1273 | lr 5.39e-04 | grad 1.55 | tok/s 8520
step    100 | loss 2.0039 | lr 5.39e-04 | grad 1.57 | tok/s 8592
step    110 | loss 2.3445 | lr 5.39e-04 | grad 6.62 | tok/s 8484
step    120 | loss 2.4512 | lr 5.39e-04 | grad 1.23 | tok/s 8069
step    130 | loss 2.1033 | lr 5.39e-04 | grad 2.20 | tok/s 8375
step    140 | loss 2.3446 | lr 5.39e-04 | grad 3.52 | tok/s 8421
step    150 | loss 1.4966 | lr 5.39e-04 | grad 2.02 | tok/s 8580
step    160 | loss 2.2128 | lr 5.39e-04 | grad 1.14 | tok/s 8230
step    170 | loss 2.2424 | lr 5.39e-04 | grad 0.87 | tok/s 8253
step    180 | loss 1.8147 | lr 5.39e-04 | grad 1.22 | tok/s 8287

Training complete! Final step: 186
