Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_78/levelE88_100m_20260126_051413
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 470,128,352 parameters
Using schedule-free AdamW (lr=0.00038185573728593)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.1872 | lr 3.82e-04 | grad 14.81 | tok/s 8992
step     20 | loss 3.1051 | lr 3.82e-04 | grad 6.19 | tok/s 16158
step     30 | loss 3.1965 | lr 3.82e-04 | grad 8.75 | tok/s 17058
step     40 | loss 4.7693 | lr 3.82e-04 | grad 38.50 | tok/s 17364
step     50 | loss 4.4284 | lr 3.82e-04 | grad 18.88 | tok/s 17565
step     60 | loss 3.4034 | lr 3.82e-04 | grad 8.50 | tok/s 17504
step     70 | loss 2.9082 | lr 3.82e-04 | grad 5.44 | tok/s 17466
step     80 | loss 2.6027 | lr 3.82e-04 | grad 5.38 | tok/s 17427
step     90 | loss 2.5088 | lr 3.82e-04 | grad 4.91 | tok/s 17423
step    100 | loss 2.2698 | lr 3.82e-04 | grad 3.61 | tok/s 17400
step    110 | loss 2.2465 | lr 3.82e-04 | grad 3.67 | tok/s 17245
step    120 | loss 2.6734 | lr 3.82e-04 | grad 2.64 | tok/s 16417
step    130 | loss 2.0643 | lr 3.82e-04 | grad 5.97 | tok/s 16803
step    140 | loss 2.3318 | lr 3.82e-04 | grad 7.06 | tok/s 16850
step    150 | loss 1.3367 | lr 3.82e-04 | grad 5.75 | tok/s 17239
step    160 | loss 2.2895 | lr 3.82e-04 | grad 2.53 | tok/s 16673
step    170 | loss 2.2698 | lr 3.82e-04 | grad 2.19 | tok/s 16432
step    180 | loss 1.7524 | lr 3.82e-04 | grad 3.31 | tok/s 16804
step    190 | loss 1.8676 | lr 3.82e-04 | grad 3.08 | tok/s 16492
step    200 | loss 1.5984 | lr 3.82e-04 | grad 2.05 | tok/s 17250
step    210 | loss 1.8481 | lr 3.82e-04 | grad 7.00 | tok/s 16376
step    220 | loss 2.1628 | lr 3.82e-04 | grad 3.55 | tok/s 16556
step    230 | loss 2.0046 | lr 3.82e-04 | grad 2.80 | tok/s 16162
step    240 | loss 2.2216 | lr 3.82e-04 | grad 5.50 | tok/s 16770
step    250 | loss 1.7307 | lr 3.82e-04 | grad 1.83 | tok/s 16670
step    260 | loss 1.8592 | lr 3.82e-04 | grad 3.31 | tok/s 17136
step    270 | loss 1.7914 | lr 3.82e-04 | grad 2.39 | tok/s 16744
step    280 | loss 1.7483 | lr 3.82e-04 | grad 1.82 | tok/s 15733
step    290 | loss 1.6407 | lr 3.82e-04 | grad 2.33 | tok/s 16263
step    300 | loss 1.9479 | lr 3.82e-04 | grad 2.16 | tok/s 16399
step    310 | loss 1.6440 | lr 3.82e-04 | grad 1.91 | tok/s 16322
step    320 | loss 1.8557 | lr 3.82e-04 | grad 3.53 | tok/s 16504
step    330 | loss 1.7017 | lr 3.82e-04 | grad 2.06 | tok/s 16679
step    340 | loss 2.0209 | lr 3.82e-04 | grad 2.14 | tok/s 16627
step    350 | loss 1.6589 | lr 3.82e-04 | grad 2.06 | tok/s 17112
step    360 | loss 1.5610 | lr 3.82e-04 | grad 1.84 | tok/s 16374
step    370 | loss 1.4482 | lr 3.82e-04 | grad 1.88 | tok/s 17245

Training complete! Final step: 371
