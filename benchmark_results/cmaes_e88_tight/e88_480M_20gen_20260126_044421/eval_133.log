Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_133/levelE88_100m_20260126_053729
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 477,074,000 parameters
Using schedule-free AdamW (lr=0.000597168306188873)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.3934 | lr 5.97e-04 | grad 12.88 | tok/s 8799
step     20 | loss 3.4401 | lr 5.97e-04 | grad 6.72 | tok/s 15791
step     30 | loss 3.3171 | lr 5.97e-04 | grad 8.38 | tok/s 16640
step     40 | loss 4.7628 | lr 5.97e-04 | grad 27.50 | tok/s 16910
step     50 | loss 4.1162 | lr 5.97e-04 | grad 14.25 | tok/s 17066
step     60 | loss 3.2659 | lr 5.97e-04 | grad 6.06 | tok/s 16982
step     70 | loss 2.8454 | lr 5.97e-04 | grad 4.34 | tok/s 16989
step     80 | loss 2.5918 | lr 5.97e-04 | grad 3.83 | tok/s 16917
step     90 | loss 2.5297 | lr 5.97e-04 | grad 3.22 | tok/s 16912
step    100 | loss 2.1358 | lr 5.97e-04 | grad 3.36 | tok/s 16900
step    110 | loss 2.1807 | lr 5.97e-04 | grad 4.47 | tok/s 16734
step    120 | loss 2.7487 | lr 5.97e-04 | grad 3.05 | tok/s 15942
step    130 | loss 2.0500 | lr 5.97e-04 | grad 5.19 | tok/s 16210
step    140 | loss 2.3171 | lr 5.97e-04 | grad 6.44 | tok/s 16332
step    150 | loss 1.5781 | lr 5.97e-04 | grad 5.88 | tok/s 16758
step    160 | loss 2.2173 | lr 5.97e-04 | grad 2.44 | tok/s 16254
step    170 | loss 2.2758 | lr 5.97e-04 | grad 2.19 | tok/s 15973
step    180 | loss 1.6938 | lr 5.97e-04 | grad 3.12 | tok/s 16382
step    190 | loss 1.8509 | lr 5.97e-04 | grad 2.83 | tok/s 16012
step    200 | loss 1.5580 | lr 5.97e-04 | grad 1.99 | tok/s 16675
step    210 | loss 1.8134 | lr 5.97e-04 | grad 6.81 | tok/s 15840
step    220 | loss 2.1167 | lr 5.97e-04 | grad 3.02 | tok/s 16049
step    230 | loss 1.9145 | lr 5.97e-04 | grad 2.23 | tok/s 14808
step    240 | loss 2.1856 | lr 5.97e-04 | grad 4.31 | tok/s 16214
step    250 | loss 1.7099 | lr 5.97e-04 | grad 2.11 | tok/s 16142
step    260 | loss 1.8164 | lr 5.97e-04 | grad 2.83 | tok/s 16614
step    270 | loss 1.7552 | lr 5.97e-04 | grad 2.12 | tok/s 16202
step    280 | loss 1.7263 | lr 5.97e-04 | grad 1.64 | tok/s 15220
step    290 | loss 1.6199 | lr 5.97e-04 | grad 2.03 | tok/s 15678
step    300 | loss 1.9187 | lr 5.97e-04 | grad 2.06 | tok/s 15852
step    310 | loss 1.6202 | lr 5.97e-04 | grad 1.84 | tok/s 15805
step    320 | loss 1.8273 | lr 5.97e-04 | grad 2.75 | tok/s 15996
step    330 | loss 1.6677 | lr 5.97e-04 | grad 2.08 | tok/s 16179
step    340 | loss 1.9841 | lr 5.97e-04 | grad 1.89 | tok/s 16115
step    350 | loss 1.6056 | lr 5.97e-04 | grad 1.84 | tok/s 16547

Training complete! Final step: 359
