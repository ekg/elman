Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_2/levelE88_100m_20260126_044428
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 480,928,392 parameters
Using schedule-free AdamW (lr=0.0003366515691411306)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.6494 | lr 3.37e-04 | grad 9.50 | tok/s 4674
step     20 | loss 2.6283 | lr 3.37e-04 | grad 4.56 | tok/s 8678
step     30 | loss 2.5599 | lr 3.37e-04 | grad 1.88 | tok/s 8756
step     40 | loss 2.3654 | lr 3.37e-04 | grad 2.11 | tok/s 8378
step     50 | loss 3.1466 | lr 3.37e-04 | grad 13.19 | tok/s 8481
step     60 | loss 2.1512 | lr 3.37e-04 | grad 3.89 | tok/s 8745
step     70 | loss 2.0103 | lr 3.37e-04 | grad 3.12 | tok/s 8882
step     80 | loss 5.0627 | lr 3.37e-04 | grad 98.50 | tok/s 8577
step     90 | loss 5.2567 | lr 3.37e-04 | grad 9.25 | tok/s 9040
step    100 | loss 4.6987 | lr 3.37e-04 | grad 12.81 | tok/s 9032
step    110 | loss 4.3491 | lr 3.37e-04 | grad 20.25 | tok/s 9019
step    120 | loss 3.9109 | lr 3.37e-04 | grad 23.62 | tok/s 9010
step    130 | loss 3.5997 | lr 3.37e-04 | grad 25.00 | tok/s 9022
step    140 | loss 2.8249 | lr 3.37e-04 | grad 13.31 | tok/s 8982
step    150 | loss 3.2904 | lr 3.37e-04 | grad 14.25 | tok/s 8979
step    160 | loss 2.4790 | lr 3.37e-04 | grad 12.25 | tok/s 8994
step    170 | loss 2.5525 | lr 3.37e-04 | grad 9.56 | tok/s 8971
step    180 | loss 2.2562 | lr 3.37e-04 | grad 3.03 | tok/s 8971
step    190 | loss 2.4809 | lr 3.37e-04 | grad 3.02 | tok/s 8952
step    200 | loss 2.1681 | lr 3.37e-04 | grad 5.38 | tok/s 8626
step    210 | loss 2.1361 | lr 3.37e-04 | grad 4.84 | tok/s 8945
step    220 | loss 2.2445 | lr 3.37e-04 | grad 1.66 | tok/s 8834
step    230 | loss 2.1417 | lr 3.37e-04 | grad 1.92 | tok/s 8736
step    240 | loss 2.2613 | lr 3.37e-04 | grad 2.64 | tok/s 8292
step    250 | loss 2.1013 | lr 3.37e-04 | grad 1.34 | tok/s 8520
step    260 | loss 1.6281 | lr 3.37e-04 | grad 1.66 | tok/s 8793
step    270 | loss 2.1119 | lr 3.37e-04 | grad 1.40 | tok/s 8672
step    280 | loss 2.2661 | lr 3.37e-04 | grad 3.41 | tok/s 8496
step    290 | loss 1.5256 | lr 3.37e-04 | grad 2.28 | tok/s 8941
step    300 | loss 0.6017 | lr 3.37e-04 | grad 2.22 | tok/s 8938
step    310 | loss 2.4232 | lr 3.37e-04 | grad 2.50 | tok/s 8791
step    320 | loss 2.0081 | lr 3.37e-04 | grad 3.31 | tok/s 8597
step    330 | loss 1.9330 | lr 3.37e-04 | grad 1.63 | tok/s 8310
step    340 | loss 2.2417 | lr 3.37e-04 | grad 1.42 | tok/s 8427
step    350 | loss 1.9147 | lr 3.37e-04 | grad 2.53 | tok/s 8652
step    360 | loss 1.2814 | lr 3.37e-04 | grad 3.91 | tok/s 8548
step    370 | loss 1.8206 | lr 3.37e-04 | grad 1.52 | tok/s 8017
step    380 | loss 1.7705 | lr 3.37e-04 | grad 1.41 | tok/s 8547

Training complete! Final step: 383
