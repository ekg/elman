Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_21/levelE88_100m_20260126_045105
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,477,538 parameters
Using schedule-free AdamW (lr=0.00042037048786847163)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1631 | lr 4.20e-04 | grad 14.88 | tok/s 5706
step     20 | loss 2.6991 | lr 4.20e-04 | grad 5.56 | tok/s 13439
step     30 | loss 2.5527 | lr 4.20e-04 | grad 3.72 | tok/s 13610
step     40 | loss 2.3915 | lr 4.20e-04 | grad 2.95 | tok/s 12966
step     50 | loss 2.9372 | lr 4.20e-04 | grad 11.19 | tok/s 13199
step     60 | loss 2.0458 | lr 4.20e-04 | grad 4.44 | tok/s 13595
step     70 | loss 1.8675 | lr 4.20e-04 | grad 4.00 | tok/s 13706
step     80 | loss 6.5417 | lr 4.20e-04 | grad 97.50 | tok/s 13777
step     90 | loss 5.8504 | lr 4.20e-04 | grad 11.12 | tok/s 14039
step    100 | loss 4.2549 | lr 4.20e-04 | grad 7.28 | tok/s 14018
step    110 | loss 3.6707 | lr 4.20e-04 | grad 12.19 | tok/s 14027
step    120 | loss 3.2755 | lr 4.20e-04 | grad 10.50 | tok/s 14002
step    130 | loss 2.8525 | lr 4.20e-04 | grad 10.81 | tok/s 13968
step    140 | loss 2.7823 | lr 4.20e-04 | grad 7.16 | tok/s 13998
step    150 | loss 2.6454 | lr 4.20e-04 | grad 11.88 | tok/s 13935
step    160 | loss 2.3077 | lr 4.20e-04 | grad 8.06 | tok/s 13942
step    170 | loss 2.3488 | lr 4.20e-04 | grad 8.00 | tok/s 13953
step    180 | loss 2.1511 | lr 4.20e-04 | grad 4.28 | tok/s 13922
step    190 | loss 2.3205 | lr 4.20e-04 | grad 3.67 | tok/s 13914
step    200 | loss 1.9705 | lr 4.20e-04 | grad 3.55 | tok/s 13887
step    210 | loss 2.0023 | lr 4.20e-04 | grad 4.50 | tok/s 13833
step    220 | loss 2.0901 | lr 4.20e-04 | grad 2.73 | tok/s 13705
step    230 | loss 2.0020 | lr 4.20e-04 | grad 3.20 | tok/s 13588
step    240 | loss 2.2734 | lr 4.20e-04 | grad 3.89 | tok/s 12929
step    250 | loss 2.0664 | lr 4.20e-04 | grad 2.05 | tok/s 12267
step    260 | loss 1.5049 | lr 4.20e-04 | grad 2.34 | tok/s 13636
step    270 | loss 2.0373 | lr 4.20e-04 | grad 2.14 | tok/s 13504
step    280 | loss 2.2203 | lr 4.20e-04 | grad 5.22 | tok/s 13245
step    290 | loss 1.3486 | lr 4.20e-04 | grad 2.55 | tok/s 13916
step    300 | loss 0.5348 | lr 4.20e-04 | grad 2.56 | tok/s 13903
step    310 | loss 2.3186 | lr 4.20e-04 | grad 3.00 | tok/s 13701
step    320 | loss 1.8480 | lr 4.20e-04 | grad 4.22 | tok/s 13422
step    330 | loss 1.9040 | lr 4.20e-04 | grad 2.17 | tok/s 12967
step    340 | loss 2.2170 | lr 4.20e-04 | grad 2.11 | tok/s 13187
step    350 | loss 1.7976 | lr 4.20e-04 | grad 3.50 | tok/s 13509
step    360 | loss 1.1228 | lr 4.20e-04 | grad 6.53 | tok/s 13818
step    370 | loss 1.7504 | lr 4.20e-04 | grad 1.91 | tok/s 12527
step    380 | loss 1.7208 | lr 4.20e-04 | grad 1.88 | tok/s 13344
step    390 | loss 1.4932 | lr 4.20e-04 | grad 1.66 | tok/s 13935
step    400 | loss 1.4564 | lr 4.20e-04 | grad 1.92 | tok/s 13815
step    410 | loss 1.2320 | lr 4.20e-04 | grad 1.49 | tok/s 13509
step    420 | loss 1.7815 | lr 4.20e-04 | grad 3.33 | tok/s 12364
step    430 | loss 2.1049 | lr 4.20e-04 | grad 2.27 | tok/s 13736
step    440 | loss 2.1147 | lr 4.20e-04 | grad 3.05 | tok/s 13018
step    450 | loss 1.9348 | lr 4.20e-04 | grad 2.00 | tok/s 13470
step    460 | loss 1.6900 | lr 4.20e-04 | grad 2.31 | tok/s 13144
step    470 | loss 1.7882 | lr 4.20e-04 | grad 1.85 | tok/s 13552
step    480 | loss 2.1576 | lr 4.20e-04 | grad 4.84 | tok/s 13584
step    490 | loss 1.7534 | lr 4.20e-04 | grad 1.89 | tok/s 12837
step    500 | loss 1.6356 | lr 4.20e-04 | grad 2.50 | tok/s 13707
step    510 | loss 1.6722 | lr 4.20e-04 | grad 1.83 | tok/s 13897
step    520 | loss 1.6127 | lr 4.20e-04 | grad 1.57 | tok/s 13856
step    530 | loss 1.8498 | lr 4.20e-04 | grad 1.87 | tok/s 13340
step    540 | loss 1.7008 | lr 4.20e-04 | grad 1.70 | tok/s 12702
step    550 | loss 1.5465 | lr 4.20e-04 | grad 2.19 | tok/s 13073
step    560 | loss 1.6898 | lr 4.20e-04 | grad 1.98 | tok/s 12733
step    570 | loss 1.6193 | lr 4.20e-04 | grad 2.66 | tok/s 13100
step    580 | loss 1.5188 | lr 4.20e-04 | grad 1.63 | tok/s 13061
step    590 | loss 1.8111 | lr 4.20e-04 | grad 2.28 | tok/s 13389

Training complete! Final step: 592
