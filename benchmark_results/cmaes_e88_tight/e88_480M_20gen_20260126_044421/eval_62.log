Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_62/levelE88_100m_20260126_050737
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 497,709,440 parameters
Using schedule-free AdamW (lr=0.000587810174307426)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2003 | lr 5.88e-04 | grad 9.81 | tok/s 5584
step     20 | loss 2.6700 | lr 5.88e-04 | grad 3.86 | tok/s 12866
step     30 | loss 2.5219 | lr 5.88e-04 | grad 2.80 | tok/s 12957
step     40 | loss 2.3765 | lr 5.88e-04 | grad 2.17 | tok/s 12394
step     50 | loss 2.9239 | lr 5.88e-04 | grad 7.88 | tok/s 12604
step     60 | loss 2.0566 | lr 5.88e-04 | grad 7.03 | tok/s 12959
step     70 | loss 1.8443 | lr 5.88e-04 | grad 2.67 | tok/s 13098
step     80 | loss 5.4152 | lr 5.88e-04 | grad 24.88 | tok/s 13164
step     90 | loss 4.1748 | lr 5.88e-04 | grad 3.47 | tok/s 13360
step    100 | loss 3.8701 | lr 5.88e-04 | grad 3.11 | tok/s 13325
step    110 | loss 3.0737 | lr 5.88e-04 | grad 10.69 | tok/s 13299
step    120 | loss 2.6943 | lr 5.88e-04 | grad 3.23 | tok/s 13266
step    130 | loss 2.4708 | lr 5.88e-04 | grad 4.00 | tok/s 12207
step    140 | loss 2.3637 | lr 5.88e-04 | grad 2.66 | tok/s 13204
step    150 | loss 2.2515 | lr 5.88e-04 | grad 2.78 | tok/s 13185
step    160 | loss 2.0493 | lr 5.88e-04 | grad 4.69 | tok/s 13171
step    170 | loss 2.0467 | lr 5.88e-04 | grad 4.00 | tok/s 13120
step    180 | loss 1.9141 | lr 5.88e-04 | grad 2.80 | tok/s 13096
step    190 | loss 2.0667 | lr 5.88e-04 | grad 6.78 | tok/s 13055
step    200 | loss 1.8195 | lr 5.88e-04 | grad 1.83 | tok/s 13053
step    210 | loss 1.8487 | lr 5.88e-04 | grad 2.61 | tok/s 13029
step    220 | loss 1.9939 | lr 5.88e-04 | grad 2.22 | tok/s 12853
step    230 | loss 2.0245 | lr 5.88e-04 | grad 1.72 | tok/s 12672
step    240 | loss 2.2166 | lr 5.88e-04 | grad 2.48 | tok/s 12058
step    250 | loss 1.9959 | lr 5.88e-04 | grad 1.37 | tok/s 12338
step    260 | loss 1.4575 | lr 5.88e-04 | grad 1.69 | tok/s 12741
step    270 | loss 1.9894 | lr 5.88e-04 | grad 1.63 | tok/s 12558
step    280 | loss 2.1740 | lr 5.88e-04 | grad 3.38 | tok/s 12330
step    290 | loss 1.3684 | lr 5.88e-04 | grad 3.92 | tok/s 12963
step    300 | loss 0.5474 | lr 5.88e-04 | grad 1.74 | tok/s 12947
step    310 | loss 2.2704 | lr 5.88e-04 | grad 2.28 | tok/s 12754
step    320 | loss 1.7782 | lr 5.88e-04 | grad 2.84 | tok/s 12477
step    330 | loss 1.8336 | lr 5.88e-04 | grad 1.60 | tok/s 12067
step    340 | loss 2.1375 | lr 5.88e-04 | grad 1.45 | tok/s 12219
step    350 | loss 1.7443 | lr 5.88e-04 | grad 1.55 | tok/s 12500
step    360 | loss 1.1033 | lr 5.88e-04 | grad 3.52 | tok/s 12806
step    370 | loss 1.7075 | lr 5.88e-04 | grad 1.30 | tok/s 11589
step    380 | loss 1.6584 | lr 5.88e-04 | grad 1.39 | tok/s 12364
step    390 | loss 1.4416 | lr 5.88e-04 | grad 1.22 | tok/s 12920
step    400 | loss 1.4131 | lr 5.88e-04 | grad 1.32 | tok/s 12773
step    410 | loss 1.2125 | lr 5.88e-04 | grad 1.11 | tok/s 12524
step    420 | loss 1.7216 | lr 5.88e-04 | grad 2.23 | tok/s 11932
step    430 | loss 2.0108 | lr 5.88e-04 | grad 1.56 | tok/s 12724
step    440 | loss 2.0597 | lr 5.88e-04 | grad 1.93 | tok/s 11981
step    450 | loss 1.7782 | lr 5.88e-04 | grad 1.27 | tok/s 12409
step    460 | loss 1.6268 | lr 5.88e-04 | grad 1.65 | tok/s 12137
step    470 | loss 1.7227 | lr 5.88e-04 | grad 1.43 | tok/s 12530
step    480 | loss 2.0579 | lr 5.88e-04 | grad 3.34 | tok/s 12515
step    490 | loss 1.6934 | lr 5.88e-04 | grad 1.37 | tok/s 11837
step    500 | loss 1.5814 | lr 5.88e-04 | grad 1.80 | tok/s 12638
step    510 | loss 1.6088 | lr 5.88e-04 | grad 1.28 | tok/s 12798
step    520 | loss 1.5708 | lr 5.88e-04 | grad 1.06 | tok/s 12779
step    530 | loss 1.7715 | lr 5.88e-04 | grad 1.22 | tok/s 12283
step    540 | loss 1.6373 | lr 5.88e-04 | grad 1.23 | tok/s 12279
step    550 | loss 1.4975 | lr 5.88e-04 | grad 1.34 | tok/s 11463

Training complete! Final step: 554
