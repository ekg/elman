Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_96/levelE88_100m_20260126_052051
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 495,486,608 parameters
Using schedule-free AdamW (lr=0.00046915382520966827)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1598 | lr 4.69e-04 | grad 7.91 | tok/s 5325
step     20 | loss 2.6236 | lr 4.69e-04 | grad 2.14 | tok/s 10643
step     30 | loss 2.4993 | lr 4.69e-04 | grad 2.38 | tok/s 10800
step     40 | loss 2.3438 | lr 4.69e-04 | grad 2.45 | tok/s 10287
step     50 | loss 2.8988 | lr 4.69e-04 | grad 8.69 | tok/s 10445
step     60 | loss 2.0512 | lr 4.69e-04 | grad 3.67 | tok/s 10753
step     70 | loss 1.8468 | lr 4.69e-04 | grad 2.58 | tok/s 10867
step     80 | loss 5.1000 | lr 4.69e-04 | grad 26.00 | tok/s 10904
step     90 | loss 4.4070 | lr 4.69e-04 | grad 3.92 | tok/s 11073
step    100 | loss 3.8435 | lr 4.69e-04 | grad 3.67 | tok/s 11110
step    110 | loss 3.1477 | lr 4.69e-04 | grad 5.72 | tok/s 11058
step    120 | loss 2.7907 | lr 4.69e-04 | grad 4.72 | tok/s 11053
step    130 | loss 2.5040 | lr 4.69e-04 | grad 3.42 | tok/s 11018
step    140 | loss 2.3428 | lr 4.69e-04 | grad 2.67 | tok/s 11014
step    150 | loss 2.3001 | lr 4.69e-04 | grad 2.73 | tok/s 11003
step    160 | loss 2.0726 | lr 4.69e-04 | grad 4.34 | tok/s 10990
step    170 | loss 2.0794 | lr 4.69e-04 | grad 4.19 | tok/s 10992
step    180 | loss 1.9369 | lr 4.69e-04 | grad 2.62 | tok/s 10993
step    190 | loss 2.0495 | lr 4.69e-04 | grad 2.53 | tok/s 10951
step    200 | loss 1.8118 | lr 4.69e-04 | grad 2.16 | tok/s 10958
step    210 | loss 1.8585 | lr 4.69e-04 | grad 3.08 | tok/s 10963
step    220 | loss 1.9651 | lr 4.69e-04 | grad 1.95 | tok/s 10816
step    230 | loss 2.0212 | lr 4.69e-04 | grad 2.72 | tok/s 10676
step    240 | loss 2.2032 | lr 4.69e-04 | grad 2.52 | tok/s 10147
step    250 | loss 2.0069 | lr 4.69e-04 | grad 1.39 | tok/s 10440
step    260 | loss 1.4747 | lr 4.69e-04 | grad 1.60 | tok/s 10777
step    270 | loss 2.0013 | lr 4.69e-04 | grad 1.52 | tok/s 10632
step    280 | loss 2.1862 | lr 4.69e-04 | grad 3.64 | tok/s 10438
step    290 | loss 1.2863 | lr 4.69e-04 | grad 2.62 | tok/s 10963
step    300 | loss 0.5224 | lr 4.69e-04 | grad 2.03 | tok/s 10959
step    310 | loss 2.2669 | lr 4.69e-04 | grad 2.34 | tok/s 10767
step    320 | loss 1.8082 | lr 4.69e-04 | grad 2.94 | tok/s 10547
step    330 | loss 1.8472 | lr 4.69e-04 | grad 1.56 | tok/s 10231
step    340 | loss 2.1514 | lr 4.69e-04 | grad 1.44 | tok/s 10346
step    350 | loss 1.7776 | lr 4.69e-04 | grad 2.06 | tok/s 10600
step    360 | loss 1.1279 | lr 4.69e-04 | grad 4.50 | tok/s 10820
step    370 | loss 1.7120 | lr 4.69e-04 | grad 1.34 | tok/s 9821
step    380 | loss 1.6700 | lr 4.69e-04 | grad 1.38 | tok/s 10462
step    390 | loss 1.4593 | lr 4.69e-04 | grad 1.12 | tok/s 10919
step    400 | loss 1.4224 | lr 4.69e-04 | grad 1.29 | tok/s 9745
step    410 | loss 1.2131 | lr 4.69e-04 | grad 1.07 | tok/s 10563
step    420 | loss 1.7274 | lr 4.69e-04 | grad 2.30 | tok/s 10109
step    430 | loss 2.0372 | lr 4.69e-04 | grad 1.57 | tok/s 10767
step    440 | loss 2.0558 | lr 4.69e-04 | grad 2.09 | tok/s 10163
step    450 | loss 1.7868 | lr 4.69e-04 | grad 1.38 | tok/s 10524
step    460 | loss 1.6412 | lr 4.69e-04 | grad 1.60 | tok/s 10307

Training complete! Final step: 468
