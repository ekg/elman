Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_118/levelE88_100m_20260126_053049
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 488,095,796 parameters
Using schedule-free AdamW (lr=0.0004291866846941655)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.0236 | lr 4.29e-04 | grad 9.31 | tok/s 5659
step     20 | loss 2.6327 | lr 4.29e-04 | grad 3.47 | tok/s 12717
step     30 | loss 2.4890 | lr 4.29e-04 | grad 3.16 | tok/s 12868
step     40 | loss 2.3681 | lr 4.29e-04 | grad 2.81 | tok/s 12351
step     50 | loss 2.9336 | lr 4.29e-04 | grad 9.75 | tok/s 12546
step     60 | loss 2.0294 | lr 4.29e-04 | grad 7.94 | tok/s 12902
step     70 | loss 1.8159 | lr 4.29e-04 | grad 3.03 | tok/s 13069
step     80 | loss 5.5627 | lr 4.29e-04 | grad 30.88 | tok/s 13146
step     90 | loss 4.5902 | lr 4.29e-04 | grad 5.69 | tok/s 13347
step    100 | loss 3.7096 | lr 4.29e-04 | grad 4.31 | tok/s 13331
step    110 | loss 3.0860 | lr 4.29e-04 | grad 12.31 | tok/s 13330
step    120 | loss 2.8612 | lr 4.29e-04 | grad 5.16 | tok/s 13301
step    130 | loss 2.6061 | lr 4.29e-04 | grad 6.78 | tok/s 13281
step    140 | loss 2.4105 | lr 4.29e-04 | grad 4.25 | tok/s 13275
step    150 | loss 2.3129 | lr 4.29e-04 | grad 3.55 | tok/s 13260
step    160 | loss 2.0632 | lr 4.29e-04 | grad 4.94 | tok/s 13264
step    170 | loss 2.0900 | lr 4.29e-04 | grad 6.19 | tok/s 13257
step    180 | loss 1.9350 | lr 4.29e-04 | grad 2.66 | tok/s 13244
step    190 | loss 2.0722 | lr 4.29e-04 | grad 3.14 | tok/s 13266
step    200 | loss 1.8388 | lr 4.29e-04 | grad 2.30 | tok/s 13251
step    210 | loss 1.8681 | lr 4.29e-04 | grad 3.56 | tok/s 13267
step    220 | loss 1.9942 | lr 4.29e-04 | grad 2.28 | tok/s 13106
step    230 | loss 1.9385 | lr 4.29e-04 | grad 2.66 | tok/s 11838
step    240 | loss 2.2176 | lr 4.29e-04 | grad 3.03 | tok/s 12299
step    250 | loss 2.0201 | lr 4.29e-04 | grad 1.59 | tok/s 12625
step    260 | loss 1.4662 | lr 4.29e-04 | grad 1.95 | tok/s 13019
step    270 | loss 2.0079 | lr 4.29e-04 | grad 1.86 | tok/s 12818
step    280 | loss 2.1706 | lr 4.29e-04 | grad 3.38 | tok/s 12585
step    290 | loss 1.2960 | lr 4.29e-04 | grad 1.98 | tok/s 13245
step    300 | loss 0.5553 | lr 4.29e-04 | grad 2.45 | tok/s 13239
step    310 | loss 2.2950 | lr 4.29e-04 | grad 2.59 | tok/s 13014
step    320 | loss 1.8192 | lr 4.29e-04 | grad 3.48 | tok/s 12763
step    330 | loss 1.8618 | lr 4.29e-04 | grad 1.91 | tok/s 12346
step    340 | loss 2.1858 | lr 4.29e-04 | grad 1.76 | tok/s 12505
step    350 | loss 1.7735 | lr 4.29e-04 | grad 2.20 | tok/s 12819
step    360 | loss 1.1268 | lr 4.29e-04 | grad 4.75 | tok/s 13097
step    370 | loss 1.7286 | lr 4.29e-04 | grad 1.61 | tok/s 11895
step    380 | loss 1.6809 | lr 4.29e-04 | grad 1.59 | tok/s 12662
step    390 | loss 1.4673 | lr 4.29e-04 | grad 1.41 | tok/s 13203
step    400 | loss 1.4259 | lr 4.29e-04 | grad 1.64 | tok/s 13096
step    410 | loss 1.2186 | lr 4.29e-04 | grad 1.28 | tok/s 12814
step    420 | loss 1.7428 | lr 4.29e-04 | grad 2.78 | tok/s 11543
step    430 | loss 2.0529 | lr 4.29e-04 | grad 1.88 | tok/s 13005
step    440 | loss 2.0773 | lr 4.29e-04 | grad 2.47 | tok/s 12293
step    450 | loss 1.8461 | lr 4.29e-04 | grad 1.59 | tok/s 12716
step    460 | loss 1.6542 | lr 4.29e-04 | grad 2.20 | tok/s 12457
step    470 | loss 1.7549 | lr 4.29e-04 | grad 1.52 | tok/s 12853
step    480 | loss 2.1044 | lr 4.29e-04 | grad 3.95 | tok/s 12833
step    490 | loss 1.7154 | lr 4.29e-04 | grad 1.59 | tok/s 12116
step    500 | loss 1.6030 | lr 4.29e-04 | grad 2.17 | tok/s 12956
step    510 | loss 1.6368 | lr 4.29e-04 | grad 1.50 | tok/s 13121
step    520 | loss 1.5929 | lr 4.29e-04 | grad 1.31 | tok/s 13087
step    530 | loss 1.8157 | lr 4.29e-04 | grad 1.52 | tok/s 12584
step    540 | loss 1.6696 | lr 4.29e-04 | grad 1.38 | tok/s 12618
step    550 | loss 1.5193 | lr 4.29e-04 | grad 1.77 | tok/s 12351
step    560 | loss 1.6551 | lr 4.29e-04 | grad 1.73 | tok/s 11351

Training complete! Final step: 562
