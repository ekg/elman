Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_82/levelE88_100m_20260126_051732
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 485,207,266 parameters
Using schedule-free AdamW (lr=0.0005155438633642565)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.2203 | lr 5.16e-04 | grad 17.50 | tok/s 5920
step     20 | loss 3.0282 | lr 5.16e-04 | grad 7.03 | tok/s 15256
step     30 | loss 2.6979 | lr 5.16e-04 | grad 6.41 | tok/s 15413
step     40 | loss 2.4717 | lr 5.16e-04 | grad 4.56 | tok/s 14792
step     50 | loss 3.1583 | lr 5.16e-04 | grad 11.62 | tok/s 14980
step     60 | loss 2.0830 | lr 5.16e-04 | grad 2.84 | tok/s 15470
step     70 | loss 1.8037 | lr 5.16e-04 | grad 3.69 | tok/s 15664
step     80 | loss 6.4186 | lr 5.16e-04 | grad 47.25 | tok/s 15786
step     90 | loss 4.7534 | lr 5.16e-04 | grad 6.19 | tok/s 16019
step    100 | loss 4.1055 | lr 5.16e-04 | grad 4.59 | tok/s 16063
step    110 | loss 3.3423 | lr 5.16e-04 | grad 11.94 | tok/s 16004
step    120 | loss 3.0688 | lr 5.16e-04 | grad 6.72 | tok/s 15976
step    130 | loss 2.8856 | lr 5.16e-04 | grad 7.03 | tok/s 15978
step    140 | loss 2.6535 | lr 5.16e-04 | grad 4.69 | tok/s 15980
step    150 | loss 2.7352 | lr 5.16e-04 | grad 12.12 | tok/s 15981
step    160 | loss 2.3321 | lr 5.16e-04 | grad 4.81 | tok/s 15973
step    170 | loss 2.3127 | lr 5.16e-04 | grad 7.09 | tok/s 15987
step    180 | loss 2.1288 | lr 5.16e-04 | grad 5.25 | tok/s 15976
step    190 | loss 2.2180 | lr 5.16e-04 | grad 9.81 | tok/s 15984
step    200 | loss 1.9465 | lr 5.16e-04 | grad 3.62 | tok/s 15985
step    210 | loss 2.0051 | lr 5.16e-04 | grad 4.62 | tok/s 15985
step    220 | loss 2.0564 | lr 5.16e-04 | grad 3.23 | tok/s 15781
step    230 | loss 2.1207 | lr 5.16e-04 | grad 2.92 | tok/s 15608
step    240 | loss 2.2616 | lr 5.16e-04 | grad 4.31 | tok/s 14809
step    250 | loss 2.0448 | lr 5.16e-04 | grad 2.25 | tok/s 15218
step    260 | loss 1.4599 | lr 5.16e-04 | grad 2.56 | tok/s 15718
step    270 | loss 2.0019 | lr 5.16e-04 | grad 2.50 | tok/s 15506
step    280 | loss 2.1744 | lr 5.16e-04 | grad 5.19 | tok/s 15207
step    290 | loss 1.4334 | lr 5.16e-04 | grad 2.38 | tok/s 15989
step    300 | loss 0.5555 | lr 5.16e-04 | grad 2.50 | tok/s 15982
step    310 | loss 2.2965 | lr 5.16e-04 | grad 3.22 | tok/s 15720
step    320 | loss 1.8160 | lr 5.16e-04 | grad 4.31 | tok/s 15404
step    330 | loss 1.8836 | lr 5.16e-04 | grad 2.19 | tok/s 14878
step    340 | loss 2.2068 | lr 5.16e-04 | grad 2.42 | tok/s 15114
step    350 | loss 1.7488 | lr 5.16e-04 | grad 2.47 | tok/s 15492
step    360 | loss 1.1130 | lr 5.16e-04 | grad 5.16 | tok/s 15843
step    370 | loss 1.7271 | lr 5.16e-04 | grad 2.03 | tok/s 14351
step    380 | loss 1.6853 | lr 5.16e-04 | grad 2.25 | tok/s 15282
step    390 | loss 1.4653 | lr 5.16e-04 | grad 2.06 | tok/s 14619
step    400 | loss 1.4301 | lr 5.16e-04 | grad 2.20 | tok/s 15842
step    410 | loss 1.2147 | lr 5.16e-04 | grad 1.70 | tok/s 15500
step    420 | loss 1.7528 | lr 5.16e-04 | grad 3.25 | tok/s 14782
step    430 | loss 2.0492 | lr 5.16e-04 | grad 2.48 | tok/s 15712
step    440 | loss 2.0942 | lr 5.16e-04 | grad 2.83 | tok/s 14860
step    450 | loss 1.8769 | lr 5.16e-04 | grad 2.05 | tok/s 15399
step    460 | loss 1.6405 | lr 5.16e-04 | grad 2.30 | tok/s 15054
step    470 | loss 1.7526 | lr 5.16e-04 | grad 2.45 | tok/s 15524
step    480 | loss 2.1146 | lr 5.16e-04 | grad 4.12 | tok/s 15532
step    490 | loss 1.7184 | lr 5.16e-04 | grad 1.89 | tok/s 14657
step    500 | loss 1.6173 | lr 5.16e-04 | grad 2.94 | tok/s 15663
step    510 | loss 1.6417 | lr 5.16e-04 | grad 2.12 | tok/s 15863
step    520 | loss 1.5857 | lr 5.16e-04 | grad 1.55 | tok/s 15771
step    530 | loss 1.8084 | lr 5.16e-04 | grad 1.85 | tok/s 15223
step    540 | loss 1.6763 | lr 5.16e-04 | grad 2.16 | tok/s 15253
step    550 | loss 1.5258 | lr 5.16e-04 | grad 2.00 | tok/s 14941
step    560 | loss 1.6613 | lr 5.16e-04 | grad 2.06 | tok/s 14520
step    570 | loss 1.5942 | lr 5.16e-04 | grad 2.38 | tok/s 14931
step    580 | loss 1.4900 | lr 5.16e-04 | grad 1.85 | tok/s 14866
step    590 | loss 1.7679 | lr 5.16e-04 | grad 2.20 | tok/s 15246
step    600 | loss 1.7731 | lr 5.16e-04 | grad 1.57 | tok/s 14686
step    610 | loss 1.5616 | lr 5.16e-04 | grad 2.05 | tok/s 15475
step    620 | loss 1.5056 | lr 5.16e-04 | grad 1.79 | tok/s 14673
step    630 | loss 1.5918 | lr 5.16e-04 | grad 3.31 | tok/s 14779
step    640 | loss 1.7420 | lr 5.16e-04 | grad 1.77 | tok/s 15148
step    650 | loss 1.6141 | lr 5.16e-04 | grad 2.09 | tok/s 15256
step    660 | loss 1.6279 | lr 5.16e-04 | grad 1.56 | tok/s 15307
step    670 | loss 1.8206 | lr 5.16e-04 | grad 15.00 | tok/s 15395

Training complete! Final step: 676
