Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_58/levelE88_100m_20260126_050736
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 482,534,272 parameters
Using schedule-free AdamW (lr=0.00047749702647774855)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1819 | lr 4.77e-04 | grad 15.06 | tok/s 5936
step     20 | loss 2.8392 | lr 4.77e-04 | grad 6.09 | tok/s 16004
step     30 | loss 2.5125 | lr 4.77e-04 | grad 6.28 | tok/s 16195
step     40 | loss 2.3786 | lr 4.77e-04 | grad 4.78 | tok/s 15525
step     50 | loss 3.0104 | lr 4.77e-04 | grad 12.44 | tok/s 15838
step     60 | loss 2.0391 | lr 4.77e-04 | grad 3.27 | tok/s 16296
step     70 | loss 1.7902 | lr 4.77e-04 | grad 4.16 | tok/s 16505
step     80 | loss 6.5485 | lr 4.77e-04 | grad 49.50 | tok/s 16575
step     90 | loss 4.9950 | lr 4.77e-04 | grad 6.78 | tok/s 16879
step    100 | loss 3.9897 | lr 4.77e-04 | grad 4.75 | tok/s 16841
step    110 | loss 3.3080 | lr 4.77e-04 | grad 20.88 | tok/s 16826
step    120 | loss 3.0349 | lr 4.77e-04 | grad 5.72 | tok/s 16832
step    130 | loss 3.0262 | lr 4.77e-04 | grad 9.44 | tok/s 16816
step    140 | loss 2.6612 | lr 4.77e-04 | grad 6.38 | tok/s 16784
step    150 | loss 2.7174 | lr 4.77e-04 | grad 10.81 | tok/s 16760
step    160 | loss 2.3283 | lr 4.77e-04 | grad 6.56 | tok/s 16768
step    170 | loss 2.3058 | lr 4.77e-04 | grad 8.25 | tok/s 15129
step    180 | loss 2.1589 | lr 4.77e-04 | grad 4.84 | tok/s 16801
step    190 | loss 2.2511 | lr 4.77e-04 | grad 9.44 | tok/s 16743
step    200 | loss 1.9746 | lr 4.77e-04 | grad 3.62 | tok/s 16739
step    210 | loss 2.0038 | lr 4.77e-04 | grad 5.28 | tok/s 16738
step    220 | loss 2.0470 | lr 4.77e-04 | grad 3.30 | tok/s 16528
step    230 | loss 2.0203 | lr 4.77e-04 | grad 3.16 | tok/s 16316
step    240 | loss 2.2568 | lr 4.77e-04 | grad 4.25 | tok/s 15508
step    250 | loss 2.0518 | lr 4.77e-04 | grad 2.41 | tok/s 15931
step    260 | loss 1.4698 | lr 4.77e-04 | grad 2.66 | tok/s 16420
step    270 | loss 2.0195 | lr 4.77e-04 | grad 2.70 | tok/s 16209
step    280 | loss 2.1801 | lr 4.77e-04 | grad 4.75 | tok/s 15902
step    290 | loss 1.4168 | lr 4.77e-04 | grad 2.80 | tok/s 16726
step    300 | loss 0.5487 | lr 4.77e-04 | grad 3.91 | tok/s 16707
step    310 | loss 2.3313 | lr 4.77e-04 | grad 3.50 | tok/s 15648
step    320 | loss 1.8300 | lr 4.77e-04 | grad 4.81 | tok/s 16089
step    330 | loss 1.8912 | lr 4.77e-04 | grad 2.34 | tok/s 15546
step    340 | loss 2.2122 | lr 4.77e-04 | grad 2.62 | tok/s 15796
step    350 | loss 1.7448 | lr 4.77e-04 | grad 2.81 | tok/s 16193
step    360 | loss 1.0813 | lr 4.77e-04 | grad 5.66 | tok/s 16554
step    370 | loss 1.7308 | lr 4.77e-04 | grad 2.02 | tok/s 15014
step    380 | loss 1.7068 | lr 4.77e-04 | grad 2.31 | tok/s 15983
step    390 | loss 1.4812 | lr 4.77e-04 | grad 2.38 | tok/s 16690
step    400 | loss 1.4367 | lr 4.77e-04 | grad 2.33 | tok/s 16549
step    410 | loss 1.2219 | lr 4.77e-04 | grad 1.73 | tok/s 16181
step    420 | loss 1.7579 | lr 4.77e-04 | grad 3.48 | tok/s 15447
step    430 | loss 2.0675 | lr 4.77e-04 | grad 2.61 | tok/s 16462
step    440 | loss 2.0994 | lr 4.77e-04 | grad 2.94 | tok/s 15559
step    450 | loss 1.9035 | lr 4.77e-04 | grad 2.16 | tok/s 16100
step    460 | loss 1.6423 | lr 4.77e-04 | grad 2.77 | tok/s 15788
step    470 | loss 1.7618 | lr 4.77e-04 | grad 2.42 | tok/s 16271
step    480 | loss 2.1077 | lr 4.77e-04 | grad 4.66 | tok/s 16298
step    490 | loss 1.7289 | lr 4.77e-04 | grad 2.03 | tok/s 15374
step    500 | loss 1.6165 | lr 4.77e-04 | grad 3.05 | tok/s 16416
step    510 | loss 1.6478 | lr 4.77e-04 | grad 2.22 | tok/s 16635
step    520 | loss 1.5932 | lr 4.77e-04 | grad 1.78 | tok/s 16621
step    530 | loss 1.8245 | lr 4.77e-04 | grad 1.94 | tok/s 15969
step    540 | loss 1.6839 | lr 4.77e-04 | grad 2.33 | tok/s 16017
step    550 | loss 1.5276 | lr 4.77e-04 | grad 2.25 | tok/s 15692
step    560 | loss 1.6641 | lr 4.77e-04 | grad 2.22 | tok/s 15273
step    570 | loss 1.5959 | lr 4.77e-04 | grad 2.61 | tok/s 15725
step    580 | loss 1.4978 | lr 4.77e-04 | grad 1.99 | tok/s 15635
step    590 | loss 1.7798 | lr 4.77e-04 | grad 2.33 | tok/s 16058
step    600 | loss 1.7777 | lr 4.77e-04 | grad 1.83 | tok/s 15520
step    610 | loss 1.5704 | lr 4.77e-04 | grad 2.03 | tok/s 16304
step    620 | loss 1.5096 | lr 4.77e-04 | grad 1.92 | tok/s 15436
step    630 | loss 1.5954 | lr 4.77e-04 | grad 3.28 | tok/s 15581
step    640 | loss 1.7445 | lr 4.77e-04 | grad 1.93 | tok/s 15989
step    650 | loss 1.6191 | lr 4.77e-04 | grad 2.39 | tok/s 16071
step    660 | loss 1.6368 | lr 4.77e-04 | grad 1.68 | tok/s 16142
step    670 | loss 1.8104 | lr 4.77e-04 | grad 3.31 | tok/s 16258
step    680 | loss 1.6755 | lr 4.77e-04 | grad 2.02 | tok/s 15930
step    690 | loss 1.7411 | lr 4.77e-04 | grad 2.78 | tok/s 16507
step    700 | loss 1.3224 | lr 4.77e-04 | grad 2.30 | tok/s 16791

Training complete! Final step: 708
