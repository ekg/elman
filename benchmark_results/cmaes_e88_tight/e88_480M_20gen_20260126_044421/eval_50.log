Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_50/levelE88_100m_20260126_050418
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 464,116,902 parameters
Using schedule-free AdamW (lr=0.0005203525222235179)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.0157 | lr 5.20e-04 | grad 7.47 | tok/s 7891
step     20 | loss 2.8047 | lr 5.20e-04 | grad 1.41 | tok/s 12440
step     30 | loss 3.0372 | lr 5.20e-04 | grad 3.23 | tok/s 13124
step     40 | loss 4.1062 | lr 5.20e-04 | grad 16.62 | tok/s 13341
step     50 | loss 3.8504 | lr 5.20e-04 | grad 7.53 | tok/s 13496
step     60 | loss 3.0760 | lr 5.20e-04 | grad 3.08 | tok/s 13472
step     70 | loss 2.5834 | lr 5.20e-04 | grad 2.94 | tok/s 13454
step     80 | loss 2.2856 | lr 5.20e-04 | grad 2.17 | tok/s 13434
step     90 | loss 2.1765 | lr 5.20e-04 | grad 2.16 | tok/s 13409
step    100 | loss 1.9676 | lr 5.20e-04 | grad 1.67 | tok/s 13423
step    110 | loss 2.0899 | lr 5.20e-04 | grad 2.61 | tok/s 13305
step    120 | loss 2.6656 | lr 5.20e-04 | grad 1.66 | tok/s 12651
step    130 | loss 2.0614 | lr 5.20e-04 | grad 3.03 | tok/s 12694
step    140 | loss 2.3026 | lr 5.20e-04 | grad 4.12 | tok/s 12997
step    150 | loss 1.4306 | lr 5.20e-04 | grad 3.89 | tok/s 13295
step    160 | loss 2.2332 | lr 5.20e-04 | grad 1.47 | tok/s 12871
step    170 | loss 2.2208 | lr 5.20e-04 | grad 1.20 | tok/s 12680
step    180 | loss 1.7418 | lr 5.20e-04 | grad 1.84 | tok/s 12980
step    190 | loss 1.8280 | lr 5.20e-04 | grad 1.77 | tok/s 12738
step    200 | loss 1.5620 | lr 5.20e-04 | grad 1.20 | tok/s 13324
step    210 | loss 1.8026 | lr 5.20e-04 | grad 3.34 | tok/s 12660
step    220 | loss 2.0941 | lr 5.20e-04 | grad 1.91 | tok/s 12790
step    230 | loss 1.8486 | lr 5.20e-04 | grad 1.54 | tok/s 12782
step    240 | loss 2.1340 | lr 5.20e-04 | grad 3.05 | tok/s 12950
step    250 | loss 1.6876 | lr 5.20e-04 | grad 1.02 | tok/s 12854
step    260 | loss 1.8027 | lr 5.20e-04 | grad 1.84 | tok/s 13212
step    270 | loss 1.7387 | lr 5.20e-04 | grad 1.31 | tok/s 12910
step    280 | loss 1.7008 | lr 5.20e-04 | grad 1.12 | tok/s 12134

Training complete! Final step: 287
