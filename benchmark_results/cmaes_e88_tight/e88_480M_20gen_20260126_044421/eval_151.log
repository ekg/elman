Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_151/levelE88_100m_20260126_054407
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 486,304,560 parameters
Using schedule-free AdamW (lr=0.0004480794793836773)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.4603 | lr 4.48e-04 | grad 18.50 | tok/s 6164
step     20 | loss 3.3365 | lr 4.48e-04 | grad 8.88 | tok/s 17355
step     30 | loss 2.9993 | lr 4.48e-04 | grad 7.28 | tok/s 17519
step     40 | loss 2.5045 | lr 4.48e-04 | grad 5.00 | tok/s 16762
step     50 | loss 3.1500 | lr 4.48e-04 | grad 11.31 | tok/s 16991
step     60 | loss 2.1509 | lr 4.48e-04 | grad 8.25 | tok/s 17495
step     70 | loss 1.8700 | lr 4.48e-04 | grad 4.56 | tok/s 17667
step     80 | loss 6.2395 | lr 4.48e-04 | grad 40.00 | tok/s 17742
step     90 | loss 5.0529 | lr 4.48e-04 | grad 6.84 | tok/s 17998
step    100 | loss 4.0679 | lr 4.48e-04 | grad 6.59 | tok/s 17979
step    110 | loss 3.5262 | lr 4.48e-04 | grad 30.38 | tok/s 17907
step    120 | loss 3.0648 | lr 4.48e-04 | grad 9.25 | tok/s 17913
step    130 | loss 3.0785 | lr 4.48e-04 | grad 9.12 | tok/s 17868
step    140 | loss 2.7842 | lr 4.48e-04 | grad 8.81 | tok/s 17877
step    150 | loss 2.7851 | lr 4.48e-04 | grad 11.44 | tok/s 17852
step    160 | loss 2.4251 | lr 4.48e-04 | grad 8.06 | tok/s 17817
step    170 | loss 2.3725 | lr 4.48e-04 | grad 8.81 | tok/s 17798
step    180 | loss 2.2600 | lr 4.48e-04 | grad 7.00 | tok/s 17792
step    190 | loss 2.4436 | lr 4.48e-04 | grad 13.31 | tok/s 17757
step    200 | loss 2.0144 | lr 4.48e-04 | grad 3.62 | tok/s 17745
step    210 | loss 2.1266 | lr 4.48e-04 | grad 5.38 | tok/s 17759
step    220 | loss 2.1170 | lr 4.48e-04 | grad 4.78 | tok/s 15964
step    230 | loss 2.1427 | lr 4.48e-04 | grad 4.03 | tok/s 17334
step    240 | loss 2.3094 | lr 4.48e-04 | grad 4.47 | tok/s 16473
step    250 | loss 2.0815 | lr 4.48e-04 | grad 2.80 | tok/s 16904
step    260 | loss 1.4890 | lr 4.48e-04 | grad 3.03 | tok/s 17452
step    270 | loss 2.0615 | lr 4.48e-04 | grad 3.39 | tok/s 17225
step    280 | loss 2.2022 | lr 4.48e-04 | grad 5.69 | tok/s 16910
step    290 | loss 1.4054 | lr 4.48e-04 | grad 11.31 | tok/s 17795
step    300 | loss 0.5611 | lr 4.48e-04 | grad 5.28 | tok/s 17765
step    310 | loss 2.3239 | lr 4.48e-04 | grad 4.91 | tok/s 17475
step    320 | loss 1.8365 | lr 4.48e-04 | grad 5.28 | tok/s 17105
step    330 | loss 1.9079 | lr 4.48e-04 | grad 2.70 | tok/s 16522
step    340 | loss 2.2544 | lr 4.48e-04 | grad 3.50 | tok/s 16792
step    350 | loss 1.7796 | lr 4.48e-04 | grad 2.69 | tok/s 17222
step    360 | loss 1.1489 | lr 4.48e-04 | grad 6.97 | tok/s 17593
step    370 | loss 1.7607 | lr 4.48e-04 | grad 2.88 | tok/s 15931
step    380 | loss 1.7009 | lr 4.48e-04 | grad 3.08 | tok/s 16976
step    390 | loss 1.4900 | lr 4.48e-04 | grad 2.69 | tok/s 17724
step    400 | loss 1.4477 | lr 4.48e-04 | grad 2.83 | tok/s 17581
step    410 | loss 1.2451 | lr 4.48e-04 | grad 2.06 | tok/s 17169
step    420 | loss 1.7725 | lr 4.48e-04 | grad 4.09 | tok/s 16419
step    430 | loss 2.1037 | lr 4.48e-04 | grad 3.30 | tok/s 17477
step    440 | loss 2.1263 | lr 4.48e-04 | grad 3.50 | tok/s 16523
step    450 | loss 2.0246 | lr 4.48e-04 | grad 2.45 | tok/s 17094
step    460 | loss 1.6778 | lr 4.48e-04 | grad 3.31 | tok/s 16710
step    470 | loss 1.7862 | lr 4.48e-04 | grad 3.02 | tok/s 17184
step    480 | loss 2.1427 | lr 4.48e-04 | grad 5.72 | tok/s 17187
step    490 | loss 1.7537 | lr 4.48e-04 | grad 2.58 | tok/s 16252
step    500 | loss 1.6386 | lr 4.48e-04 | grad 3.89 | tok/s 17316
step    510 | loss 1.6700 | lr 4.48e-04 | grad 2.95 | tok/s 17572
step    520 | loss 1.6152 | lr 4.48e-04 | grad 2.14 | tok/s 17551
step    530 | loss 1.8504 | lr 4.48e-04 | grad 2.25 | tok/s 16863
step    540 | loss 1.7012 | lr 4.48e-04 | grad 2.88 | tok/s 16864
step    550 | loss 1.5438 | lr 4.48e-04 | grad 2.69 | tok/s 16544
step    560 | loss 1.6923 | lr 4.48e-04 | grad 2.69 | tok/s 16119
step    570 | loss 1.6298 | lr 4.48e-04 | grad 3.00 | tok/s 16564
step    580 | loss 1.5214 | lr 4.48e-04 | grad 2.39 | tok/s 16503
step    590 | loss 1.7902 | lr 4.48e-04 | grad 2.80 | tok/s 16967
step    600 | loss 1.8059 | lr 4.48e-04 | grad 2.08 | tok/s 16378
step    610 | loss 1.5858 | lr 4.48e-04 | grad 2.78 | tok/s 17214
step    620 | loss 1.5285 | lr 4.48e-04 | grad 2.33 | tok/s 16304
step    630 | loss 1.6228 | lr 4.48e-04 | grad 4.16 | tok/s 16433
step    640 | loss 1.7610 | lr 4.48e-04 | grad 2.41 | tok/s 16886
step    650 | loss 1.6397 | lr 4.48e-04 | grad 2.97 | tok/s 16967
step    660 | loss 1.6580 | lr 4.48e-04 | grad 2.06 | tok/s 17026
step    670 | loss 1.8703 | lr 4.48e-04 | grad 2.88 | tok/s 17153
step    680 | loss 1.6964 | lr 4.48e-04 | grad 2.22 | tok/s 16803
step    690 | loss 1.7701 | lr 4.48e-04 | grad 2.95 | tok/s 17377
step    700 | loss 1.3260 | lr 4.48e-04 | grad 2.47 | tok/s 17711
step    710 | loss 1.5550 | lr 4.48e-04 | grad 2.59 | tok/s 16542
step    720 | loss 1.4413 | lr 4.48e-04 | grad 3.77 | tok/s 16318
step    730 | loss 1.2512 | lr 4.48e-04 | grad 2.81 | tok/s 17654
step    740 | loss 1.4564 | lr 4.48e-04 | grad 2.27 | tok/s 17474
step    750 | loss 1.1528 | lr 4.48e-04 | grad 2.55 | tok/s 17744

Training complete! Final step: 752
