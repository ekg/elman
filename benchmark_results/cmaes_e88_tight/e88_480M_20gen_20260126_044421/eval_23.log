Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_23/levelE88_100m_20260126_045104
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 492,177,828 parameters
Using schedule-free AdamW (lr=0.0004511365922420242)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 3.0 minutes
step     10 | loss 4.1914 | lr 4.51e-04 | grad 5.28 | tok/s 4504
step     20 | loss 2.6255 | lr 4.51e-04 | grad 3.23 | tok/s 7926
step     30 | loss 2.5091 | lr 4.51e-04 | grad 1.27 | tok/s 7997
step     40 | loss 2.3286 | lr 4.51e-04 | grad 1.59 | tok/s 7624
step     50 | loss 2.8708 | lr 4.51e-04 | grad 8.12 | tok/s 7722
step     60 | loss 2.0887 | lr 4.51e-04 | grad 5.41 | tok/s 7953
step     70 | loss 1.9546 | lr 4.51e-04 | grad 2.09 | tok/s 8049
step     80 | loss 4.9509 | lr 4.51e-04 | grad 41.25 | tok/s 8071
step     90 | loss 4.7745 | lr 4.51e-04 | grad 4.69 | tok/s 7924
step    100 | loss 3.9739 | lr 4.51e-04 | grad 3.64 | tok/s 8195
step    110 | loss 3.3875 | lr 4.51e-04 | grad 10.50 | tok/s 8168
step    120 | loss 2.9048 | lr 4.51e-04 | grad 4.50 | tok/s 8157
step    130 | loss 2.6300 | lr 4.51e-04 | grad 4.59 | tok/s 8163
step    140 | loss 2.3814 | lr 4.51e-04 | grad 2.67 | tok/s 8161
step    150 | loss 2.2844 | lr 4.51e-04 | grad 2.94 | tok/s 8164
step    160 | loss 2.0638 | lr 4.51e-04 | grad 2.86 | tok/s 8156
step    170 | loss 2.1002 | lr 4.51e-04 | grad 3.30 | tok/s 8167
step    180 | loss 1.9232 | lr 4.51e-04 | grad 2.45 | tok/s 7982
step    190 | loss 2.0780 | lr 4.51e-04 | grad 2.69 | tok/s 8176
step    200 | loss 1.8346 | lr 4.51e-04 | grad 1.69 | tok/s 8181
step    210 | loss 1.8724 | lr 4.51e-04 | grad 1.96 | tok/s 8182
step    220 | loss 2.0231 | lr 4.51e-04 | grad 1.48 | tok/s 8089
step    230 | loss 2.0631 | lr 4.51e-04 | grad 1.55 | tok/s 7988
step    240 | loss 2.2041 | lr 4.51e-04 | grad 2.02 | tok/s 7595
step    250 | loss 2.0391 | lr 4.51e-04 | grad 1.10 | tok/s 7829
step    260 | loss 1.5430 | lr 4.51e-04 | grad 1.34 | tok/s 8081
step    270 | loss 2.0181 | lr 4.51e-04 | grad 1.18 | tok/s 7752
step    280 | loss 2.2045 | lr 4.51e-04 | grad 2.53 | tok/s 7818
step    290 | loss 1.3821 | lr 4.51e-04 | grad 1.61 | tok/s 8217
step    300 | loss 0.5707 | lr 4.51e-04 | grad 1.34 | tok/s 8234
step    310 | loss 2.3585 | lr 4.51e-04 | grad 2.28 | tok/s 8091
step    320 | loss 1.9033 | lr 4.51e-04 | grad 2.44 | tok/s 7923
step    330 | loss 1.8607 | lr 4.51e-04 | grad 1.30 | tok/s 7654
step    340 | loss 2.1543 | lr 4.51e-04 | grad 1.14 | tok/s 7783
step    350 | loss 1.8071 | lr 4.51e-04 | grad 2.12 | tok/s 7985

Training complete! Final step: 350
