Using device: cuda
Output directory: benchmark_results/cmaes_e88_tight/e88_480M_20gen_20260126_044421/eval_1/levelE88_100m_20260126_044428
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88, 460,873,842 parameters
Using schedule-free AdamW (lr=0.0003793920540414954)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 3.0 minutes
step     10 | loss 4.4513 | lr 3.79e-04 | grad 7.84 | tok/s 6289
step     20 | loss 2.7302 | lr 3.79e-04 | grad 2.05 | tok/s 8467
step     30 | loss 3.0774 | lr 3.79e-04 | grad 5.03 | tok/s 8869
step     40 | loss 4.2327 | lr 3.79e-04 | grad 42.25 | tok/s 9010
step     50 | loss 4.7623 | lr 3.79e-04 | grad 17.00 | tok/s 9100
step     60 | loss 3.8770 | lr 3.79e-04 | grad 11.56 | tok/s 9089
step     70 | loss 2.9897 | lr 3.79e-04 | grad 5.84 | tok/s 9057
step     80 | loss 2.6039 | lr 3.79e-04 | grad 3.53 | tok/s 9040
step     90 | loss 2.3913 | lr 3.79e-04 | grad 2.88 | tok/s 8928
step    100 | loss 2.1560 | lr 3.79e-04 | grad 1.60 | tok/s 9029
step    110 | loss 2.2460 | lr 3.79e-04 | grad 1.90 | tok/s 8946
step    120 | loss 2.7101 | lr 3.79e-04 | grad 1.09 | tok/s 8525
step    130 | loss 2.1557 | lr 3.79e-04 | grad 3.30 | tok/s 8742
step    140 | loss 2.3928 | lr 3.79e-04 | grad 5.22 | tok/s 8757
step    150 | loss 1.4392 | lr 3.79e-04 | grad 3.22 | tok/s 8963
step    160 | loss 2.3428 | lr 3.79e-04 | grad 1.19 | tok/s 8674
step    170 | loss 2.2793 | lr 3.79e-04 | grad 0.96 | tok/s 8549
step    180 | loss 1.9131 | lr 3.79e-04 | grad 1.73 | tok/s 8752
step    190 | loss 1.9357 | lr 3.79e-04 | grad 1.27 | tok/s 8583

Training complete! Final step: 195
