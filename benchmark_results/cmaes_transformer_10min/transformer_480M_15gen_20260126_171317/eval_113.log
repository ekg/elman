Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_113/levelllama_100m_20260126_193723
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 447,161,856 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 6.0662 | lr 3.00e-04 | grad 4.53 | tok/s 6302
step     20 | loss 2.7668 | lr 3.00e-04 | grad 2.98 | tok/s 18256
step     30 | loss 2.8448 | lr 3.00e-04 | grad 2.30 | tok/s 18439
step     40 | loss 3.0385 | lr 3.00e-04 | grad 2.73 | tok/s 17656
step     50 | loss 3.2385 | lr 3.00e-04 | grad 2.86 | tok/s 17921
step     60 | loss 2.5766 | lr 3.00e-04 | grad 5.81 | tok/s 18483
step     70 | loss 2.5244 | lr 3.00e-04 | grad 1.77 | tok/s 18699
step     80 | loss 7.0648 | lr 3.00e-04 | grad 11.25 | tok/s 18783
step     90 | loss 5.0377 | lr 3.00e-04 | grad 3.41 | tok/s 19124
step    100 | loss 3.7572 | lr 3.00e-04 | grad 2.98 | tok/s 19111
step    110 | loss 3.5981 | lr 3.00e-04 | grad 6.59 | tok/s 19089
step    120 | loss 3.4466 | lr 3.00e-04 | grad 8.31 | tok/s 19048
step    130 | loss 3.3099 | lr 3.00e-04 | grad 3.80 | tok/s 19029
step    140 | loss 2.8418 | lr 3.00e-04 | grad 2.89 | tok/s 18979
step    150 | loss 3.0783 | lr 3.00e-04 | grad 5.78 | tok/s 18936
step    160 | loss 2.5933 | lr 3.00e-04 | grad 4.50 | tok/s 18893
step    170 | loss 2.6274 | lr 3.00e-04 | grad 4.28 | tok/s 18801
step    180 | loss 2.4151 | lr 3.00e-04 | grad 4.94 | tok/s 18776
step    190 | loss 2.6161 | lr 3.00e-04 | grad 2.58 | tok/s 18759
step    200 | loss 2.3345 | lr 3.00e-04 | grad 3.42 | tok/s 18703
step    210 | loss 2.3228 | lr 3.00e-04 | grad 3.19 | tok/s 18664
step    220 | loss 2.5962 | lr 3.00e-04 | grad 1.96 | tok/s 18410
step    230 | loss 3.4357 | lr 3.00e-04 | grad 3.28 | tok/s 18176
step    240 | loss 2.6456 | lr 3.00e-04 | grad 3.34 | tok/s 17256
step    250 | loss 2.4865 | lr 3.00e-04 | grad 2.33 | tok/s 17701
step    260 | loss 2.2858 | lr 3.00e-04 | grad 2.06 | tok/s 18243
step    270 | loss 2.5644 | lr 3.00e-04 | grad 3.12 | tok/s 17955
step    280 | loss 2.6729 | lr 3.00e-04 | grad 3.28 | tok/s 17595
step    290 | loss 2.6040 | lr 3.00e-04 | grad 3.69 | tok/s 18512
step    300 | loss 1.6294 | lr 3.00e-04 | grad 2.03 | tok/s 18465
step    310 | loss 2.8215 | lr 3.00e-04 | grad 2.03 | tok/s 18152
step    320 | loss 2.5526 | lr 3.00e-04 | grad 3.48 | tok/s 17694
step    330 | loss 2.3614 | lr 3.00e-04 | grad 1.48 | tok/s 17069
step    340 | loss 2.6408 | lr 3.00e-04 | grad 1.76 | tok/s 17317
step    350 | loss 2.4481 | lr 3.00e-04 | grad 2.20 | tok/s 17719
step    360 | loss 2.4855 | lr 3.00e-04 | grad 2.89 | tok/s 18090
step    370 | loss 2.2469 | lr 3.00e-04 | grad 1.74 | tok/s 16389
step    380 | loss 2.2098 | lr 3.00e-04 | grad 1.70 | tok/s 17477
step    390 | loss 2.0907 | lr 3.00e-04 | grad 1.27 | tok/s 18232
step    400 | loss 2.0796 | lr 3.00e-04 | grad 1.89 | tok/s 18056
step    410 | loss 1.9937 | lr 3.00e-04 | grad 1.66 | tok/s 17613
step    420 | loss 2.1857 | lr 3.00e-04 | grad 2.17 | tok/s 16789
step    430 | loss 2.4676 | lr 3.00e-04 | grad 2.67 | tok/s 17898
step    440 | loss 2.4944 | lr 3.00e-04 | grad 1.87 | tok/s 16880
step    450 | loss 2.3758 | lr 3.00e-04 | grad 1.29 | tok/s 17455
step    460 | loss 2.2011 | lr 3.00e-04 | grad 2.52 | tok/s 17054
step    470 | loss 2.2700 | lr 3.00e-04 | grad 2.00 | tok/s 17590
step    480 | loss 2.6588 | lr 3.00e-04 | grad 3.05 | tok/s 17550
step    490 | loss 2.1628 | lr 3.00e-04 | grad 2.06 | tok/s 16576
step    500 | loss 2.1379 | lr 3.00e-04 | grad 2.05 | tok/s 17697
step    510 | loss 2.1549 | lr 3.00e-04 | grad 1.37 | tok/s 17973
step    520 | loss 2.1358 | lr 3.00e-04 | grad 1.45 | tok/s 17894
step    530 | loss 2.2791 | lr 3.00e-04 | grad 1.63 | tok/s 17217
step    540 | loss 2.0622 | lr 3.00e-04 | grad 1.85 | tok/s 17185
step    550 | loss 1.9180 | lr 3.00e-04 | grad 1.48 | tok/s 16838
step    560 | loss 2.0773 | lr 3.00e-04 | grad 1.52 | tok/s 16410
step    570 | loss 2.0302 | lr 3.00e-04 | grad 2.58 | tok/s 16876
step    580 | loss 1.9132 | lr 3.00e-04 | grad 1.51 | tok/s 16799
step    590 | loss 2.2390 | lr 3.00e-04 | grad 1.45 | tok/s 17266
step    600 | loss 2.1475 | lr 3.00e-04 | grad 1.25 | tok/s 16656
step    610 | loss 1.9787 | lr 3.00e-04 | grad 1.50 | tok/s 17452
step    620 | loss 1.8486 | lr 3.00e-04 | grad 1.17 | tok/s 16585
step    630 | loss 1.9975 | lr 3.00e-04 | grad 2.27 | tok/s 16718
step    640 | loss 2.1670 | lr 3.00e-04 | grad 1.53 | tok/s 17173
step    650 | loss 1.9944 | lr 3.00e-04 | grad 1.71 | tok/s 17258
step    660 | loss 2.0228 | lr 3.00e-04 | grad 1.16 | tok/s 17367
step    670 | loss 2.2302 | lr 3.00e-04 | grad 3.20 | tok/s 17500
step    680 | loss 1.9966 | lr 3.00e-04 | grad 1.20 | tok/s 17129
step    690 | loss 2.2768 | lr 3.00e-04 | grad 1.84 | tok/s 17703
step    700 | loss 2.0235 | lr 3.00e-04 | grad 1.66 | tok/s 18056
step    710 | loss 1.9171 | lr 3.00e-04 | grad 1.87 | tok/s 16863
step    720 | loss 1.7609 | lr 3.00e-04 | grad 1.55 | tok/s 16618
step    730 | loss 1.8178 | lr 3.00e-04 | grad 1.90 | tok/s 18014
step    740 | loss 1.8816 | lr 3.00e-04 | grad 1.55 | tok/s 17798
step    750 | loss 1.6661 | lr 3.00e-04 | grad 1.45 | tok/s 18069
step    760 | loss 1.5260 | lr 3.00e-04 | grad 1.62 | tok/s 18054
step    770 | loss 1.4853 | lr 3.00e-04 | grad 1.20 | tok/s 18032
step    780 | loss 1.4210 | lr 3.00e-04 | grad 1.51 | tok/s 18040
step    790 | loss 1.4900 | lr 3.00e-04 | grad 1.77 | tok/s 17472
step    800 | loss 2.2092 | lr 3.00e-04 | grad 2.78 | tok/s 17413
step    810 | loss 1.9469 | lr 3.00e-04 | grad 1.47 | tok/s 17334
step    820 | loss 1.9575 | lr 3.00e-04 | grad 1.86 | tok/s 16653
step    830 | loss 1.9852 | lr 3.00e-04 | grad 1.37 | tok/s 17874
step    840 | loss 1.8909 | lr 3.00e-04 | grad 1.38 | tok/s 18047
step    850 | loss 1.9187 | lr 3.00e-04 | grad 1.64 | tok/s 17971
step    860 | loss 1.8674 | lr 3.00e-04 | grad 2.16 | tok/s 17754
step    870 | loss 1.8105 | lr 3.00e-04 | grad 1.41 | tok/s 17118
step    880 | loss 1.9663 | lr 3.00e-04 | grad 1.26 | tok/s 17190
step    890 | loss 1.9428 | lr 3.00e-04 | grad 1.72 | tok/s 17434
step    900 | loss 1.8139 | lr 3.00e-04 | grad 1.32 | tok/s 17456
step    910 | loss 1.6769 | lr 3.00e-04 | grad 1.78 | tok/s 17078
step    920 | loss 1.8587 | lr 3.00e-04 | grad 2.42 | tok/s 17764
step    930 | loss 1.8508 | lr 3.00e-04 | grad 1.97 | tok/s 16980
step    940 | loss 1.7756 | lr 3.00e-04 | grad 1.30 | tok/s 17900
step    950 | loss 1.7924 | lr 3.00e-04 | grad 1.57 | tok/s 17969
step    960 | loss 1.6890 | lr 3.00e-04 | grad 1.59 | tok/s 17985
step    970 | loss 1.9212 | lr 3.00e-04 | grad 1.75 | tok/s 16936
step    980 | loss 1.8515 | lr 3.00e-04 | grad 1.25 | tok/s 17358
step    990 | loss 1.7468 | lr 3.00e-04 | grad 1.41 | tok/s 17667
step   1000 | loss 2.0856 | lr 3.00e-04 | grad 5.41 | tok/s 16960
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0856.pt
step   1010 | loss 1.9371 | lr 3.00e-04 | grad 1.57 | tok/s 6675
step   1020 | loss 1.8409 | lr 3.00e-04 | grad 1.27 | tok/s 16656
step   1030 | loss 1.6977 | lr 3.00e-04 | grad 1.42 | tok/s 17324
step   1040 | loss 1.7008 | lr 3.00e-04 | grad 1.26 | tok/s 17873
step   1050 | loss 1.8044 | lr 3.00e-04 | grad 1.71 | tok/s 16527
step   1060 | loss 1.9769 | lr 3.00e-04 | grad 1.64 | tok/s 17846
step   1070 | loss 2.0019 | lr 3.00e-04 | grad 1.41 | tok/s 17756
step   1080 | loss 1.6098 | lr 3.00e-04 | grad 1.15 | tok/s 16137
step   1090 | loss 1.3478 | lr 3.00e-04 | grad 1.22 | tok/s 17766
step   1100 | loss 1.6336 | lr 3.00e-04 | grad 2.14 | tok/s 17236
step   1110 | loss 1.6944 | lr 3.00e-04 | grad 1.40 | tok/s 18082
step   1120 | loss 1.5899 | lr 3.00e-04 | grad 1.80 | tok/s 18107
step   1130 | loss 1.5358 | lr 3.00e-04 | grad 1.32 | tok/s 18074
step   1140 | loss 1.5054 | lr 3.00e-04 | grad 1.59 | tok/s 18090
step   1150 | loss 1.5253 | lr 3.00e-04 | grad 1.38 | tok/s 18073
step   1160 | loss 1.4285 | lr 3.00e-04 | grad 1.31 | tok/s 18074
step   1170 | loss 1.4451 | lr 3.00e-04 | grad 1.30 | tok/s 18081
step   1180 | loss 1.5756 | lr 3.00e-04 | grad 1.01 | tok/s 18054
step   1190 | loss 1.4504 | lr 3.00e-04 | grad 1.44 | tok/s 18064
step   1200 | loss 1.4403 | lr 3.00e-04 | grad 1.43 | tok/s 18064
step   1210 | loss 1.4728 | lr 3.00e-04 | grad 1.48 | tok/s 18074
step   1220 | loss 1.4753 | lr 3.00e-04 | grad 1.55 | tok/s 18063
step   1230 | loss 1.4540 | lr 3.00e-04 | grad 1.22 | tok/s 18077
step   1240 | loss 1.4028 | lr 3.00e-04 | grad 1.17 | tok/s 18089
step   1250 | loss 2.0804 | lr 3.00e-04 | grad 1.80 | tok/s 17118
step   1260 | loss 1.5419 | lr 3.00e-04 | grad 1.81 | tok/s 16925
step   1270 | loss 1.8251 | lr 3.00e-04 | grad 3.09 | tok/s 16908
step   1280 | loss 1.8171 | lr 3.00e-04 | grad 1.36 | tok/s 17426
step   1290 | loss 1.6590 | lr 3.00e-04 | grad 1.43 | tok/s 17318
step   1300 | loss 1.7206 | lr 3.00e-04 | grad 1.40 | tok/s 17458
step   1310 | loss 1.6855 | lr 3.00e-04 | grad 1.48 | tok/s 17740
step   1320 | loss 1.7712 | lr 3.00e-04 | grad 1.35 | tok/s 17777
step   1330 | loss 1.7775 | lr 3.00e-04 | grad 1.41 | tok/s 17827
step   1340 | loss 1.6568 | lr 3.00e-04 | grad 4.62 | tok/s 16997
step   1350 | loss 1.8778 | lr 3.00e-04 | grad 1.45 | tok/s 16444
step   1360 | loss 1.7390 | lr 3.00e-04 | grad 1.57 | tok/s 17476
step   1370 | loss 1.5664 | lr 3.00e-04 | grad 1.00 | tok/s 17212
step   1380 | loss 1.8619 | lr 3.00e-04 | grad 1.15 | tok/s 16569
step   1390 | loss 1.7083 | lr 3.00e-04 | grad 1.30 | tok/s 17591
step   1400 | loss 1.5951 | lr 3.00e-04 | grad 1.15 | tok/s 16943
step   1410 | loss 1.6328 | lr 3.00e-04 | grad 1.94 | tok/s 16995
step   1420 | loss 1.8873 | lr 3.00e-04 | grad 3.17 | tok/s 17031
step   1430 | loss 1.5965 | lr 3.00e-04 | grad 1.23 | tok/s 17322
step   1440 | loss 1.3533 | lr 3.00e-04 | grad 1.32 | tok/s 17913
step   1450 | loss 1.3421 | lr 3.00e-04 | grad 2.88 | tok/s 18025
step   1460 | loss 1.8060 | lr 3.00e-04 | grad 1.38 | tok/s 17041
step   1470 | loss 1.7279 | lr 3.00e-04 | grad 1.15 | tok/s 17647
step   1480 | loss 2.1706 | lr 3.00e-04 | grad 2.31 | tok/s 17725
step   1490 | loss 1.9482 | lr 3.00e-04 | grad 1.35 | tok/s 17973
step   1500 | loss 1.5996 | lr 3.00e-04 | grad 1.22 | tok/s 18079
step   1510 | loss 1.6900 | lr 3.00e-04 | grad 1.45 | tok/s 17842
step   1520 | loss 1.6063 | lr 3.00e-04 | grad 2.42 | tok/s 17453
step   1530 | loss 1.5812 | lr 3.00e-04 | grad 1.26 | tok/s 17886
step   1540 | loss 1.7659 | lr 3.00e-04 | grad 1.32 | tok/s 16849
step   1550 | loss 1.5105 | lr 3.00e-04 | grad 1.78 | tok/s 17940
step   1560 | loss 1.7108 | lr 3.00e-04 | grad 1.62 | tok/s 16990
step   1570 | loss 1.5325 | lr 3.00e-04 | grad 1.36 | tok/s 18103
step   1580 | loss 2.0266 | lr 3.00e-04 | grad 3.06 | tok/s 17703
step   1590 | loss 1.9156 | lr 3.00e-04 | grad 1.50 | tok/s 17004
step   1600 | loss 1.2123 | lr 3.00e-04 | grad 1.03 | tok/s 18154
step   1610 | loss 1.1684 | lr 3.00e-04 | grad 1.68 | tok/s 17587
step   1620 | loss 1.6000 | lr 3.00e-04 | grad 1.80 | tok/s 16489
step   1630 | loss 1.6322 | lr 3.00e-04 | grad 1.38 | tok/s 17617
step   1640 | loss 1.4972 | lr 3.00e-04 | grad 1.49 | tok/s 17103
step   1650 | loss 1.6498 | lr 3.00e-04 | grad 1.37 | tok/s 16461
step   1660 | loss 1.6169 | lr 3.00e-04 | grad 1.21 | tok/s 17556
step   1670 | loss 1.6080 | lr 3.00e-04 | grad 4.62 | tok/s 17546
step   1680 | loss 1.8681 | lr 3.00e-04 | grad 1.32 | tok/s 16884
step   1690 | loss 1.6478 | lr 3.00e-04 | grad 2.81 | tok/s 17184
step   1700 | loss 1.6503 | lr 3.00e-04 | grad 1.49 | tok/s 17550
step   1710 | loss 1.6647 | lr 3.00e-04 | grad 1.19 | tok/s 17206
step   1720 | loss 1.7287 | lr 3.00e-04 | grad 1.61 | tok/s 17959
step   1730 | loss 1.6255 | lr 3.00e-04 | grad 1.54 | tok/s 18112
step   1740 | loss 1.6289 | lr 3.00e-04 | grad 1.70 | tok/s 17644
step   1750 | loss 1.7260 | lr 3.00e-04 | grad 1.59 | tok/s 17338
step   1760 | loss 1.6621 | lr 3.00e-04 | grad 1.29 | tok/s 17400
step   1770 | loss 1.5736 | lr 3.00e-04 | grad 1.40 | tok/s 17113
step   1780 | loss 1.6291 | lr 3.00e-04 | grad 1.31 | tok/s 17788
step   1790 | loss 1.5532 | lr 3.00e-04 | grad 1.19 | tok/s 17316
step   1800 | loss 1.7424 | lr 3.00e-04 | grad 1.38 | tok/s 17483
step   1810 | loss 1.5779 | lr 3.00e-04 | grad 1.36 | tok/s 16892
step   1820 | loss 1.6632 | lr 3.00e-04 | grad 3.09 | tok/s 17096
step   1830 | loss 1.5787 | lr 3.00e-04 | grad 1.50 | tok/s 17799
step   1840 | loss 1.6599 | lr 3.00e-04 | grad 1.62 | tok/s 17018
step   1850 | loss 1.5515 | lr 3.00e-04 | grad 1.21 | tok/s 17823
step   1860 | loss 1.4752 | lr 3.00e-04 | grad 1.38 | tok/s 17211
step   1870 | loss 1.5678 | lr 3.00e-04 | grad 2.06 | tok/s 17343
step   1880 | loss 1.4110 | lr 3.00e-04 | grad 1.30 | tok/s 17007
step   1890 | loss 1.6674 | lr 3.00e-04 | grad 1.19 | tok/s 16165
step   1900 | loss 1.5209 | lr 3.00e-04 | grad 1.45 | tok/s 17467
step   1910 | loss 1.5757 | lr 3.00e-04 | grad 1.35 | tok/s 16552
step   1920 | loss 1.5578 | lr 3.00e-04 | grad 1.38 | tok/s 18165
step   1930 | loss 1.5813 | lr 3.00e-04 | grad 1.62 | tok/s 17034
step   1940 | loss 1.5699 | lr 3.00e-04 | grad 1.32 | tok/s 17726
step   1950 | loss 2.1713 | lr 3.00e-04 | grad 1.85 | tok/s 17962
step   1960 | loss 1.8977 | lr 3.00e-04 | grad 2.08 | tok/s 18183
step   1970 | loss 1.7457 | lr 3.00e-04 | grad 1.58 | tok/s 17690
step   1980 | loss 1.7007 | lr 3.00e-04 | grad 1.27 | tok/s 16903
step   1990 | loss 1.7123 | lr 3.00e-04 | grad 3.36 | tok/s 17239
step   2000 | loss 1.6065 | lr 3.00e-04 | grad 1.33 | tok/s 17483
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6065.pt
step   2010 | loss 1.1994 | lr 3.00e-04 | grad 1.23 | tok/s 7521
step   2020 | loss 1.4332 | lr 3.00e-04 | grad 1.27 | tok/s 17370
step   2030 | loss 1.2972 | lr 3.00e-04 | grad 0.49 | tok/s 18298
step   2040 | loss 1.5150 | lr 3.00e-04 | grad 1.18 | tok/s 18221
step   2050 | loss 1.4870 | lr 3.00e-04 | grad 1.32 | tok/s 17842
step   2060 | loss 1.6899 | lr 3.00e-04 | grad 1.31 | tok/s 16803
step   2070 | loss 1.7708 | lr 3.00e-04 | grad 1.68 | tok/s 17432
step   2080 | loss 2.4119 | lr 3.00e-04 | grad 2.84 | tok/s 17995
step   2090 | loss 1.8951 | lr 3.00e-04 | grad 2.11 | tok/s 18068
step   2100 | loss 1.5725 | lr 3.00e-04 | grad 1.65 | tok/s 17625
step   2110 | loss 1.7283 | lr 3.00e-04 | grad 5.22 | tok/s 17297
step   2120 | loss 1.0820 | lr 3.00e-04 | grad 1.54 | tok/s 17882
step   2130 | loss 1.2035 | lr 3.00e-04 | grad 1.87 | tok/s 17861
step   2140 | loss 1.6668 | lr 3.00e-04 | grad 1.28 | tok/s 17288
step   2150 | loss 1.4136 | lr 3.00e-04 | grad 1.36 | tok/s 18159
step   2160 | loss 1.3246 | lr 3.00e-04 | grad 1.19 | tok/s 18158
step   2170 | loss 1.3689 | lr 3.00e-04 | grad 1.12 | tok/s 18155
step   2180 | loss 1.3245 | lr 3.00e-04 | grad 1.33 | tok/s 18143
step   2190 | loss 1.3333 | lr 3.00e-04 | grad 1.05 | tok/s 18147
step   2200 | loss 1.3312 | lr 3.00e-04 | grad 1.21 | tok/s 18161
step   2210 | loss 1.2634 | lr 3.00e-04 | grad 1.06 | tok/s 18134
step   2220 | loss 1.2643 | lr 3.00e-04 | grad 1.22 | tok/s 18125
step   2230 | loss 1.4627 | lr 3.00e-04 | grad 1.23 | tok/s 17788
step   2240 | loss 1.4829 | lr 3.00e-04 | grad 1.23 | tok/s 18112
step   2250 | loss 1.7538 | lr 3.00e-04 | grad 2.39 | tok/s 17547
step   2260 | loss 1.7465 | lr 3.00e-04 | grad 1.29 | tok/s 17713
step   2270 | loss 2.0527 | lr 3.00e-04 | grad 2.06 | tok/s 18023
step   2280 | loss 1.6478 | lr 3.00e-04 | grad 1.47 | tok/s 17981
step   2290 | loss 1.4774 | lr 3.00e-04 | grad 1.49 | tok/s 17550
step   2300 | loss 2.0960 | lr 3.00e-04 | grad 3.95 | tok/s 17966
step   2310 | loss 1.5942 | lr 3.00e-04 | grad 1.12 | tok/s 17024
step   2320 | loss 1.7350 | lr 3.00e-04 | grad 3.14 | tok/s 17082
step   2330 | loss 1.8630 | lr 3.00e-04 | grad 1.71 | tok/s 17388
step   2340 | loss 1.5677 | lr 3.00e-04 | grad 3.52 | tok/s 16916
step   2350 | loss 1.4862 | lr 3.00e-04 | grad 2.66 | tok/s 17778
step   2360 | loss 1.4952 | lr 3.00e-04 | grad 1.45 | tok/s 17977
step   2370 | loss 1.6097 | lr 3.00e-04 | grad 2.09 | tok/s 17778
step   2380 | loss 1.7677 | lr 3.00e-04 | grad 1.50 | tok/s 18168
step   2390 | loss 1.4192 | lr 3.00e-04 | grad 1.27 | tok/s 18107
step   2400 | loss 1.2749 | lr 3.00e-04 | grad 1.22 | tok/s 18113
step   2410 | loss 1.2032 | lr 3.00e-04 | grad 1.23 | tok/s 17568
step   2420 | loss 1.5749 | lr 3.00e-04 | grad 2.09 | tok/s 16985
step   2430 | loss 1.5496 | lr 3.00e-04 | grad 1.42 | tok/s 17298
step   2440 | loss 1.3937 | lr 3.00e-04 | grad 2.34 | tok/s 17757
step   2450 | loss 1.5713 | lr 3.00e-04 | grad 1.48 | tok/s 17287
step   2460 | loss 1.5014 | lr 3.00e-04 | grad 1.54 | tok/s 17987
step   2470 | loss 1.3077 | lr 3.00e-04 | grad 1.59 | tok/s 17924
step   2480 | loss 1.3877 | lr 3.00e-04 | grad 1.05 | tok/s 18149
step   2490 | loss 1.4423 | lr 3.00e-04 | grad 1.38 | tok/s 17161
step   2500 | loss 1.6888 | lr 3.00e-04 | grad 1.41 | tok/s 17700
step   2510 | loss 1.4048 | lr 3.00e-04 | grad 2.05 | tok/s 18102
step   2520 | loss 1.6274 | lr 3.00e-04 | grad 2.48 | tok/s 18036
step   2530 | loss 1.4664 | lr 3.00e-04 | grad 1.36 | tok/s 17421
step   2540 | loss 1.5150 | lr 3.00e-04 | grad 1.77 | tok/s 17313
step   2550 | loss 1.3451 | lr 3.00e-04 | grad 1.28 | tok/s 17977
step   2560 | loss 1.5322 | lr 3.00e-04 | grad 3.42 | tok/s 16890
step   2570 | loss 1.5527 | lr 3.00e-04 | grad 1.17 | tok/s 17593
step   2580 | loss 1.4805 | lr 3.00e-04 | grad 1.41 | tok/s 16495

Training complete! Final step: 2582
