Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_40/levelllama_100m_20260126_175426
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 286,561,408 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.8591 | lr 3.00e-04 | grad 2.25 | tok/s 9884
step     20 | loss 2.9394 | lr 3.00e-04 | grad 0.92 | tok/s 18895
step     30 | loss 3.4091 | lr 3.00e-04 | grad 1.22 | tok/s 20035
step     40 | loss 4.4707 | lr 3.00e-04 | grad 5.03 | tok/s 20458
step     50 | loss 4.2540 | lr 3.00e-04 | grad 3.34 | tok/s 20749
step     60 | loss 4.0460 | lr 3.00e-04 | grad 2.83 | tok/s 20714
step     70 | loss 3.6873 | lr 3.00e-04 | grad 2.50 | tok/s 20714
step     80 | loss 3.6021 | lr 3.00e-04 | grad 2.06 | tok/s 20688
step     90 | loss 3.3365 | lr 3.00e-04 | grad 1.62 | tok/s 20679
step    100 | loss 3.1470 | lr 3.00e-04 | grad 1.80 | tok/s 20647
step    110 | loss 2.9289 | lr 3.00e-04 | grad 1.58 | tok/s 20451
step    120 | loss 3.2065 | lr 3.00e-04 | grad 0.68 | tok/s 19446
step    130 | loss 2.6119 | lr 3.00e-04 | grad 1.95 | tok/s 19892
step    140 | loss 2.8949 | lr 3.00e-04 | grad 3.94 | tok/s 19922
step    150 | loss 2.9481 | lr 3.00e-04 | grad 3.75 | tok/s 20395
step    160 | loss 2.8479 | lr 3.00e-04 | grad 1.39 | tok/s 19725
step    170 | loss 2.6852 | lr 3.00e-04 | grad 0.79 | tok/s 19419
step    180 | loss 2.8995 | lr 3.00e-04 | grad 1.16 | tok/s 19865
step    190 | loss 2.4466 | lr 3.00e-04 | grad 0.87 | tok/s 19498
step    200 | loss 2.3949 | lr 3.00e-04 | grad 0.78 | tok/s 20387
step    210 | loss 2.4002 | lr 3.00e-04 | grad 2.12 | tok/s 19324
step    220 | loss 2.7135 | lr 3.00e-04 | grad 4.47 | tok/s 19520
step    230 | loss 2.5809 | lr 3.00e-04 | grad 1.41 | tok/s 19501
step    240 | loss 2.7603 | lr 3.00e-04 | grad 1.77 | tok/s 19735
step    250 | loss 2.3067 | lr 3.00e-04 | grad 0.68 | tok/s 19599
step    260 | loss 2.4438 | lr 3.00e-04 | grad 1.45 | tok/s 20157
step    270 | loss 2.3086 | lr 3.00e-04 | grad 0.90 | tok/s 19685
step    280 | loss 2.2470 | lr 3.00e-04 | grad 0.62 | tok/s 18465
step    290 | loss 2.1791 | lr 3.00e-04 | grad 1.41 | tok/s 19099
step    300 | loss 2.4304 | lr 3.00e-04 | grad 0.68 | tok/s 19249
step    310 | loss 2.1362 | lr 3.00e-04 | grad 0.62 | tok/s 19159
step    320 | loss 2.3935 | lr 3.00e-04 | grad 2.47 | tok/s 19369
step    330 | loss 2.1940 | lr 3.00e-04 | grad 0.85 | tok/s 19574
step    340 | loss 2.4955 | lr 3.00e-04 | grad 1.34 | tok/s 19482
step    350 | loss 2.5153 | lr 3.00e-04 | grad 0.88 | tok/s 20048
step    360 | loss 2.1152 | lr 3.00e-04 | grad 0.90 | tok/s 19168
step    370 | loss 2.1757 | lr 3.00e-04 | grad 0.76 | tok/s 20203
step    380 | loss 2.0203 | lr 3.00e-04 | grad 1.12 | tok/s 20372
step    390 | loss 1.9765 | lr 3.00e-04 | grad 0.75 | tok/s 20372
step    400 | loss 2.2771 | lr 3.00e-04 | grad 0.86 | tok/s 19286
step    410 | loss 2.1787 | lr 3.00e-04 | grad 1.01 | tok/s 19455
step    420 | loss 2.3616 | lr 3.00e-04 | grad 1.07 | tok/s 20292
step    430 | loss 2.2005 | lr 3.00e-04 | grad 1.70 | tok/s 19953
step    440 | loss 2.1807 | lr 3.00e-04 | grad 1.07 | tok/s 19336
step    450 | loss 2.0732 | lr 3.00e-04 | grad 0.66 | tok/s 19539
step    460 | loss 2.1684 | lr 3.00e-04 | grad 0.75 | tok/s 19825
step    470 | loss 2.1633 | lr 3.00e-04 | grad 1.51 | tok/s 19690
step    480 | loss 2.1985 | lr 3.00e-04 | grad 1.27 | tok/s 20109
step    490 | loss 2.1214 | lr 3.00e-04 | grad 1.13 | tok/s 19305
step    500 | loss 2.2470 | lr 3.00e-04 | grad 0.85 | tok/s 19626
step    510 | loss 2.1191 | lr 3.00e-04 | grad 0.77 | tok/s 18721
step    520 | loss 1.9860 | lr 3.00e-04 | grad 0.86 | tok/s 19619
step    530 | loss 2.1339 | lr 3.00e-04 | grad 0.96 | tok/s 19266
step    540 | loss 2.1059 | lr 3.00e-04 | grad 0.80 | tok/s 18883
step    550 | loss 1.7854 | lr 3.00e-04 | grad 1.55 | tok/s 19701
step    560 | loss 1.9895 | lr 3.00e-04 | grad 1.01 | tok/s 20287
step    570 | loss 1.8776 | lr 3.00e-04 | grad 1.01 | tok/s 20306
step    580 | loss 1.8117 | lr 3.00e-04 | grad 0.63 | tok/s 20298
step    590 | loss 1.8685 | lr 3.00e-04 | grad 0.95 | tok/s 20304
step    600 | loss 1.8254 | lr 3.00e-04 | grad 0.74 | tok/s 20324
step    610 | loss 1.7936 | lr 3.00e-04 | grad 0.76 | tok/s 20312
step    620 | loss 1.7708 | lr 3.00e-04 | grad 0.64 | tok/s 20226
step    630 | loss 1.9701 | lr 3.00e-04 | grad 2.03 | tok/s 19099
step    640 | loss 2.1327 | lr 3.00e-04 | grad 1.52 | tok/s 19364
step    650 | loss 1.9488 | lr 3.00e-04 | grad 0.85 | tok/s 19344
step    660 | loss 2.0190 | lr 3.00e-04 | grad 1.06 | tok/s 20084
step    670 | loss 2.0531 | lr 3.00e-04 | grad 2.23 | tok/s 19408
step    680 | loss 2.0295 | lr 3.00e-04 | grad 1.01 | tok/s 19098
step    690 | loss 1.9937 | lr 3.00e-04 | grad 0.93 | tok/s 18940
step    700 | loss 1.9121 | lr 3.00e-04 | grad 0.91 | tok/s 19365
step    710 | loss 2.0432 | lr 3.00e-04 | grad 1.90 | tok/s 19070
step    720 | loss 1.8266 | lr 3.00e-04 | grad 1.26 | tok/s 19808
step    730 | loss 1.9000 | lr 3.00e-04 | grad 0.75 | tok/s 19498
step    740 | loss 2.3547 | lr 3.00e-04 | grad 1.86 | tok/s 20018
step    750 | loss 2.1576 | lr 3.00e-04 | grad 1.22 | tok/s 20250
step    760 | loss 1.9347 | lr 3.00e-04 | grad 1.92 | tok/s 19831
step    770 | loss 1.9400 | lr 3.00e-04 | grad 0.91 | tok/s 19500
step    780 | loss 1.8830 | lr 3.00e-04 | grad 1.12 | tok/s 19629
step    790 | loss 2.2262 | lr 3.00e-04 | grad 1.89 | tok/s 20068
step    800 | loss 1.7782 | lr 3.00e-04 | grad 0.83 | tok/s 19705
step    810 | loss 1.7002 | lr 3.00e-04 | grad 1.59 | tok/s 19048
step    820 | loss 1.8669 | lr 3.00e-04 | grad 1.12 | tok/s 19437
step    830 | loss 1.9071 | lr 3.00e-04 | grad 1.04 | tok/s 19178
step    840 | loss 2.0304 | lr 3.00e-04 | grad 0.91 | tok/s 19088
step    850 | loss 2.0056 | lr 3.00e-04 | grad 0.82 | tok/s 19488
step    860 | loss 2.0373 | lr 3.00e-04 | grad 1.25 | tok/s 19809
step    870 | loss 2.1196 | lr 3.00e-04 | grad 1.11 | tok/s 19965
step    880 | loss 1.9247 | lr 3.00e-04 | grad 0.86 | tok/s 19573
step    890 | loss 1.8193 | lr 3.00e-04 | grad 0.77 | tok/s 19487
step    900 | loss 1.8725 | lr 3.00e-04 | grad 0.83 | tok/s 19407
step    910 | loss 1.9406 | lr 3.00e-04 | grad 3.11 | tok/s 19202
step    920 | loss 1.8302 | lr 3.00e-04 | grad 0.93 | tok/s 19400
step    930 | loss 1.8013 | lr 3.00e-04 | grad 0.93 | tok/s 19650
step    940 | loss 1.7715 | lr 3.00e-04 | grad 0.89 | tok/s 19202
step    950 | loss 1.8361 | lr 3.00e-04 | grad 1.22 | tok/s 18883
step    960 | loss 1.7864 | lr 3.00e-04 | grad 0.93 | tok/s 19399
step    970 | loss 1.7797 | lr 3.00e-04 | grad 1.00 | tok/s 19421
step    980 | loss 2.5906 | lr 3.00e-04 | grad 1.51 | tok/s 20201
step    990 | loss 1.9863 | lr 3.00e-04 | grad 1.16 | tok/s 19362
step   1000 | loss 1.9080 | lr 3.00e-04 | grad 1.09 | tok/s 19432
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9080.pt
step   1010 | loss 1.6380 | lr 3.00e-04 | grad 1.19 | tok/s 13411
step   1020 | loss 1.7357 | lr 3.00e-04 | grad 1.02 | tok/s 20316
step   1030 | loss 1.8980 | lr 3.00e-04 | grad 1.08 | tok/s 19332
step   1040 | loss 2.3905 | lr 3.00e-04 | grad 1.52 | tok/s 19795
step   1050 | loss 1.9894 | lr 3.00e-04 | grad 1.18 | tok/s 19928
step   1060 | loss 1.6296 | lr 3.00e-04 | grad 0.82 | tok/s 19638
step   1070 | loss 1.7224 | lr 3.00e-04 | grad 1.35 | tok/s 19620
step   1080 | loss 1.6291 | lr 3.00e-04 | grad 1.06 | tok/s 20299
step   1090 | loss 1.6018 | lr 3.00e-04 | grad 0.84 | tok/s 20265
step   1100 | loss 1.5662 | lr 3.00e-04 | grad 0.91 | tok/s 20304
step   1110 | loss 1.5193 | lr 3.00e-04 | grad 1.16 | tok/s 20305
step   1120 | loss 1.7325 | lr 3.00e-04 | grad 0.94 | tok/s 19748
step   1130 | loss 2.1879 | lr 3.00e-04 | grad 1.05 | tok/s 19959
step   1140 | loss 2.1076 | lr 3.00e-04 | grad 1.09 | tok/s 20180
step   1150 | loss 2.3124 | lr 3.00e-04 | grad 1.73 | tok/s 19740
step   1160 | loss 2.0034 | lr 3.00e-04 | grad 2.70 | tok/s 19091
step   1170 | loss 1.8454 | lr 3.00e-04 | grad 1.15 | tok/s 19055
step   1180 | loss 1.7576 | lr 3.00e-04 | grad 0.95 | tok/s 20050
step   1190 | loss 2.0963 | lr 3.00e-04 | grad 1.28 | tok/s 20068
step   1200 | loss 1.6120 | lr 3.00e-04 | grad 1.25 | tok/s 20266
step   1210 | loss 1.6488 | lr 3.00e-04 | grad 1.04 | tok/s 19082
step   1220 | loss 1.7393 | lr 3.00e-04 | grad 1.11 | tok/s 19721
step   1230 | loss 1.7746 | lr 3.00e-04 | grad 0.90 | tok/s 19850
step   1240 | loss 1.6814 | lr 3.00e-04 | grad 1.52 | tok/s 20081
step   1250 | loss 1.8912 | lr 3.00e-04 | grad 1.23 | tok/s 19613
step   1260 | loss 2.0303 | lr 3.00e-04 | grad 1.03 | tok/s 20156
step   1270 | loss 1.7171 | lr 3.00e-04 | grad 1.54 | tok/s 19519
step   1280 | loss 1.7377 | lr 3.00e-04 | grad 1.24 | tok/s 19563
step   1290 | loss 1.7285 | lr 3.00e-04 | grad 1.17 | tok/s 19131
step   1300 | loss 1.9696 | lr 3.00e-04 | grad 2.88 | tok/s 19080
step   1310 | loss 2.0077 | lr 3.00e-04 | grad 1.05 | tok/s 19860
step   1320 | loss 1.8228 | lr 3.00e-04 | grad 1.97 | tok/s 19806
step   1330 | loss 1.8707 | lr 3.00e-04 | grad 1.20 | tok/s 19767
step   1340 | loss 1.9059 | lr 3.00e-04 | grad 0.95 | tok/s 19140
step   1350 | loss 1.7955 | lr 3.00e-04 | grad 1.06 | tok/s 19897
step   1360 | loss 1.8542 | lr 3.00e-04 | grad 1.16 | tok/s 18694
step   1370 | loss 1.9866 | lr 3.00e-04 | grad 2.20 | tok/s 19967
step   1380 | loss 1.8114 | lr 3.00e-04 | grad 1.21 | tok/s 19330
step   1390 | loss 1.8236 | lr 3.00e-04 | grad 1.16 | tok/s 19716
step   1400 | loss 1.8277 | lr 3.00e-04 | grad 1.07 | tok/s 19282
step   1410 | loss 1.6739 | lr 3.00e-04 | grad 1.41 | tok/s 18933
step   1420 | loss 1.7563 | lr 3.00e-04 | grad 1.15 | tok/s 20110
step   1430 | loss 2.0822 | lr 3.00e-04 | grad 1.03 | tok/s 19466
step   1440 | loss 1.7863 | lr 3.00e-04 | grad 1.35 | tok/s 19624
step   1450 | loss 1.7937 | lr 3.00e-04 | grad 1.02 | tok/s 19835
step   1460 | loss 1.8503 | lr 3.00e-04 | grad 1.22 | tok/s 19246

Training complete! Final step: 1468
