Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_73/levelllama_100m_20260126_184546
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 281,704,320 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.3487 | lr 3.00e-04 | grad 2.16 | tok/s 10950
step     20 | loss 2.9434 | lr 3.00e-04 | grad 1.05 | tok/s 24139
step     30 | loss 3.2531 | lr 3.00e-04 | grad 1.61 | tok/s 25515
step     40 | loss 5.5425 | lr 3.00e-04 | grad 6.28 | tok/s 26019
step     50 | loss 4.2137 | lr 3.00e-04 | grad 4.19 | tok/s 26339
step     60 | loss 3.9275 | lr 3.00e-04 | grad 3.27 | tok/s 26363
step     70 | loss 3.5544 | lr 3.00e-04 | grad 3.25 | tok/s 26350
step     80 | loss 3.5062 | lr 3.00e-04 | grad 2.72 | tok/s 26336
step     90 | loss 3.2998 | lr 3.00e-04 | grad 1.81 | tok/s 26281
step    100 | loss 3.1180 | lr 3.00e-04 | grad 2.17 | tok/s 26244
step    110 | loss 2.9035 | lr 3.00e-04 | grad 1.85 | tok/s 26005
step    120 | loss 3.1762 | lr 3.00e-04 | grad 1.02 | tok/s 24708
step    130 | loss 2.5786 | lr 3.00e-04 | grad 2.19 | tok/s 25271
step    140 | loss 2.8308 | lr 3.00e-04 | grad 4.19 | tok/s 25311
step    150 | loss 2.7404 | lr 3.00e-04 | grad 4.44 | tok/s 25863
step    160 | loss 2.8274 | lr 3.00e-04 | grad 1.38 | tok/s 24981
step    170 | loss 2.6491 | lr 3.00e-04 | grad 1.05 | tok/s 24602
step    180 | loss 2.8484 | lr 3.00e-04 | grad 1.40 | tok/s 25158
step    190 | loss 2.3927 | lr 3.00e-04 | grad 1.40 | tok/s 24647
step    200 | loss 2.3322 | lr 3.00e-04 | grad 0.99 | tok/s 25746
step    210 | loss 2.3478 | lr 3.00e-04 | grad 2.16 | tok/s 24377
step    220 | loss 2.6272 | lr 3.00e-04 | grad 1.85 | tok/s 24627
step    230 | loss 2.4795 | lr 3.00e-04 | grad 1.53 | tok/s 24582
step    240 | loss 2.7043 | lr 3.00e-04 | grad 2.02 | tok/s 24884
step    250 | loss 2.2387 | lr 3.00e-04 | grad 1.00 | tok/s 24656
step    260 | loss 2.3899 | lr 3.00e-04 | grad 1.52 | tok/s 25330
step    270 | loss 2.2483 | lr 3.00e-04 | grad 1.94 | tok/s 24733
step    280 | loss 2.1869 | lr 3.00e-04 | grad 0.77 | tok/s 23206
step    290 | loss 2.1159 | lr 3.00e-04 | grad 1.41 | tok/s 23977
step    300 | loss 2.3794 | lr 3.00e-04 | grad 1.01 | tok/s 24147
step    310 | loss 2.0650 | lr 3.00e-04 | grad 0.85 | tok/s 23994
step    320 | loss 2.3291 | lr 3.00e-04 | grad 2.73 | tok/s 24241
step    330 | loss 2.1284 | lr 3.00e-04 | grad 1.16 | tok/s 24474
step    340 | loss 2.4390 | lr 3.00e-04 | grad 1.57 | tok/s 24356
step    350 | loss 2.4236 | lr 3.00e-04 | grad 1.04 | tok/s 25055
step    360 | loss 2.0409 | lr 3.00e-04 | grad 1.03 | tok/s 23935
step    370 | loss 2.0873 | lr 3.00e-04 | grad 1.02 | tok/s 25244
step    380 | loss 1.8969 | lr 3.00e-04 | grad 1.39 | tok/s 25417
step    390 | loss 1.8242 | lr 3.00e-04 | grad 1.15 | tok/s 25380
step    400 | loss 2.1967 | lr 3.00e-04 | grad 0.99 | tok/s 24036
step    410 | loss 2.1083 | lr 3.00e-04 | grad 1.10 | tok/s 24281
step    420 | loss 2.2728 | lr 3.00e-04 | grad 1.41 | tok/s 25312
step    430 | loss 2.1192 | lr 3.00e-04 | grad 1.41 | tok/s 24884
step    440 | loss 2.1084 | lr 3.00e-04 | grad 1.32 | tok/s 24089
step    450 | loss 2.0023 | lr 3.00e-04 | grad 0.91 | tok/s 24356
step    460 | loss 2.0764 | lr 3.00e-04 | grad 0.95 | tok/s 24698
step    470 | loss 2.0748 | lr 3.00e-04 | grad 1.88 | tok/s 24491
step    480 | loss 2.1049 | lr 3.00e-04 | grad 1.73 | tok/s 25033
step    490 | loss 2.0383 | lr 3.00e-04 | grad 1.53 | tok/s 24017
step    500 | loss 2.1927 | lr 3.00e-04 | grad 0.94 | tok/s 24410
step    510 | loss 2.0484 | lr 3.00e-04 | grad 0.78 | tok/s 23310
step    520 | loss 1.8897 | lr 3.00e-04 | grad 0.99 | tok/s 24383
step    530 | loss 2.0575 | lr 3.00e-04 | grad 1.05 | tok/s 23974
step    540 | loss 2.0198 | lr 3.00e-04 | grad 0.75 | tok/s 23460
step    550 | loss 1.7133 | lr 3.00e-04 | grad 1.77 | tok/s 24509
step    560 | loss 1.8784 | lr 3.00e-04 | grad 1.36 | tok/s 25235
step    570 | loss 1.7620 | lr 3.00e-04 | grad 1.09 | tok/s 25244
step    580 | loss 1.6942 | lr 3.00e-04 | grad 0.77 | tok/s 25249
step    590 | loss 1.7483 | lr 3.00e-04 | grad 1.26 | tok/s 25257
step    600 | loss 1.7043 | lr 3.00e-04 | grad 1.18 | tok/s 25223
step    610 | loss 1.6706 | lr 3.00e-04 | grad 0.96 | tok/s 25216
step    620 | loss 1.6605 | lr 3.00e-04 | grad 1.16 | tok/s 25112
step    630 | loss 1.9449 | lr 3.00e-04 | grad 2.55 | tok/s 23752
step    640 | loss 2.0697 | lr 3.00e-04 | grad 1.71 | tok/s 24057
step    650 | loss 1.8694 | lr 3.00e-04 | grad 0.99 | tok/s 24028
step    660 | loss 1.9331 | lr 3.00e-04 | grad 1.17 | tok/s 24961
step    670 | loss 1.9690 | lr 3.00e-04 | grad 2.44 | tok/s 24130
step    680 | loss 1.9555 | lr 3.00e-04 | grad 1.48 | tok/s 23714
step    690 | loss 1.9360 | lr 3.00e-04 | grad 1.05 | tok/s 23549
step    700 | loss 1.8270 | lr 3.00e-04 | grad 0.98 | tok/s 24061
step    710 | loss 1.9698 | lr 3.00e-04 | grad 2.11 | tok/s 23658
step    720 | loss 1.7091 | lr 3.00e-04 | grad 1.15 | tok/s 24574
step    730 | loss 1.8119 | lr 3.00e-04 | grad 0.87 | tok/s 24180
step    740 | loss 2.2663 | lr 3.00e-04 | grad 1.97 | tok/s 24840
step    750 | loss 2.0495 | lr 3.00e-04 | grad 1.40 | tok/s 25095
step    760 | loss 1.8409 | lr 3.00e-04 | grad 2.05 | tok/s 24569
step    770 | loss 1.8548 | lr 3.00e-04 | grad 1.11 | tok/s 24164
step    780 | loss 1.7989 | lr 3.00e-04 | grad 1.30 | tok/s 24323
step    790 | loss 2.1326 | lr 3.00e-04 | grad 1.96 | tok/s 24868
step    800 | loss 1.6884 | lr 3.00e-04 | grad 0.84 | tok/s 24435
step    810 | loss 1.6319 | lr 3.00e-04 | grad 1.80 | tok/s 23616
step    820 | loss 1.7681 | lr 3.00e-04 | grad 1.12 | tok/s 24072
step    830 | loss 1.8235 | lr 3.00e-04 | grad 1.09 | tok/s 23775
step    840 | loss 1.9540 | lr 3.00e-04 | grad 1.12 | tok/s 23651
step    850 | loss 1.9166 | lr 3.00e-04 | grad 1.04 | tok/s 24166
step    860 | loss 1.9587 | lr 3.00e-04 | grad 1.30 | tok/s 24555
step    870 | loss 1.9968 | lr 3.00e-04 | grad 1.12 | tok/s 24726
step    880 | loss 1.8512 | lr 3.00e-04 | grad 0.92 | tok/s 24218
step    890 | loss 1.7420 | lr 3.00e-04 | grad 1.02 | tok/s 24109
step    900 | loss 1.8025 | lr 3.00e-04 | grad 0.97 | tok/s 24005
step    910 | loss 1.8595 | lr 3.00e-04 | grad 3.30 | tok/s 23751
step    920 | loss 1.7495 | lr 3.00e-04 | grad 1.17 | tok/s 24024
step    930 | loss 1.7170 | lr 3.00e-04 | grad 1.06 | tok/s 24362
step    940 | loss 1.6785 | lr 3.00e-04 | grad 1.03 | tok/s 23795
step    950 | loss 1.7650 | lr 3.00e-04 | grad 1.42 | tok/s 23392
step    960 | loss 1.7118 | lr 3.00e-04 | grad 1.16 | tok/s 24030
step    970 | loss 1.7066 | lr 3.00e-04 | grad 1.06 | tok/s 24073
step    980 | loss 2.4583 | lr 3.00e-04 | grad 2.03 | tok/s 25039
step    990 | loss 1.9107 | lr 3.00e-04 | grad 1.22 | tok/s 23999
step   1000 | loss 1.8532 | lr 3.00e-04 | grad 1.32 | tok/s 24064
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8532.pt
step   1010 | loss 1.5665 | lr 3.00e-04 | grad 1.35 | tok/s 14359
step   1020 | loss 1.6596 | lr 3.00e-04 | grad 1.19 | tok/s 25283
step   1030 | loss 1.8089 | lr 3.00e-04 | grad 1.16 | tok/s 24000
step   1040 | loss 2.3293 | lr 3.00e-04 | grad 1.55 | tok/s 24582
step   1050 | loss 1.9105 | lr 3.00e-04 | grad 1.05 | tok/s 24742
step   1060 | loss 1.5638 | lr 3.00e-04 | grad 0.85 | tok/s 24431
step   1070 | loss 1.6463 | lr 3.00e-04 | grad 1.48 | tok/s 24371
step   1080 | loss 1.5327 | lr 3.00e-04 | grad 1.22 | tok/s 25193
step   1090 | loss 1.5094 | lr 3.00e-04 | grad 1.28 | tok/s 25191
step   1100 | loss 1.4789 | lr 3.00e-04 | grad 1.36 | tok/s 25208
step   1110 | loss 1.4260 | lr 3.00e-04 | grad 1.12 | tok/s 25187
step   1120 | loss 1.6489 | lr 3.00e-04 | grad 1.12 | tok/s 24521
step   1130 | loss 2.1033 | lr 3.00e-04 | grad 1.25 | tok/s 24777
step   1140 | loss 2.0509 | lr 3.00e-04 | grad 1.19 | tok/s 25039
step   1150 | loss 2.2147 | lr 3.00e-04 | grad 1.80 | tok/s 24482
step   1160 | loss 1.9434 | lr 3.00e-04 | grad 2.92 | tok/s 23697
step   1170 | loss 1.7854 | lr 3.00e-04 | grad 1.38 | tok/s 23613
step   1180 | loss 1.6718 | lr 3.00e-04 | grad 1.27 | tok/s 24849
step   1190 | loss 2.0048 | lr 3.00e-04 | grad 1.41 | tok/s 24925
step   1200 | loss 1.5265 | lr 3.00e-04 | grad 1.27 | tok/s 25141
step   1210 | loss 1.5760 | lr 3.00e-04 | grad 1.09 | tok/s 23688
step   1220 | loss 1.6594 | lr 3.00e-04 | grad 1.43 | tok/s 24462
step   1230 | loss 1.6980 | lr 3.00e-04 | grad 0.96 | tok/s 24636
step   1240 | loss 1.5770 | lr 3.00e-04 | grad 1.58 | tok/s 24896
step   1250 | loss 1.8161 | lr 3.00e-04 | grad 1.45 | tok/s 24316
step   1260 | loss 1.9185 | lr 3.00e-04 | grad 1.06 | tok/s 25009
step   1270 | loss 1.6445 | lr 3.00e-04 | grad 1.67 | tok/s 24235
step   1280 | loss 1.6558 | lr 3.00e-04 | grad 1.39 | tok/s 24317
step   1290 | loss 1.6523 | lr 3.00e-04 | grad 1.29 | tok/s 23742
step   1300 | loss 1.8974 | lr 3.00e-04 | grad 3.44 | tok/s 23658
step   1310 | loss 1.9088 | lr 3.00e-04 | grad 1.05 | tok/s 24643
step   1320 | loss 1.7486 | lr 3.00e-04 | grad 2.14 | tok/s 24579
step   1330 | loss 1.7900 | lr 3.00e-04 | grad 1.20 | tok/s 24544
step   1340 | loss 1.8463 | lr 3.00e-04 | grad 1.00 | tok/s 23755
step   1350 | loss 1.7203 | lr 3.00e-04 | grad 1.13 | tok/s 24682
step   1360 | loss 1.7721 | lr 3.00e-04 | grad 1.26 | tok/s 23182
step   1370 | loss 1.8840 | lr 3.00e-04 | grad 2.22 | tok/s 24765
step   1380 | loss 1.7465 | lr 3.00e-04 | grad 1.31 | tok/s 23996
step   1390 | loss 1.7241 | lr 3.00e-04 | grad 1.35 | tok/s 24452
step   1400 | loss 1.7815 | lr 3.00e-04 | grad 1.16 | tok/s 23926
step   1410 | loss 1.5996 | lr 3.00e-04 | grad 1.48 | tok/s 23501
step   1420 | loss 1.5996 | lr 3.00e-04 | grad 1.33 | tok/s 24953
step   1430 | loss 2.0275 | lr 3.00e-04 | grad 1.05 | tok/s 24140
step   1440 | loss 1.7155 | lr 3.00e-04 | grad 1.50 | tok/s 24339
step   1450 | loss 1.7163 | lr 3.00e-04 | grad 1.06 | tok/s 24615
step   1460 | loss 1.7918 | lr 3.00e-04 | grad 1.34 | tok/s 23891
step   1470 | loss 1.6332 | lr 3.00e-04 | grad 1.30 | tok/s 23557
step   1480 | loss 1.6497 | lr 3.00e-04 | grad 1.74 | tok/s 24410
step   1490 | loss 1.8709 | lr 3.00e-04 | grad 1.38 | tok/s 24228
step   1500 | loss 1.9017 | lr 3.00e-04 | grad 1.25 | tok/s 24589
step   1510 | loss 1.5453 | lr 3.00e-04 | grad 1.01 | tok/s 24097
step   1520 | loss 1.6971 | lr 3.00e-04 | grad 1.49 | tok/s 23989
step   1530 | loss 1.6336 | lr 3.00e-04 | grad 1.31 | tok/s 24638
step   1540 | loss 1.7430 | lr 3.00e-04 | grad 1.09 | tok/s 24684
step   1550 | loss 1.7168 | lr 3.00e-04 | grad 2.27 | tok/s 24231
step   1560 | loss 1.5068 | lr 3.00e-04 | grad 1.28 | tok/s 25025
step   1570 | loss 1.6140 | lr 3.00e-04 | grad 0.98 | tok/s 24444
step   1580 | loss 1.5162 | lr 3.00e-04 | grad 1.32 | tok/s 24577
step   1590 | loss 1.7019 | lr 3.00e-04 | grad 1.99 | tok/s 23628
step   1600 | loss 1.4957 | lr 3.00e-04 | grad 3.66 | tok/s 24949
step   1610 | loss 2.1346 | lr 3.00e-04 | grad 2.55 | tok/s 24352
step   1620 | loss 2.3230 | lr 3.00e-04 | grad 1.75 | tok/s 25138
step   1630 | loss 2.0499 | lr 3.00e-04 | grad 1.31 | tok/s 25148
step   1640 | loss 1.8901 | lr 3.00e-04 | grad 1.46 | tok/s 25159
step   1650 | loss 1.8085 | lr 3.00e-04 | grad 1.37 | tok/s 25150
step   1660 | loss 1.7620 | lr 3.00e-04 | grad 1.39 | tok/s 25140
step   1670 | loss 1.8619 | lr 3.00e-04 | grad 1.50 | tok/s 24390
step   1680 | loss 1.6553 | lr 3.00e-04 | grad 1.11 | tok/s 24152
step   1690 | loss 1.6743 | lr 3.00e-04 | grad 1.45 | tok/s 23597
step   1700 | loss 1.5475 | lr 3.00e-04 | grad 1.24 | tok/s 24473
step   1710 | loss 1.5192 | lr 3.00e-04 | grad 1.49 | tok/s 24578
step   1720 | loss 1.6521 | lr 3.00e-04 | grad 1.09 | tok/s 23857
step   1730 | loss 1.7263 | lr 3.00e-04 | grad 2.30 | tok/s 24603
step   1740 | loss 1.6377 | lr 3.00e-04 | grad 1.30 | tok/s 24718
step   1750 | loss 1.5276 | lr 3.00e-04 | grad 1.20 | tok/s 24212
step   1760 | loss 1.6112 | lr 3.00e-04 | grad 1.30 | tok/s 23941
step   1770 | loss 1.9366 | lr 3.00e-04 | grad 1.23 | tok/s 24532
step   1780 | loss 1.9282 | lr 3.00e-04 | grad 1.27 | tok/s 23156
step   1790 | loss 1.5278 | lr 3.00e-04 | grad 1.09 | tok/s 23533
step   1800 | loss 1.5605 | lr 3.00e-04 | grad 1.14 | tok/s 24105
step   1810 | loss 1.6317 | lr 3.00e-04 | grad 1.43 | tok/s 24312
step   1820 | loss 1.7319 | lr 3.00e-04 | grad 1.71 | tok/s 23969

Training complete! Final step: 1824
