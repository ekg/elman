Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_21/levelllama_100m_20260126_173355
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 314,909,696 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.0666 | lr 3.00e-04 | grad 2.08 | tok/s 10044
step     20 | loss 2.9533 | lr 3.00e-04 | grad 0.80 | tok/s 19529
step     30 | loss 3.3328 | lr 3.00e-04 | grad 1.39 | tok/s 20654
step     40 | loss 5.5883 | lr 3.00e-04 | grad 6.53 | tok/s 21056
step     50 | loss 4.9158 | lr 3.00e-04 | grad 3.56 | tok/s 21339
step     60 | loss 3.8783 | lr 3.00e-04 | grad 2.64 | tok/s 21323
step     70 | loss 3.4274 | lr 3.00e-04 | grad 2.88 | tok/s 21286
step     80 | loss 3.2843 | lr 3.00e-04 | grad 1.88 | tok/s 21236
step     90 | loss 3.0358 | lr 3.00e-04 | grad 1.40 | tok/s 21214
step    100 | loss 2.8165 | lr 3.00e-04 | grad 1.91 | tok/s 21185
step    110 | loss 2.7520 | lr 3.00e-04 | grad 2.70 | tok/s 21021
step    120 | loss 3.3896 | lr 3.00e-04 | grad 0.78 | tok/s 19972
step    130 | loss 2.6147 | lr 3.00e-04 | grad 2.05 | tok/s 20451
step    140 | loss 2.9067 | lr 3.00e-04 | grad 4.53 | tok/s 20511
step    150 | loss 2.7867 | lr 3.00e-04 | grad 4.44 | tok/s 21021
step    160 | loss 2.8731 | lr 3.00e-04 | grad 1.41 | tok/s 20309
step    170 | loss 2.7044 | lr 3.00e-04 | grad 0.84 | tok/s 20021
step    180 | loss 2.8773 | lr 3.00e-04 | grad 1.30 | tok/s 20484
step    190 | loss 2.4458 | lr 3.00e-04 | grad 0.96 | tok/s 20071
step    200 | loss 2.3840 | lr 3.00e-04 | grad 0.84 | tok/s 20988
step    210 | loss 2.4040 | lr 3.00e-04 | grad 2.25 | tok/s 19931
step    220 | loss 2.6600 | lr 3.00e-04 | grad 2.61 | tok/s 20116
step    230 | loss 2.4441 | lr 3.00e-04 | grad 1.57 | tok/s 20094
step    240 | loss 2.7554 | lr 3.00e-04 | grad 1.81 | tok/s 20358
step    250 | loss 2.2999 | lr 3.00e-04 | grad 0.79 | tok/s 20221
step    260 | loss 2.4424 | lr 3.00e-04 | grad 1.82 | tok/s 20785
step    270 | loss 2.3059 | lr 3.00e-04 | grad 1.98 | tok/s 20306
step    280 | loss 2.2377 | lr 3.00e-04 | grad 0.79 | tok/s 19057
step    290 | loss 2.1739 | lr 3.00e-04 | grad 1.53 | tok/s 19713
step    300 | loss 2.4319 | lr 3.00e-04 | grad 1.08 | tok/s 19881
step    310 | loss 2.1279 | lr 3.00e-04 | grad 0.81 | tok/s 19779
step    320 | loss 2.3949 | lr 3.00e-04 | grad 2.58 | tok/s 20020
step    330 | loss 2.1893 | lr 3.00e-04 | grad 0.92 | tok/s 20237
step    340 | loss 2.4851 | lr 3.00e-04 | grad 1.43 | tok/s 20179
step    350 | loss 2.4795 | lr 3.00e-04 | grad 1.08 | tok/s 20756
step    360 | loss 2.0990 | lr 3.00e-04 | grad 0.95 | tok/s 19845
step    370 | loss 2.1677 | lr 3.00e-04 | grad 0.96 | tok/s 20917
step    380 | loss 1.9903 | lr 3.00e-04 | grad 1.10 | tok/s 21103
step    390 | loss 1.9392 | lr 3.00e-04 | grad 0.84 | tok/s 21103
step    400 | loss 2.2663 | lr 3.00e-04 | grad 0.89 | tok/s 19981
step    410 | loss 2.1654 | lr 3.00e-04 | grad 1.06 | tok/s 20170
step    420 | loss 2.3415 | lr 3.00e-04 | grad 1.33 | tok/s 20982
step    430 | loss 2.1805 | lr 3.00e-04 | grad 1.62 | tok/s 20697
step    440 | loss 2.1695 | lr 3.00e-04 | grad 1.09 | tok/s 20051
step    450 | loss 2.0624 | lr 3.00e-04 | grad 1.05 | tok/s 20276
step    460 | loss 2.1576 | lr 3.00e-04 | grad 0.96 | tok/s 20579
step    470 | loss 2.1408 | lr 3.00e-04 | grad 1.49 | tok/s 20445
step    480 | loss 2.1664 | lr 3.00e-04 | grad 1.40 | tok/s 20916
step    490 | loss 2.1015 | lr 3.00e-04 | grad 1.41 | tok/s 20080
step    500 | loss 2.2402 | lr 3.00e-04 | grad 0.90 | tok/s 20392
step    510 | loss 2.1008 | lr 3.00e-04 | grad 0.82 | tok/s 19473
step    520 | loss 1.9693 | lr 3.00e-04 | grad 0.98 | tok/s 20420
step    530 | loss 2.1172 | lr 3.00e-04 | grad 1.02 | tok/s 20091
step    540 | loss 2.0850 | lr 3.00e-04 | grad 0.67 | tok/s 19655
step    550 | loss 1.7709 | lr 3.00e-04 | grad 1.71 | tok/s 20516
step    560 | loss 1.9728 | lr 3.00e-04 | grad 0.91 | tok/s 21134
step    570 | loss 1.8531 | lr 3.00e-04 | grad 1.06 | tok/s 21123
step    580 | loss 1.7838 | lr 3.00e-04 | grad 1.00 | tok/s 21135
step    590 | loss 1.8348 | lr 3.00e-04 | grad 1.00 | tok/s 21155
step    600 | loss 1.7906 | lr 3.00e-04 | grad 0.94 | tok/s 21123
step    610 | loss 1.7533 | lr 3.00e-04 | grad 0.78 | tok/s 21143
step    620 | loss 1.7397 | lr 3.00e-04 | grad 1.15 | tok/s 21067
step    630 | loss 1.9704 | lr 3.00e-04 | grad 2.12 | tok/s 19902
step    640 | loss 2.1065 | lr 3.00e-04 | grad 1.49 | tok/s 20151
step    650 | loss 1.9281 | lr 3.00e-04 | grad 0.95 | tok/s 20141
step    660 | loss 1.9937 | lr 3.00e-04 | grad 1.08 | tok/s 20911
step    670 | loss 2.0302 | lr 3.00e-04 | grad 2.30 | tok/s 20219
step    680 | loss 2.0078 | lr 3.00e-04 | grad 1.19 | tok/s 19889
step    690 | loss 1.9724 | lr 3.00e-04 | grad 1.05 | tok/s 19640
step    700 | loss 1.8889 | lr 3.00e-04 | grad 0.93 | tok/s 20095
step    710 | loss 2.0247 | lr 3.00e-04 | grad 2.05 | tok/s 19739
step    720 | loss 1.7886 | lr 3.00e-04 | grad 1.21 | tok/s 20575
step    730 | loss 1.8686 | lr 3.00e-04 | grad 0.84 | tok/s 20266
step    740 | loss 2.3044 | lr 3.00e-04 | grad 1.84 | tok/s 20811
step    750 | loss 2.1063 | lr 3.00e-04 | grad 1.45 | tok/s 21102
step    760 | loss 1.9079 | lr 3.00e-04 | grad 2.05 | tok/s 20630
step    770 | loss 1.9056 | lr 3.00e-04 | grad 0.98 | tok/s 20287
step    780 | loss 1.8549 | lr 3.00e-04 | grad 1.20 | tok/s 20437
step    790 | loss 2.1779 | lr 3.00e-04 | grad 1.98 | tok/s 20905
step    800 | loss 1.7430 | lr 3.00e-04 | grad 1.05 | tok/s 20524
step    810 | loss 1.6764 | lr 3.00e-04 | grad 1.74 | tok/s 19831
step    820 | loss 1.8357 | lr 3.00e-04 | grad 1.10 | tok/s 20240
step    830 | loss 1.8746 | lr 3.00e-04 | grad 1.17 | tok/s 19979
step    840 | loss 1.9894 | lr 3.00e-04 | grad 0.95 | tok/s 19869
step    850 | loss 1.9566 | lr 3.00e-04 | grad 1.09 | tok/s 20293
step    860 | loss 2.0128 | lr 3.00e-04 | grad 1.25 | tok/s 20618
step    870 | loss 2.0528 | lr 3.00e-04 | grad 1.09 | tok/s 20786
step    880 | loss 1.9003 | lr 3.00e-04 | grad 0.92 | tok/s 20382
step    890 | loss 1.7885 | lr 3.00e-04 | grad 1.03 | tok/s 20269
step    900 | loss 1.8448 | lr 3.00e-04 | grad 0.86 | tok/s 20199
step    910 | loss 1.9044 | lr 3.00e-04 | grad 3.50 | tok/s 19991
step    920 | loss 1.8015 | lr 3.00e-04 | grad 1.02 | tok/s 20221
step    930 | loss 1.7641 | lr 3.00e-04 | grad 0.98 | tok/s 20492
step    940 | loss 1.7393 | lr 3.00e-04 | grad 0.95 | tok/s 20008
step    950 | loss 1.8140 | lr 3.00e-04 | grad 1.25 | tok/s 19672
step    960 | loss 1.7553 | lr 3.00e-04 | grad 1.05 | tok/s 20217
step    970 | loss 1.7534 | lr 3.00e-04 | grad 1.01 | tok/s 20227
step    980 | loss 2.5492 | lr 3.00e-04 | grad 1.79 | tok/s 21045
step    990 | loss 1.9569 | lr 3.00e-04 | grad 1.11 | tok/s 20176
step   1000 | loss 1.8751 | lr 3.00e-04 | grad 1.20 | tok/s 20236
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8751.pt
step   1010 | loss 1.6178 | lr 3.00e-04 | grad 1.23 | tok/s 13290
step   1020 | loss 1.7074 | lr 3.00e-04 | grad 1.13 | tok/s 21225
step   1030 | loss 1.8601 | lr 3.00e-04 | grad 1.15 | tok/s 20155
step   1040 | loss 2.3337 | lr 3.00e-04 | grad 1.54 | tok/s 20642
step   1050 | loss 1.9482 | lr 3.00e-04 | grad 0.99 | tok/s 20788
step   1060 | loss 1.5968 | lr 3.00e-04 | grad 0.80 | tok/s 20473
step   1070 | loss 1.6883 | lr 3.00e-04 | grad 1.39 | tok/s 20426
step   1080 | loss 1.5942 | lr 3.00e-04 | grad 1.10 | tok/s 21127
step   1090 | loss 1.5668 | lr 3.00e-04 | grad 1.20 | tok/s 21151
step   1100 | loss 1.5341 | lr 3.00e-04 | grad 0.98 | tok/s 21156
step   1110 | loss 1.4771 | lr 3.00e-04 | grad 1.24 | tok/s 21165
step   1120 | loss 1.6921 | lr 3.00e-04 | grad 1.04 | tok/s 20594
step   1130 | loss 2.1242 | lr 3.00e-04 | grad 1.02 | tok/s 20815
step   1140 | loss 2.0846 | lr 3.00e-04 | grad 1.07 | tok/s 21041
step   1150 | loss 2.2559 | lr 3.00e-04 | grad 1.79 | tok/s 20576
step   1160 | loss 1.9635 | lr 3.00e-04 | grad 2.66 | tok/s 19886
step   1170 | loss 1.8146 | lr 3.00e-04 | grad 1.28 | tok/s 19844
step   1180 | loss 1.7291 | lr 3.00e-04 | grad 1.25 | tok/s 20892
step   1190 | loss 2.0534 | lr 3.00e-04 | grad 1.31 | tok/s 20928
step   1200 | loss 1.5665 | lr 3.00e-04 | grad 1.23 | tok/s 21127
step   1210 | loss 1.6176 | lr 3.00e-04 | grad 1.02 | tok/s 19887
step   1220 | loss 1.7053 | lr 3.00e-04 | grad 1.23 | tok/s 20541
step   1230 | loss 1.7444 | lr 3.00e-04 | grad 0.94 | tok/s 20690
step   1240 | loss 1.6355 | lr 3.00e-04 | grad 1.40 | tok/s 20925
step   1250 | loss 1.8517 | lr 3.00e-04 | grad 1.36 | tok/s 20440
step   1260 | loss 1.9699 | lr 3.00e-04 | grad 1.10 | tok/s 20993
step   1270 | loss 1.6847 | lr 3.00e-04 | grad 1.54 | tok/s 20356
step   1280 | loss 1.6966 | lr 3.00e-04 | grad 1.28 | tok/s 20390
step   1290 | loss 1.6969 | lr 3.00e-04 | grad 1.32 | tok/s 19941
step   1300 | loss 1.9286 | lr 3.00e-04 | grad 3.03 | tok/s 19883
step   1310 | loss 1.9426 | lr 3.00e-04 | grad 1.07 | tok/s 20719
step   1320 | loss 1.7880 | lr 3.00e-04 | grad 2.02 | tok/s 20649
step   1330 | loss 1.8253 | lr 3.00e-04 | grad 1.22 | tok/s 20603
step   1340 | loss 1.8779 | lr 3.00e-04 | grad 0.98 | tok/s 19944
step   1350 | loss 1.7587 | lr 3.00e-04 | grad 0.99 | tok/s 20733
step   1360 | loss 1.8025 | lr 3.00e-04 | grad 1.25 | tok/s 19480
step   1370 | loss 1.9360 | lr 3.00e-04 | grad 2.44 | tok/s 20811
step   1380 | loss 1.7802 | lr 3.00e-04 | grad 1.34 | tok/s 20159
step   1390 | loss 1.7740 | lr 3.00e-04 | grad 1.22 | tok/s 20551
step   1400 | loss 1.8164 | lr 3.00e-04 | grad 1.05 | tok/s 20108
step   1410 | loss 1.6405 | lr 3.00e-04 | grad 1.39 | tok/s 19752
step   1420 | loss 1.6684 | lr 3.00e-04 | grad 1.25 | tok/s 20958
step   1430 | loss 2.0480 | lr 3.00e-04 | grad 1.04 | tok/s 20234
step   1440 | loss 1.7470 | lr 3.00e-04 | grad 1.41 | tok/s 20420
step   1450 | loss 1.7566 | lr 3.00e-04 | grad 1.09 | tok/s 20669
step   1460 | loss 1.8286 | lr 3.00e-04 | grad 1.34 | tok/s 20063
step   1470 | loss 1.6656 | lr 3.00e-04 | grad 1.32 | tok/s 19789
step   1480 | loss 1.6964 | lr 3.00e-04 | grad 1.73 | tok/s 20496
step   1490 | loss 1.8984 | lr 3.00e-04 | grad 2.17 | tok/s 20341
step   1500 | loss 1.9372 | lr 3.00e-04 | grad 1.22 | tok/s 20659
step   1510 | loss 1.5847 | lr 3.00e-04 | grad 0.98 | tok/s 20248
step   1520 | loss 1.7234 | lr 3.00e-04 | grad 1.50 | tok/s 20177

Training complete! Final step: 1523
