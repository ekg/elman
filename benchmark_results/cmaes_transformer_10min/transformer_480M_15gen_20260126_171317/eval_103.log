Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_103/levelllama_100m_20260126_191643
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 659,940,736 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 6.1916 | lr 3.00e-04 | grad 6.81 | tok/s 5504
step     20 | loss 2.8900 | lr 3.00e-04 | grad 2.89 | tok/s 11890
step     30 | loss 2.8604 | lr 3.00e-04 | grad 2.80 | tok/s 11983
step     40 | loss 3.0051 | lr 3.00e-04 | grad 2.72 | tok/s 11451
step     50 | loss 3.2263 | lr 3.00e-04 | grad 4.06 | tok/s 11626
step     60 | loss 2.6107 | lr 3.00e-04 | grad 3.55 | tok/s 11995
step     70 | loss 2.5557 | lr 3.00e-04 | grad 2.19 | tok/s 12116
step     80 | loss 6.8136 | lr 3.00e-04 | grad 11.12 | tok/s 12189
step     90 | loss 5.0324 | lr 3.00e-04 | grad 3.47 | tok/s 12402
step    100 | loss 3.8422 | lr 3.00e-04 | grad 3.34 | tok/s 12365
step    110 | loss 3.7490 | lr 3.00e-04 | grad 9.75 | tok/s 12363
step    120 | loss 3.5427 | lr 3.00e-04 | grad 6.88 | tok/s 12327
step    130 | loss 3.3992 | lr 3.00e-04 | grad 4.78 | tok/s 12310
step    140 | loss 2.9720 | lr 3.00e-04 | grad 2.75 | tok/s 12271
step    150 | loss 3.2119 | lr 3.00e-04 | grad 5.91 | tok/s 12277
step    160 | loss 2.6232 | lr 3.00e-04 | grad 3.38 | tok/s 12236
step    170 | loss 2.6643 | lr 3.00e-04 | grad 6.12 | tok/s 12219
step    180 | loss 2.4601 | lr 3.00e-04 | grad 3.69 | tok/s 12219
step    190 | loss 2.6337 | lr 3.00e-04 | grad 4.72 | tok/s 12209
step    200 | loss 2.3371 | lr 3.00e-04 | grad 3.05 | tok/s 12208
step    210 | loss 2.3726 | lr 3.00e-04 | grad 6.19 | tok/s 12197
step    220 | loss 2.6018 | lr 3.00e-04 | grad 3.97 | tok/s 12057
step    230 | loss 3.3739 | lr 3.00e-04 | grad 2.58 | tok/s 11918
step    240 | loss 2.6610 | lr 3.00e-04 | grad 3.55 | tok/s 11305
step    250 | loss 2.5092 | lr 3.00e-04 | grad 2.69 | tok/s 11630
step    260 | loss 2.3144 | lr 3.00e-04 | grad 2.95 | tok/s 11976
step    270 | loss 2.5627 | lr 3.00e-04 | grad 1.83 | tok/s 11838
step    280 | loss 2.6802 | lr 3.00e-04 | grad 3.55 | tok/s 11612
step    290 | loss 2.5670 | lr 3.00e-04 | grad 1.64 | tok/s 12205
step    300 | loss 1.5767 | lr 3.00e-04 | grad 3.48 | tok/s 12205
step    310 | loss 2.8446 | lr 3.00e-04 | grad 2.55 | tok/s 11989
step    320 | loss 2.5587 | lr 3.00e-04 | grad 3.47 | tok/s 11751
step    330 | loss 2.3577 | lr 3.00e-04 | grad 1.52 | tok/s 11341
step    340 | loss 2.6367 | lr 3.00e-04 | grad 2.22 | tok/s 11502
step    350 | loss 2.4453 | lr 3.00e-04 | grad 2.64 | tok/s 11798
step    360 | loss 2.4660 | lr 3.00e-04 | grad 2.94 | tok/s 12061
step    370 | loss 2.2471 | lr 3.00e-04 | grad 1.98 | tok/s 10936
step    380 | loss 2.2220 | lr 3.00e-04 | grad 1.55 | tok/s 11646
step    390 | loss 2.0914 | lr 3.00e-04 | grad 1.31 | tok/s 12172
step    400 | loss 2.0842 | lr 3.00e-04 | grad 2.31 | tok/s 12044
step    410 | loss 2.0129 | lr 3.00e-04 | grad 1.59 | tok/s 11791
step    420 | loss 2.1845 | lr 3.00e-04 | grad 2.25 | tok/s 11250
step    430 | loss 2.4607 | lr 3.00e-04 | grad 2.91 | tok/s 11981
step    440 | loss 2.5088 | lr 3.00e-04 | grad 2.06 | tok/s 11332
step    450 | loss 2.3883 | lr 3.00e-04 | grad 1.12 | tok/s 11713
step    460 | loss 2.1772 | lr 3.00e-04 | grad 3.05 | tok/s 11477
step    470 | loss 2.2765 | lr 3.00e-04 | grad 2.33 | tok/s 11823
step    480 | loss 2.6409 | lr 3.00e-04 | grad 3.11 | tok/s 11819
step    490 | loss 2.1618 | lr 3.00e-04 | grad 2.00 | tok/s 11167
step    500 | loss 2.1391 | lr 3.00e-04 | grad 1.89 | tok/s 11922
step    510 | loss 2.1476 | lr 3.00e-04 | grad 1.52 | tok/s 12084
step    520 | loss 2.1277 | lr 3.00e-04 | grad 1.55 | tok/s 12057
step    530 | loss 2.2701 | lr 3.00e-04 | grad 1.88 | tok/s 11595
step    540 | loss 2.0616 | lr 3.00e-04 | grad 2.09 | tok/s 11592
step    550 | loss 1.9153 | lr 3.00e-04 | grad 1.82 | tok/s 11351
step    560 | loss 2.0767 | lr 3.00e-04 | grad 1.70 | tok/s 11073
step    570 | loss 2.0177 | lr 3.00e-04 | grad 2.58 | tok/s 11359
step    580 | loss 1.9112 | lr 3.00e-04 | grad 1.80 | tok/s 11309
step    590 | loss 2.2343 | lr 3.00e-04 | grad 1.54 | tok/s 11604
step    600 | loss 2.1317 | lr 3.00e-04 | grad 1.37 | tok/s 11202
step    610 | loss 1.9666 | lr 3.00e-04 | grad 1.68 | tok/s 11775
step    620 | loss 1.8403 | lr 3.00e-04 | grad 1.12 | tok/s 11160
step    630 | loss 1.9884 | lr 3.00e-04 | grad 2.39 | tok/s 11259
step    640 | loss 2.1529 | lr 3.00e-04 | grad 1.63 | tok/s 11562
step    650 | loss 1.9748 | lr 3.00e-04 | grad 1.82 | tok/s 11606
step    660 | loss 2.0117 | lr 3.00e-04 | grad 1.32 | tok/s 11645
step    670 | loss 2.2272 | lr 3.00e-04 | grad 1.75 | tok/s 11742
step    680 | loss 1.9875 | lr 3.00e-04 | grad 1.34 | tok/s 11504
step    690 | loss 2.2473 | lr 3.00e-04 | grad 1.79 | tok/s 11906
step    700 | loss 1.9910 | lr 3.00e-04 | grad 1.65 | tok/s 12129
step    710 | loss 1.8997 | lr 3.00e-04 | grad 2.08 | tok/s 11337
step    720 | loss 1.7473 | lr 3.00e-04 | grad 1.87 | tok/s 11190
step    730 | loss 1.7894 | lr 3.00e-04 | grad 1.94 | tok/s 12110
step    740 | loss 1.8721 | lr 3.00e-04 | grad 1.67 | tok/s 11959
step    750 | loss 1.6394 | lr 3.00e-04 | grad 1.55 | tok/s 12144
step    760 | loss 1.4991 | lr 3.00e-04 | grad 1.52 | tok/s 12149
step    770 | loss 1.4607 | lr 3.00e-04 | grad 1.37 | tok/s 12144
step    780 | loss 1.4004 | lr 3.00e-04 | grad 1.28 | tok/s 12137
step    790 | loss 1.4616 | lr 3.00e-04 | grad 1.85 | tok/s 11758
step    800 | loss 2.1855 | lr 3.00e-04 | grad 2.48 | tok/s 11740
step    810 | loss 1.9342 | lr 3.00e-04 | grad 1.51 | tok/s 11662
step    820 | loss 1.9420 | lr 3.00e-04 | grad 1.92 | tok/s 11196
step    830 | loss 1.9551 | lr 3.00e-04 | grad 1.40 | tok/s 12020
step    840 | loss 1.8500 | lr 3.00e-04 | grad 1.34 | tok/s 12141
step    850 | loss 1.8706 | lr 3.00e-04 | grad 1.80 | tok/s 12100
step    860 | loss 1.8453 | lr 3.00e-04 | grad 2.27 | tok/s 11949
step    870 | loss 1.7984 | lr 3.00e-04 | grad 1.57 | tok/s 11508
step    880 | loss 1.9411 | lr 3.00e-04 | grad 1.23 | tok/s 11562
step    890 | loss 1.9330 | lr 3.00e-04 | grad 1.78 | tok/s 11712
step    900 | loss 1.7976 | lr 3.00e-04 | grad 1.27 | tok/s 11734
step    910 | loss 1.6528 | lr 3.00e-04 | grad 1.67 | tok/s 11491
step    920 | loss 1.8249 | lr 3.00e-04 | grad 2.34 | tok/s 11940
step    930 | loss 1.8414 | lr 3.00e-04 | grad 2.17 | tok/s 11407
step    940 | loss 1.7412 | lr 3.00e-04 | grad 1.39 | tok/s 12019
step    950 | loss 1.7750 | lr 3.00e-04 | grad 1.81 | tok/s 12084
step    960 | loss 1.6446 | lr 3.00e-04 | grad 1.52 | tok/s 12117
step    970 | loss 1.9025 | lr 3.00e-04 | grad 1.79 | tok/s 11388
step    980 | loss 1.8356 | lr 3.00e-04 | grad 1.21 | tok/s 11702
step    990 | loss 1.7291 | lr 3.00e-04 | grad 1.44 | tok/s 11910
step   1000 | loss 2.0694 | lr 3.00e-04 | grad 5.22 | tok/s 11428
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0694.pt
step   1010 | loss 1.9107 | lr 3.00e-04 | grad 1.85 | tok/s 4727
step   1020 | loss 1.8240 | lr 3.00e-04 | grad 1.46 | tok/s 11192
step   1030 | loss 1.6387 | lr 3.00e-04 | grad 1.52 | tok/s 11810
step   1040 | loss 1.7043 | lr 3.00e-04 | grad 1.30 | tok/s 11743
step   1050 | loss 1.8134 | lr 3.00e-04 | grad 1.66 | tok/s 11422
step   1060 | loss 1.9292 | lr 3.00e-04 | grad 1.49 | tok/s 11925
step   1070 | loss 1.9858 | lr 3.00e-04 | grad 1.46 | tok/s 11685
step   1080 | loss 1.6011 | lr 3.00e-04 | grad 1.31 | tok/s 10998
step   1090 | loss 1.2315 | lr 3.00e-04 | grad 1.95 | tok/s 12187
step   1100 | loss 1.7316 | lr 3.00e-04 | grad 1.47 | tok/s 11693
step   1110 | loss 1.6245 | lr 3.00e-04 | grad 1.59 | tok/s 12218
step   1120 | loss 1.5695 | lr 3.00e-04 | grad 1.57 | tok/s 12193
step   1130 | loss 1.4970 | lr 3.00e-04 | grad 1.44 | tok/s 12200
step   1140 | loss 1.4919 | lr 3.00e-04 | grad 1.41 | tok/s 12194
step   1150 | loss 1.4934 | lr 3.00e-04 | grad 1.23 | tok/s 12183
step   1160 | loss 1.3974 | lr 3.00e-04 | grad 1.41 | tok/s 12174
step   1170 | loss 1.4682 | lr 3.00e-04 | grad 1.40 | tok/s 12185
step   1180 | loss 1.5129 | lr 3.00e-04 | grad 1.74 | tok/s 12178
step   1190 | loss 1.4221 | lr 3.00e-04 | grad 1.20 | tok/s 12180
step   1200 | loss 1.4359 | lr 3.00e-04 | grad 1.34 | tok/s 12177
step   1210 | loss 1.4472 | lr 3.00e-04 | grad 1.45 | tok/s 12180
step   1220 | loss 1.4444 | lr 3.00e-04 | grad 1.41 | tok/s 12164
step   1230 | loss 1.4384 | lr 3.00e-04 | grad 1.17 | tok/s 12182
step   1240 | loss 1.6047 | lr 3.00e-04 | grad 5.53 | tok/s 11923
step   1250 | loss 1.9100 | lr 3.00e-04 | grad 1.32 | tok/s 11580
step   1260 | loss 1.4645 | lr 3.00e-04 | grad 1.47 | tok/s 11428
step   1270 | loss 1.8765 | lr 3.00e-04 | grad 1.52 | tok/s 11292
step   1280 | loss 1.7881 | lr 3.00e-04 | grad 1.41 | tok/s 11937
step   1290 | loss 1.6909 | lr 3.00e-04 | grad 1.75 | tok/s 11738
step   1300 | loss 1.6916 | lr 3.00e-04 | grad 2.22 | tok/s 11551
step   1310 | loss 1.6626 | lr 3.00e-04 | grad 2.16 | tok/s 12114
step   1320 | loss 1.8005 | lr 3.00e-04 | grad 2.12 | tok/s 11988
step   1330 | loss 1.5721 | lr 3.00e-04 | grad 1.71 | tok/s 11972
step   1340 | loss 1.8071 | lr 3.00e-04 | grad 1.12 | tok/s 11097
step   1350 | loss 1.8704 | lr 3.00e-04 | grad 1.80 | tok/s 11300
step   1360 | loss 1.6893 | lr 3.00e-04 | grad 1.20 | tok/s 11810
step   1370 | loss 1.7116 | lr 3.00e-04 | grad 3.88 | tok/s 11576
step   1380 | loss 1.7447 | lr 3.00e-04 | grad 1.63 | tok/s 11147
step   1390 | loss 1.5919 | lr 3.00e-04 | grad 1.21 | tok/s 11567
step   1400 | loss 1.5839 | lr 3.00e-04 | grad 1.48 | tok/s 11657
step   1410 | loss 1.7647 | lr 3.00e-04 | grad 2.00 | tok/s 11432
step   1420 | loss 1.7714 | lr 3.00e-04 | grad 1.51 | tok/s 11416
step   1430 | loss 1.5407 | lr 3.00e-04 | grad 1.50 | tok/s 11587
step   1440 | loss 1.3283 | lr 3.00e-04 | grad 1.45 | tok/s 12157
step   1450 | loss 1.4409 | lr 3.00e-04 | grad 4.12 | tok/s 12008
step   1460 | loss 1.8095 | lr 3.00e-04 | grad 2.62 | tok/s 11341
step   1470 | loss 1.6322 | lr 3.00e-04 | grad 1.34 | tok/s 12060
step   1480 | loss 2.2573 | lr 3.00e-04 | grad 1.80 | tok/s 11934
step   1490 | loss 1.8683 | lr 3.00e-04 | grad 2.73 | tok/s 12110
step   1500 | loss 1.4615 | lr 3.00e-04 | grad 1.28 | tok/s 12162
step   1510 | loss 1.7155 | lr 3.00e-04 | grad 1.62 | tok/s 12008
step   1520 | loss 1.6022 | lr 3.00e-04 | grad 1.51 | tok/s 11771
step   1530 | loss 1.5352 | lr 3.00e-04 | grad 1.32 | tok/s 11784
step   1540 | loss 1.7933 | lr 3.00e-04 | grad 1.23 | tok/s 11561
step   1550 | loss 1.4439 | lr 3.00e-04 | grad 1.30 | tok/s 12062
step   1560 | loss 1.7245 | lr 3.00e-04 | grad 1.38 | tok/s 11436
step   1570 | loss 1.5579 | lr 3.00e-04 | grad 1.78 | tok/s 12037
step   1580 | loss 2.1071 | lr 3.00e-04 | grad 1.91 | tok/s 11980
step   1590 | loss 1.7280 | lr 3.00e-04 | grad 1.39 | tok/s 11409
step   1600 | loss 1.0164 | lr 3.00e-04 | grad 0.91 | tok/s 12217
step   1610 | loss 1.3142 | lr 3.00e-04 | grad 1.28 | tok/s 11281
step   1620 | loss 1.6102 | lr 3.00e-04 | grad 1.59 | tok/s 11527
step   1630 | loss 1.5773 | lr 3.00e-04 | grad 1.66 | tok/s 11807
step   1640 | loss 1.5744 | lr 3.00e-04 | grad 2.19 | tok/s 11419
step   1650 | loss 1.6901 | lr 3.00e-04 | grad 2.06 | tok/s 10814
step   1660 | loss 1.4694 | lr 3.00e-04 | grad 1.20 | tok/s 12122
step   1670 | loss 1.8004 | lr 3.00e-04 | grad 3.81 | tok/s 11528
step   1680 | loss 1.6799 | lr 3.00e-04 | grad 1.64 | tok/s 11290
step   1690 | loss 1.5836 | lr 3.00e-04 | grad 1.70 | tok/s 11706
step   1700 | loss 1.6654 | lr 3.00e-04 | grad 1.29 | tok/s 11386
step   1710 | loss 1.6604 | lr 3.00e-04 | grad 1.35 | tok/s 11756
step   1720 | loss 1.7357 | lr 3.00e-04 | grad 1.55 | tok/s 12149
step   1730 | loss 1.5926 | lr 3.00e-04 | grad 2.42 | tok/s 12142

Training complete! Final step: 1735
