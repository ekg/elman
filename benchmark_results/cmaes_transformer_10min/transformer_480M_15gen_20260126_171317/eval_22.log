Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_22/levelllama_100m_20260126_173355
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 306,071,808 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.8469 | lr 3.00e-04 | grad 1.65 | tok/s 10111
step     20 | loss 2.9243 | lr 3.00e-04 | grad 0.89 | tok/s 19880
step     30 | loss 3.2964 | lr 3.00e-04 | grad 1.34 | tok/s 21050
step     40 | loss 4.7987 | lr 3.00e-04 | grad 5.09 | tok/s 21486
step     50 | loss 4.2675 | lr 3.00e-04 | grad 3.47 | tok/s 21786
step     60 | loss 4.0468 | lr 3.00e-04 | grad 2.84 | tok/s 21764
step     70 | loss 3.7476 | lr 3.00e-04 | grad 2.80 | tok/s 21729
step     80 | loss 3.6974 | lr 3.00e-04 | grad 2.33 | tok/s 21693
step     90 | loss 3.4314 | lr 3.00e-04 | grad 1.76 | tok/s 21692
step    100 | loss 3.2309 | lr 3.00e-04 | grad 1.82 | tok/s 21652
step    110 | loss 2.9829 | lr 3.00e-04 | grad 1.76 | tok/s 21436
step    120 | loss 3.1490 | lr 3.00e-04 | grad 0.68 | tok/s 20397
step    130 | loss 2.6126 | lr 3.00e-04 | grad 1.98 | tok/s 20862
step    140 | loss 2.8654 | lr 3.00e-04 | grad 3.88 | tok/s 20904
step    150 | loss 2.8882 | lr 3.00e-04 | grad 3.95 | tok/s 21387
step    160 | loss 2.8397 | lr 3.00e-04 | grad 1.45 | tok/s 20673
step    170 | loss 2.6811 | lr 3.00e-04 | grad 0.67 | tok/s 20342
step    180 | loss 2.8905 | lr 3.00e-04 | grad 1.23 | tok/s 20823
step    190 | loss 2.4425 | lr 3.00e-04 | grad 0.74 | tok/s 20413
step    200 | loss 2.3954 | lr 3.00e-04 | grad 0.88 | tok/s 21336
step    210 | loss 2.4063 | lr 3.00e-04 | grad 2.12 | tok/s 20218
step    220 | loss 2.7196 | lr 3.00e-04 | grad 6.62 | tok/s 20436
step    230 | loss 2.5888 | lr 3.00e-04 | grad 1.48 | tok/s 20379
step    240 | loss 2.7655 | lr 3.00e-04 | grad 1.80 | tok/s 20660
step    250 | loss 2.3173 | lr 3.00e-04 | grad 0.76 | tok/s 20488
step    260 | loss 2.4528 | lr 3.00e-04 | grad 1.61 | tok/s 21045
step    270 | loss 2.3162 | lr 3.00e-04 | grad 1.55 | tok/s 20551
step    280 | loss 2.2483 | lr 3.00e-04 | grad 0.62 | tok/s 19294
step    290 | loss 2.1883 | lr 3.00e-04 | grad 0.78 | tok/s 19941
step    300 | loss 2.4376 | lr 3.00e-04 | grad 0.88 | tok/s 20107
step    310 | loss 2.1421 | lr 3.00e-04 | grad 0.62 | tok/s 20004
step    320 | loss 2.4063 | lr 3.00e-04 | grad 2.53 | tok/s 20235
step    330 | loss 2.2027 | lr 3.00e-04 | grad 0.77 | tok/s 20443
step    340 | loss 2.4981 | lr 3.00e-04 | grad 1.37 | tok/s 20356
step    350 | loss 2.5221 | lr 3.00e-04 | grad 0.88 | tok/s 20934
step    360 | loss 2.1234 | lr 3.00e-04 | grad 0.87 | tok/s 20015
step    370 | loss 2.1881 | lr 3.00e-04 | grad 0.79 | tok/s 21099
step    380 | loss 2.0328 | lr 3.00e-04 | grad 2.05 | tok/s 21291
step    390 | loss 1.9990 | lr 3.00e-04 | grad 1.30 | tok/s 21305
step    400 | loss 2.2875 | lr 3.00e-04 | grad 0.85 | tok/s 20167
step    410 | loss 2.1752 | lr 3.00e-04 | grad 0.98 | tok/s 20341
step    420 | loss 2.3763 | lr 3.00e-04 | grad 1.21 | tok/s 21212
step    430 | loss 2.2167 | lr 3.00e-04 | grad 1.59 | tok/s 20851
step    440 | loss 2.1868 | lr 3.00e-04 | grad 1.05 | tok/s 20223
step    450 | loss 2.0835 | lr 3.00e-04 | grad 0.66 | tok/s 20449
step    460 | loss 2.1951 | lr 3.00e-04 | grad 0.80 | tok/s 20730
step    470 | loss 2.1832 | lr 3.00e-04 | grad 1.80 | tok/s 20567
step    480 | loss 2.2292 | lr 3.00e-04 | grad 1.12 | tok/s 21049
step    490 | loss 2.1230 | lr 3.00e-04 | grad 1.16 | tok/s 20221
step    500 | loss 2.2687 | lr 3.00e-04 | grad 0.80 | tok/s 20542
step    510 | loss 2.1211 | lr 3.00e-04 | grad 0.86 | tok/s 19618
step    520 | loss 1.9966 | lr 3.00e-04 | grad 0.92 | tok/s 20544
step    530 | loss 2.1395 | lr 3.00e-04 | grad 0.96 | tok/s 20203
step    540 | loss 2.1138 | lr 3.00e-04 | grad 0.75 | tok/s 19789
step    550 | loss 1.7908 | lr 3.00e-04 | grad 1.50 | tok/s 20673
step    560 | loss 1.9988 | lr 3.00e-04 | grad 1.17 | tok/s 21283
step    570 | loss 1.8822 | lr 3.00e-04 | grad 0.84 | tok/s 21270
step    580 | loss 1.8185 | lr 3.00e-04 | grad 0.55 | tok/s 21269
step    590 | loss 1.8746 | lr 3.00e-04 | grad 1.00 | tok/s 21278
step    600 | loss 1.8391 | lr 3.00e-04 | grad 0.78 | tok/s 21274
step    610 | loss 1.7985 | lr 3.00e-04 | grad 1.22 | tok/s 21276
step    620 | loss 1.7860 | lr 3.00e-04 | grad 1.00 | tok/s 21201
step    630 | loss 1.9779 | lr 3.00e-04 | grad 2.02 | tok/s 20018
step    640 | loss 2.1481 | lr 3.00e-04 | grad 1.73 | tok/s 20262
step    650 | loss 1.9563 | lr 3.00e-04 | grad 0.80 | tok/s 20259
step    660 | loss 2.0343 | lr 3.00e-04 | grad 1.14 | tok/s 21040
step    670 | loss 2.0644 | lr 3.00e-04 | grad 2.27 | tok/s 20356
step    680 | loss 2.0417 | lr 3.00e-04 | grad 1.15 | tok/s 20040
step    690 | loss 2.0031 | lr 3.00e-04 | grad 0.94 | tok/s 19882
step    700 | loss 1.9256 | lr 3.00e-04 | grad 0.89 | tok/s 20346
step    710 | loss 2.0483 | lr 3.00e-04 | grad 1.98 | tok/s 20022
step    720 | loss 1.8391 | lr 3.00e-04 | grad 1.21 | tok/s 20778
step    730 | loss 1.9173 | lr 3.00e-04 | grad 0.73 | tok/s 20453
step    740 | loss 2.3494 | lr 3.00e-04 | grad 1.83 | tok/s 21010
step    750 | loss 2.1628 | lr 3.00e-04 | grad 0.98 | tok/s 21249
step    760 | loss 1.9468 | lr 3.00e-04 | grad 1.88 | tok/s 20807
step    770 | loss 1.9463 | lr 3.00e-04 | grad 0.88 | tok/s 20463
step    780 | loss 1.8993 | lr 3.00e-04 | grad 1.15 | tok/s 20595
step    790 | loss 2.2289 | lr 3.00e-04 | grad 1.84 | tok/s 21058
step    800 | loss 1.7909 | lr 3.00e-04 | grad 0.95 | tok/s 20669
step    810 | loss 1.7134 | lr 3.00e-04 | grad 1.63 | tok/s 20001
step    820 | loss 1.8714 | lr 3.00e-04 | grad 1.02 | tok/s 20396
step    830 | loss 1.9091 | lr 3.00e-04 | grad 1.12 | tok/s 20132
step    840 | loss 2.0376 | lr 3.00e-04 | grad 0.89 | tok/s 20053
step    850 | loss 2.0193 | lr 3.00e-04 | grad 0.91 | tok/s 20467
step    860 | loss 2.0491 | lr 3.00e-04 | grad 1.23 | tok/s 20800
step    870 | loss 2.1269 | lr 3.00e-04 | grad 1.09 | tok/s 20967
step    880 | loss 1.9371 | lr 3.00e-04 | grad 0.84 | tok/s 20540
step    890 | loss 1.8257 | lr 3.00e-04 | grad 0.70 | tok/s 20442
step    900 | loss 1.8827 | lr 3.00e-04 | grad 0.84 | tok/s 20351
step    910 | loss 1.9450 | lr 3.00e-04 | grad 3.17 | tok/s 20145
step    920 | loss 1.8442 | lr 3.00e-04 | grad 0.98 | tok/s 20379
step    930 | loss 1.8080 | lr 3.00e-04 | grad 0.90 | tok/s 20637
step    940 | loss 1.7883 | lr 3.00e-04 | grad 0.96 | tok/s 20160
step    950 | loss 1.8472 | lr 3.00e-04 | grad 1.23 | tok/s 19826
step    960 | loss 1.8013 | lr 3.00e-04 | grad 0.95 | tok/s 20374
step    970 | loss 1.7868 | lr 3.00e-04 | grad 0.96 | tok/s 20386
step    980 | loss 2.5858 | lr 3.00e-04 | grad 1.57 | tok/s 21210
step    990 | loss 1.9984 | lr 3.00e-04 | grad 1.00 | tok/s 20333
step   1000 | loss 1.9276 | lr 3.00e-04 | grad 1.18 | tok/s 20389
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9276.pt
step   1010 | loss 1.6581 | lr 3.00e-04 | grad 1.13 | tok/s 13467
step   1020 | loss 1.7463 | lr 3.00e-04 | grad 1.05 | tok/s 21373
step   1030 | loss 1.9077 | lr 3.00e-04 | grad 0.95 | tok/s 20318
step   1040 | loss 2.3923 | lr 3.00e-04 | grad 1.52 | tok/s 20793
step   1050 | loss 2.0025 | lr 3.00e-04 | grad 1.03 | tok/s 20926
step   1060 | loss 1.6378 | lr 3.00e-04 | grad 0.66 | tok/s 20637
step   1070 | loss 1.7273 | lr 3.00e-04 | grad 1.11 | tok/s 20601
step   1080 | loss 1.6402 | lr 3.00e-04 | grad 1.34 | tok/s 21266
step   1090 | loss 1.6170 | lr 3.00e-04 | grad 0.75 | tok/s 21298
step   1100 | loss 1.5770 | lr 3.00e-04 | grad 1.19 | tok/s 21316
step   1110 | loss 1.5282 | lr 3.00e-04 | grad 1.01 | tok/s 21298
step   1120 | loss 1.7494 | lr 3.00e-04 | grad 0.86 | tok/s 20719
step   1130 | loss 2.1962 | lr 3.00e-04 | grad 1.13 | tok/s 20972
step   1140 | loss 2.1171 | lr 3.00e-04 | grad 1.10 | tok/s 21150
step   1150 | loss 2.3352 | lr 3.00e-04 | grad 1.75 | tok/s 20679
step   1160 | loss 2.0188 | lr 3.00e-04 | grad 2.70 | tok/s 20019
step   1170 | loss 1.8655 | lr 3.00e-04 | grad 1.14 | tok/s 19956
step   1180 | loss 1.7654 | lr 3.00e-04 | grad 1.02 | tok/s 20990
step   1190 | loss 2.1023 | lr 3.00e-04 | grad 1.26 | tok/s 21043
step   1200 | loss 1.6280 | lr 3.00e-04 | grad 1.20 | tok/s 21228
step   1210 | loss 1.6575 | lr 3.00e-04 | grad 0.91 | tok/s 20005
step   1220 | loss 1.7472 | lr 3.00e-04 | grad 1.21 | tok/s 20663
step   1230 | loss 1.7880 | lr 3.00e-04 | grad 0.94 | tok/s 20804
step   1240 | loss 1.6942 | lr 3.00e-04 | grad 1.52 | tok/s 21032
step   1250 | loss 1.9010 | lr 3.00e-04 | grad 1.25 | tok/s 20544
step   1260 | loss 2.0473 | lr 3.00e-04 | grad 0.98 | tok/s 21096
step   1270 | loss 1.7290 | lr 3.00e-04 | grad 1.48 | tok/s 20459
step   1280 | loss 1.7516 | lr 3.00e-04 | grad 1.23 | tok/s 20511
step   1290 | loss 1.7393 | lr 3.00e-04 | grad 1.12 | tok/s 20050
step   1300 | loss 1.9842 | lr 3.00e-04 | grad 2.89 | tok/s 19980
step   1310 | loss 2.0383 | lr 3.00e-04 | grad 1.13 | tok/s 20823
step   1320 | loss 1.8347 | lr 3.00e-04 | grad 1.87 | tok/s 20750
step   1330 | loss 1.8817 | lr 3.00e-04 | grad 1.31 | tok/s 20696
step   1340 | loss 1.9219 | lr 3.00e-04 | grad 0.97 | tok/s 20064
step   1350 | loss 1.8078 | lr 3.00e-04 | grad 1.06 | tok/s 20849
step   1360 | loss 1.8573 | lr 3.00e-04 | grad 1.02 | tok/s 19584
step   1370 | loss 1.9957 | lr 3.00e-04 | grad 2.31 | tok/s 20913
step   1380 | loss 1.8240 | lr 3.00e-04 | grad 1.22 | tok/s 20263
step   1390 | loss 1.8281 | lr 3.00e-04 | grad 1.20 | tok/s 20634
step   1400 | loss 1.8338 | lr 3.00e-04 | grad 1.30 | tok/s 20216
step   1410 | loss 1.6825 | lr 3.00e-04 | grad 1.43 | tok/s 19842
step   1420 | loss 1.7484 | lr 3.00e-04 | grad 1.16 | tok/s 21083
step   1430 | loss 2.0930 | lr 3.00e-04 | grad 1.03 | tok/s 20364
step   1440 | loss 1.8021 | lr 3.00e-04 | grad 1.41 | tok/s 20516
step   1450 | loss 1.8076 | lr 3.00e-04 | grad 0.95 | tok/s 20773
step   1460 | loss 1.8574 | lr 3.00e-04 | grad 1.25 | tok/s 20190
step   1470 | loss 1.7027 | lr 3.00e-04 | grad 1.27 | tok/s 19927
step   1480 | loss 1.7412 | lr 3.00e-04 | grad 1.84 | tok/s 20644
step   1490 | loss 1.9570 | lr 3.00e-04 | grad 2.44 | tok/s 20494
step   1500 | loss 2.0075 | lr 3.00e-04 | grad 1.15 | tok/s 20797
step   1510 | loss 1.6271 | lr 3.00e-04 | grad 1.00 | tok/s 20373
step   1520 | loss 1.7583 | lr 3.00e-04 | grad 1.41 | tok/s 20303
step   1530 | loss 1.7106 | lr 3.00e-04 | grad 1.18 | tok/s 20845

Training complete! Final step: 1538
