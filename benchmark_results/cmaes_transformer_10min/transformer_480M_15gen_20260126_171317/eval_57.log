Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_57/levelllama_100m_20260126_182511
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 363,769,472 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.0395 | lr 3.00e-04 | grad 2.44 | tok/s 11034
step     20 | loss 2.8848 | lr 3.00e-04 | grad 2.45 | tok/s 24251
step     30 | loss 3.2718 | lr 3.00e-04 | grad 2.11 | tok/s 25550
step     40 | loss 5.5477 | lr 3.00e-04 | grad 7.34 | tok/s 25916
step     50 | loss 4.6524 | lr 3.00e-04 | grad 4.81 | tok/s 26236
step     60 | loss 3.8386 | lr 3.00e-04 | grad 3.66 | tok/s 26106
step     70 | loss 3.3287 | lr 3.00e-04 | grad 3.41 | tok/s 26018
step     80 | loss 3.0480 | lr 3.00e-04 | grad 2.09 | tok/s 25948
step     90 | loss 2.7477 | lr 3.00e-04 | grad 2.08 | tok/s 25874
step    100 | loss 2.5458 | lr 3.00e-04 | grad 2.23 | tok/s 25816
step    110 | loss 2.6458 | lr 3.00e-04 | grad 2.28 | tok/s 25582
step    120 | loss 3.4071 | lr 3.00e-04 | grad 1.03 | tok/s 24403
step    130 | loss 2.6294 | lr 3.00e-04 | grad 3.75 | tok/s 24925
step    140 | loss 2.9197 | lr 3.00e-04 | grad 5.00 | tok/s 24971
step    150 | loss 2.6614 | lr 3.00e-04 | grad 4.38 | tok/s 25608
step    160 | loss 2.7930 | lr 3.00e-04 | grad 1.55 | tok/s 24706
step    170 | loss 2.7059 | lr 3.00e-04 | grad 1.33 | tok/s 24313
step    180 | loss 2.8173 | lr 3.00e-04 | grad 1.62 | tok/s 24928
step    190 | loss 2.4455 | lr 3.00e-04 | grad 1.85 | tok/s 24422
step    200 | loss 2.3702 | lr 3.00e-04 | grad 1.59 | tok/s 25536
step    210 | loss 2.3833 | lr 3.00e-04 | grad 3.03 | tok/s 24229
step    220 | loss 2.6457 | lr 3.00e-04 | grad 2.53 | tok/s 24455
step    230 | loss 2.4795 | lr 3.00e-04 | grad 2.11 | tok/s 24375
step    240 | loss 2.7337 | lr 3.00e-04 | grad 2.31 | tok/s 24707
step    250 | loss 2.2816 | lr 3.00e-04 | grad 1.89 | tok/s 24555
step    260 | loss 2.4179 | lr 3.00e-04 | grad 1.70 | tok/s 25192
step    270 | loss 2.2779 | lr 3.00e-04 | grad 1.73 | tok/s 24661
step    280 | loss 2.2135 | lr 3.00e-04 | grad 0.92 | tok/s 23178
step    290 | loss 2.1498 | lr 3.00e-04 | grad 1.48 | tok/s 23971
step    300 | loss 2.4086 | lr 3.00e-04 | grad 1.59 | tok/s 24149
step    310 | loss 2.0973 | lr 3.00e-04 | grad 1.60 | tok/s 24017
step    320 | loss 2.3515 | lr 3.00e-04 | grad 2.84 | tok/s 24330
step    330 | loss 2.1516 | lr 3.00e-04 | grad 1.43 | tok/s 24602
step    340 | loss 2.4548 | lr 3.00e-04 | grad 1.58 | tok/s 24495
step    350 | loss 2.4027 | lr 3.00e-04 | grad 1.48 | tok/s 25239
step    360 | loss 2.0571 | lr 3.00e-04 | grad 1.22 | tok/s 24137
step    370 | loss 2.1000 | lr 3.00e-04 | grad 1.22 | tok/s 25427
step    380 | loss 1.8917 | lr 3.00e-04 | grad 1.45 | tok/s 25605
step    390 | loss 1.8132 | lr 3.00e-04 | grad 1.45 | tok/s 25617
step    400 | loss 2.2118 | lr 3.00e-04 | grad 1.21 | tok/s 24275
step    410 | loss 2.1280 | lr 3.00e-04 | grad 1.23 | tok/s 24502
step    420 | loss 2.2555 | lr 3.00e-04 | grad 1.67 | tok/s 25540
step    430 | loss 2.0923 | lr 3.00e-04 | grad 1.68 | tok/s 25117
step    440 | loss 2.1197 | lr 3.00e-04 | grad 1.60 | tok/s 24381
step    450 | loss 2.0146 | lr 3.00e-04 | grad 1.19 | tok/s 24639
step    460 | loss 2.0768 | lr 3.00e-04 | grad 1.27 | tok/s 25000
step    470 | loss 2.0618 | lr 3.00e-04 | grad 1.76 | tok/s 24814
step    480 | loss 2.0782 | lr 3.00e-04 | grad 2.23 | tok/s 25372
step    490 | loss 2.0526 | lr 3.00e-04 | grad 1.69 | tok/s 24364
step    500 | loss 2.1851 | lr 3.00e-04 | grad 1.05 | tok/s 24755
step    510 | loss 2.0592 | lr 3.00e-04 | grad 1.12 | tok/s 23650
step    520 | loss 1.9041 | lr 3.00e-04 | grad 1.05 | tok/s 24787
step    530 | loss 2.0632 | lr 3.00e-04 | grad 1.11 | tok/s 24408
step    540 | loss 2.0142 | lr 3.00e-04 | grad 0.93 | tok/s 23843
step    550 | loss 1.7064 | lr 3.00e-04 | grad 2.11 | tok/s 24956
step    560 | loss 1.8664 | lr 3.00e-04 | grad 1.58 | tok/s 25673
step    570 | loss 1.7571 | lr 3.00e-04 | grad 1.30 | tok/s 25666
step    580 | loss 1.6925 | lr 3.00e-04 | grad 1.14 | tok/s 25645
step    590 | loss 1.7424 | lr 3.00e-04 | grad 1.23 | tok/s 25606
step    600 | loss 1.6845 | lr 3.00e-04 | grad 1.17 | tok/s 25599
step    610 | loss 1.6610 | lr 3.00e-04 | grad 1.15 | tok/s 25609
step    620 | loss 1.6500 | lr 3.00e-04 | grad 1.25 | tok/s 25484
step    630 | loss 1.9250 | lr 3.00e-04 | grad 3.20 | tok/s 24139
step    640 | loss 2.0498 | lr 3.00e-04 | grad 1.28 | tok/s 24457
step    650 | loss 1.8673 | lr 3.00e-04 | grad 1.14 | tok/s 24437
step    660 | loss 1.9343 | lr 3.00e-04 | grad 1.24 | tok/s 25380
step    670 | loss 1.9501 | lr 3.00e-04 | grad 2.66 | tok/s 24550
step    680 | loss 1.9482 | lr 3.00e-04 | grad 1.52 | tok/s 24155
step    690 | loss 1.9067 | lr 3.00e-04 | grad 1.17 | tok/s 23995
step    700 | loss 1.8111 | lr 3.00e-04 | grad 1.02 | tok/s 24514
step    710 | loss 1.9575 | lr 3.00e-04 | grad 2.19 | tok/s 24127
step    720 | loss 1.6989 | lr 3.00e-04 | grad 1.64 | tok/s 25049
step    730 | loss 1.7871 | lr 3.00e-04 | grad 0.93 | tok/s 24654
step    740 | loss 2.2237 | lr 3.00e-04 | grad 2.22 | tok/s 25316
step    750 | loss 1.9994 | lr 3.00e-04 | grad 1.23 | tok/s 25632
step    760 | loss 1.8316 | lr 3.00e-04 | grad 2.12 | tok/s 25069
step    770 | loss 1.8478 | lr 3.00e-04 | grad 1.43 | tok/s 24657
step    780 | loss 1.7942 | lr 3.00e-04 | grad 1.45 | tok/s 24846
step    790 | loss 2.0947 | lr 3.00e-04 | grad 2.55 | tok/s 25404
step    800 | loss 1.6641 | lr 3.00e-04 | grad 1.00 | tok/s 24981
step    810 | loss 1.6178 | lr 3.00e-04 | grad 1.91 | tok/s 24171
step    820 | loss 1.7562 | lr 3.00e-04 | grad 1.20 | tok/s 24637
step    830 | loss 1.8054 | lr 3.00e-04 | grad 1.34 | tok/s 24313
step    840 | loss 1.9316 | lr 3.00e-04 | grad 1.25 | tok/s 24188
step    850 | loss 1.8831 | lr 3.00e-04 | grad 1.25 | tok/s 24689
step    860 | loss 1.9396 | lr 3.00e-04 | grad 1.40 | tok/s 25105
step    870 | loss 1.9240 | lr 3.00e-04 | grad 1.22 | tok/s 25283
step    880 | loss 1.8382 | lr 3.00e-04 | grad 0.99 | tok/s 24776
step    890 | loss 1.7294 | lr 3.00e-04 | grad 1.08 | tok/s 24656
step    900 | loss 1.7924 | lr 3.00e-04 | grad 1.02 | tok/s 24550
step    910 | loss 1.8376 | lr 3.00e-04 | grad 3.31 | tok/s 24287
step    920 | loss 1.7394 | lr 3.00e-04 | grad 1.37 | tok/s 24576
step    930 | loss 1.7000 | lr 3.00e-04 | grad 1.27 | tok/s 24860
step    940 | loss 1.6586 | lr 3.00e-04 | grad 1.17 | tok/s 24316
step    950 | loss 1.7553 | lr 3.00e-04 | grad 1.55 | tok/s 23920
step    960 | loss 1.7028 | lr 3.00e-04 | grad 1.06 | tok/s 24569
step    970 | loss 1.7002 | lr 3.00e-04 | grad 1.16 | tok/s 24597
step    980 | loss 2.3520 | lr 3.00e-04 | grad 1.78 | tok/s 25580
step    990 | loss 1.8972 | lr 3.00e-04 | grad 1.42 | tok/s 24555
step   1000 | loss 1.8114 | lr 3.00e-04 | grad 1.50 | tok/s 24628
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8114.pt
step   1010 | loss 1.6376 | lr 3.00e-04 | grad 1.59 | tok/s 13439
step   1020 | loss 1.6089 | lr 3.00e-04 | grad 0.92 | tok/s 25981
step   1030 | loss 1.8172 | lr 3.00e-04 | grad 1.17 | tok/s 24598
step   1040 | loss 2.3397 | lr 3.00e-04 | grad 2.44 | tok/s 25151
step   1050 | loss 1.8375 | lr 3.00e-04 | grad 1.86 | tok/s 25325
step   1060 | loss 1.4005 | lr 3.00e-04 | grad 1.24 | tok/s 25059
step   1070 | loss 1.6906 | lr 3.00e-04 | grad 1.61 | tok/s 24889
step   1080 | loss 1.5151 | lr 3.00e-04 | grad 1.24 | tok/s 25734
step   1090 | loss 1.4755 | lr 3.00e-04 | grad 1.20 | tok/s 25701
step   1100 | loss 1.4640 | lr 3.00e-04 | grad 1.21 | tok/s 25680
step   1110 | loss 1.3954 | lr 3.00e-04 | grad 1.13 | tok/s 25687
step   1120 | loss 1.6906 | lr 3.00e-04 | grad 2.38 | tok/s 24987
step   1130 | loss 2.0122 | lr 3.00e-04 | grad 1.38 | tok/s 25256
step   1140 | loss 2.0217 | lr 3.00e-04 | grad 1.46 | tok/s 25549
step   1150 | loss 2.1442 | lr 3.00e-04 | grad 1.32 | tok/s 24732
step   1160 | loss 1.9707 | lr 3.00e-04 | grad 1.51 | tok/s 24341
step   1170 | loss 1.7014 | lr 3.00e-04 | grad 1.29 | tok/s 24043
step   1180 | loss 1.6293 | lr 3.00e-04 | grad 1.82 | tok/s 25279
step   1190 | loss 1.9853 | lr 3.00e-04 | grad 1.57 | tok/s 25499
step   1200 | loss 1.4310 | lr 3.00e-04 | grad 1.46 | tok/s 25569
step   1210 | loss 1.5917 | lr 3.00e-04 | grad 1.21 | tok/s 23875
step   1220 | loss 1.6465 | lr 3.00e-04 | grad 1.38 | tok/s 25048
step   1230 | loss 1.6192 | lr 3.00e-04 | grad 0.93 | tok/s 25117
step   1240 | loss 1.5730 | lr 3.00e-04 | grad 1.38 | tok/s 25218
step   1250 | loss 1.7879 | lr 3.00e-04 | grad 1.52 | tok/s 24845
step   1260 | loss 1.7731 | lr 3.00e-04 | grad 1.72 | tok/s 25395
step   1270 | loss 1.6115 | lr 3.00e-04 | grad 1.46 | tok/s 24680
step   1280 | loss 1.6413 | lr 3.00e-04 | grad 1.21 | tok/s 24381
step   1290 | loss 1.6442 | lr 3.00e-04 | grad 1.49 | tok/s 24437
step   1300 | loss 1.9261 | lr 3.00e-04 | grad 3.48 | tok/s 24064
step   1310 | loss 1.7873 | lr 3.00e-04 | grad 1.42 | tok/s 25007
step   1320 | loss 1.7223 | lr 3.00e-04 | grad 1.16 | tok/s 25018
step   1330 | loss 1.7325 | lr 3.00e-04 | grad 1.21 | tok/s 24758
step   1340 | loss 1.8319 | lr 3.00e-04 | grad 1.47 | tok/s 24284
step   1350 | loss 1.6781 | lr 3.00e-04 | grad 0.99 | tok/s 24791
step   1360 | loss 1.7553 | lr 3.00e-04 | grad 1.30 | tok/s 23900
step   1370 | loss 1.8741 | lr 3.00e-04 | grad 1.38 | tok/s 25177
step   1380 | loss 1.6690 | lr 3.00e-04 | grad 1.15 | tok/s 24055
step   1390 | loss 1.6827 | lr 3.00e-04 | grad 1.95 | tok/s 25205
step   1400 | loss 1.7463 | lr 3.00e-04 | grad 1.33 | tok/s 24343
step   1410 | loss 1.6278 | lr 3.00e-04 | grad 1.98 | tok/s 23776
step   1420 | loss 1.5361 | lr 3.00e-04 | grad 3.14 | tok/s 25325
step   1430 | loss 1.9612 | lr 3.00e-04 | grad 1.38 | tok/s 24397
step   1440 | loss 1.6939 | lr 3.00e-04 | grad 1.33 | tok/s 25009
step   1450 | loss 1.7563 | lr 3.00e-04 | grad 5.81 | tok/s 25010
step   1460 | loss 1.7510 | lr 3.00e-04 | grad 2.28 | tok/s 24283
step   1470 | loss 1.5614 | lr 3.00e-04 | grad 1.20 | tok/s 23717
step   1480 | loss 1.6123 | lr 3.00e-04 | grad 1.12 | tok/s 25027
step   1490 | loss 2.0446 | lr 3.00e-04 | grad 5.12 | tok/s 24631
step   1500 | loss 1.6786 | lr 3.00e-04 | grad 1.17 | tok/s 24683
step   1510 | loss 1.4933 | lr 3.00e-04 | grad 1.32 | tok/s 24711
step   1520 | loss 1.6714 | lr 3.00e-04 | grad 1.22 | tok/s 24527
step   1530 | loss 1.6166 | lr 3.00e-04 | grad 1.18 | tok/s 24889
step   1540 | loss 1.7086 | lr 3.00e-04 | grad 1.28 | tok/s 25159
step   1550 | loss 1.6936 | lr 3.00e-04 | grad 1.26 | tok/s 24656
step   1560 | loss 1.4376 | lr 3.00e-04 | grad 1.47 | tok/s 25615
step   1570 | loss 1.5571 | lr 3.00e-04 | grad 1.03 | tok/s 24870
step   1580 | loss 1.5234 | lr 3.00e-04 | grad 1.41 | tok/s 24750
step   1590 | loss 1.6491 | lr 3.00e-04 | grad 1.38 | tok/s 24285
step   1600 | loss 1.4915 | lr 3.00e-04 | grad 1.72 | tok/s 25191
step   1610 | loss 2.1529 | lr 3.00e-04 | grad 2.05 | tok/s 24923
step   1620 | loss 2.2264 | lr 3.00e-04 | grad 1.99 | tok/s 25582
step   1630 | loss 1.9993 | lr 3.00e-04 | grad 1.80 | tok/s 25579
step   1640 | loss 1.8500 | lr 3.00e-04 | grad 1.26 | tok/s 25536
step   1650 | loss 1.7695 | lr 3.00e-04 | grad 1.50 | tok/s 25527
step   1660 | loss 1.7112 | lr 3.00e-04 | grad 1.37 | tok/s 25554
step   1670 | loss 1.8166 | lr 3.00e-04 | grad 1.68 | tok/s 24778
step   1680 | loss 1.6312 | lr 3.00e-04 | grad 1.31 | tok/s 24565
step   1690 | loss 1.6470 | lr 3.00e-04 | grad 1.23 | tok/s 23842
step   1700 | loss 1.5035 | lr 3.00e-04 | grad 0.95 | tok/s 25045
step   1710 | loss 1.5027 | lr 3.00e-04 | grad 1.23 | tok/s 24796
step   1720 | loss 1.6547 | lr 3.00e-04 | grad 1.28 | tok/s 24438
step   1730 | loss 1.6839 | lr 3.00e-04 | grad 1.70 | tok/s 24808
step   1740 | loss 1.6093 | lr 3.00e-04 | grad 1.20 | tok/s 25288
step   1750 | loss 1.4480 | lr 3.00e-04 | grad 1.06 | tok/s 24477
step   1760 | loss 1.6408 | lr 3.00e-04 | grad 1.26 | tok/s 24142
step   1770 | loss 1.8610 | lr 3.00e-04 | grad 1.20 | tok/s 25060
step   1780 | loss 1.8802 | lr 3.00e-04 | grad 1.19 | tok/s 23314
step   1790 | loss 1.5154 | lr 3.00e-04 | grad 1.48 | tok/s 24226
step   1800 | loss 1.5477 | lr 3.00e-04 | grad 1.11 | tok/s 24393
step   1810 | loss 1.5722 | lr 3.00e-04 | grad 1.11 | tok/s 24530
step   1820 | loss 1.7430 | lr 3.00e-04 | grad 1.12 | tok/s 24410
step   1830 | loss 1.5556 | lr 3.00e-04 | grad 1.00 | tok/s 23588
step   1840 | loss 1.6019 | lr 3.00e-04 | grad 1.47 | tok/s 24607

Training complete! Final step: 1843
