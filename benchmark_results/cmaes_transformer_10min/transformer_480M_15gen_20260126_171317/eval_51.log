Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_51/levelllama_100m_20260126_181456
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 274,924,800 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.4802 | lr 3.00e-04 | grad 2.95 | tok/s 11534
step     20 | loss 2.9346 | lr 3.00e-04 | grad 1.31 | tok/s 27104
step     30 | loss 3.3094 | lr 3.00e-04 | grad 1.50 | tok/s 28695
step     40 | loss 5.4798 | lr 3.00e-04 | grad 6.81 | tok/s 29229
step     50 | loss 4.9895 | lr 3.00e-04 | grad 4.53 | tok/s 29633
step     60 | loss 3.9521 | lr 3.00e-04 | grad 3.34 | tok/s 29598
step     70 | loss 3.5209 | lr 3.00e-04 | grad 2.80 | tok/s 29527
step     80 | loss 3.2298 | lr 3.00e-04 | grad 1.42 | tok/s 29460
step     90 | loss 2.8725 | lr 3.00e-04 | grad 1.52 | tok/s 29399
step    100 | loss 2.6716 | lr 3.00e-04 | grad 1.77 | tok/s 29365
step    110 | loss 2.6801 | lr 3.00e-04 | grad 1.85 | tok/s 29084
step    120 | loss 3.4514 | lr 3.00e-04 | grad 1.26 | tok/s 27652
step    130 | loss 2.6227 | lr 3.00e-04 | grad 2.25 | tok/s 28269
step    140 | loss 2.9297 | lr 3.00e-04 | grad 4.91 | tok/s 28318
step    150 | loss 2.7427 | lr 3.00e-04 | grad 4.38 | tok/s 28982
step    160 | loss 2.8478 | lr 3.00e-04 | grad 1.53 | tok/s 27958
step    170 | loss 2.7166 | lr 3.00e-04 | grad 0.82 | tok/s 27469
step    180 | loss 2.8532 | lr 3.00e-04 | grad 1.40 | tok/s 28092
step    190 | loss 2.4384 | lr 3.00e-04 | grad 1.63 | tok/s 27558
step    200 | loss 2.3695 | lr 3.00e-04 | grad 1.08 | tok/s 28714
step    210 | loss 2.3929 | lr 3.00e-04 | grad 2.28 | tok/s 27235
step    220 | loss 2.6683 | lr 3.00e-04 | grad 3.95 | tok/s 27481
step    230 | loss 2.4331 | lr 3.00e-04 | grad 1.49 | tok/s 27394
step    240 | loss 2.7453 | lr 3.00e-04 | grad 2.02 | tok/s 27733
step    250 | loss 2.2907 | lr 3.00e-04 | grad 1.61 | tok/s 27482
step    260 | loss 2.4323 | lr 3.00e-04 | grad 1.89 | tok/s 28231
step    270 | loss 2.2930 | lr 3.00e-04 | grad 1.82 | tok/s 27531
step    280 | loss 2.2226 | lr 3.00e-04 | grad 0.76 | tok/s 25844
step    290 | loss 2.1614 | lr 3.00e-04 | grad 1.71 | tok/s 26702
step    300 | loss 2.4222 | lr 3.00e-04 | grad 1.05 | tok/s 26855
step    310 | loss 2.1109 | lr 3.00e-04 | grad 0.94 | tok/s 26713
step    320 | loss 2.3782 | lr 3.00e-04 | grad 2.75 | tok/s 27032
step    330 | loss 2.1717 | lr 3.00e-04 | grad 1.12 | tok/s 27289
step    340 | loss 2.4670 | lr 3.00e-04 | grad 1.54 | tok/s 27121
step    350 | loss 2.4492 | lr 3.00e-04 | grad 1.16 | tok/s 27844
step    360 | loss 2.0826 | lr 3.00e-04 | grad 1.16 | tok/s 26665
step    370 | loss 2.1356 | lr 3.00e-04 | grad 1.25 | tok/s 28131
step    380 | loss 1.9593 | lr 3.00e-04 | grad 1.30 | tok/s 28345
step    390 | loss 1.8821 | lr 3.00e-04 | grad 1.35 | tok/s 28284
step    400 | loss 2.2410 | lr 3.00e-04 | grad 1.05 | tok/s 26777
step    410 | loss 2.1519 | lr 3.00e-04 | grad 1.16 | tok/s 27012
step    420 | loss 2.3089 | lr 3.00e-04 | grad 1.54 | tok/s 28135
step    430 | loss 2.1382 | lr 3.00e-04 | grad 1.16 | tok/s 27673
step    440 | loss 2.1468 | lr 3.00e-04 | grad 1.24 | tok/s 26796
step    450 | loss 2.0428 | lr 3.00e-04 | grad 0.92 | tok/s 27078
step    460 | loss 2.1285 | lr 3.00e-04 | grad 0.95 | tok/s 27452
step    470 | loss 2.1065 | lr 3.00e-04 | grad 1.64 | tok/s 27228
step    480 | loss 2.1427 | lr 3.00e-04 | grad 1.80 | tok/s 27801
step    490 | loss 2.0777 | lr 3.00e-04 | grad 1.53 | tok/s 26688
step    500 | loss 2.2172 | lr 3.00e-04 | grad 0.94 | tok/s 27099
step    510 | loss 2.0725 | lr 3.00e-04 | grad 0.95 | tok/s 25879
step    520 | loss 1.9419 | lr 3.00e-04 | grad 1.01 | tok/s 27035
step    530 | loss 2.0918 | lr 3.00e-04 | grad 1.12 | tok/s 26605
step    540 | loss 2.0545 | lr 3.00e-04 | grad 0.91 | tok/s 26042
step    550 | loss 1.7442 | lr 3.00e-04 | grad 1.99 | tok/s 27210
step    560 | loss 1.9280 | lr 3.00e-04 | grad 1.05 | tok/s 27980
step    570 | loss 1.8201 | lr 3.00e-04 | grad 1.18 | tok/s 27942
step    580 | loss 1.7453 | lr 3.00e-04 | grad 0.69 | tok/s 27912
step    590 | loss 1.7976 | lr 3.00e-04 | grad 1.16 | tok/s 27912
step    600 | loss 1.7597 | lr 3.00e-04 | grad 1.52 | tok/s 27956
step    610 | loss 1.7228 | lr 3.00e-04 | grad 0.94 | tok/s 27938
step    620 | loss 1.7071 | lr 3.00e-04 | grad 1.19 | tok/s 27781
step    630 | loss 1.9531 | lr 3.00e-04 | grad 2.73 | tok/s 26281
step    640 | loss 2.0853 | lr 3.00e-04 | grad 1.41 | tok/s 26572
step    650 | loss 1.9051 | lr 3.00e-04 | grad 1.05 | tok/s 26542
step    660 | loss 1.9708 | lr 3.00e-04 | grad 1.16 | tok/s 27558
step    670 | loss 1.9992 | lr 3.00e-04 | grad 2.50 | tok/s 26610
step    680 | loss 1.9833 | lr 3.00e-04 | grad 1.37 | tok/s 26204
step    690 | loss 1.9469 | lr 3.00e-04 | grad 1.00 | tok/s 26014
step    700 | loss 1.8501 | lr 3.00e-04 | grad 1.05 | tok/s 26593
step    710 | loss 1.9987 | lr 3.00e-04 | grad 2.20 | tok/s 26188
step    720 | loss 1.7548 | lr 3.00e-04 | grad 1.16 | tok/s 27165
step    730 | loss 1.8390 | lr 3.00e-04 | grad 0.87 | tok/s 26707
step    740 | loss 2.2694 | lr 3.00e-04 | grad 1.90 | tok/s 27430
step    750 | loss 2.0631 | lr 3.00e-04 | grad 1.19 | tok/s 27730
step    760 | loss 1.8790 | lr 3.00e-04 | grad 1.91 | tok/s 27137
step    770 | loss 1.8888 | lr 3.00e-04 | grad 1.09 | tok/s 26707
step    780 | loss 1.8295 | lr 3.00e-04 | grad 1.30 | tok/s 26879
step    790 | loss 2.1366 | lr 3.00e-04 | grad 2.08 | tok/s 27437
step    800 | loss 1.7126 | lr 3.00e-04 | grad 0.93 | tok/s 26966
step    810 | loss 1.6517 | lr 3.00e-04 | grad 1.79 | tok/s 26076
step    820 | loss 1.7982 | lr 3.00e-04 | grad 1.17 | tok/s 26558
step    830 | loss 1.8548 | lr 3.00e-04 | grad 1.19 | tok/s 26211
step    840 | loss 1.9644 | lr 3.00e-04 | grad 1.08 | tok/s 26085
step    850 | loss 1.9269 | lr 3.00e-04 | grad 1.09 | tok/s 26616
step    860 | loss 1.9794 | lr 3.00e-04 | grad 1.26 | tok/s 27043
step    870 | loss 2.0165 | lr 3.00e-04 | grad 1.16 | tok/s 27246
step    880 | loss 1.8769 | lr 3.00e-04 | grad 1.02 | tok/s 26750
step    890 | loss 1.7661 | lr 3.00e-04 | grad 1.06 | tok/s 26616
step    900 | loss 1.8234 | lr 3.00e-04 | grad 0.94 | tok/s 26493
step    910 | loss 1.8811 | lr 3.00e-04 | grad 3.45 | tok/s 26233
step    920 | loss 1.7711 | lr 3.00e-04 | grad 1.05 | tok/s 26538
step    930 | loss 1.7419 | lr 3.00e-04 | grad 1.12 | tok/s 26858
step    940 | loss 1.7042 | lr 3.00e-04 | grad 1.04 | tok/s 26252
step    950 | loss 1.7902 | lr 3.00e-04 | grad 1.38 | tok/s 25792
step    960 | loss 1.7338 | lr 3.00e-04 | grad 1.17 | tok/s 26543
step    970 | loss 1.7278 | lr 3.00e-04 | grad 1.07 | tok/s 26548
step    980 | loss 2.4883 | lr 3.00e-04 | grad 2.11 | tok/s 27602
step    990 | loss 1.9319 | lr 3.00e-04 | grad 1.27 | tok/s 26492
step   1000 | loss 1.8487 | lr 3.00e-04 | grad 1.26 | tok/s 26568
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8487.pt
step   1010 | loss 1.5886 | lr 3.00e-04 | grad 1.34 | tok/s 16058
step   1020 | loss 1.6716 | lr 3.00e-04 | grad 1.20 | tok/s 27929
step   1030 | loss 1.8319 | lr 3.00e-04 | grad 1.17 | tok/s 26513
step   1040 | loss 2.3116 | lr 3.00e-04 | grad 1.69 | tok/s 27178
step   1050 | loss 1.9159 | lr 3.00e-04 | grad 1.01 | tok/s 27304
step   1060 | loss 1.5715 | lr 3.00e-04 | grad 0.83 | tok/s 26988
step   1070 | loss 1.6614 | lr 3.00e-04 | grad 1.31 | tok/s 26911
step   1080 | loss 1.5629 | lr 3.00e-04 | grad 1.09 | tok/s 27831
step   1090 | loss 1.5369 | lr 3.00e-04 | grad 1.12 | tok/s 27777
step   1100 | loss 1.5083 | lr 3.00e-04 | grad 1.23 | tok/s 27775
step   1110 | loss 1.4507 | lr 3.00e-04 | grad 1.10 | tok/s 27762
step   1120 | loss 1.6743 | lr 3.00e-04 | grad 1.09 | tok/s 26999
step   1130 | loss 2.0987 | lr 3.00e-04 | grad 1.23 | tok/s 27311
step   1140 | loss 2.0621 | lr 3.00e-04 | grad 1.14 | tok/s 27616
step   1150 | loss 2.2290 | lr 3.00e-04 | grad 1.83 | tok/s 27010
step   1160 | loss 1.9497 | lr 3.00e-04 | grad 2.95 | tok/s 26172
step   1170 | loss 1.7922 | lr 3.00e-04 | grad 1.44 | tok/s 26085
step   1180 | loss 1.6948 | lr 3.00e-04 | grad 1.34 | tok/s 27454
step   1190 | loss 2.0165 | lr 3.00e-04 | grad 1.43 | tok/s 27533
step   1200 | loss 1.5385 | lr 3.00e-04 | grad 1.37 | tok/s 27788
step   1210 | loss 1.5952 | lr 3.00e-04 | grad 1.14 | tok/s 26151
step   1220 | loss 1.6753 | lr 3.00e-04 | grad 1.34 | tok/s 27037
step   1230 | loss 1.7165 | lr 3.00e-04 | grad 1.04 | tok/s 27223
step   1240 | loss 1.5934 | lr 3.00e-04 | grad 1.83 | tok/s 27529
step   1250 | loss 1.8263 | lr 3.00e-04 | grad 1.52 | tok/s 26854
step   1260 | loss 1.9176 | lr 3.00e-04 | grad 1.14 | tok/s 27602
step   1270 | loss 1.6667 | lr 3.00e-04 | grad 1.68 | tok/s 26768
step   1280 | loss 1.6713 | lr 3.00e-04 | grad 1.41 | tok/s 26849
step   1290 | loss 1.6738 | lr 3.00e-04 | grad 1.40 | tok/s 26249
step   1300 | loss 1.9103 | lr 3.00e-04 | grad 3.38 | tok/s 26167
step   1310 | loss 1.9304 | lr 3.00e-04 | grad 1.04 | tok/s 27208
step   1320 | loss 1.7619 | lr 3.00e-04 | grad 2.14 | tok/s 27135
step   1330 | loss 1.7958 | lr 3.00e-04 | grad 1.34 | tok/s 27080
step   1340 | loss 1.8423 | lr 3.00e-04 | grad 1.03 | tok/s 26235
step   1350 | loss 1.7391 | lr 3.00e-04 | grad 1.22 | tok/s 27253
step   1360 | loss 1.7928 | lr 3.00e-04 | grad 1.33 | tok/s 25655
step   1370 | loss 1.9053 | lr 3.00e-04 | grad 2.55 | tok/s 27332
step   1380 | loss 1.7567 | lr 3.00e-04 | grad 1.30 | tok/s 26522
step   1390 | loss 1.7331 | lr 3.00e-04 | grad 1.33 | tok/s 27003
step   1400 | loss 1.7719 | lr 3.00e-04 | grad 1.12 | tok/s 26411
step   1410 | loss 1.6131 | lr 3.00e-04 | grad 1.48 | tok/s 25967
step   1420 | loss 1.6393 | lr 3.00e-04 | grad 1.41 | tok/s 27524
step   1430 | loss 2.0301 | lr 3.00e-04 | grad 1.05 | tok/s 26656
step   1440 | loss 1.7323 | lr 3.00e-04 | grad 1.59 | tok/s 26874
step   1450 | loss 1.7334 | lr 3.00e-04 | grad 1.12 | tok/s 27164
step   1460 | loss 1.7986 | lr 3.00e-04 | grad 1.46 | tok/s 26393
step   1470 | loss 1.6492 | lr 3.00e-04 | grad 1.61 | tok/s 26074
step   1480 | loss 1.6764 | lr 3.00e-04 | grad 1.91 | tok/s 27011
step   1490 | loss 1.8732 | lr 3.00e-04 | grad 4.19 | tok/s 26787
step   1500 | loss 1.8923 | lr 3.00e-04 | grad 1.27 | tok/s 27173
step   1510 | loss 1.5515 | lr 3.00e-04 | grad 1.02 | tok/s 26650
step   1520 | loss 1.7086 | lr 3.00e-04 | grad 1.45 | tok/s 26533
step   1530 | loss 1.6500 | lr 3.00e-04 | grad 1.32 | tok/s 27270
step   1540 | loss 1.7360 | lr 3.00e-04 | grad 1.02 | tok/s 27254
step   1550 | loss 1.7217 | lr 3.00e-04 | grad 2.50 | tok/s 26756
step   1560 | loss 1.5243 | lr 3.00e-04 | grad 1.20 | tok/s 27670
step   1570 | loss 1.6261 | lr 3.00e-04 | grad 1.18 | tok/s 27032
step   1580 | loss 1.5333 | lr 3.00e-04 | grad 1.53 | tok/s 27205
step   1590 | loss 1.6995 | lr 3.00e-04 | grad 2.14 | tok/s 26208
step   1600 | loss 1.5086 | lr 3.00e-04 | grad 4.16 | tok/s 27616
step   1610 | loss 2.1312 | lr 3.00e-04 | grad 2.59 | tok/s 26945
step   1620 | loss 2.3131 | lr 3.00e-04 | grad 1.68 | tok/s 27842
step   1630 | loss 2.0659 | lr 3.00e-04 | grad 1.41 | tok/s 27842
step   1640 | loss 1.9089 | lr 3.00e-04 | grad 1.66 | tok/s 27848
step   1650 | loss 1.8366 | lr 3.00e-04 | grad 1.76 | tok/s 27834
step   1660 | loss 1.7842 | lr 3.00e-04 | grad 1.23 | tok/s 27828
step   1670 | loss 1.8710 | lr 3.00e-04 | grad 1.48 | tok/s 26970
step   1680 | loss 1.6611 | lr 3.00e-04 | grad 1.18 | tok/s 26711
step   1690 | loss 1.6853 | lr 3.00e-04 | grad 1.47 | tok/s 26085
step   1700 | loss 1.5633 | lr 3.00e-04 | grad 1.28 | tok/s 27077
step   1710 | loss 1.5328 | lr 3.00e-04 | grad 1.55 | tok/s 27249
step   1720 | loss 1.6649 | lr 3.00e-04 | grad 1.12 | tok/s 26343
step   1730 | loss 1.7278 | lr 3.00e-04 | grad 2.08 | tok/s 27211
step   1740 | loss 1.6536 | lr 3.00e-04 | grad 1.21 | tok/s 27393
step   1750 | loss 1.5420 | lr 3.00e-04 | grad 1.25 | tok/s 26804
step   1760 | loss 1.6295 | lr 3.00e-04 | grad 1.20 | tok/s 26539
step   1770 | loss 1.9139 | lr 3.00e-04 | grad 1.36 | tok/s 27165
step   1780 | loss 1.9457 | lr 3.00e-04 | grad 1.22 | tok/s 25645
step   1790 | loss 1.5322 | lr 3.00e-04 | grad 1.14 | tok/s 26059
step   1800 | loss 1.5735 | lr 3.00e-04 | grad 1.27 | tok/s 26688
step   1810 | loss 1.6403 | lr 3.00e-04 | grad 1.38 | tok/s 26918
step   1820 | loss 1.7434 | lr 3.00e-04 | grad 1.77 | tok/s 26581
step   1830 | loss 1.5849 | lr 3.00e-04 | grad 1.05 | tok/s 25835
step   1840 | loss 1.6671 | lr 3.00e-04 | grad 1.09 | tok/s 26779
step   1850 | loss 1.6836 | lr 3.00e-04 | grad 1.30 | tok/s 26597
step   1860 | loss 1.6408 | lr 3.00e-04 | grad 1.49 | tok/s 26506
step   1870 | loss 1.6428 | lr 3.00e-04 | grad 2.36 | tok/s 26951
step   1880 | loss 1.6997 | lr 3.00e-04 | grad 1.22 | tok/s 27109
step   1890 | loss 1.5088 | lr 3.00e-04 | grad 1.11 | tok/s 27842
step   1900 | loss 1.4393 | lr 3.00e-04 | grad 1.00 | tok/s 27836
step   1910 | loss 1.4137 | lr 3.00e-04 | grad 0.82 | tok/s 27854
step   1920 | loss 1.3979 | lr 3.00e-04 | grad 1.20 | tok/s 27849
step   1930 | loss 1.4251 | lr 3.00e-04 | grad 1.95 | tok/s 27756
step   1940 | loss 1.7558 | lr 3.00e-04 | grad 1.35 | tok/s 26431
step   1950 | loss 1.6379 | lr 3.00e-04 | grad 0.97 | tok/s 26244
step   1960 | loss 1.7530 | lr 3.00e-04 | grad 1.55 | tok/s 26539
step   1970 | loss 1.7805 | lr 3.00e-04 | grad 1.30 | tok/s 27088
step   1980 | loss 1.6696 | lr 3.00e-04 | grad 1.46 | tok/s 26248
step   1990 | loss 1.8783 | lr 3.00e-04 | grad 2.12 | tok/s 27021
step   2000 | loss 1.4047 | lr 3.00e-04 | grad 1.14 | tok/s 27892
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4047.pt
step   2010 | loss 1.6172 | lr 3.00e-04 | grad 1.73 | tok/s 15775

Training complete! Final step: 2013
