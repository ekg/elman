Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_25/levelllama_100m_20260126_174411
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 331,945,984 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.0940 | lr 3.00e-04 | grad 2.02 | tok/s 9399
step     20 | loss 2.9770 | lr 3.00e-04 | grad 0.66 | tok/s 16949
step     30 | loss 3.3188 | lr 3.00e-04 | grad 1.38 | tok/s 17925
step     40 | loss 5.4176 | lr 3.00e-04 | grad 5.34 | tok/s 18286
step     50 | loss 4.5431 | lr 3.00e-04 | grad 3.89 | tok/s 18530
step     60 | loss 4.0177 | lr 3.00e-04 | grad 2.97 | tok/s 18530
step     70 | loss 3.6349 | lr 3.00e-04 | grad 2.66 | tok/s 18527
step     80 | loss 3.5198 | lr 3.00e-04 | grad 2.00 | tok/s 18519
step     90 | loss 3.2840 | lr 3.00e-04 | grad 1.66 | tok/s 18501
step    100 | loss 3.1336 | lr 3.00e-04 | grad 1.41 | tok/s 18486
step    110 | loss 2.9615 | lr 3.00e-04 | grad 2.03 | tok/s 18324
step    120 | loss 3.2525 | lr 3.00e-04 | grad 0.96 | tok/s 17431
step    130 | loss 2.6268 | lr 3.00e-04 | grad 2.12 | tok/s 17844
step    140 | loss 2.8748 | lr 3.00e-04 | grad 4.28 | tok/s 17901
step    150 | loss 2.7556 | lr 3.00e-04 | grad 4.22 | tok/s 18332
step    160 | loss 2.8477 | lr 3.00e-04 | grad 1.34 | tok/s 17723
step    170 | loss 2.6836 | lr 3.00e-04 | grad 0.67 | tok/s 17459
step    180 | loss 2.8791 | lr 3.00e-04 | grad 1.27 | tok/s 17885
step    190 | loss 2.4476 | lr 3.00e-04 | grad 1.32 | tok/s 17530
step    200 | loss 2.3963 | lr 3.00e-04 | grad 0.93 | tok/s 18343
step    210 | loss 2.3953 | lr 3.00e-04 | grad 2.02 | tok/s 17383
step    220 | loss 2.6721 | lr 3.00e-04 | grad 1.64 | tok/s 17553
step    230 | loss 2.5485 | lr 3.00e-04 | grad 1.24 | tok/s 17541
step    240 | loss 2.7507 | lr 3.00e-04 | grad 1.84 | tok/s 17799
step    250 | loss 2.3085 | lr 3.00e-04 | grad 1.00 | tok/s 17669
step    260 | loss 2.4383 | lr 3.00e-04 | grad 1.48 | tok/s 18155
step    270 | loss 2.3060 | lr 3.00e-04 | grad 1.91 | tok/s 17730
step    280 | loss 2.2413 | lr 3.00e-04 | grad 0.69 | tok/s 16651
step    290 | loss 2.1718 | lr 3.00e-04 | grad 1.43 | tok/s 17216
step    300 | loss 2.4212 | lr 3.00e-04 | grad 0.80 | tok/s 17361
step    310 | loss 2.1273 | lr 3.00e-04 | grad 0.66 | tok/s 17275
step    320 | loss 2.3867 | lr 3.00e-04 | grad 2.62 | tok/s 17478
step    330 | loss 2.1839 | lr 3.00e-04 | grad 0.93 | tok/s 17671
step    340 | loss 2.4771 | lr 3.00e-04 | grad 1.41 | tok/s 17601
step    350 | loss 2.4715 | lr 3.00e-04 | grad 0.90 | tok/s 18116
step    360 | loss 2.0971 | lr 3.00e-04 | grad 0.91 | tok/s 17323
step    370 | loss 2.1612 | lr 3.00e-04 | grad 0.96 | tok/s 18240
step    380 | loss 1.9816 | lr 3.00e-04 | grad 1.18 | tok/s 18395
step    390 | loss 1.9258 | lr 3.00e-04 | grad 1.06 | tok/s 18394
step    400 | loss 2.2489 | lr 3.00e-04 | grad 0.95 | tok/s 17431
step    410 | loss 2.1555 | lr 3.00e-04 | grad 1.06 | tok/s 17605
step    420 | loss 2.3372 | lr 3.00e-04 | grad 1.34 | tok/s 18364
step    430 | loss 2.1645 | lr 3.00e-04 | grad 1.79 | tok/s 18061
step    440 | loss 2.1529 | lr 3.00e-04 | grad 1.20 | tok/s 17495
step    450 | loss 2.0537 | lr 3.00e-04 | grad 0.77 | tok/s 17691
step    460 | loss 2.1403 | lr 3.00e-04 | grad 0.90 | tok/s 17955
step    470 | loss 2.1290 | lr 3.00e-04 | grad 1.49 | tok/s 17833
step    480 | loss 2.1528 | lr 3.00e-04 | grad 1.47 | tok/s 18215
step    490 | loss 2.0895 | lr 3.00e-04 | grad 1.46 | tok/s 17497
step    500 | loss 2.2306 | lr 3.00e-04 | grad 0.96 | tok/s 17770
step    510 | loss 2.0934 | lr 3.00e-04 | grad 0.88 | tok/s 16972
step    520 | loss 1.9525 | lr 3.00e-04 | grad 0.92 | tok/s 17791
step    530 | loss 2.1033 | lr 3.00e-04 | grad 0.99 | tok/s 17510
step    540 | loss 2.0761 | lr 3.00e-04 | grad 0.69 | tok/s 17143
step    550 | loss 1.7512 | lr 3.00e-04 | grad 1.59 | tok/s 17881
step    560 | loss 1.9421 | lr 3.00e-04 | grad 0.98 | tok/s 18413
step    570 | loss 1.8276 | lr 3.00e-04 | grad 0.95 | tok/s 18404
step    580 | loss 1.7563 | lr 3.00e-04 | grad 0.77 | tok/s 18365
step    590 | loss 1.8105 | lr 3.00e-04 | grad 0.82 | tok/s 18369
step    600 | loss 1.7640 | lr 3.00e-04 | grad 1.36 | tok/s 18381
step    610 | loss 1.7308 | lr 3.00e-04 | grad 0.92 | tok/s 18390
step    620 | loss 1.7162 | lr 3.00e-04 | grad 1.02 | tok/s 18298
step    630 | loss 1.9362 | lr 3.00e-04 | grad 2.02 | tok/s 17244
step    640 | loss 2.0991 | lr 3.00e-04 | grad 1.40 | tok/s 17476
step    650 | loss 1.9132 | lr 3.00e-04 | grad 0.88 | tok/s 17485
step    660 | loss 1.9825 | lr 3.00e-04 | grad 1.07 | tok/s 18123
step    670 | loss 2.0134 | lr 3.00e-04 | grad 2.36 | tok/s 17494
step    680 | loss 1.9938 | lr 3.00e-04 | grad 1.12 | tok/s 17268
step    690 | loss 1.9635 | lr 3.00e-04 | grad 0.92 | tok/s 17162
step    700 | loss 1.8686 | lr 3.00e-04 | grad 0.92 | tok/s 17541
step    710 | loss 2.0080 | lr 3.00e-04 | grad 1.93 | tok/s 17284
step    720 | loss 1.7749 | lr 3.00e-04 | grad 1.36 | tok/s 17949
step    730 | loss 1.8575 | lr 3.00e-04 | grad 0.77 | tok/s 17662
step    740 | loss 2.2842 | lr 3.00e-04 | grad 1.78 | tok/s 18142
step    750 | loss 2.0912 | lr 3.00e-04 | grad 1.50 | tok/s 18365
step    760 | loss 1.8930 | lr 3.00e-04 | grad 2.05 | tok/s 17971
step    770 | loss 1.8945 | lr 3.00e-04 | grad 1.02 | tok/s 17667
step    780 | loss 1.8453 | lr 3.00e-04 | grad 1.16 | tok/s 17791
step    790 | loss 2.1596 | lr 3.00e-04 | grad 2.09 | tok/s 18185
step    800 | loss 1.7281 | lr 3.00e-04 | grad 1.17 | tok/s 17843
step    810 | loss 1.6602 | lr 3.00e-04 | grad 1.62 | tok/s 17252
step    820 | loss 1.8151 | lr 3.00e-04 | grad 1.08 | tok/s 17589
step    830 | loss 1.8642 | lr 3.00e-04 | grad 0.91 | tok/s 17357
step    840 | loss 1.9860 | lr 3.00e-04 | grad 0.97 | tok/s 17281
step    850 | loss 1.9439 | lr 3.00e-04 | grad 0.94 | tok/s 17644
step    860 | loss 1.9932 | lr 3.00e-04 | grad 1.16 | tok/s 17957
step    870 | loss 2.0483 | lr 3.00e-04 | grad 1.02 | tok/s 18090
step    880 | loss 1.8911 | lr 3.00e-04 | grad 0.83 | tok/s 17735
step    890 | loss 1.7732 | lr 3.00e-04 | grad 0.82 | tok/s 17644
step    900 | loss 1.8379 | lr 3.00e-04 | grad 0.88 | tok/s 17565
step    910 | loss 1.8908 | lr 3.00e-04 | grad 2.97 | tok/s 17388
step    920 | loss 1.7914 | lr 3.00e-04 | grad 1.07 | tok/s 17581
step    930 | loss 1.7511 | lr 3.00e-04 | grad 0.92 | tok/s 17803
step    940 | loss 1.7208 | lr 3.00e-04 | grad 0.94 | tok/s 17373
step    950 | loss 1.7943 | lr 3.00e-04 | grad 1.17 | tok/s 17134
step    960 | loss 1.7471 | lr 3.00e-04 | grad 1.18 | tok/s 17585
step    970 | loss 1.7419 | lr 3.00e-04 | grad 0.99 | tok/s 17532
step    980 | loss 2.5383 | lr 3.00e-04 | grad 2.00 | tok/s 18270
step    990 | loss 1.9390 | lr 3.00e-04 | grad 1.02 | tok/s 17554
step   1000 | loss 1.8708 | lr 3.00e-04 | grad 1.15 | tok/s 17616
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8708.pt
step   1010 | loss 1.6019 | lr 3.00e-04 | grad 1.32 | tok/s 11774
step   1020 | loss 1.6830 | lr 3.00e-04 | grad 1.09 | tok/s 18497
step   1030 | loss 1.8467 | lr 3.00e-04 | grad 1.11 | tok/s 17582
step   1040 | loss 2.3555 | lr 3.00e-04 | grad 1.37 | tok/s 18005
step   1050 | loss 1.9468 | lr 3.00e-04 | grad 1.12 | tok/s 18119
step   1060 | loss 1.5854 | lr 3.00e-04 | grad 0.79 | tok/s 17859
step   1070 | loss 1.6772 | lr 3.00e-04 | grad 1.34 | tok/s 17844
step   1080 | loss 1.5826 | lr 3.00e-04 | grad 1.37 | tok/s 18457
step   1090 | loss 1.5472 | lr 3.00e-04 | grad 0.81 | tok/s 18461
step   1100 | loss 1.5270 | lr 3.00e-04 | grad 1.11 | tok/s 18453
step   1110 | loss 1.4650 | lr 3.00e-04 | grad 1.12 | tok/s 18445
step   1120 | loss 1.6864 | lr 3.00e-04 | grad 1.02 | tok/s 17946
step   1130 | loss 2.1216 | lr 3.00e-04 | grad 1.15 | tok/s 18145
step   1140 | loss 2.0679 | lr 3.00e-04 | grad 1.10 | tok/s 18345
step   1150 | loss 2.2503 | lr 3.00e-04 | grad 1.68 | tok/s 17939
step   1160 | loss 1.9713 | lr 3.00e-04 | grad 2.81 | tok/s 17362
step   1170 | loss 1.8083 | lr 3.00e-04 | grad 1.16 | tok/s 17316
step   1180 | loss 1.7098 | lr 3.00e-04 | grad 1.06 | tok/s 18225
step   1190 | loss 2.0315 | lr 3.00e-04 | grad 1.33 | tok/s 18260
step   1200 | loss 1.5580 | lr 3.00e-04 | grad 1.13 | tok/s 18424
step   1210 | loss 1.6086 | lr 3.00e-04 | grad 1.02 | tok/s 17357
step   1220 | loss 1.6938 | lr 3.00e-04 | grad 1.23 | tok/s 17924
step   1230 | loss 1.7315 | lr 3.00e-04 | grad 0.95 | tok/s 18048
step   1240 | loss 1.6233 | lr 3.00e-04 | grad 1.41 | tok/s 18253
step   1250 | loss 1.8418 | lr 3.00e-04 | grad 1.59 | tok/s 17817
step   1260 | loss 1.9639 | lr 3.00e-04 | grad 0.98 | tok/s 18302
step   1270 | loss 1.6757 | lr 3.00e-04 | grad 1.52 | tok/s 17742
step   1280 | loss 1.6878 | lr 3.00e-04 | grad 1.23 | tok/s 17784
step   1290 | loss 1.6832 | lr 3.00e-04 | grad 1.16 | tok/s 17398
step   1300 | loss 1.9242 | lr 3.00e-04 | grad 3.16 | tok/s 17340
step   1310 | loss 1.9439 | lr 3.00e-04 | grad 1.07 | tok/s 18051
step   1320 | loss 1.7830 | lr 3.00e-04 | grad 2.14 | tok/s 17990

Training complete! Final step: 1327
