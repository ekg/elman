Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_110/levelllama_100m_20260126_192704
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 507,232,000 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 8, Chunk size: 512
Gradient accumulation: 1, Effective batch: 8

Time-based training: 10.0 minutes
step     10 | loss 5.7447 | lr 3.00e-04 | grad 3.77 | tok/s 5824
step     20 | loss 2.8443 | lr 3.00e-04 | grad 3.27 | tok/s 14218
step     30 | loss 2.8915 | lr 3.00e-04 | grad 2.61 | tok/s 14347
step     40 | loss 2.8825 | lr 3.00e-04 | grad 1.85 | tok/s 13798
step     50 | loss 3.2460 | lr 3.00e-04 | grad 3.62 | tok/s 14026
step     60 | loss 2.5381 | lr 3.00e-04 | grad 7.25 | tok/s 14484
step     70 | loss 2.5149 | lr 3.00e-04 | grad 3.11 | tok/s 14617
step     80 | loss 6.0246 | lr 3.00e-04 | grad 8.44 | tok/s 14762
step     90 | loss 4.5735 | lr 3.00e-04 | grad 2.70 | tok/s 15011
step    100 | loss 3.7475 | lr 3.00e-04 | grad 2.83 | tok/s 14999
step    110 | loss 3.5959 | lr 3.00e-04 | grad 6.53 | tok/s 14989
step    120 | loss 3.4492 | lr 3.00e-04 | grad 7.66 | tok/s 14968
step    130 | loss 3.3126 | lr 3.00e-04 | grad 4.12 | tok/s 14950
step    140 | loss 2.8418 | lr 3.00e-04 | grad 2.48 | tok/s 14937
step    150 | loss 3.0782 | lr 3.00e-04 | grad 6.94 | tok/s 14926
step    160 | loss 2.5968 | lr 3.00e-04 | grad 3.75 | tok/s 14899
step    170 | loss 2.6095 | lr 3.00e-04 | grad 5.12 | tok/s 14887
step    180 | loss 2.4248 | lr 3.00e-04 | grad 4.56 | tok/s 14864
step    190 | loss 2.5759 | lr 3.00e-04 | grad 2.61 | tok/s 14850
step    200 | loss 2.3230 | lr 3.00e-04 | grad 2.52 | tok/s 14846
step    210 | loss 2.3541 | lr 3.00e-04 | grad 3.00 | tok/s 14828
step    220 | loss 2.5703 | lr 3.00e-04 | grad 2.42 | tok/s 14654
step    230 | loss 3.4395 | lr 3.00e-04 | grad 3.64 | tok/s 14469
step    240 | loss 2.6313 | lr 3.00e-04 | grad 2.98 | tok/s 13744
step    250 | loss 2.4774 | lr 3.00e-04 | grad 1.99 | tok/s 14109
step    260 | loss 2.2838 | lr 3.00e-04 | grad 1.84 | tok/s 14570
step    270 | loss 2.5294 | lr 3.00e-04 | grad 1.73 | tok/s 14371
step    280 | loss 2.6699 | lr 3.00e-04 | grad 2.92 | tok/s 14079
step    290 | loss 2.5971 | lr 3.00e-04 | grad 2.67 | tok/s 14797
step    300 | loss 1.6232 | lr 3.00e-04 | grad 1.62 | tok/s 14788
step    310 | loss 2.8540 | lr 3.00e-04 | grad 1.76 | tok/s 14516
step    320 | loss 2.5476 | lr 3.00e-04 | grad 3.41 | tok/s 14222
step    330 | loss 2.3549 | lr 3.00e-04 | grad 1.27 | tok/s 13741
step    340 | loss 2.6255 | lr 3.00e-04 | grad 1.46 | tok/s 13935
step    350 | loss 2.4389 | lr 3.00e-04 | grad 2.12 | tok/s 14282
step    360 | loss 2.4948 | lr 3.00e-04 | grad 2.77 | tok/s 14586
step    370 | loss 2.2467 | lr 3.00e-04 | grad 1.68 | tok/s 13230
step    380 | loss 2.2206 | lr 3.00e-04 | grad 2.27 | tok/s 14083
step    390 | loss 2.1063 | lr 3.00e-04 | grad 2.41 | tok/s 14708
step    400 | loss 2.0781 | lr 3.00e-04 | grad 1.62 | tok/s 14580
step    410 | loss 1.9946 | lr 3.00e-04 | grad 1.38 | tok/s 14254
step    420 | loss 2.1825 | lr 3.00e-04 | grad 1.98 | tok/s 13601
step    430 | loss 2.4647 | lr 3.00e-04 | grad 2.23 | tok/s 14483
step    440 | loss 2.5011 | lr 3.00e-04 | grad 1.76 | tok/s 13675
step    450 | loss 2.3318 | lr 3.00e-04 | grad 1.10 | tok/s 14149
step    460 | loss 2.1932 | lr 3.00e-04 | grad 2.44 | tok/s 13842
step    470 | loss 2.2732 | lr 3.00e-04 | grad 1.71 | tok/s 14271
step    480 | loss 2.6700 | lr 3.00e-04 | grad 2.70 | tok/s 14282
step    490 | loss 2.1601 | lr 3.00e-04 | grad 1.92 | tok/s 13492
step    500 | loss 2.1496 | lr 3.00e-04 | grad 1.91 | tok/s 14400
step    510 | loss 2.1538 | lr 3.00e-04 | grad 1.18 | tok/s 14591
step    520 | loss 2.1448 | lr 3.00e-04 | grad 1.23 | tok/s 14583
step    530 | loss 2.2854 | lr 3.00e-04 | grad 1.59 | tok/s 14012
step    540 | loss 2.0628 | lr 3.00e-04 | grad 1.61 | tok/s 14004
step    550 | loss 1.9159 | lr 3.00e-04 | grad 1.55 | tok/s 13723
step    560 | loss 2.0856 | lr 3.00e-04 | grad 1.48 | tok/s 13369
step    570 | loss 2.0232 | lr 3.00e-04 | grad 2.11 | tok/s 13724
step    580 | loss 1.9161 | lr 3.00e-04 | grad 1.24 | tok/s 13682
step    590 | loss 2.2525 | lr 3.00e-04 | grad 1.30 | tok/s 14011
step    600 | loss 2.1467 | lr 3.00e-04 | grad 1.15 | tok/s 13567
step    610 | loss 1.9767 | lr 3.00e-04 | grad 1.51 | tok/s 14257
step    620 | loss 1.8507 | lr 3.00e-04 | grad 1.11 | tok/s 13507
step    630 | loss 1.9978 | lr 3.00e-04 | grad 2.25 | tok/s 13581
step    640 | loss 2.1678 | lr 3.00e-04 | grad 1.45 | tok/s 13958
step    650 | loss 2.0032 | lr 3.00e-04 | grad 1.65 | tok/s 14060
step    660 | loss 2.0265 | lr 3.00e-04 | grad 0.96 | tok/s 14082
step    670 | loss 2.2447 | lr 3.00e-04 | grad 4.34 | tok/s 14133
step    680 | loss 2.0077 | lr 3.00e-04 | grad 1.14 | tok/s 13849
step    690 | loss 2.2811 | lr 3.00e-04 | grad 1.61 | tok/s 14309
step    700 | loss 2.0460 | lr 3.00e-04 | grad 1.71 | tok/s 14601
step    710 | loss 1.9171 | lr 3.00e-04 | grad 1.56 | tok/s 13631
step    720 | loss 1.7623 | lr 3.00e-04 | grad 1.38 | tok/s 13436
step    730 | loss 1.8241 | lr 3.00e-04 | grad 1.73 | tok/s 14579
step    740 | loss 1.8932 | lr 3.00e-04 | grad 1.44 | tok/s 14393
step    750 | loss 1.6868 | lr 3.00e-04 | grad 1.34 | tok/s 14594
step    760 | loss 1.5470 | lr 3.00e-04 | grad 1.57 | tok/s 14641
step    770 | loss 1.5146 | lr 3.00e-04 | grad 1.37 | tok/s 14624
step    780 | loss 1.4538 | lr 3.00e-04 | grad 1.20 | tok/s 14620
step    790 | loss 1.5134 | lr 3.00e-04 | grad 1.69 | tok/s 14184
step    800 | loss 2.2326 | lr 3.00e-04 | grad 2.66 | tok/s 14111
step    810 | loss 1.9576 | lr 3.00e-04 | grad 1.32 | tok/s 14051
step    820 | loss 1.9635 | lr 3.00e-04 | grad 1.70 | tok/s 13496
step    830 | loss 2.0082 | lr 3.00e-04 | grad 1.22 | tok/s 14470
step    840 | loss 1.9053 | lr 3.00e-04 | grad 1.20 | tok/s 14608
step    850 | loss 1.9338 | lr 3.00e-04 | grad 1.50 | tok/s 14519
step    860 | loss 1.8840 | lr 3.00e-04 | grad 2.06 | tok/s 14372
step    870 | loss 1.8131 | lr 3.00e-04 | grad 1.38 | tok/s 13822
step    880 | loss 1.9827 | lr 3.00e-04 | grad 1.14 | tok/s 13886
step    890 | loss 1.9507 | lr 3.00e-04 | grad 1.59 | tok/s 14105
step    900 | loss 1.8296 | lr 3.00e-04 | grad 1.23 | tok/s 14092
step    910 | loss 1.6889 | lr 3.00e-04 | grad 1.65 | tok/s 13794
step    920 | loss 1.8806 | lr 3.00e-04 | grad 2.23 | tok/s 14339
step    930 | loss 1.8588 | lr 3.00e-04 | grad 1.88 | tok/s 13683
step    940 | loss 1.7965 | lr 3.00e-04 | grad 1.22 | tok/s 14453
step    950 | loss 1.8378 | lr 3.00e-04 | grad 1.52 | tok/s 14591
step    960 | loss 1.7270 | lr 3.00e-04 | grad 1.46 | tok/s 14530
step    970 | loss 1.9265 | lr 3.00e-04 | grad 1.66 | tok/s 13692
step    980 | loss 1.8617 | lr 3.00e-04 | grad 1.22 | tok/s 14052
step    990 | loss 1.7684 | lr 3.00e-04 | grad 1.31 | tok/s 14270
step   1000 | loss 2.1115 | lr 3.00e-04 | grad 5.19 | tok/s 13741
  >>> saved checkpoint: checkpoint_step_001000_loss_2.1115.pt
step   1010 | loss 1.9612 | lr 3.00e-04 | grad 1.43 | tok/s 6230
step   1020 | loss 1.8480 | lr 3.00e-04 | grad 1.16 | tok/s 13396
step   1030 | loss 1.7062 | lr 3.00e-04 | grad 1.29 | tok/s 13952
step   1040 | loss 1.7113 | lr 3.00e-04 | grad 1.27 | tok/s 14371
step   1050 | loss 1.8116 | lr 3.00e-04 | grad 1.77 | tok/s 13281
step   1060 | loss 1.9875 | lr 3.00e-04 | grad 1.58 | tok/s 14397
step   1070 | loss 2.0227 | lr 3.00e-04 | grad 1.34 | tok/s 14319
step   1080 | loss 1.6202 | lr 3.00e-04 | grad 1.09 | tok/s 13005
step   1090 | loss 1.3619 | lr 3.00e-04 | grad 0.91 | tok/s 14329
step   1100 | loss 1.6428 | lr 3.00e-04 | grad 2.03 | tok/s 13905
step   1110 | loss 1.7078 | lr 3.00e-04 | grad 1.20 | tok/s 14635
step   1120 | loss 1.5993 | lr 3.00e-04 | grad 1.57 | tok/s 14610
step   1130 | loss 1.5438 | lr 3.00e-04 | grad 1.32 | tok/s 14634
step   1140 | loss 1.5205 | lr 3.00e-04 | grad 1.53 | tok/s 14637
step   1150 | loss 1.5384 | lr 3.00e-04 | grad 1.33 | tok/s 14614
step   1160 | loss 1.4436 | lr 3.00e-04 | grad 1.23 | tok/s 14605
step   1170 | loss 1.4584 | lr 3.00e-04 | grad 1.35 | tok/s 14609
step   1180 | loss 1.5867 | lr 3.00e-04 | grad 0.99 | tok/s 14649
step   1190 | loss 1.4640 | lr 3.00e-04 | grad 1.38 | tok/s 14650
step   1200 | loss 1.4553 | lr 3.00e-04 | grad 1.44 | tok/s 14645
step   1210 | loss 1.4872 | lr 3.00e-04 | grad 1.49 | tok/s 14655
step   1220 | loss 1.4915 | lr 3.00e-04 | grad 1.38 | tok/s 14656
step   1230 | loss 1.4697 | lr 3.00e-04 | grad 1.23 | tok/s 14663
step   1240 | loss 1.4190 | lr 3.00e-04 | grad 1.16 | tok/s 14592
step   1250 | loss 2.0346 | lr 3.00e-04 | grad 1.67 | tok/s 13826
step   1260 | loss 1.5676 | lr 3.00e-04 | grad 1.81 | tok/s 13698
step   1270 | loss 1.8362 | lr 3.00e-04 | grad 2.80 | tok/s 13686
step   1280 | loss 1.8313 | lr 3.00e-04 | grad 1.31 | tok/s 14092
step   1290 | loss 1.6786 | lr 3.00e-04 | grad 1.42 | tok/s 13943
step   1300 | loss 1.7398 | lr 3.00e-04 | grad 1.42 | tok/s 14053
step   1310 | loss 1.7017 | lr 3.00e-04 | grad 1.38 | tok/s 14314
step   1320 | loss 1.7876 | lr 3.00e-04 | grad 1.25 | tok/s 14367
step   1330 | loss 1.7928 | lr 3.00e-04 | grad 1.41 | tok/s 14387
step   1340 | loss 1.6831 | lr 3.00e-04 | grad 4.28 | tok/s 13751
step   1350 | loss 1.8841 | lr 3.00e-04 | grad 1.41 | tok/s 13317
step   1360 | loss 1.7584 | lr 3.00e-04 | grad 1.52 | tok/s 14093
step   1370 | loss 1.5812 | lr 3.00e-04 | grad 0.95 | tok/s 13873
step   1380 | loss 1.8788 | lr 3.00e-04 | grad 1.14 | tok/s 13411
step   1390 | loss 1.7224 | lr 3.00e-04 | grad 1.25 | tok/s 14252
step   1400 | loss 1.6189 | lr 3.00e-04 | grad 1.12 | tok/s 13724
step   1410 | loss 1.6522 | lr 3.00e-04 | grad 1.66 | tok/s 13768
step   1420 | loss 1.9068 | lr 3.00e-04 | grad 3.23 | tok/s 13818
step   1430 | loss 1.6153 | lr 3.00e-04 | grad 1.10 | tok/s 14035
step   1440 | loss 1.3859 | lr 3.00e-04 | grad 1.37 | tok/s 14442
step   1450 | loss 1.3632 | lr 3.00e-04 | grad 2.64 | tok/s 14501
step   1460 | loss 1.8349 | lr 3.00e-04 | grad 1.27 | tok/s 13710
step   1470 | loss 1.7484 | lr 3.00e-04 | grad 1.13 | tok/s 14212
step   1480 | loss 2.2081 | lr 3.00e-04 | grad 2.19 | tok/s 14321
step   1490 | loss 1.9895 | lr 3.00e-04 | grad 1.31 | tok/s 14516
step   1500 | loss 1.6142 | lr 3.00e-04 | grad 1.08 | tok/s 14577
step   1510 | loss 1.7071 | lr 3.00e-04 | grad 1.30 | tok/s 14395
step   1520 | loss 1.6255 | lr 3.00e-04 | grad 2.34 | tok/s 14100
step   1530 | loss 1.5925 | lr 3.00e-04 | grad 1.20 | tok/s 14459
step   1540 | loss 1.7807 | lr 3.00e-04 | grad 1.29 | tok/s 13563
step   1550 | loss 1.5308 | lr 3.00e-04 | grad 1.62 | tok/s 14447
step   1560 | loss 1.7271 | lr 3.00e-04 | grad 1.55 | tok/s 13725
step   1570 | loss 1.5560 | lr 3.00e-04 | grad 1.21 | tok/s 14554
step   1580 | loss 2.0302 | lr 3.00e-04 | grad 2.70 | tok/s 14224
step   1590 | loss 1.9451 | lr 3.00e-04 | grad 1.36 | tok/s 13674
step   1600 | loss 1.2191 | lr 3.00e-04 | grad 0.89 | tok/s 14591
step   1610 | loss 1.1792 | lr 3.00e-04 | grad 1.60 | tok/s 14167
step   1620 | loss 1.6198 | lr 3.00e-04 | grad 1.59 | tok/s 13228
step   1630 | loss 1.6546 | lr 3.00e-04 | grad 1.30 | tok/s 14176
step   1640 | loss 1.5065 | lr 3.00e-04 | grad 1.39 | tok/s 13842
step   1650 | loss 1.6630 | lr 3.00e-04 | grad 1.32 | tok/s 13276
step   1660 | loss 1.6316 | lr 3.00e-04 | grad 1.06 | tok/s 14157
step   1670 | loss 1.6323 | lr 3.00e-04 | grad 4.53 | tok/s 14118
step   1680 | loss 1.8654 | lr 3.00e-04 | grad 1.25 | tok/s 13566
step   1690 | loss 1.6704 | lr 3.00e-04 | grad 2.91 | tok/s 13784
step   1700 | loss 1.6712 | lr 3.00e-04 | grad 1.39 | tok/s 14089
step   1710 | loss 1.6805 | lr 3.00e-04 | grad 1.15 | tok/s 13845
step   1720 | loss 1.7567 | lr 3.00e-04 | grad 1.52 | tok/s 14498
step   1730 | loss 1.6617 | lr 3.00e-04 | grad 1.33 | tok/s 14622
step   1740 | loss 1.6567 | lr 3.00e-04 | grad 1.54 | tok/s 14282
step   1750 | loss 1.7382 | lr 3.00e-04 | grad 1.51 | tok/s 14032
step   1760 | loss 1.6751 | lr 3.00e-04 | grad 1.34 | tok/s 14091
step   1770 | loss 1.5846 | lr 3.00e-04 | grad 1.27 | tok/s 13847
step   1780 | loss 1.6473 | lr 3.00e-04 | grad 1.20 | tok/s 14390
step   1790 | loss 1.5701 | lr 3.00e-04 | grad 1.07 | tok/s 14038
step   1800 | loss 1.7567 | lr 3.00e-04 | grad 1.36 | tok/s 14145
step   1810 | loss 1.5879 | lr 3.00e-04 | grad 1.22 | tok/s 13619
step   1820 | loss 1.6871 | lr 3.00e-04 | grad 2.95 | tok/s 13801
step   1830 | loss 1.5990 | lr 3.00e-04 | grad 1.36 | tok/s 14338
step   1840 | loss 1.6614 | lr 3.00e-04 | grad 1.49 | tok/s 13690
step   1850 | loss 1.5641 | lr 3.00e-04 | grad 1.13 | tok/s 14333
step   1860 | loss 1.4855 | lr 3.00e-04 | grad 1.29 | tok/s 13860
step   1870 | loss 1.5806 | lr 3.00e-04 | grad 1.75 | tok/s 13943
step   1880 | loss 1.4277 | lr 3.00e-04 | grad 1.23 | tok/s 13646
step   1890 | loss 1.6807 | lr 3.00e-04 | grad 1.19 | tok/s 12965
step   1900 | loss 1.5314 | lr 3.00e-04 | grad 1.36 | tok/s 14029
step   1910 | loss 1.5857 | lr 3.00e-04 | grad 1.34 | tok/s 13314
step   1920 | loss 1.5744 | lr 3.00e-04 | grad 1.34 | tok/s 14593
step   1930 | loss 1.5881 | lr 3.00e-04 | grad 1.44 | tok/s 13663
step   1940 | loss 1.5761 | lr 3.00e-04 | grad 1.22 | tok/s 14224
step   1950 | loss 2.2049 | lr 3.00e-04 | grad 1.94 | tok/s 14438
step   1960 | loss 1.9348 | lr 3.00e-04 | grad 2.03 | tok/s 14613
step   1970 | loss 1.7697 | lr 3.00e-04 | grad 1.59 | tok/s 14257
step   1980 | loss 1.7191 | lr 3.00e-04 | grad 1.34 | tok/s 13621
step   1990 | loss 1.7296 | lr 3.00e-04 | grad 3.27 | tok/s 13856
step   2000 | loss 1.6275 | lr 3.00e-04 | grad 1.25 | tok/s 14083
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6275.pt
step   2010 | loss 1.1939 | lr 3.00e-04 | grad 1.10 | tok/s 6738
step   2020 | loss 1.4506 | lr 3.00e-04 | grad 1.13 | tok/s 13978
step   2030 | loss 1.3170 | lr 3.00e-04 | grad 0.47 | tok/s 14679
step   2040 | loss 1.5321 | lr 3.00e-04 | grad 1.17 | tok/s 14613
step   2050 | loss 1.5066 | lr 3.00e-04 | grad 1.23 | tok/s 14311
step   2060 | loss 1.7085 | lr 3.00e-04 | grad 1.23 | tok/s 13481
step   2070 | loss 1.7849 | lr 3.00e-04 | grad 1.54 | tok/s 13977
step   2080 | loss 2.3997 | lr 3.00e-04 | grad 2.59 | tok/s 14464

Training complete! Final step: 2084
