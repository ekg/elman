Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_47/levelllama_100m_20260126_180441
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 271,087,488 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.3865 | lr 3.00e-04 | grad 3.39 | tok/s 11152
step     20 | loss 2.9131 | lr 3.00e-04 | grad 1.41 | tok/s 25521
step     30 | loss 3.3403 | lr 3.00e-04 | grad 1.43 | tok/s 26949
step     40 | loss 5.5473 | lr 3.00e-04 | grad 35.00 | tok/s 27410
step     50 | loss 4.8415 | lr 3.00e-04 | grad 3.92 | tok/s 27785
step     60 | loss 3.9179 | lr 3.00e-04 | grad 3.11 | tok/s 27724
step     70 | loss 3.3521 | lr 3.00e-04 | grad 2.66 | tok/s 27620
step     80 | loss 3.1274 | lr 3.00e-04 | grad 1.63 | tok/s 27593
step     90 | loss 2.8183 | lr 3.00e-04 | grad 1.41 | tok/s 27498
step    100 | loss 2.6101 | lr 3.00e-04 | grad 2.05 | tok/s 27373
step    110 | loss 2.6506 | lr 3.00e-04 | grad 1.78 | tok/s 27115
step    120 | loss 3.4311 | lr 3.00e-04 | grad 1.20 | tok/s 25801
step    130 | loss 2.6256 | lr 3.00e-04 | grad 2.33 | tok/s 26353
step    140 | loss 2.9396 | lr 3.00e-04 | grad 4.75 | tok/s 26366
step    150 | loss 2.7533 | lr 3.00e-04 | grad 4.28 | tok/s 27012
step    160 | loss 2.8654 | lr 3.00e-04 | grad 1.62 | tok/s 26030
step    170 | loss 2.7172 | lr 3.00e-04 | grad 0.93 | tok/s 25595
step    180 | loss 2.8680 | lr 3.00e-04 | grad 1.46 | tok/s 26220
step    190 | loss 2.4424 | lr 3.00e-04 | grad 1.37 | tok/s 25681
step    200 | loss 2.3757 | lr 3.00e-04 | grad 1.10 | tok/s 26820
step    210 | loss 2.3872 | lr 3.00e-04 | grad 2.30 | tok/s 25444
step    220 | loss 2.6765 | lr 3.00e-04 | grad 3.88 | tok/s 25679
step    230 | loss 2.4348 | lr 3.00e-04 | grad 1.78 | tok/s 25664
step    240 | loss 2.7510 | lr 3.00e-04 | grad 2.11 | tok/s 26011
step    250 | loss 2.2905 | lr 3.00e-04 | grad 1.43 | tok/s 25842
step    260 | loss 2.4277 | lr 3.00e-04 | grad 1.77 | tok/s 26577
step    270 | loss 2.2901 | lr 3.00e-04 | grad 1.46 | tok/s 25969
step    280 | loss 2.2233 | lr 3.00e-04 | grad 0.79 | tok/s 24354
step    290 | loss 2.1564 | lr 3.00e-04 | grad 1.51 | tok/s 25202
step    300 | loss 2.4151 | lr 3.00e-04 | grad 1.11 | tok/s 25371
step    310 | loss 2.1056 | lr 3.00e-04 | grad 1.23 | tok/s 25294
step    320 | loss 2.3733 | lr 3.00e-04 | grad 2.69 | tok/s 25595
step    330 | loss 2.1622 | lr 3.00e-04 | grad 1.09 | tok/s 25855
step    340 | loss 2.4589 | lr 3.00e-04 | grad 1.51 | tok/s 25753
step    350 | loss 2.4261 | lr 3.00e-04 | grad 1.11 | tok/s 26563
step    360 | loss 2.0700 | lr 3.00e-04 | grad 1.06 | tok/s 25398
step    370 | loss 2.1339 | lr 3.00e-04 | grad 0.96 | tok/s 26753
step    380 | loss 1.9516 | lr 3.00e-04 | grad 1.36 | tok/s 27022
step    390 | loss 1.8814 | lr 3.00e-04 | grad 1.05 | tok/s 27018
step    400 | loss 2.2396 | lr 3.00e-04 | grad 1.00 | tok/s 25626
step    410 | loss 2.1392 | lr 3.00e-04 | grad 1.12 | tok/s 25850
step    420 | loss 2.3039 | lr 3.00e-04 | grad 1.41 | tok/s 26963
step    430 | loss 2.1371 | lr 3.00e-04 | grad 1.24 | tok/s 26514
step    440 | loss 2.1422 | lr 3.00e-04 | grad 1.25 | tok/s 25731
step    450 | loss 2.0325 | lr 3.00e-04 | grad 0.80 | tok/s 26013
step    460 | loss 2.1112 | lr 3.00e-04 | grad 1.10 | tok/s 26443
step    470 | loss 2.1023 | lr 3.00e-04 | grad 1.68 | tok/s 26235
step    480 | loss 2.1385 | lr 3.00e-04 | grad 1.74 | tok/s 26817
step    490 | loss 2.0791 | lr 3.00e-04 | grad 1.35 | tok/s 25750
step    500 | loss 2.2124 | lr 3.00e-04 | grad 1.03 | tok/s 26158
step    510 | loss 2.0730 | lr 3.00e-04 | grad 0.83 | tok/s 25004
step    520 | loss 1.9317 | lr 3.00e-04 | grad 1.01 | tok/s 26178
step    530 | loss 2.0837 | lr 3.00e-04 | grad 1.12 | tok/s 25788
step    540 | loss 2.0520 | lr 3.00e-04 | grad 0.79 | tok/s 25256
step    550 | loss 1.7388 | lr 3.00e-04 | grad 1.96 | tok/s 26364
step    560 | loss 1.9225 | lr 3.00e-04 | grad 1.52 | tok/s 27172
step    570 | loss 1.8057 | lr 3.00e-04 | grad 1.38 | tok/s 27143
step    580 | loss 1.7380 | lr 3.00e-04 | grad 0.74 | tok/s 27178
step    590 | loss 1.7861 | lr 3.00e-04 | grad 1.16 | tok/s 27176
step    600 | loss 1.7372 | lr 3.00e-04 | grad 0.92 | tok/s 27176
step    610 | loss 1.7093 | lr 3.00e-04 | grad 0.92 | tok/s 27205
step    620 | loss 1.6937 | lr 3.00e-04 | grad 1.01 | tok/s 27079
step    630 | loss 1.9633 | lr 3.00e-04 | grad 2.44 | tok/s 25629
step    640 | loss 2.0822 | lr 3.00e-04 | grad 1.48 | tok/s 25956
step    650 | loss 1.8925 | lr 3.00e-04 | grad 0.97 | tok/s 25940
step    660 | loss 1.9636 | lr 3.00e-04 | grad 1.37 | tok/s 26942
step    670 | loss 1.9829 | lr 3.00e-04 | grad 2.56 | tok/s 26041
step    680 | loss 1.9769 | lr 3.00e-04 | grad 1.41 | tok/s 25635
step    690 | loss 1.9458 | lr 3.00e-04 | grad 1.12 | tok/s 25427
step    700 | loss 1.8447 | lr 3.00e-04 | grad 1.08 | tok/s 25988
step    710 | loss 1.9976 | lr 3.00e-04 | grad 2.23 | tok/s 25580
step    720 | loss 1.7292 | lr 3.00e-04 | grad 1.04 | tok/s 26600
step    730 | loss 1.8222 | lr 3.00e-04 | grad 0.88 | tok/s 26164
step    740 | loss 2.2629 | lr 3.00e-04 | grad 2.00 | tok/s 26905
step    750 | loss 2.0560 | lr 3.00e-04 | grad 1.51 | tok/s 27196
step    760 | loss 1.8644 | lr 3.00e-04 | grad 1.92 | tok/s 26670
step    770 | loss 1.8718 | lr 3.00e-04 | grad 1.10 | tok/s 26199
step    780 | loss 1.8220 | lr 3.00e-04 | grad 1.30 | tok/s 26338
step    790 | loss 2.1318 | lr 3.00e-04 | grad 1.99 | tok/s 26957
step    800 | loss 1.7005 | lr 3.00e-04 | grad 1.06 | tok/s 26456
step    810 | loss 1.6509 | lr 3.00e-04 | grad 1.84 | tok/s 25585
step    820 | loss 1.7979 | lr 3.00e-04 | grad 1.18 | tok/s 26117
step    830 | loss 1.8357 | lr 3.00e-04 | grad 1.11 | tok/s 25783
step    840 | loss 1.9654 | lr 3.00e-04 | grad 1.10 | tok/s 25637
step    850 | loss 1.9200 | lr 3.00e-04 | grad 1.12 | tok/s 26194
step    860 | loss 1.9675 | lr 3.00e-04 | grad 1.30 | tok/s 26620
step    870 | loss 1.9998 | lr 3.00e-04 | grad 1.17 | tok/s 26862
step    880 | loss 1.8723 | lr 3.00e-04 | grad 1.03 | tok/s 26332
step    890 | loss 1.7601 | lr 3.00e-04 | grad 1.05 | tok/s 26201
step    900 | loss 1.8148 | lr 3.00e-04 | grad 1.02 | tok/s 26061
step    910 | loss 1.8714 | lr 3.00e-04 | grad 3.23 | tok/s 25829
step    920 | loss 1.7687 | lr 3.00e-04 | grad 1.16 | tok/s 26104
step    930 | loss 1.7388 | lr 3.00e-04 | grad 1.10 | tok/s 26412
step    940 | loss 1.6946 | lr 3.00e-04 | grad 1.09 | tok/s 25853
step    950 | loss 1.7858 | lr 3.00e-04 | grad 1.44 | tok/s 25430
step    960 | loss 1.7267 | lr 3.00e-04 | grad 1.07 | tok/s 26096
step    970 | loss 1.7223 | lr 3.00e-04 | grad 1.09 | tok/s 26131
step    980 | loss 2.4684 | lr 3.00e-04 | grad 1.99 | tok/s 27174
step    990 | loss 1.9234 | lr 3.00e-04 | grad 1.30 | tok/s 26040
step   1000 | loss 1.8355 | lr 3.00e-04 | grad 1.18 | tok/s 26134
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8355.pt
step   1010 | loss 1.5751 | lr 3.00e-04 | grad 1.28 | tok/s 16652
step   1020 | loss 1.6639 | lr 3.00e-04 | grad 1.26 | tok/s 27486
step   1030 | loss 1.8248 | lr 3.00e-04 | grad 1.20 | tok/s 26120
step   1040 | loss 2.2925 | lr 3.00e-04 | grad 1.82 | tok/s 26762
step   1050 | loss 1.9100 | lr 3.00e-04 | grad 1.08 | tok/s 26894
step   1060 | loss 1.5527 | lr 3.00e-04 | grad 0.82 | tok/s 26521
step   1070 | loss 1.6569 | lr 3.00e-04 | grad 1.18 | tok/s 26502
step   1080 | loss 1.5544 | lr 3.00e-04 | grad 1.30 | tok/s 27386
step   1090 | loss 1.5312 | lr 3.00e-04 | grad 1.34 | tok/s 27362
step   1100 | loss 1.5031 | lr 3.00e-04 | grad 1.18 | tok/s 27339
step   1110 | loss 1.4407 | lr 3.00e-04 | grad 1.06 | tok/s 27343
step   1120 | loss 1.6563 | lr 3.00e-04 | grad 1.05 | tok/s 26607
step   1130 | loss 2.0658 | lr 3.00e-04 | grad 1.19 | tok/s 26883
step   1140 | loss 2.0529 | lr 3.00e-04 | grad 1.23 | tok/s 27200
step   1150 | loss 2.2160 | lr 3.00e-04 | grad 1.78 | tok/s 26607
step   1160 | loss 1.9369 | lr 3.00e-04 | grad 3.08 | tok/s 25755
step   1170 | loss 1.7861 | lr 3.00e-04 | grad 1.34 | tok/s 25700
step   1180 | loss 1.6928 | lr 3.00e-04 | grad 1.26 | tok/s 27032
step   1190 | loss 1.9965 | lr 3.00e-04 | grad 1.51 | tok/s 27102
step   1200 | loss 1.5232 | lr 3.00e-04 | grad 1.21 | tok/s 27351
step   1210 | loss 1.5883 | lr 3.00e-04 | grad 1.11 | tok/s 25725
step   1220 | loss 1.6648 | lr 3.00e-04 | grad 1.39 | tok/s 26599
step   1230 | loss 1.7081 | lr 3.00e-04 | grad 1.03 | tok/s 26754
step   1240 | loss 1.5876 | lr 3.00e-04 | grad 1.68 | tok/s 27083
step   1250 | loss 1.8135 | lr 3.00e-04 | grad 1.47 | tok/s 26453
step   1260 | loss 1.8780 | lr 3.00e-04 | grad 1.16 | tok/s 27194
step   1270 | loss 1.6565 | lr 3.00e-04 | grad 1.66 | tok/s 26339
step   1280 | loss 1.6629 | lr 3.00e-04 | grad 1.37 | tok/s 26357
step   1290 | loss 1.6563 | lr 3.00e-04 | grad 1.32 | tok/s 25771
step   1300 | loss 1.8982 | lr 3.00e-04 | grad 3.30 | tok/s 25683
step   1310 | loss 1.9234 | lr 3.00e-04 | grad 1.16 | tok/s 26786
step   1320 | loss 1.7500 | lr 3.00e-04 | grad 2.17 | tok/s 26735
step   1330 | loss 1.7801 | lr 3.00e-04 | grad 1.26 | tok/s 26642
step   1340 | loss 1.8442 | lr 3.00e-04 | grad 1.05 | tok/s 25787
step   1350 | loss 1.7320 | lr 3.00e-04 | grad 1.21 | tok/s 26854
step   1360 | loss 1.7826 | lr 3.00e-04 | grad 1.33 | tok/s 25218
step   1370 | loss 1.8907 | lr 3.00e-04 | grad 2.39 | tok/s 26957
step   1380 | loss 1.7475 | lr 3.00e-04 | grad 1.36 | tok/s 26097
step   1390 | loss 1.7321 | lr 3.00e-04 | grad 1.41 | tok/s 26610
step   1400 | loss 1.7642 | lr 3.00e-04 | grad 1.17 | tok/s 26044
step   1410 | loss 1.6095 | lr 3.00e-04 | grad 1.61 | tok/s 25613
step   1420 | loss 1.6229 | lr 3.00e-04 | grad 1.39 | tok/s 27189
step   1430 | loss 2.0180 | lr 3.00e-04 | grad 0.99 | tok/s 26258
step   1440 | loss 1.7217 | lr 3.00e-04 | grad 1.58 | tok/s 26460
step   1450 | loss 1.7238 | lr 3.00e-04 | grad 1.13 | tok/s 26754
step   1460 | loss 1.8010 | lr 3.00e-04 | grad 1.45 | tok/s 25951
step   1470 | loss 1.6378 | lr 3.00e-04 | grad 1.40 | tok/s 25592
step   1480 | loss 1.6608 | lr 3.00e-04 | grad 1.94 | tok/s 26562
step   1490 | loss 1.8615 | lr 3.00e-04 | grad 3.64 | tok/s 26327
step   1500 | loss 1.8806 | lr 3.00e-04 | grad 1.25 | tok/s 26721
step   1510 | loss 1.5479 | lr 3.00e-04 | grad 1.09 | tok/s 26204
step   1520 | loss 1.6968 | lr 3.00e-04 | grad 1.53 | tok/s 26066
step   1530 | loss 1.6397 | lr 3.00e-04 | grad 1.29 | tok/s 26796
step   1540 | loss 1.7310 | lr 3.00e-04 | grad 0.94 | tok/s 26841
step   1550 | loss 1.7179 | lr 3.00e-04 | grad 2.34 | tok/s 26326
step   1560 | loss 1.5103 | lr 3.00e-04 | grad 1.23 | tok/s 27203
step   1570 | loss 1.6073 | lr 3.00e-04 | grad 1.11 | tok/s 26554
step   1580 | loss 1.5257 | lr 3.00e-04 | grad 1.37 | tok/s 26687
step   1590 | loss 1.6958 | lr 3.00e-04 | grad 2.00 | tok/s 25703
step   1600 | loss 1.4892 | lr 3.00e-04 | grad 4.38 | tok/s 27123
step   1610 | loss 2.1409 | lr 3.00e-04 | grad 2.59 | tok/s 26464
step   1620 | loss 2.3146 | lr 3.00e-04 | grad 1.84 | tok/s 27345
step   1630 | loss 2.0671 | lr 3.00e-04 | grad 1.59 | tok/s 27347
step   1640 | loss 1.9100 | lr 3.00e-04 | grad 1.35 | tok/s 27326
step   1650 | loss 1.8245 | lr 3.00e-04 | grad 1.58 | tok/s 27322
step   1660 | loss 1.7735 | lr 3.00e-04 | grad 1.27 | tok/s 27326
step   1670 | loss 1.8598 | lr 3.00e-04 | grad 1.54 | tok/s 26472
step   1680 | loss 1.6624 | lr 3.00e-04 | grad 1.23 | tok/s 26235
step   1690 | loss 1.6790 | lr 3.00e-04 | grad 1.51 | tok/s 25639
step   1700 | loss 1.5519 | lr 3.00e-04 | grad 1.16 | tok/s 26606
step   1710 | loss 1.5218 | lr 3.00e-04 | grad 1.49 | tok/s 26717
step   1720 | loss 1.6589 | lr 3.00e-04 | grad 1.16 | tok/s 25880
step   1730 | loss 1.7274 | lr 3.00e-04 | grad 2.11 | tok/s 26709
step   1740 | loss 1.6442 | lr 3.00e-04 | grad 1.27 | tok/s 26848
step   1750 | loss 1.5350 | lr 3.00e-04 | grad 1.29 | tok/s 26271
step   1760 | loss 1.6216 | lr 3.00e-04 | grad 1.27 | tok/s 26005
step   1770 | loss 1.9108 | lr 3.00e-04 | grad 1.24 | tok/s 26631
step   1780 | loss 1.9413 | lr 3.00e-04 | grad 1.34 | tok/s 25193
step   1790 | loss 1.5251 | lr 3.00e-04 | grad 1.09 | tok/s 25549
step   1800 | loss 1.5646 | lr 3.00e-04 | grad 1.12 | tok/s 26173
step   1810 | loss 1.6308 | lr 3.00e-04 | grad 1.41 | tok/s 26372
step   1820 | loss 1.7361 | lr 3.00e-04 | grad 1.82 | tok/s 26055
step   1830 | loss 1.5807 | lr 3.00e-04 | grad 1.05 | tok/s 25325
step   1840 | loss 1.6511 | lr 3.00e-04 | grad 1.23 | tok/s 26229
step   1850 | loss 1.6692 | lr 3.00e-04 | grad 1.40 | tok/s 26072
step   1860 | loss 1.6356 | lr 3.00e-04 | grad 1.35 | tok/s 25936
step   1870 | loss 1.6358 | lr 3.00e-04 | grad 1.69 | tok/s 26384
step   1880 | loss 1.6960 | lr 3.00e-04 | grad 1.16 | tok/s 26574
step   1890 | loss 1.4987 | lr 3.00e-04 | grad 1.15 | tok/s 27298
step   1900 | loss 1.4355 | lr 3.00e-04 | grad 1.18 | tok/s 27310
step   1910 | loss 1.4088 | lr 3.00e-04 | grad 0.90 | tok/s 27276
step   1920 | loss 1.3890 | lr 3.00e-04 | grad 1.09 | tok/s 27330
step   1930 | loss 1.4216 | lr 3.00e-04 | grad 2.09 | tok/s 27174
step   1940 | loss 1.7507 | lr 3.00e-04 | grad 1.43 | tok/s 25886
step   1950 | loss 1.6348 | lr 3.00e-04 | grad 0.96 | tok/s 25639
step   1960 | loss 1.7436 | lr 3.00e-04 | grad 1.65 | tok/s 25987

Training complete! Final step: 1964
