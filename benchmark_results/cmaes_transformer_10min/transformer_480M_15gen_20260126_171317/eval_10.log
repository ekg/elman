Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_10/levelllama_100m_20260126_172339
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 318,287,232 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.3337 | lr 3.00e-04 | grad 3.20 | tok/s 10924
step     20 | loss 2.9687 | lr 3.00e-04 | grad 1.03 | tok/s 23643
step     30 | loss 3.3029 | lr 3.00e-04 | grad 1.55 | tok/s 25004
step     40 | loss 5.0972 | lr 3.00e-04 | grad 5.94 | tok/s 25542
step     50 | loss 4.2796 | lr 3.00e-04 | grad 4.03 | tok/s 25903
step     60 | loss 4.0213 | lr 3.00e-04 | grad 3.36 | tok/s 25869
step     70 | loss 3.7041 | lr 3.00e-04 | grad 3.09 | tok/s 25849
step     80 | loss 3.6719 | lr 3.00e-04 | grad 2.70 | tok/s 25822
step     90 | loss 3.4298 | lr 3.00e-04 | grad 2.16 | tok/s 25780
step    100 | loss 3.2605 | lr 3.00e-04 | grad 2.00 | tok/s 25775
step    110 | loss 3.0065 | lr 3.00e-04 | grad 2.67 | tok/s 25525
step    120 | loss 3.1356 | lr 3.00e-04 | grad 0.91 | tok/s 24288
step    130 | loss 2.5910 | lr 3.00e-04 | grad 2.25 | tok/s 24817
step    140 | loss 2.8438 | lr 3.00e-04 | grad 4.25 | tok/s 24833
step    150 | loss 2.7643 | lr 3.00e-04 | grad 4.47 | tok/s 25364
step    160 | loss 2.8613 | lr 3.00e-04 | grad 1.42 | tok/s 24509
step    170 | loss 2.6611 | lr 3.00e-04 | grad 0.77 | tok/s 24152
step    180 | loss 2.8652 | lr 3.00e-04 | grad 1.41 | tok/s 24718
step    190 | loss 2.4117 | lr 3.00e-04 | grad 1.16 | tok/s 24241
step    200 | loss 2.3455 | lr 3.00e-04 | grad 1.38 | tok/s 25304
step    210 | loss 2.3659 | lr 3.00e-04 | grad 2.16 | tok/s 23979
step    220 | loss 2.7297 | lr 3.00e-04 | grad 3.52 | tok/s 24236
step    230 | loss 2.6205 | lr 3.00e-04 | grad 1.56 | tok/s 24169
step    240 | loss 2.7245 | lr 3.00e-04 | grad 2.02 | tok/s 24469
step    250 | loss 2.2715 | lr 3.00e-04 | grad 0.88 | tok/s 24281
step    260 | loss 2.4158 | lr 3.00e-04 | grad 1.48 | tok/s 24959
step    270 | loss 2.2744 | lr 3.00e-04 | grad 2.00 | tok/s 24353
step    280 | loss 2.2097 | lr 3.00e-04 | grad 0.71 | tok/s 22885
step    290 | loss 2.1459 | lr 3.00e-04 | grad 1.40 | tok/s 23612
step    300 | loss 2.3974 | lr 3.00e-04 | grad 1.11 | tok/s 23783
step    310 | loss 2.0961 | lr 3.00e-04 | grad 0.75 | tok/s 23681
step    320 | loss 2.3461 | lr 3.00e-04 | grad 2.77 | tok/s 23941
step    330 | loss 2.1558 | lr 3.00e-04 | grad 1.10 | tok/s 24160
step    340 | loss 2.4457 | lr 3.00e-04 | grad 1.46 | tok/s 23959
step    350 | loss 2.4417 | lr 3.00e-04 | grad 1.02 | tok/s 24611
step    360 | loss 2.0677 | lr 3.00e-04 | grad 1.18 | tok/s 23534
step    370 | loss 2.1151 | lr 3.00e-04 | grad 0.96 | tok/s 24867
step    380 | loss 1.9413 | lr 3.00e-04 | grad 1.37 | tok/s 25081
step    390 | loss 1.8698 | lr 3.00e-04 | grad 1.20 | tok/s 25116
step    400 | loss 2.2259 | lr 3.00e-04 | grad 1.02 | tok/s 23799
step    410 | loss 2.1370 | lr 3.00e-04 | grad 1.08 | tok/s 24021
step    420 | loss 2.2943 | lr 3.00e-04 | grad 1.42 | tok/s 25038
step    430 | loss 2.1303 | lr 3.00e-04 | grad 1.48 | tok/s 24641
step    440 | loss 2.1305 | lr 3.00e-04 | grad 1.40 | tok/s 23861
step    450 | loss 2.0237 | lr 3.00e-04 | grad 0.83 | tok/s 24108
step    460 | loss 2.1124 | lr 3.00e-04 | grad 1.03 | tok/s 24464
step    470 | loss 2.0970 | lr 3.00e-04 | grad 1.61 | tok/s 24278
step    480 | loss 2.1145 | lr 3.00e-04 | grad 1.65 | tok/s 24800
step    490 | loss 2.0618 | lr 3.00e-04 | grad 1.42 | tok/s 23793
step    500 | loss 2.2142 | lr 3.00e-04 | grad 1.12 | tok/s 24212
step    510 | loss 2.0748 | lr 3.00e-04 | grad 0.88 | tok/s 23133
step    520 | loss 1.9261 | lr 3.00e-04 | grad 0.98 | tok/s 24186
step    530 | loss 2.0789 | lr 3.00e-04 | grad 1.08 | tok/s 23788
step    540 | loss 2.0470 | lr 3.00e-04 | grad 0.80 | tok/s 23328
step    550 | loss 1.7318 | lr 3.00e-04 | grad 1.87 | tok/s 24377
step    560 | loss 1.9161 | lr 3.00e-04 | grad 1.56 | tok/s 25001
step    570 | loss 1.8005 | lr 3.00e-04 | grad 1.46 | tok/s 25020
step    580 | loss 1.7323 | lr 3.00e-04 | grad 0.75 | tok/s 25037
step    590 | loss 1.7825 | lr 3.00e-04 | grad 1.12 | tok/s 25046
step    600 | loss 1.7379 | lr 3.00e-04 | grad 1.27 | tok/s 25072
step    610 | loss 1.7024 | lr 3.00e-04 | grad 0.99 | tok/s 25076
step    620 | loss 1.6870 | lr 3.00e-04 | grad 1.12 | tok/s 24937
step    630 | loss 1.9397 | lr 3.00e-04 | grad 2.36 | tok/s 23500
step    640 | loss 2.0743 | lr 3.00e-04 | grad 1.34 | tok/s 23775
step    650 | loss 1.8941 | lr 3.00e-04 | grad 0.96 | tok/s 23866
step    660 | loss 1.9571 | lr 3.00e-04 | grad 1.20 | tok/s 24776
step    670 | loss 1.9777 | lr 3.00e-04 | grad 2.39 | tok/s 23905
step    680 | loss 1.9758 | lr 3.00e-04 | grad 1.24 | tok/s 23599
step    690 | loss 1.9390 | lr 3.00e-04 | grad 0.97 | tok/s 23424
step    700 | loss 1.8457 | lr 3.00e-04 | grad 0.96 | tok/s 23917
step    710 | loss 1.9828 | lr 3.00e-04 | grad 2.17 | tok/s 23522
step    720 | loss 1.7441 | lr 3.00e-04 | grad 1.32 | tok/s 24424
step    730 | loss 1.8334 | lr 3.00e-04 | grad 0.84 | tok/s 24004
step    740 | loss 2.2741 | lr 3.00e-04 | grad 2.08 | tok/s 24667
step    750 | loss 2.0790 | lr 3.00e-04 | grad 1.56 | tok/s 24978
step    760 | loss 1.8732 | lr 3.00e-04 | grad 2.25 | tok/s 24461
step    770 | loss 1.8788 | lr 3.00e-04 | grad 1.07 | tok/s 24038
step    780 | loss 1.8240 | lr 3.00e-04 | grad 1.24 | tok/s 24193
step    790 | loss 2.1491 | lr 3.00e-04 | grad 1.99 | tok/s 24742
step    800 | loss 1.7126 | lr 3.00e-04 | grad 0.93 | tok/s 24329
step    810 | loss 1.6436 | lr 3.00e-04 | grad 1.82 | tok/s 23498
step    820 | loss 1.7967 | lr 3.00e-04 | grad 1.15 | tok/s 23985
step    830 | loss 1.8323 | lr 3.00e-04 | grad 1.05 | tok/s 23583
step    840 | loss 1.9664 | lr 3.00e-04 | grad 1.07 | tok/s 23523
step    850 | loss 1.9193 | lr 3.00e-04 | grad 1.05 | tok/s 24022
step    860 | loss 1.9785 | lr 3.00e-04 | grad 1.35 | tok/s 24397
step    870 | loss 2.0200 | lr 3.00e-04 | grad 1.09 | tok/s 24570
step    880 | loss 1.8686 | lr 3.00e-04 | grad 0.93 | tok/s 24129
step    890 | loss 1.7550 | lr 3.00e-04 | grad 0.92 | tok/s 24004
step    900 | loss 1.8158 | lr 3.00e-04 | grad 0.93 | tok/s 23954
step    910 | loss 1.8822 | lr 3.00e-04 | grad 3.41 | tok/s 23695
step    920 | loss 1.7665 | lr 3.00e-04 | grad 1.05 | tok/s 23965
step    930 | loss 1.7314 | lr 3.00e-04 | grad 1.00 | tok/s 24263
step    940 | loss 1.6980 | lr 3.00e-04 | grad 1.06 | tok/s 23689
step    950 | loss 1.7770 | lr 3.00e-04 | grad 1.30 | tok/s 23309
step    960 | loss 1.7249 | lr 3.00e-04 | grad 1.12 | tok/s 23929
step    970 | loss 1.7196 | lr 3.00e-04 | grad 1.11 | tok/s 23942
step    980 | loss 2.4931 | lr 3.00e-04 | grad 1.74 | tok/s 24931
step    990 | loss 1.9222 | lr 3.00e-04 | grad 1.35 | tok/s 23898
step   1000 | loss 1.8670 | lr 3.00e-04 | grad 1.27 | tok/s 23968
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8670.pt
step   1010 | loss 1.5806 | lr 3.00e-04 | grad 1.34 | tok/s 14360
step   1020 | loss 1.6710 | lr 3.00e-04 | grad 1.16 | tok/s 25147
step   1030 | loss 1.8288 | lr 3.00e-04 | grad 1.13 | tok/s 23844
step   1040 | loss 2.3344 | lr 3.00e-04 | grad 1.94 | tok/s 24439
step   1050 | loss 1.9227 | lr 3.00e-04 | grad 1.16 | tok/s 24588
step   1060 | loss 1.5559 | lr 3.00e-04 | grad 1.00 | tok/s 24271
step   1070 | loss 1.6579 | lr 3.00e-04 | grad 1.48 | tok/s 24248
step   1080 | loss 1.5482 | lr 3.00e-04 | grad 1.41 | tok/s 25070
step   1090 | loss 1.5307 | lr 3.00e-04 | grad 1.35 | tok/s 25042
step   1100 | loss 1.5007 | lr 3.00e-04 | grad 1.25 | tok/s 25008
step   1110 | loss 1.4408 | lr 3.00e-04 | grad 1.11 | tok/s 24925
step   1120 | loss 1.6618 | lr 3.00e-04 | grad 1.05 | tok/s 24259
step   1130 | loss 2.1088 | lr 3.00e-04 | grad 1.16 | tok/s 24604
step   1140 | loss 2.0535 | lr 3.00e-04 | grad 1.20 | tok/s 24854
step   1150 | loss 2.2193 | lr 3.00e-04 | grad 1.71 | tok/s 24314
step   1160 | loss 1.9477 | lr 3.00e-04 | grad 2.84 | tok/s 23554
step   1170 | loss 1.7923 | lr 3.00e-04 | grad 1.23 | tok/s 23544
step   1180 | loss 1.6943 | lr 3.00e-04 | grad 1.26 | tok/s 24764
step   1190 | loss 2.0089 | lr 3.00e-04 | grad 1.32 | tok/s 24834
step   1200 | loss 1.5300 | lr 3.00e-04 | grad 1.20 | tok/s 25063
step   1210 | loss 1.5900 | lr 3.00e-04 | grad 1.08 | tok/s 23576
step   1220 | loss 1.6746 | lr 3.00e-04 | grad 1.23 | tok/s 24319
step   1230 | loss 1.7147 | lr 3.00e-04 | grad 0.98 | tok/s 24522
step   1240 | loss 1.5983 | lr 3.00e-04 | grad 1.59 | tok/s 24753
step   1250 | loss 1.8221 | lr 3.00e-04 | grad 1.31 | tok/s 24186
step   1260 | loss 1.9044 | lr 3.00e-04 | grad 1.09 | tok/s 24850
step   1270 | loss 1.6518 | lr 3.00e-04 | grad 1.49 | tok/s 24143
step   1280 | loss 1.6697 | lr 3.00e-04 | grad 1.36 | tok/s 24203
step   1290 | loss 1.6683 | lr 3.00e-04 | grad 1.23 | tok/s 23638
step   1300 | loss 1.9073 | lr 3.00e-04 | grad 3.22 | tok/s 23502
step   1310 | loss 1.9265 | lr 3.00e-04 | grad 1.12 | tok/s 24513
step   1320 | loss 1.7585 | lr 3.00e-04 | grad 2.12 | tok/s 24448
step   1330 | loss 1.7967 | lr 3.00e-04 | grad 1.22 | tok/s 24365
step   1340 | loss 1.8571 | lr 3.00e-04 | grad 1.07 | tok/s 23588
step   1350 | loss 1.7323 | lr 3.00e-04 | grad 1.06 | tok/s 24527
step   1360 | loss 1.7790 | lr 3.00e-04 | grad 1.25 | tok/s 23099
step   1370 | loss 1.8944 | lr 3.00e-04 | grad 2.30 | tok/s 24678
step   1380 | loss 1.7550 | lr 3.00e-04 | grad 1.34 | tok/s 23829
step   1390 | loss 1.7317 | lr 3.00e-04 | grad 1.36 | tok/s 24284
step   1400 | loss 1.7774 | lr 3.00e-04 | grad 1.27 | tok/s 23697
step   1410 | loss 1.6065 | lr 3.00e-04 | grad 1.43 | tok/s 23360
step   1420 | loss 1.6036 | lr 3.00e-04 | grad 1.33 | tok/s 24729
step   1430 | loss 2.0326 | lr 3.00e-04 | grad 1.01 | tok/s 23988
step   1440 | loss 1.7318 | lr 3.00e-04 | grad 1.52 | tok/s 24172
step   1450 | loss 1.7266 | lr 3.00e-04 | grad 1.20 | tok/s 24457
step   1460 | loss 1.7929 | lr 3.00e-04 | grad 1.36 | tok/s 23714
step   1470 | loss 1.6467 | lr 3.00e-04 | grad 1.41 | tok/s 23440
step   1480 | loss 1.6642 | lr 3.00e-04 | grad 1.87 | tok/s 24311
step   1490 | loss 1.8898 | lr 3.00e-04 | grad 1.53 | tok/s 24157
step   1500 | loss 1.9209 | lr 3.00e-04 | grad 1.24 | tok/s 24542
step   1510 | loss 1.5425 | lr 3.00e-04 | grad 1.00 | tok/s 24028
step   1520 | loss 1.7032 | lr 3.00e-04 | grad 1.40 | tok/s 23891
step   1530 | loss 1.6461 | lr 3.00e-04 | grad 1.26 | tok/s 24590
step   1540 | loss 1.7320 | lr 3.00e-04 | grad 1.04 | tok/s 24610
step   1550 | loss 1.7129 | lr 3.00e-04 | grad 2.30 | tok/s 24144
step   1560 | loss 1.5181 | lr 3.00e-04 | grad 1.23 | tok/s 24939
step   1570 | loss 1.6355 | lr 3.00e-04 | grad 1.05 | tok/s 24338
step   1580 | loss 1.5275 | lr 3.00e-04 | grad 1.33 | tok/s 24469
step   1590 | loss 1.7089 | lr 3.00e-04 | grad 1.97 | tok/s 23518
step   1600 | loss 1.5040 | lr 3.00e-04 | grad 4.44 | tok/s 24802
step   1610 | loss 2.1368 | lr 3.00e-04 | grad 2.36 | tok/s 24269
step   1620 | loss 2.3186 | lr 3.00e-04 | grad 1.67 | tok/s 25044
step   1630 | loss 2.0741 | lr 3.00e-04 | grad 1.41 | tok/s 25071
step   1640 | loss 1.9125 | lr 3.00e-04 | grad 1.59 | tok/s 25013
step   1650 | loss 1.8274 | lr 3.00e-04 | grad 1.62 | tok/s 25016
step   1660 | loss 1.7732 | lr 3.00e-04 | grad 1.25 | tok/s 25028
step   1670 | loss 1.8672 | lr 3.00e-04 | grad 1.59 | tok/s 24251
step   1680 | loss 1.6663 | lr 3.00e-04 | grad 1.20 | tok/s 24054
step   1690 | loss 1.6850 | lr 3.00e-04 | grad 1.51 | tok/s 23537
step   1700 | loss 1.5602 | lr 3.00e-04 | grad 1.31 | tok/s 24451
step   1710 | loss 1.5309 | lr 3.00e-04 | grad 1.54 | tok/s 24572
step   1720 | loss 1.6629 | lr 3.00e-04 | grad 1.07 | tok/s 23800
step   1730 | loss 1.7301 | lr 3.00e-04 | grad 2.17 | tok/s 24552
step   1740 | loss 1.6492 | lr 3.00e-04 | grad 1.34 | tok/s 24695
step   1750 | loss 1.5393 | lr 3.00e-04 | grad 1.11 | tok/s 24184
step   1760 | loss 1.6214 | lr 3.00e-04 | grad 1.22 | tok/s 23945
step   1770 | loss 1.9453 | lr 3.00e-04 | grad 1.19 | tok/s 24505
step   1780 | loss 1.9462 | lr 3.00e-04 | grad 1.24 | tok/s 23154
step   1790 | loss 1.5282 | lr 3.00e-04 | grad 1.12 | tok/s 23509
step   1800 | loss 1.5759 | lr 3.00e-04 | grad 1.12 | tok/s 24099

Training complete! Final step: 1809
