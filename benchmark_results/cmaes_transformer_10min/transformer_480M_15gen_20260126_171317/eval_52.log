Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_52/levelllama_100m_20260126_181456
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 311,877,120 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.2637 | lr 3.00e-04 | grad 3.47 | tok/s 11427
step     20 | loss 2.9533 | lr 3.00e-04 | grad 2.42 | tok/s 27007
step     30 | loss 3.2719 | lr 3.00e-04 | grad 2.02 | tok/s 28520
step     40 | loss 5.6078 | lr 3.00e-04 | grad 7.50 | tok/s 29053
step     50 | loss 4.4753 | lr 3.00e-04 | grad 3.77 | tok/s 29412
step     60 | loss 3.7529 | lr 3.00e-04 | grad 3.69 | tok/s 29374
step     70 | loss 3.2758 | lr 3.00e-04 | grad 3.55 | tok/s 29310
step     80 | loss 2.9756 | lr 3.00e-04 | grad 2.06 | tok/s 29223
step     90 | loss 2.7420 | lr 3.00e-04 | grad 2.16 | tok/s 29134
step    100 | loss 2.5484 | lr 3.00e-04 | grad 3.25 | tok/s 29104
step    110 | loss 2.6653 | lr 3.00e-04 | grad 2.77 | tok/s 28870
step    120 | loss 3.4456 | lr 3.00e-04 | grad 1.23 | tok/s 27504
step    130 | loss 2.6555 | lr 3.00e-04 | grad 4.34 | tok/s 28141
step    140 | loss 2.9416 | lr 3.00e-04 | grad 5.22 | tok/s 28200
step    150 | loss 2.7063 | lr 3.00e-04 | grad 4.50 | tok/s 28871
step    160 | loss 2.7962 | lr 3.00e-04 | grad 1.60 | tok/s 27894
step    170 | loss 2.7137 | lr 3.00e-04 | grad 1.74 | tok/s 27463
step    180 | loss 2.8786 | lr 3.00e-04 | grad 1.74 | tok/s 28100
step    190 | loss 2.4640 | lr 3.00e-04 | grad 2.28 | tok/s 27583
step    200 | loss 2.4001 | lr 3.00e-04 | grad 1.48 | tok/s 28864
step    210 | loss 2.4126 | lr 3.00e-04 | grad 2.92 | tok/s 27342
step    220 | loss 2.7084 | lr 3.00e-04 | grad 3.58 | tok/s 27642
step    230 | loss 2.4882 | lr 3.00e-04 | grad 2.61 | tok/s 27593
step    240 | loss 2.7705 | lr 3.00e-04 | grad 2.44 | tok/s 27982
step    250 | loss 2.3186 | lr 3.00e-04 | grad 2.31 | tok/s 27741
step    260 | loss 2.4508 | lr 3.00e-04 | grad 2.03 | tok/s 28527
step    270 | loss 2.3117 | lr 3.00e-04 | grad 1.82 | tok/s 27894
step    280 | loss 2.2486 | lr 3.00e-04 | grad 0.97 | tok/s 26183
step    290 | loss 2.1832 | lr 3.00e-04 | grad 1.94 | tok/s 27073
step    300 | loss 2.4370 | lr 3.00e-04 | grad 1.73 | tok/s 27263
step    310 | loss 2.1270 | lr 3.00e-04 | grad 1.80 | tok/s 27135
step    320 | loss 2.3761 | lr 3.00e-04 | grad 2.81 | tok/s 27456
step    330 | loss 2.1810 | lr 3.00e-04 | grad 1.89 | tok/s 27736
step    340 | loss 2.4791 | lr 3.00e-04 | grad 1.63 | tok/s 27618
step    350 | loss 2.4133 | lr 3.00e-04 | grad 1.45 | tok/s 28399
step    360 | loss 2.0827 | lr 3.00e-04 | grad 1.45 | tok/s 27208
step    370 | loss 2.1283 | lr 3.00e-04 | grad 1.73 | tok/s 28666
step    380 | loss 1.9333 | lr 3.00e-04 | grad 2.02 | tok/s 28839
step    390 | loss 1.8432 | lr 3.00e-04 | grad 1.66 | tok/s 28857
step    400 | loss 2.2324 | lr 3.00e-04 | grad 1.28 | tok/s 27361
step    410 | loss 2.1509 | lr 3.00e-04 | grad 1.28 | tok/s 27604
step    420 | loss 2.2716 | lr 3.00e-04 | grad 1.62 | tok/s 28793
step    430 | loss 2.1077 | lr 3.00e-04 | grad 1.79 | tok/s 28326
step    440 | loss 2.1338 | lr 3.00e-04 | grad 1.65 | tok/s 27455
step    450 | loss 2.0269 | lr 3.00e-04 | grad 1.31 | tok/s 27769
step    460 | loss 2.0908 | lr 3.00e-04 | grad 1.27 | tok/s 28162
step    470 | loss 2.0803 | lr 3.00e-04 | grad 2.02 | tok/s 27947
step    480 | loss 2.1094 | lr 3.00e-04 | grad 2.09 | tok/s 28543
step    490 | loss 2.0597 | lr 3.00e-04 | grad 1.72 | tok/s 27416
step    500 | loss 2.1933 | lr 3.00e-04 | grad 1.37 | tok/s 27877
step    510 | loss 2.0559 | lr 3.00e-04 | grad 1.32 | tok/s 26617
step    520 | loss 1.9100 | lr 3.00e-04 | grad 1.10 | tok/s 27871
step    530 | loss 2.0673 | lr 3.00e-04 | grad 1.18 | tok/s 27444
step    540 | loss 2.0158 | lr 3.00e-04 | grad 0.93 | tok/s 26817
step    550 | loss 1.7089 | lr 3.00e-04 | grad 2.56 | tok/s 28044
step    560 | loss 1.8641 | lr 3.00e-04 | grad 1.34 | tok/s 28881
step    570 | loss 1.7531 | lr 3.00e-04 | grad 1.62 | tok/s 28881
step    580 | loss 1.6835 | lr 3.00e-04 | grad 1.15 | tok/s 28852
step    590 | loss 1.7267 | lr 3.00e-04 | grad 1.39 | tok/s 28833
step    600 | loss 1.6664 | lr 3.00e-04 | grad 1.59 | tok/s 28868
step    610 | loss 1.6582 | lr 3.00e-04 | grad 1.26 | tok/s 28861
step    620 | loss 1.6384 | lr 3.00e-04 | grad 1.46 | tok/s 28742
step    630 | loss 1.9293 | lr 3.00e-04 | grad 3.53 | tok/s 27140
step    640 | loss 2.0395 | lr 3.00e-04 | grad 1.41 | tok/s 27520
step    650 | loss 1.8570 | lr 3.00e-04 | grad 1.34 | tok/s 27508
step    660 | loss 1.9196 | lr 3.00e-04 | grad 1.30 | tok/s 28521
step    670 | loss 1.9417 | lr 3.00e-04 | grad 2.92 | tok/s 27608
step    680 | loss 1.9416 | lr 3.00e-04 | grad 1.73 | tok/s 27147
step    690 | loss 1.8950 | lr 3.00e-04 | grad 1.26 | tok/s 26980
step    700 | loss 1.7905 | lr 3.00e-04 | grad 1.07 | tok/s 27556
step    710 | loss 1.9425 | lr 3.00e-04 | grad 2.28 | tok/s 27097
step    720 | loss 1.6786 | lr 3.00e-04 | grad 1.38 | tok/s 28184
step    730 | loss 1.7735 | lr 3.00e-04 | grad 1.06 | tok/s 27679
step    740 | loss 2.1972 | lr 3.00e-04 | grad 2.48 | tok/s 28487
step    750 | loss 1.9790 | lr 3.00e-04 | grad 1.78 | tok/s 28786
step    760 | loss 1.8202 | lr 3.00e-04 | grad 2.33 | tok/s 28208
step    770 | loss 1.8316 | lr 3.00e-04 | grad 1.39 | tok/s 27696
step    780 | loss 1.7707 | lr 3.00e-04 | grad 1.47 | tok/s 27883
step    790 | loss 2.0915 | lr 3.00e-04 | grad 3.19 | tok/s 28492
step    800 | loss 1.6537 | lr 3.00e-04 | grad 1.26 | tok/s 28032
step    810 | loss 1.6067 | lr 3.00e-04 | grad 1.91 | tok/s 27079
step    820 | loss 1.7408 | lr 3.00e-04 | grad 1.30 | tok/s 27622
step    830 | loss 1.7906 | lr 3.00e-04 | grad 1.31 | tok/s 27227
step    840 | loss 1.9115 | lr 3.00e-04 | grad 1.25 | tok/s 27138
step    850 | loss 1.8593 | lr 3.00e-04 | grad 1.18 | tok/s 27670
step    860 | loss 1.9050 | lr 3.00e-04 | grad 1.73 | tok/s 28135
step    870 | loss 1.9117 | lr 3.00e-04 | grad 1.33 | tok/s 28343
step    880 | loss 1.8214 | lr 3.00e-04 | grad 1.13 | tok/s 27801
step    890 | loss 1.7115 | lr 3.00e-04 | grad 1.17 | tok/s 27666
step    900 | loss 1.7697 | lr 3.00e-04 | grad 1.08 | tok/s 27586
step    910 | loss 1.8135 | lr 3.00e-04 | grad 3.56 | tok/s 27282
step    920 | loss 1.7195 | lr 3.00e-04 | grad 1.34 | tok/s 27545
step    930 | loss 1.6790 | lr 3.00e-04 | grad 1.38 | tok/s 27939
step    940 | loss 1.6364 | lr 3.00e-04 | grad 1.22 | tok/s 27331
step    950 | loss 1.7342 | lr 3.00e-04 | grad 1.66 | tok/s 26818
step    960 | loss 1.6759 | lr 3.00e-04 | grad 1.21 | tok/s 27585
step    970 | loss 1.6788 | lr 3.00e-04 | grad 1.23 | tok/s 27583
step    980 | loss 2.3786 | lr 3.00e-04 | grad 2.42 | tok/s 28692
step    990 | loss 1.8692 | lr 3.00e-04 | grad 1.38 | tok/s 27553
step   1000 | loss 1.7953 | lr 3.00e-04 | grad 1.56 | tok/s 27636
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7953.pt
step   1010 | loss 1.5306 | lr 3.00e-04 | grad 1.55 | tok/s 15751
step   1020 | loss 1.5958 | lr 3.00e-04 | grad 1.29 | tok/s 29022
step   1030 | loss 1.7552 | lr 3.00e-04 | grad 1.34 | tok/s 27575
step   1040 | loss 2.2829 | lr 3.00e-04 | grad 2.78 | tok/s 28208
step   1050 | loss 1.8506 | lr 3.00e-04 | grad 1.29 | tok/s 28385
step   1060 | loss 1.5090 | lr 3.00e-04 | grad 0.96 | tok/s 28033
step   1070 | loss 1.6055 | lr 3.00e-04 | grad 1.34 | tok/s 27971
step   1080 | loss 1.4859 | lr 3.00e-04 | grad 1.30 | tok/s 28848
step   1090 | loss 1.4700 | lr 3.00e-04 | grad 1.38 | tok/s 28867
step   1100 | loss 1.4399 | lr 3.00e-04 | grad 1.30 | tok/s 28897
step   1110 | loss 1.3797 | lr 3.00e-04 | grad 1.21 | tok/s 28874
step   1120 | loss 1.6056 | lr 3.00e-04 | grad 1.24 | tok/s 28104
step   1130 | loss 2.0051 | lr 3.00e-04 | grad 1.31 | tok/s 28414
step   1140 | loss 2.0037 | lr 3.00e-04 | grad 1.34 | tok/s 28772
step   1150 | loss 2.1164 | lr 3.00e-04 | grad 1.78 | tok/s 28122
step   1160 | loss 1.8857 | lr 3.00e-04 | grad 2.92 | tok/s 27184
step   1170 | loss 1.7334 | lr 3.00e-04 | grad 1.34 | tok/s 27134
step   1180 | loss 1.6227 | lr 3.00e-04 | grad 1.26 | tok/s 28548
step   1190 | loss 1.9036 | lr 3.00e-04 | grad 1.62 | tok/s 28599
step   1200 | loss 1.4457 | lr 3.00e-04 | grad 1.33 | tok/s 28884
step   1210 | loss 1.5364 | lr 3.00e-04 | grad 1.18 | tok/s 27181
step   1220 | loss 1.6102 | lr 3.00e-04 | grad 1.56 | tok/s 28084
step   1230 | loss 1.6513 | lr 3.00e-04 | grad 1.05 | tok/s 28286
step   1240 | loss 1.5059 | lr 3.00e-04 | grad 1.77 | tok/s 28622
step   1250 | loss 1.7533 | lr 3.00e-04 | grad 1.63 | tok/s 27945
step   1260 | loss 1.7738 | lr 3.00e-04 | grad 1.15 | tok/s 28716
step   1270 | loss 1.5978 | lr 3.00e-04 | grad 1.66 | tok/s 27845
step   1280 | loss 1.6095 | lr 3.00e-04 | grad 1.39 | tok/s 27921
step   1290 | loss 1.6009 | lr 3.00e-04 | grad 1.45 | tok/s 27269
step   1300 | loss 1.8360 | lr 3.00e-04 | grad 3.75 | tok/s 27180
step   1310 | loss 1.8257 | lr 3.00e-04 | grad 1.16 | tok/s 28326
step   1320 | loss 1.6993 | lr 3.00e-04 | grad 2.41 | tok/s 28201
step   1330 | loss 1.7170 | lr 3.00e-04 | grad 1.36 | tok/s 28134
step   1340 | loss 1.7905 | lr 3.00e-04 | grad 1.24 | tok/s 27275
step   1350 | loss 1.6709 | lr 3.00e-04 | grad 1.23 | tok/s 28352
step   1360 | loss 1.7260 | lr 3.00e-04 | grad 1.30 | tok/s 26658
step   1370 | loss 1.8150 | lr 3.00e-04 | grad 2.47 | tok/s 28448
step   1380 | loss 1.6935 | lr 3.00e-04 | grad 1.44 | tok/s 27571
step   1390 | loss 1.6353 | lr 3.00e-04 | grad 1.48 | tok/s 28130
step   1400 | loss 1.7281 | lr 3.00e-04 | grad 1.20 | tok/s 27509
step   1410 | loss 1.5551 | lr 3.00e-04 | grad 1.51 | tok/s 27005
step   1420 | loss 1.5204 | lr 3.00e-04 | grad 1.57 | tok/s 28681
step   1430 | loss 1.9636 | lr 3.00e-04 | grad 1.12 | tok/s 27756
step   1440 | loss 1.6747 | lr 3.00e-04 | grad 1.66 | tok/s 27971
step   1450 | loss 1.6615 | lr 3.00e-04 | grad 1.20 | tok/s 28294
step   1460 | loss 1.7503 | lr 3.00e-04 | grad 1.56 | tok/s 27502
step   1470 | loss 1.5911 | lr 3.00e-04 | grad 1.42 | tok/s 27092
step   1480 | loss 1.5929 | lr 3.00e-04 | grad 1.94 | tok/s 28119
step   1490 | loss 1.8349 | lr 3.00e-04 | grad 6.16 | tok/s 27897
step   1500 | loss 1.8413 | lr 3.00e-04 | grad 1.33 | tok/s 28315
step   1510 | loss 1.4819 | lr 3.00e-04 | grad 1.10 | tok/s 27720
step   1520 | loss 1.6542 | lr 3.00e-04 | grad 1.59 | tok/s 27595
step   1530 | loss 1.5877 | lr 3.00e-04 | grad 1.41 | tok/s 28378
step   1540 | loss 1.6925 | lr 3.00e-04 | grad 1.17 | tok/s 28376
step   1550 | loss 1.6666 | lr 3.00e-04 | grad 2.52 | tok/s 27848
step   1560 | loss 1.4036 | lr 3.00e-04 | grad 1.08 | tok/s 28786
step   1570 | loss 1.5337 | lr 3.00e-04 | grad 1.13 | tok/s 28101
step   1580 | loss 1.4555 | lr 3.00e-04 | grad 1.41 | tok/s 28258
step   1590 | loss 1.6513 | lr 3.00e-04 | grad 2.08 | tok/s 27220
step   1600 | loss 1.4378 | lr 3.00e-04 | grad 4.53 | tok/s 28716
step   1610 | loss 2.0645 | lr 3.00e-04 | grad 2.45 | tok/s 27994
step   1620 | loss 2.2547 | lr 3.00e-04 | grad 1.70 | tok/s 28931
step   1630 | loss 1.9972 | lr 3.00e-04 | grad 1.49 | tok/s 28889
step   1640 | loss 1.8325 | lr 3.00e-04 | grad 1.60 | tok/s 28935
step   1650 | loss 1.7466 | lr 3.00e-04 | grad 1.48 | tok/s 28918
step   1660 | loss 1.6908 | lr 3.00e-04 | grad 1.41 | tok/s 28934
step   1670 | loss 1.7990 | lr 3.00e-04 | grad 1.80 | tok/s 28035
step   1680 | loss 1.6055 | lr 3.00e-04 | grad 1.40 | tok/s 27771
step   1690 | loss 1.6325 | lr 3.00e-04 | grad 1.55 | tok/s 27130
step   1700 | loss 1.5015 | lr 3.00e-04 | grad 1.38 | tok/s 28182
step   1710 | loss 1.4600 | lr 3.00e-04 | grad 1.62 | tok/s 28273
step   1720 | loss 1.6177 | lr 3.00e-04 | grad 1.16 | tok/s 27394
step   1730 | loss 1.6671 | lr 3.00e-04 | grad 2.34 | tok/s 28266
step   1740 | loss 1.5853 | lr 3.00e-04 | grad 1.39 | tok/s 28427
step   1750 | loss 1.4573 | lr 3.00e-04 | grad 1.27 | tok/s 27834
step   1760 | loss 1.5725 | lr 3.00e-04 | grad 1.23 | tok/s 27562
step   1770 | loss 1.8382 | lr 3.00e-04 | grad 1.27 | tok/s 28219
step   1780 | loss 1.8972 | lr 3.00e-04 | grad 1.34 | tok/s 26691
step   1790 | loss 1.4827 | lr 3.00e-04 | grad 1.22 | tok/s 27084
step   1800 | loss 1.5029 | lr 3.00e-04 | grad 1.19 | tok/s 27748
step   1810 | loss 1.5843 | lr 3.00e-04 | grad 1.35 | tok/s 27981
step   1820 | loss 1.6915 | lr 3.00e-04 | grad 1.94 | tok/s 27590
step   1830 | loss 1.5370 | lr 3.00e-04 | grad 1.11 | tok/s 26832
step   1840 | loss 1.5725 | lr 3.00e-04 | grad 1.25 | tok/s 27811
step   1850 | loss 1.6117 | lr 3.00e-04 | grad 1.45 | tok/s 27623
step   1860 | loss 1.5958 | lr 3.00e-04 | grad 1.41 | tok/s 27531
step   1870 | loss 1.6008 | lr 3.00e-04 | grad 2.95 | tok/s 27968
step   1880 | loss 1.6465 | lr 3.00e-04 | grad 1.41 | tok/s 28209
step   1890 | loss 1.4441 | lr 3.00e-04 | grad 1.28 | tok/s 28948
step   1900 | loss 1.3819 | lr 3.00e-04 | grad 1.15 | tok/s 28930
step   1910 | loss 1.3538 | lr 3.00e-04 | grad 1.11 | tok/s 28962
step   1920 | loss 1.3322 | lr 3.00e-04 | grad 1.23 | tok/s 28939
step   1930 | loss 1.3629 | lr 3.00e-04 | grad 2.09 | tok/s 28871
step   1940 | loss 1.7056 | lr 3.00e-04 | grad 1.45 | tok/s 27468
step   1950 | loss 1.5770 | lr 3.00e-04 | grad 1.05 | tok/s 27236
step   1960 | loss 1.6842 | lr 3.00e-04 | grad 1.70 | tok/s 27613
step   1970 | loss 1.7153 | lr 3.00e-04 | grad 1.36 | tok/s 28142
step   1980 | loss 1.6052 | lr 3.00e-04 | grad 1.73 | tok/s 27305
step   1990 | loss 1.7897 | lr 3.00e-04 | grad 2.28 | tok/s 28072
step   2000 | loss 1.3460 | lr 3.00e-04 | grad 1.13 | tok/s 28981
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3460.pt
step   2010 | loss 1.5336 | lr 3.00e-04 | grad 1.75 | tok/s 15420
step   2020 | loss 1.5301 | lr 3.00e-04 | grad 1.47 | tok/s 27567
step   2030 | loss 1.8826 | lr 3.00e-04 | grad 11.06 | tok/s 27221
step   2040 | loss 1.6200 | lr 3.00e-04 | grad 1.55 | tok/s 27736
step   2050 | loss 1.4921 | lr 3.00e-04 | grad 1.15 | tok/s 28112
step   2060 | loss 1.7206 | lr 3.00e-04 | grad 2.20 | tok/s 28162
step   2070 | loss 1.4374 | lr 3.00e-04 | grad 1.62 | tok/s 28145

Training complete! Final step: 2072
