Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_80/levelllama_100m_20260126_184546
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 330,751,488 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.2006 | lr 3.00e-04 | grad 3.89 | tok/s 10894
step     20 | loss 2.9816 | lr 3.00e-04 | grad 2.27 | tok/s 24231
step     30 | loss 3.2880 | lr 3.00e-04 | grad 2.28 | tok/s 25602
step     40 | loss 5.4513 | lr 3.00e-04 | grad 7.53 | tok/s 26049
step     50 | loss 4.5496 | lr 3.00e-04 | grad 4.56 | tok/s 26385
step     60 | loss 3.7185 | lr 3.00e-04 | grad 2.98 | tok/s 26291
step     70 | loss 3.2073 | lr 3.00e-04 | grad 4.53 | tok/s 26233
step     80 | loss 2.9074 | lr 3.00e-04 | grad 2.81 | tok/s 26160
step     90 | loss 2.6687 | lr 3.00e-04 | grad 2.34 | tok/s 26073
step    100 | loss 2.5178 | lr 3.00e-04 | grad 1.85 | tok/s 26008
step    110 | loss 2.6440 | lr 3.00e-04 | grad 2.88 | tok/s 25799
step    120 | loss 3.4435 | lr 3.00e-04 | grad 1.45 | tok/s 24543
step    130 | loss 2.6678 | lr 3.00e-04 | grad 2.62 | tok/s 25127
step    140 | loss 2.9595 | lr 3.00e-04 | grad 5.72 | tok/s 25202
step    150 | loss 2.7241 | lr 3.00e-04 | grad 4.41 | tok/s 25803
step    160 | loss 2.7861 | lr 3.00e-04 | grad 1.60 | tok/s 24920
step    170 | loss 2.7170 | lr 3.00e-04 | grad 1.85 | tok/s 24523
step    180 | loss 2.8784 | lr 3.00e-04 | grad 1.67 | tok/s 25102
step    190 | loss 2.4733 | lr 3.00e-04 | grad 2.11 | tok/s 24590
step    200 | loss 2.4213 | lr 3.00e-04 | grad 1.47 | tok/s 25715
step    210 | loss 2.4238 | lr 3.00e-04 | grad 3.66 | tok/s 24371
step    220 | loss 2.6692 | lr 3.00e-04 | grad 2.28 | tok/s 24635
step    230 | loss 2.5318 | lr 3.00e-04 | grad 2.72 | tok/s 24605
step    240 | loss 2.7762 | lr 3.00e-04 | grad 2.50 | tok/s 24933
step    250 | loss 2.3292 | lr 3.00e-04 | grad 2.19 | tok/s 24760
step    260 | loss 2.4597 | lr 3.00e-04 | grad 2.02 | tok/s 25455
step    270 | loss 2.3068 | lr 3.00e-04 | grad 1.75 | tok/s 24882
step    280 | loss 2.2431 | lr 3.00e-04 | grad 0.96 | tok/s 23363
step    290 | loss 2.1736 | lr 3.00e-04 | grad 2.14 | tok/s 24151
step    300 | loss 2.4363 | lr 3.00e-04 | grad 1.95 | tok/s 24290
step    310 | loss 2.1175 | lr 3.00e-04 | grad 2.02 | tok/s 24213
step    320 | loss 2.3726 | lr 3.00e-04 | grad 2.88 | tok/s 24481
step    330 | loss 2.1736 | lr 3.00e-04 | grad 1.91 | tok/s 24734
step    340 | loss 2.4726 | lr 3.00e-04 | grad 1.71 | tok/s 24614
step    350 | loss 2.4004 | lr 3.00e-04 | grad 1.44 | tok/s 25346
step    360 | loss 2.0657 | lr 3.00e-04 | grad 1.44 | tok/s 24250
step    370 | loss 2.1172 | lr 3.00e-04 | grad 1.38 | tok/s 25539
step    380 | loss 1.9037 | lr 3.00e-04 | grad 1.70 | tok/s 25709
step    390 | loss 1.8196 | lr 3.00e-04 | grad 1.80 | tok/s 25737
step    400 | loss 2.2236 | lr 3.00e-04 | grad 1.17 | tok/s 24380
step    410 | loss 2.1319 | lr 3.00e-04 | grad 1.29 | tok/s 24646
step    420 | loss 2.2660 | lr 3.00e-04 | grad 1.97 | tok/s 25688
step    430 | loss 2.0982 | lr 3.00e-04 | grad 2.12 | tok/s 25254
step    440 | loss 2.1193 | lr 3.00e-04 | grad 1.83 | tok/s 24490
step    450 | loss 2.0137 | lr 3.00e-04 | grad 1.38 | tok/s 24768
step    460 | loss 2.0744 | lr 3.00e-04 | grad 1.35 | tok/s 25108
step    470 | loss 2.0567 | lr 3.00e-04 | grad 2.03 | tok/s 24929
step    480 | loss 2.0814 | lr 3.00e-04 | grad 2.22 | tok/s 25505
step    490 | loss 2.0424 | lr 3.00e-04 | grad 1.77 | tok/s 24468
step    500 | loss 2.1836 | lr 3.00e-04 | grad 1.23 | tok/s 24914
step    510 | loss 2.0495 | lr 3.00e-04 | grad 1.32 | tok/s 23795
step    520 | loss 1.8903 | lr 3.00e-04 | grad 1.08 | tok/s 24905
step    530 | loss 2.0476 | lr 3.00e-04 | grad 1.20 | tok/s 24460
step    540 | loss 2.0036 | lr 3.00e-04 | grad 0.94 | tok/s 23931
step    550 | loss 1.6848 | lr 3.00e-04 | grad 2.48 | tok/s 25014
step    560 | loss 1.8538 | lr 3.00e-04 | grad 1.40 | tok/s 25747
step    570 | loss 1.7427 | lr 3.00e-04 | grad 1.77 | tok/s 25759
step    580 | loss 1.6686 | lr 3.00e-04 | grad 1.23 | tok/s 25745
step    590 | loss 1.7105 | lr 3.00e-04 | grad 1.26 | tok/s 25760
step    600 | loss 1.6573 | lr 3.00e-04 | grad 1.43 | tok/s 25791
step    610 | loss 1.6428 | lr 3.00e-04 | grad 1.16 | tok/s 25768
step    620 | loss 1.6234 | lr 3.00e-04 | grad 1.42 | tok/s 25651
step    630 | loss 1.9504 | lr 3.00e-04 | grad 3.34 | tok/s 24274
step    640 | loss 2.0302 | lr 3.00e-04 | grad 1.26 | tok/s 24603
step    650 | loss 1.8457 | lr 3.00e-04 | grad 1.16 | tok/s 24588
step    660 | loss 1.9082 | lr 3.00e-04 | grad 1.29 | tok/s 25529
step    670 | loss 1.9262 | lr 3.00e-04 | grad 2.80 | tok/s 24698
step    680 | loss 1.9305 | lr 3.00e-04 | grad 1.63 | tok/s 24281
step    690 | loss 1.8853 | lr 3.00e-04 | grad 1.20 | tok/s 24104
step    700 | loss 1.7890 | lr 3.00e-04 | grad 1.06 | tok/s 24629
step    710 | loss 1.9386 | lr 3.00e-04 | grad 2.30 | tok/s 24242
step    720 | loss 1.6656 | lr 3.00e-04 | grad 1.41 | tok/s 25196
step    730 | loss 1.7665 | lr 3.00e-04 | grad 1.00 | tok/s 24790
step    740 | loss 2.1871 | lr 3.00e-04 | grad 2.52 | tok/s 25473
step    750 | loss 1.9591 | lr 3.00e-04 | grad 1.55 | tok/s 25765
step    760 | loss 1.8122 | lr 3.00e-04 | grad 2.22 | tok/s 25216
step    770 | loss 1.8211 | lr 3.00e-04 | grad 1.38 | tok/s 24797
step    780 | loss 1.7614 | lr 3.00e-04 | grad 1.42 | tok/s 24950
step    790 | loss 2.0808 | lr 3.00e-04 | grad 3.12 | tok/s 25518
step    800 | loss 1.6410 | lr 3.00e-04 | grad 1.01 | tok/s 25084
step    810 | loss 1.5929 | lr 3.00e-04 | grad 1.95 | tok/s 24220
step    820 | loss 1.7242 | lr 3.00e-04 | grad 1.30 | tok/s 24701
step    830 | loss 1.7778 | lr 3.00e-04 | grad 1.26 | tok/s 24361
step    840 | loss 1.9086 | lr 3.00e-04 | grad 1.29 | tok/s 24249
step    850 | loss 1.8483 | lr 3.00e-04 | grad 1.20 | tok/s 24777
step    860 | loss 1.9029 | lr 3.00e-04 | grad 1.75 | tok/s 25195
step    870 | loss 1.8920 | lr 3.00e-04 | grad 1.30 | tok/s 25390
step    880 | loss 1.8161 | lr 3.00e-04 | grad 1.17 | tok/s 24886
step    890 | loss 1.7045 | lr 3.00e-04 | grad 1.19 | tok/s 24780
step    900 | loss 1.7598 | lr 3.00e-04 | grad 1.04 | tok/s 24678
step    910 | loss 1.8062 | lr 3.00e-04 | grad 3.55 | tok/s 24402
step    920 | loss 1.7061 | lr 3.00e-04 | grad 1.27 | tok/s 24686
step    930 | loss 1.6689 | lr 3.00e-04 | grad 1.33 | tok/s 25005
step    940 | loss 1.6237 | lr 3.00e-04 | grad 1.15 | tok/s 24423
step    950 | loss 1.7228 | lr 3.00e-04 | grad 1.64 | tok/s 24018
step    960 | loss 1.6673 | lr 3.00e-04 | grad 1.21 | tok/s 24682
step    970 | loss 1.6689 | lr 3.00e-04 | grad 1.20 | tok/s 24698
step    980 | loss 2.3571 | lr 3.00e-04 | grad 2.16 | tok/s 25695
step    990 | loss 1.8563 | lr 3.00e-04 | grad 1.52 | tok/s 24624
step   1000 | loss 1.7849 | lr 3.00e-04 | grad 1.54 | tok/s 24702
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7849.pt
step   1010 | loss 1.5217 | lr 3.00e-04 | grad 1.52 | tok/s 14183
step   1020 | loss 1.5796 | lr 3.00e-04 | grad 1.27 | tok/s 26028
step   1030 | loss 1.7447 | lr 3.00e-04 | grad 1.36 | tok/s 24709
step   1040 | loss 2.2858 | lr 3.00e-04 | grad 2.83 | tok/s 25278
step   1050 | loss 1.8388 | lr 3.00e-04 | grad 1.23 | tok/s 25432
step   1060 | loss 1.5014 | lr 3.00e-04 | grad 0.86 | tok/s 25093
step   1070 | loss 1.5924 | lr 3.00e-04 | grad 1.62 | tok/s 25050
step   1080 | loss 1.4848 | lr 3.00e-04 | grad 1.38 | tok/s 25890
step   1090 | loss 1.4624 | lr 3.00e-04 | grad 1.29 | tok/s 25831
step   1100 | loss 1.4348 | lr 3.00e-04 | grad 1.29 | tok/s 25864
step   1110 | loss 1.3719 | lr 3.00e-04 | grad 1.19 | tok/s 25892
step   1120 | loss 1.5969 | lr 3.00e-04 | grad 1.28 | tok/s 25158
step   1130 | loss 1.9737 | lr 3.00e-04 | grad 1.27 | tok/s 25430
step   1140 | loss 1.9897 | lr 3.00e-04 | grad 1.29 | tok/s 25736
step   1150 | loss 2.0869 | lr 3.00e-04 | grad 1.62 | tok/s 25147
step   1160 | loss 1.8755 | lr 3.00e-04 | grad 2.83 | tok/s 24335
step   1170 | loss 1.7313 | lr 3.00e-04 | grad 1.45 | tok/s 24309
step   1180 | loss 1.6144 | lr 3.00e-04 | grad 1.23 | tok/s 25528
step   1190 | loss 1.9145 | lr 3.00e-04 | grad 1.70 | tok/s 25614
step   1200 | loss 1.4398 | lr 3.00e-04 | grad 1.20 | tok/s 25849
step   1210 | loss 1.5308 | lr 3.00e-04 | grad 1.21 | tok/s 24315
step   1220 | loss 1.6011 | lr 3.00e-04 | grad 1.48 | tok/s 25083
step   1230 | loss 1.6416 | lr 3.00e-04 | grad 1.02 | tok/s 25314
step   1240 | loss 1.4939 | lr 3.00e-04 | grad 1.80 | tok/s 25592
step   1250 | loss 1.7440 | lr 3.00e-04 | grad 1.56 | tok/s 24971
step   1260 | loss 1.7520 | lr 3.00e-04 | grad 1.11 | tok/s 25633
step   1270 | loss 1.5880 | lr 3.00e-04 | grad 1.58 | tok/s 24862
step   1280 | loss 1.5894 | lr 3.00e-04 | grad 1.43 | tok/s 24917
step   1290 | loss 1.5921 | lr 3.00e-04 | grad 1.38 | tok/s 24366
step   1300 | loss 1.8260 | lr 3.00e-04 | grad 3.66 | tok/s 24267
step   1310 | loss 1.7996 | lr 3.00e-04 | grad 1.23 | tok/s 25284
step   1320 | loss 1.6896 | lr 3.00e-04 | grad 2.47 | tok/s 25219
step   1330 | loss 1.7106 | lr 3.00e-04 | grad 1.24 | tok/s 25151
step   1340 | loss 1.7812 | lr 3.00e-04 | grad 1.23 | tok/s 24367
step   1350 | loss 1.6595 | lr 3.00e-04 | grad 1.12 | tok/s 25384
step   1360 | loss 1.7091 | lr 3.00e-04 | grad 1.31 | tok/s 23810
step   1370 | loss 1.8023 | lr 3.00e-04 | grad 2.31 | tok/s 25440
step   1380 | loss 1.6844 | lr 3.00e-04 | grad 1.48 | tok/s 24611
step   1390 | loss 1.6217 | lr 3.00e-04 | grad 1.46 | tok/s 25100
step   1400 | loss 1.7364 | lr 3.00e-04 | grad 1.16 | tok/s 24541
step   1410 | loss 1.5480 | lr 3.00e-04 | grad 1.48 | tok/s 24110
step   1420 | loss 1.4957 | lr 3.00e-04 | grad 1.48 | tok/s 25605
step   1430 | loss 1.9630 | lr 3.00e-04 | grad 1.04 | tok/s 24764
step   1440 | loss 1.6689 | lr 3.00e-04 | grad 1.59 | tok/s 24973
step   1450 | loss 1.6488 | lr 3.00e-04 | grad 1.28 | tok/s 25253
step   1460 | loss 1.7429 | lr 3.00e-04 | grad 1.48 | tok/s 24504
step   1470 | loss 1.5879 | lr 3.00e-04 | grad 1.32 | tok/s 24161
step   1480 | loss 1.5851 | lr 3.00e-04 | grad 1.73 | tok/s 25067
step   1490 | loss 1.8230 | lr 3.00e-04 | grad 5.81 | tok/s 24867
step   1500 | loss 1.8285 | lr 3.00e-04 | grad 1.40 | tok/s 25248
step   1510 | loss 1.4831 | lr 3.00e-04 | grad 1.09 | tok/s 24740
step   1520 | loss 1.6478 | lr 3.00e-04 | grad 1.55 | tok/s 24648
step   1530 | loss 1.5820 | lr 3.00e-04 | grad 1.41 | tok/s 25317
step   1540 | loss 1.6758 | lr 3.00e-04 | grad 1.10 | tok/s 25340
step   1550 | loss 1.6596 | lr 3.00e-04 | grad 2.48 | tok/s 24864
step   1560 | loss 1.4030 | lr 3.00e-04 | grad 1.29 | tok/s 25678
step   1570 | loss 1.5224 | lr 3.00e-04 | grad 1.02 | tok/s 25101
step   1580 | loss 1.4452 | lr 3.00e-04 | grad 1.43 | tok/s 25247
step   1590 | loss 1.6470 | lr 3.00e-04 | grad 2.14 | tok/s 24269
step   1600 | loss 1.4332 | lr 3.00e-04 | grad 4.16 | tok/s 25647
step   1610 | loss 2.0566 | lr 3.00e-04 | grad 2.50 | tok/s 24988
step   1620 | loss 2.2432 | lr 3.00e-04 | grad 1.70 | tok/s 25832
step   1630 | loss 1.9915 | lr 3.00e-04 | grad 1.62 | tok/s 25826
step   1640 | loss 1.8344 | lr 3.00e-04 | grad 1.64 | tok/s 25830
step   1650 | loss 1.7469 | lr 3.00e-04 | grad 1.68 | tok/s 25830
step   1660 | loss 1.6910 | lr 3.00e-04 | grad 1.56 | tok/s 25830
step   1670 | loss 1.7697 | lr 3.00e-04 | grad 1.49 | tok/s 25028
step   1680 | loss 1.5883 | lr 3.00e-04 | grad 1.28 | tok/s 24807
step   1690 | loss 1.6213 | lr 3.00e-04 | grad 1.48 | tok/s 24251
step   1700 | loss 1.4900 | lr 3.00e-04 | grad 1.32 | tok/s 25139
step   1710 | loss 1.4464 | lr 3.00e-04 | grad 1.67 | tok/s 25260
step   1720 | loss 1.6074 | lr 3.00e-04 | grad 1.12 | tok/s 24456
step   1730 | loss 1.6542 | lr 3.00e-04 | grad 2.39 | tok/s 25251
step   1740 | loss 1.5810 | lr 3.00e-04 | grad 1.47 | tok/s 25368
step   1750 | loss 1.4504 | lr 3.00e-04 | grad 1.21 | tok/s 24848
step   1760 | loss 1.5724 | lr 3.00e-04 | grad 1.28 | tok/s 24585
step   1770 | loss 1.8198 | lr 3.00e-04 | grad 1.23 | tok/s 25175
step   1780 | loss 1.8961 | lr 3.00e-04 | grad 1.32 | tok/s 23754
step   1790 | loss 1.4747 | lr 3.00e-04 | grad 1.19 | tok/s 24132
step   1800 | loss 1.4836 | lr 3.00e-04 | grad 1.20 | tok/s 24735
step   1810 | loss 1.5779 | lr 3.00e-04 | grad 1.32 | tok/s 24953
step   1820 | loss 1.6798 | lr 3.00e-04 | grad 1.87 | tok/s 24612
step   1830 | loss 1.5318 | lr 3.00e-04 | grad 1.15 | tok/s 23919
step   1840 | loss 1.5648 | lr 3.00e-04 | grad 1.20 | tok/s 24780
step   1850 | loss 1.6049 | lr 3.00e-04 | grad 1.40 | tok/s 24608

Training complete! Final step: 1858
