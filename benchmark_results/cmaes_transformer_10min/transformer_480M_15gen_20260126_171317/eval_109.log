Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_109/levelllama_100m_20260126_192704
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 292,334,976 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.5046 | lr 3.00e-04 | grad 2.22 | tok/s 11531
step     20 | loss 2.9250 | lr 3.00e-04 | grad 1.20 | tok/s 27205
step     30 | loss 3.2623 | lr 3.00e-04 | grad 1.55 | tok/s 28754
step     40 | loss 5.4838 | lr 3.00e-04 | grad 6.69 | tok/s 29253
step     50 | loss 4.6394 | lr 3.00e-04 | grad 3.81 | tok/s 29617
step     60 | loss 3.7534 | lr 3.00e-04 | grad 2.41 | tok/s 29483
step     70 | loss 3.2619 | lr 3.00e-04 | grad 3.33 | tok/s 29410
step     80 | loss 3.0168 | lr 3.00e-04 | grad 1.50 | tok/s 29265
step     90 | loss 2.7788 | lr 3.00e-04 | grad 1.62 | tok/s 29176
step    100 | loss 2.5616 | lr 3.00e-04 | grad 1.84 | tok/s 29123
step    110 | loss 2.6300 | lr 3.00e-04 | grad 1.66 | tok/s 28863
step    120 | loss 3.4481 | lr 3.00e-04 | grad 0.92 | tok/s 27453
step    130 | loss 2.5917 | lr 3.00e-04 | grad 2.16 | tok/s 28097
step    140 | loss 2.8953 | lr 3.00e-04 | grad 4.88 | tok/s 28158
step    150 | loss 2.7824 | lr 3.00e-04 | grad 4.59 | tok/s 28784
step    160 | loss 2.8394 | lr 3.00e-04 | grad 1.41 | tok/s 27856
step    170 | loss 2.6970 | lr 3.00e-04 | grad 0.73 | tok/s 27360
step    180 | loss 2.8851 | lr 3.00e-04 | grad 1.49 | tok/s 28027
step    190 | loss 2.4351 | lr 3.00e-04 | grad 1.13 | tok/s 27473
step    200 | loss 2.3696 | lr 3.00e-04 | grad 0.88 | tok/s 28683
step    210 | loss 2.3894 | lr 3.00e-04 | grad 3.05 | tok/s 27230
step    220 | loss 2.6439 | lr 3.00e-04 | grad 2.50 | tok/s 27487
step    230 | loss 2.4294 | lr 3.00e-04 | grad 1.73 | tok/s 27428
step    240 | loss 2.7512 | lr 3.00e-04 | grad 2.05 | tok/s 27768
step    250 | loss 2.2821 | lr 3.00e-04 | grad 1.05 | tok/s 27573
step    260 | loss 2.4239 | lr 3.00e-04 | grad 1.61 | tok/s 28365
step    270 | loss 2.2832 | lr 3.00e-04 | grad 1.76 | tok/s 27736
step    280 | loss 2.2198 | lr 3.00e-04 | grad 0.75 | tok/s 26031
step    290 | loss 2.1519 | lr 3.00e-04 | grad 1.26 | tok/s 26843
step    300 | loss 2.4193 | lr 3.00e-04 | grad 1.09 | tok/s 27126
step    310 | loss 2.1052 | lr 3.00e-04 | grad 0.88 | tok/s 26995
step    320 | loss 2.3667 | lr 3.00e-04 | grad 2.75 | tok/s 27262
step    330 | loss 2.1681 | lr 3.00e-04 | grad 1.12 | tok/s 27497
step    340 | loss 2.4651 | lr 3.00e-04 | grad 1.55 | tok/s 27413
step    350 | loss 2.4343 | lr 3.00e-04 | grad 1.16 | tok/s 28191
step    360 | loss 2.0783 | lr 3.00e-04 | grad 1.12 | tok/s 26995
step    370 | loss 2.1195 | lr 3.00e-04 | grad 1.12 | tok/s 28427
step    380 | loss 1.9404 | lr 3.00e-04 | grad 1.10 | tok/s 28636
step    390 | loss 1.8706 | lr 3.00e-04 | grad 1.22 | tok/s 28604
step    400 | loss 2.2315 | lr 3.00e-04 | grad 1.00 | tok/s 27088
step    410 | loss 2.1389 | lr 3.00e-04 | grad 1.10 | tok/s 27340
step    420 | loss 2.3058 | lr 3.00e-04 | grad 1.45 | tok/s 28532
step    430 | loss 2.1278 | lr 3.00e-04 | grad 0.90 | tok/s 28080
step    440 | loss 2.1383 | lr 3.00e-04 | grad 1.27 | tok/s 27203
step    450 | loss 2.0290 | lr 3.00e-04 | grad 0.90 | tok/s 27487
step    460 | loss 2.1111 | lr 3.00e-04 | grad 0.93 | tok/s 27881
step    470 | loss 2.1008 | lr 3.00e-04 | grad 1.91 | tok/s 27705
step    480 | loss 2.1332 | lr 3.00e-04 | grad 1.51 | tok/s 28320
step    490 | loss 2.0647 | lr 3.00e-04 | grad 1.52 | tok/s 27148
step    500 | loss 2.2153 | lr 3.00e-04 | grad 1.00 | tok/s 27598
step    510 | loss 2.0671 | lr 3.00e-04 | grad 0.95 | tok/s 26341
step    520 | loss 1.9249 | lr 3.00e-04 | grad 1.02 | tok/s 27572
step    530 | loss 2.0798 | lr 3.00e-04 | grad 1.12 | tok/s 27094
step    540 | loss 2.0514 | lr 3.00e-04 | grad 0.94 | tok/s 26558
step    550 | loss 1.7376 | lr 3.00e-04 | grad 1.89 | tok/s 27766
step    560 | loss 1.9106 | lr 3.00e-04 | grad 1.45 | tok/s 28556
step    570 | loss 1.8007 | lr 3.00e-04 | grad 1.45 | tok/s 28516
step    580 | loss 1.7292 | lr 3.00e-04 | grad 0.68 | tok/s 28532
step    590 | loss 1.7830 | lr 3.00e-04 | grad 1.23 | tok/s 28563
step    600 | loss 1.7438 | lr 3.00e-04 | grad 1.05 | tok/s 28591
step    610 | loss 1.7050 | lr 3.00e-04 | grad 0.98 | tok/s 28567
step    620 | loss 1.6945 | lr 3.00e-04 | grad 1.16 | tok/s 28451
step    630 | loss 1.9302 | lr 3.00e-04 | grad 2.44 | tok/s 26899
step    640 | loss 2.0880 | lr 3.00e-04 | grad 1.50 | tok/s 27256
step    650 | loss 1.8978 | lr 3.00e-04 | grad 0.93 | tok/s 27238
step    660 | loss 1.9641 | lr 3.00e-04 | grad 1.20 | tok/s 28284
step    670 | loss 1.9924 | lr 3.00e-04 | grad 2.55 | tok/s 27316
step    680 | loss 1.9768 | lr 3.00e-04 | grad 1.40 | tok/s 26904
step    690 | loss 1.9412 | lr 3.00e-04 | grad 1.02 | tok/s 26688
step    700 | loss 1.8504 | lr 3.00e-04 | grad 1.03 | tok/s 27271
step    710 | loss 1.9930 | lr 3.00e-04 | grad 2.23 | tok/s 26855
step    720 | loss 1.7410 | lr 3.00e-04 | grad 1.67 | tok/s 27895
step    730 | loss 1.8342 | lr 3.00e-04 | grad 0.83 | tok/s 27419
step    740 | loss 2.2656 | lr 3.00e-04 | grad 1.94 | tok/s 28187
step    750 | loss 2.0576 | lr 3.00e-04 | grad 1.36 | tok/s 28533
step    760 | loss 1.8675 | lr 3.00e-04 | grad 1.97 | tok/s 27898
step    770 | loss 1.8747 | lr 3.00e-04 | grad 1.09 | tok/s 27433
step    780 | loss 1.8298 | lr 3.00e-04 | grad 1.24 | tok/s 27631
step    790 | loss 2.1381 | lr 3.00e-04 | grad 2.03 | tok/s 28255
step    800 | loss 1.7067 | lr 3.00e-04 | grad 1.04 | tok/s 27765
step    810 | loss 1.6478 | lr 3.00e-04 | grad 1.85 | tok/s 26835
step    820 | loss 1.7974 | lr 3.00e-04 | grad 1.18 | tok/s 27350
step    830 | loss 1.8407 | lr 3.00e-04 | grad 1.03 | tok/s 26960
step    840 | loss 1.9553 | lr 3.00e-04 | grad 1.11 | tok/s 26840
step    850 | loss 1.9177 | lr 3.00e-04 | grad 1.06 | tok/s 27435
step    860 | loss 1.9666 | lr 3.00e-04 | grad 1.34 | tok/s 27885
step    870 | loss 2.0045 | lr 3.00e-04 | grad 1.20 | tok/s 28115
step    880 | loss 1.8732 | lr 3.00e-04 | grad 1.02 | tok/s 27582
step    890 | loss 1.7618 | lr 3.00e-04 | grad 0.93 | tok/s 27433
step    900 | loss 1.8209 | lr 3.00e-04 | grad 0.99 | tok/s 27330
step    910 | loss 1.8767 | lr 3.00e-04 | grad 3.52 | tok/s 27032
step    920 | loss 1.7698 | lr 3.00e-04 | grad 1.09 | tok/s 27322
step    930 | loss 1.7380 | lr 3.00e-04 | grad 1.00 | tok/s 27664
step    940 | loss 1.6979 | lr 3.00e-04 | grad 1.05 | tok/s 27021
step    950 | loss 1.7886 | lr 3.00e-04 | grad 1.45 | tok/s 26587
step    960 | loss 1.7329 | lr 3.00e-04 | grad 1.12 | tok/s 27304
step    970 | loss 1.7269 | lr 3.00e-04 | grad 1.03 | tok/s 27323
step    980 | loss 2.4638 | lr 3.00e-04 | grad 2.06 | tok/s 28447
step    990 | loss 1.9309 | lr 3.00e-04 | grad 1.15 | tok/s 27251
step   1000 | loss 1.8370 | lr 3.00e-04 | grad 1.22 | tok/s 27336
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8370.pt
step   1010 | loss 1.5856 | lr 3.00e-04 | grad 1.41 | tok/s 14432
step   1020 | loss 1.6726 | lr 3.00e-04 | grad 1.20 | tok/s 28833
step   1030 | loss 1.8307 | lr 3.00e-04 | grad 1.06 | tok/s 27391
step   1040 | loss 2.3166 | lr 3.00e-04 | grad 2.25 | tok/s 28020
step   1050 | loss 1.9275 | lr 3.00e-04 | grad 1.06 | tok/s 28190
step   1060 | loss 1.5666 | lr 3.00e-04 | grad 0.84 | tok/s 27825
step   1070 | loss 1.6651 | lr 3.00e-04 | grad 1.12 | tok/s 27780
step   1080 | loss 1.5549 | lr 3.00e-04 | grad 1.22 | tok/s 28669
step   1090 | loss 1.5382 | lr 3.00e-04 | grad 1.14 | tok/s 28632
step   1100 | loss 1.5075 | lr 3.00e-04 | grad 1.23 | tok/s 28676
step   1110 | loss 1.4483 | lr 3.00e-04 | grad 1.11 | tok/s 28650
step   1120 | loss 1.6686 | lr 3.00e-04 | grad 1.02 | tok/s 27895
step   1130 | loss 2.0914 | lr 3.00e-04 | grad 1.22 | tok/s 28154
step   1140 | loss 2.0668 | lr 3.00e-04 | grad 1.15 | tok/s 28437
step   1150 | loss 2.2213 | lr 3.00e-04 | grad 1.81 | tok/s 27855
step   1160 | loss 1.9485 | lr 3.00e-04 | grad 2.84 | tok/s 26947
step   1170 | loss 1.7858 | lr 3.00e-04 | grad 1.26 | tok/s 26883
step   1180 | loss 1.6987 | lr 3.00e-04 | grad 1.17 | tok/s 28251
step   1190 | loss 2.0067 | lr 3.00e-04 | grad 1.48 | tok/s 28382
step   1200 | loss 1.5293 | lr 3.00e-04 | grad 1.23 | tok/s 28655
step   1210 | loss 1.5899 | lr 3.00e-04 | grad 1.01 | tok/s 26974
step   1220 | loss 1.6708 | lr 3.00e-04 | grad 1.34 | tok/s 27878
step   1230 | loss 1.7096 | lr 3.00e-04 | grad 1.01 | tok/s 28029
step   1240 | loss 1.5982 | lr 3.00e-04 | grad 1.94 | tok/s 28355
step   1250 | loss 1.8249 | lr 3.00e-04 | grad 1.43 | tok/s 27649
step   1260 | loss 1.9010 | lr 3.00e-04 | grad 1.11 | tok/s 28412
step   1270 | loss 1.6630 | lr 3.00e-04 | grad 1.59 | tok/s 27501
step   1280 | loss 1.6675 | lr 3.00e-04 | grad 1.38 | tok/s 27602
step   1290 | loss 1.6735 | lr 3.00e-04 | grad 1.32 | tok/s 26994
step   1300 | loss 1.9062 | lr 3.00e-04 | grad 3.39 | tok/s 26906
step   1310 | loss 1.9143 | lr 3.00e-04 | grad 1.06 | tok/s 28046
step   1320 | loss 1.7623 | lr 3.00e-04 | grad 2.06 | tok/s 27954
step   1330 | loss 1.7919 | lr 3.00e-04 | grad 1.27 | tok/s 27876
step   1340 | loss 1.8520 | lr 3.00e-04 | grad 1.12 | tok/s 26994
step   1350 | loss 1.7375 | lr 3.00e-04 | grad 1.13 | tok/s 28090
step   1360 | loss 1.7908 | lr 3.00e-04 | grad 1.32 | tok/s 26406
step   1370 | loss 1.9210 | lr 3.00e-04 | grad 2.41 | tok/s 28155
step   1380 | loss 1.7608 | lr 3.00e-04 | grad 1.35 | tok/s 27279
step   1390 | loss 1.7432 | lr 3.00e-04 | grad 1.39 | tok/s 27798
step   1400 | loss 1.7654 | lr 3.00e-04 | grad 1.15 | tok/s 27215
step   1410 | loss 1.6140 | lr 3.00e-04 | grad 1.43 | tok/s 26716
step   1420 | loss 1.6269 | lr 3.00e-04 | grad 1.34 | tok/s 28353
step   1430 | loss 2.0410 | lr 3.00e-04 | grad 1.00 | tok/s 27457
step   1440 | loss 1.7308 | lr 3.00e-04 | grad 1.57 | tok/s 27623
step   1450 | loss 1.7345 | lr 3.00e-04 | grad 1.05 | tok/s 27928
step   1460 | loss 1.8117 | lr 3.00e-04 | grad 1.41 | tok/s 27106
step   1470 | loss 1.6537 | lr 3.00e-04 | grad 1.41 | tok/s 26756
step   1480 | loss 1.6735 | lr 3.00e-04 | grad 1.91 | tok/s 27777
step   1490 | loss 1.8702 | lr 3.00e-04 | grad 2.73 | tok/s 27563
step   1500 | loss 1.8830 | lr 3.00e-04 | grad 1.16 | tok/s 27984
step   1510 | loss 1.5522 | lr 3.00e-04 | grad 1.03 | tok/s 27392
step   1520 | loss 1.7113 | lr 3.00e-04 | grad 1.46 | tok/s 27296
step   1530 | loss 1.6518 | lr 3.00e-04 | grad 1.27 | tok/s 28016
step   1540 | loss 1.7512 | lr 3.00e-04 | grad 0.91 | tok/s 28055
step   1550 | loss 1.7240 | lr 3.00e-04 | grad 2.34 | tok/s 27514
step   1560 | loss 1.5310 | lr 3.00e-04 | grad 1.17 | tok/s 28419
step   1570 | loss 1.6261 | lr 3.00e-04 | grad 1.07 | tok/s 27777
step   1580 | loss 1.5283 | lr 3.00e-04 | grad 1.45 | tok/s 27943
step   1590 | loss 1.7054 | lr 3.00e-04 | grad 2.03 | tok/s 26884
step   1600 | loss 1.5072 | lr 3.00e-04 | grad 4.41 | tok/s 28353
step   1610 | loss 2.1314 | lr 3.00e-04 | grad 2.52 | tok/s 27661
step   1620 | loss 2.3208 | lr 3.00e-04 | grad 1.54 | tok/s 28604
step   1630 | loss 2.0690 | lr 3.00e-04 | grad 1.45 | tok/s 28525
step   1640 | loss 1.9119 | lr 3.00e-04 | grad 1.59 | tok/s 28487
step   1650 | loss 1.8260 | lr 3.00e-04 | grad 1.46 | tok/s 28534
step   1660 | loss 1.7795 | lr 3.00e-04 | grad 1.28 | tok/s 28505
step   1670 | loss 1.8786 | lr 3.00e-04 | grad 1.59 | tok/s 27641
step   1680 | loss 1.6661 | lr 3.00e-04 | grad 1.16 | tok/s 27377
step   1690 | loss 1.6875 | lr 3.00e-04 | grad 1.41 | tok/s 26753
step   1700 | loss 1.5619 | lr 3.00e-04 | grad 1.23 | tok/s 27755
step   1710 | loss 1.5313 | lr 3.00e-04 | grad 1.47 | tok/s 27831
step   1720 | loss 1.6655 | lr 3.00e-04 | grad 1.09 | tok/s 26986
step   1730 | loss 1.7244 | lr 3.00e-04 | grad 2.34 | tok/s 27859
step   1740 | loss 1.6542 | lr 3.00e-04 | grad 1.28 | tok/s 28040
step   1750 | loss 1.5365 | lr 3.00e-04 | grad 1.24 | tok/s 27418
step   1760 | loss 1.6275 | lr 3.00e-04 | grad 1.25 | tok/s 27176
step   1770 | loss 1.9276 | lr 3.00e-04 | grad 1.30 | tok/s 27845
step   1780 | loss 1.9406 | lr 3.00e-04 | grad 1.28 | tok/s 26339
step   1790 | loss 1.5366 | lr 3.00e-04 | grad 1.12 | tok/s 26748
step   1800 | loss 1.5798 | lr 3.00e-04 | grad 1.16 | tok/s 27399
step   1810 | loss 1.6328 | lr 3.00e-04 | grad 1.34 | tok/s 27596
step   1820 | loss 1.7429 | lr 3.00e-04 | grad 1.70 | tok/s 27201
step   1830 | loss 1.5886 | lr 3.00e-04 | grad 1.02 | tok/s 26439
step   1840 | loss 1.6598 | lr 3.00e-04 | grad 1.11 | tok/s 27414
step   1850 | loss 1.6804 | lr 3.00e-04 | grad 1.34 | tok/s 27266
step   1860 | loss 1.6409 | lr 3.00e-04 | grad 1.34 | tok/s 27168
step   1870 | loss 1.6449 | lr 3.00e-04 | grad 2.23 | tok/s 27629
step   1880 | loss 1.6980 | lr 3.00e-04 | grad 1.13 | tok/s 27809
step   1890 | loss 1.5036 | lr 3.00e-04 | grad 1.10 | tok/s 28582
step   1900 | loss 1.4362 | lr 3.00e-04 | grad 0.98 | tok/s 28553
step   1910 | loss 1.4114 | lr 3.00e-04 | grad 0.94 | tok/s 28499
step   1920 | loss 1.3928 | lr 3.00e-04 | grad 1.26 | tok/s 28536
step   1930 | loss 1.4253 | lr 3.00e-04 | grad 2.00 | tok/s 28446
step   1940 | loss 1.7568 | lr 3.00e-04 | grad 1.51 | tok/s 27039
step   1950 | loss 1.6413 | lr 3.00e-04 | grad 0.93 | tok/s 26873
step   1960 | loss 1.7592 | lr 3.00e-04 | grad 1.55 | tok/s 27168
step   1970 | loss 1.7809 | lr 3.00e-04 | grad 1.23 | tok/s 27718
step   1980 | loss 1.6680 | lr 3.00e-04 | grad 1.40 | tok/s 26877
step   1990 | loss 1.8757 | lr 3.00e-04 | grad 1.99 | tok/s 27613
step   2000 | loss 1.4022 | lr 3.00e-04 | grad 1.16 | tok/s 28479
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4022.pt
step   2010 | loss 1.6145 | lr 3.00e-04 | grad 1.76 | tok/s 15031
step   2020 | loss 1.5934 | lr 3.00e-04 | grad 1.29 | tok/s 27087
step   2030 | loss 1.9516 | lr 3.00e-04 | grad 8.00 | tok/s 26783
step   2040 | loss 1.6714 | lr 3.00e-04 | grad 1.37 | tok/s 27270
step   2050 | loss 1.5410 | lr 3.00e-04 | grad 1.11 | tok/s 27676

Training complete! Final step: 2052
