Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_31/levelllama_100m_20260126_174411
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 301,799,424 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.0805 | lr 3.00e-04 | grad 1.45 | tok/s 10330
step     20 | loss 2.9426 | lr 3.00e-04 | grad 1.04 | tok/s 20712
step     30 | loss 3.2837 | lr 3.00e-04 | grad 1.29 | tok/s 21927
step     40 | loss 4.9020 | lr 3.00e-04 | grad 5.62 | tok/s 22362
step     50 | loss 4.2638 | lr 3.00e-04 | grad 3.73 | tok/s 22677
step     60 | loss 4.0299 | lr 3.00e-04 | grad 3.11 | tok/s 22642
step     70 | loss 3.7172 | lr 3.00e-04 | grad 2.92 | tok/s 22629
step     80 | loss 3.6653 | lr 3.00e-04 | grad 2.48 | tok/s 22592
step     90 | loss 3.3966 | lr 3.00e-04 | grad 1.82 | tok/s 22580
step    100 | loss 3.2207 | lr 3.00e-04 | grad 2.20 | tok/s 22552
step    110 | loss 2.9787 | lr 3.00e-04 | grad 2.72 | tok/s 22342
step    120 | loss 3.1373 | lr 3.00e-04 | grad 0.84 | tok/s 21244
step    130 | loss 2.5976 | lr 3.00e-04 | grad 2.12 | tok/s 21707
step    140 | loss 2.8306 | lr 3.00e-04 | grad 3.83 | tok/s 21773
step    150 | loss 2.7390 | lr 3.00e-04 | grad 4.03 | tok/s 22259
step    160 | loss 2.8364 | lr 3.00e-04 | grad 1.51 | tok/s 21515
step    170 | loss 2.6711 | lr 3.00e-04 | grad 0.75 | tok/s 21166
step    180 | loss 2.8945 | lr 3.00e-04 | grad 1.26 | tok/s 21647
step    190 | loss 2.4208 | lr 3.00e-04 | grad 1.07 | tok/s 21236
step    200 | loss 2.3711 | lr 3.00e-04 | grad 0.93 | tok/s 22185
step    210 | loss 2.3815 | lr 3.00e-04 | grad 2.47 | tok/s 21036
step    220 | loss 2.7107 | lr 3.00e-04 | grad 4.31 | tok/s 21249
step    230 | loss 2.5606 | lr 3.00e-04 | grad 1.38 | tok/s 21205
step    240 | loss 2.7342 | lr 3.00e-04 | grad 1.87 | tok/s 21465
step    250 | loss 2.2810 | lr 3.00e-04 | grad 0.95 | tok/s 21305
step    260 | loss 2.4206 | lr 3.00e-04 | grad 1.44 | tok/s 21877
step    270 | loss 2.2842 | lr 3.00e-04 | grad 2.33 | tok/s 21326
step    280 | loss 2.2159 | lr 3.00e-04 | grad 0.70 | tok/s 20017
step    290 | loss 2.1503 | lr 3.00e-04 | grad 1.28 | tok/s 20725
step    300 | loss 2.4046 | lr 3.00e-04 | grad 0.78 | tok/s 20864
step    310 | loss 2.1030 | lr 3.00e-04 | grad 0.73 | tok/s 20748
step    320 | loss 2.3626 | lr 3.00e-04 | grad 2.62 | tok/s 20974
step    330 | loss 2.1664 | lr 3.00e-04 | grad 1.05 | tok/s 21163
step    340 | loss 2.4607 | lr 3.00e-04 | grad 1.43 | tok/s 21062
step    350 | loss 2.4626 | lr 3.00e-04 | grad 0.95 | tok/s 21668
step    360 | loss 2.0822 | lr 3.00e-04 | grad 1.12 | tok/s 20724
step    370 | loss 2.1426 | lr 3.00e-04 | grad 0.96 | tok/s 21825
step    380 | loss 1.9577 | lr 3.00e-04 | grad 1.20 | tok/s 21977
step    390 | loss 1.9018 | lr 3.00e-04 | grad 0.84 | tok/s 21968
step    400 | loss 2.2338 | lr 3.00e-04 | grad 0.97 | tok/s 20851
step    410 | loss 2.1455 | lr 3.00e-04 | grad 0.98 | tok/s 21031
step    420 | loss 2.3232 | lr 3.00e-04 | grad 1.27 | tok/s 21934
step    430 | loss 2.1637 | lr 3.00e-04 | grad 0.86 | tok/s 21560
step    440 | loss 2.1424 | lr 3.00e-04 | grad 1.23 | tok/s 20880
step    450 | loss 2.0405 | lr 3.00e-04 | grad 0.72 | tok/s 21090
step    460 | loss 2.1259 | lr 3.00e-04 | grad 0.84 | tok/s 21380
step    470 | loss 2.1226 | lr 3.00e-04 | grad 1.47 | tok/s 21221
step    480 | loss 2.1537 | lr 3.00e-04 | grad 1.41 | tok/s 21673
step    490 | loss 2.0795 | lr 3.00e-04 | grad 1.39 | tok/s 20818
step    500 | loss 2.2264 | lr 3.00e-04 | grad 0.86 | tok/s 21167
step    510 | loss 2.0827 | lr 3.00e-04 | grad 0.85 | tok/s 20205
step    520 | loss 1.9457 | lr 3.00e-04 | grad 0.98 | tok/s 21152
step    530 | loss 2.0934 | lr 3.00e-04 | grad 0.98 | tok/s 20806
step    540 | loss 2.0651 | lr 3.00e-04 | grad 0.72 | tok/s 20366
step    550 | loss 1.7489 | lr 3.00e-04 | grad 1.69 | tok/s 21286
step    560 | loss 1.9407 | lr 3.00e-04 | grad 0.95 | tok/s 21912
step    570 | loss 1.8192 | lr 3.00e-04 | grad 1.03 | tok/s 21914
step    580 | loss 1.7552 | lr 3.00e-04 | grad 1.02 | tok/s 21911
step    590 | loss 1.8041 | lr 3.00e-04 | grad 0.82 | tok/s 21907
step    600 | loss 1.7619 | lr 3.00e-04 | grad 1.27 | tok/s 21900
step    610 | loss 1.7334 | lr 3.00e-04 | grad 0.87 | tok/s 21891
step    620 | loss 1.7126 | lr 3.00e-04 | grad 0.75 | tok/s 21765
step    630 | loss 1.9447 | lr 3.00e-04 | grad 2.17 | tok/s 20572
step    640 | loss 2.0958 | lr 3.00e-04 | grad 1.42 | tok/s 20829
step    650 | loss 1.9100 | lr 3.00e-04 | grad 0.93 | tok/s 20832
step    660 | loss 1.9767 | lr 3.00e-04 | grad 1.08 | tok/s 21615
step    670 | loss 2.0061 | lr 3.00e-04 | grad 2.20 | tok/s 20909
step    680 | loss 1.9906 | lr 3.00e-04 | grad 1.23 | tok/s 20560
step    690 | loss 1.9520 | lr 3.00e-04 | grad 0.89 | tok/s 20400
step    700 | loss 1.8648 | lr 3.00e-04 | grad 0.93 | tok/s 20837
step    710 | loss 1.9999 | lr 3.00e-04 | grad 1.96 | tok/s 20481
step    720 | loss 1.7757 | lr 3.00e-04 | grad 1.33 | tok/s 21280
step    730 | loss 1.8589 | lr 3.00e-04 | grad 0.75 | tok/s 20958
step    740 | loss 2.2946 | lr 3.00e-04 | grad 1.86 | tok/s 21530
step    750 | loss 2.0921 | lr 3.00e-04 | grad 1.34 | tok/s 21826
step    760 | loss 1.8871 | lr 3.00e-04 | grad 1.86 | tok/s 21337
step    770 | loss 1.8983 | lr 3.00e-04 | grad 0.99 | tok/s 20968
step    780 | loss 1.8373 | lr 3.00e-04 | grad 1.21 | tok/s 21113
step    790 | loss 2.1678 | lr 3.00e-04 | grad 1.89 | tok/s 21598
step    800 | loss 1.7297 | lr 3.00e-04 | grad 1.02 | tok/s 21214
step    810 | loss 1.6626 | lr 3.00e-04 | grad 1.71 | tok/s 20504
step    820 | loss 1.8157 | lr 3.00e-04 | grad 1.04 | tok/s 20947
step    830 | loss 1.8633 | lr 3.00e-04 | grad 1.20 | tok/s 20671
step    840 | loss 1.9888 | lr 3.00e-04 | grad 1.05 | tok/s 20598
step    850 | loss 1.9580 | lr 3.00e-04 | grad 0.98 | tok/s 20997
step    860 | loss 1.9956 | lr 3.00e-04 | grad 1.33 | tok/s 21333
step    870 | loss 2.0559 | lr 3.00e-04 | grad 1.09 | tok/s 21484
step    880 | loss 1.8906 | lr 3.00e-04 | grad 0.88 | tok/s 21100
step    890 | loss 1.7786 | lr 3.00e-04 | grad 0.88 | tok/s 21011
step    900 | loss 1.8369 | lr 3.00e-04 | grad 0.88 | tok/s 20921
step    910 | loss 1.9040 | lr 3.00e-04 | grad 3.56 | tok/s 20699
step    920 | loss 1.7941 | lr 3.00e-04 | grad 1.08 | tok/s 20914
step    930 | loss 1.7564 | lr 3.00e-04 | grad 0.95 | tok/s 21155
step    940 | loss 1.7202 | lr 3.00e-04 | grad 0.99 | tok/s 20675
step    950 | loss 1.8032 | lr 3.00e-04 | grad 1.25 | tok/s 20348
step    960 | loss 1.7489 | lr 3.00e-04 | grad 1.01 | tok/s 20922
step    970 | loss 1.7406 | lr 3.00e-04 | grad 1.05 | tok/s 20936
step    980 | loss 2.5212 | lr 3.00e-04 | grad 1.88 | tok/s 21793
step    990 | loss 1.9381 | lr 3.00e-04 | grad 1.11 | tok/s 20884
step   1000 | loss 1.8797 | lr 3.00e-04 | grad 1.21 | tok/s 20950
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8797.pt
step   1010 | loss 1.5992 | lr 3.00e-04 | grad 1.20 | tok/s 13507
step   1020 | loss 1.6981 | lr 3.00e-04 | grad 1.08 | tok/s 21957
step   1030 | loss 1.8472 | lr 3.00e-04 | grad 1.01 | tok/s 20868
step   1040 | loss 2.3437 | lr 3.00e-04 | grad 1.35 | tok/s 21374
step   1050 | loss 1.9457 | lr 3.00e-04 | grad 1.03 | tok/s 21520
step   1060 | loss 1.5842 | lr 3.00e-04 | grad 0.66 | tok/s 21201
step   1070 | loss 1.6836 | lr 3.00e-04 | grad 1.39 | tok/s 21172
step   1080 | loss 1.5762 | lr 3.00e-04 | grad 1.16 | tok/s 21893
step   1090 | loss 1.5579 | lr 3.00e-04 | grad 1.36 | tok/s 21906
step   1100 | loss 1.5266 | lr 3.00e-04 | grad 1.23 | tok/s 21891
step   1110 | loss 1.4608 | lr 3.00e-04 | grad 0.79 | tok/s 21885
step   1120 | loss 1.6847 | lr 3.00e-04 | grad 1.06 | tok/s 21264
step   1130 | loss 2.1309 | lr 3.00e-04 | grad 1.18 | tok/s 21532
step   1140 | loss 2.0724 | lr 3.00e-04 | grad 1.14 | tok/s 21782
step   1150 | loss 2.2529 | lr 3.00e-04 | grad 1.73 | tok/s 21251
step   1160 | loss 1.9782 | lr 3.00e-04 | grad 2.80 | tok/s 20572
step   1170 | loss 1.8123 | lr 3.00e-04 | grad 1.22 | tok/s 20549
step   1180 | loss 1.7125 | lr 3.00e-04 | grad 1.32 | tok/s 21623
step   1190 | loss 2.0406 | lr 3.00e-04 | grad 1.29 | tok/s 21666
step   1200 | loss 1.5644 | lr 3.00e-04 | grad 1.36 | tok/s 21873
step   1210 | loss 1.6111 | lr 3.00e-04 | grad 1.03 | tok/s 20600
step   1220 | loss 1.6915 | lr 3.00e-04 | grad 1.41 | tok/s 21279
step   1230 | loss 1.7352 | lr 3.00e-04 | grad 0.98 | tok/s 21423
step   1240 | loss 1.6311 | lr 3.00e-04 | grad 1.61 | tok/s 21653
step   1250 | loss 1.8395 | lr 3.00e-04 | grad 1.32 | tok/s 21165
step   1260 | loss 1.9371 | lr 3.00e-04 | grad 1.05 | tok/s 21722
step   1270 | loss 1.6754 | lr 3.00e-04 | grad 1.50 | tok/s 21071
step   1280 | loss 1.6968 | lr 3.00e-04 | grad 1.32 | tok/s 21118
step   1290 | loss 1.6911 | lr 3.00e-04 | grad 1.20 | tok/s 20651
step   1300 | loss 1.9321 | lr 3.00e-04 | grad 2.91 | tok/s 20579
step   1310 | loss 1.9366 | lr 3.00e-04 | grad 1.07 | tok/s 21415
step   1320 | loss 1.7821 | lr 3.00e-04 | grad 2.08 | tok/s 21372
step   1330 | loss 1.8240 | lr 3.00e-04 | grad 1.20 | tok/s 21314
step   1340 | loss 1.8670 | lr 3.00e-04 | grad 0.98 | tok/s 20644
step   1350 | loss 1.7481 | lr 3.00e-04 | grad 1.04 | tok/s 21456
step   1360 | loss 1.7945 | lr 3.00e-04 | grad 1.12 | tok/s 20166
step   1370 | loss 1.9225 | lr 3.00e-04 | grad 2.16 | tok/s 21538
step   1380 | loss 1.7733 | lr 3.00e-04 | grad 1.30 | tok/s 20872
step   1390 | loss 1.7685 | lr 3.00e-04 | grad 1.21 | tok/s 21287
step   1400 | loss 1.7974 | lr 3.00e-04 | grad 1.18 | tok/s 20803
step   1410 | loss 1.6320 | lr 3.00e-04 | grad 1.41 | tok/s 20444
step   1420 | loss 1.6513 | lr 3.00e-04 | grad 1.27 | tok/s 21688
step   1430 | loss 2.0344 | lr 3.00e-04 | grad 0.97 | tok/s 21002
step   1440 | loss 1.7441 | lr 3.00e-04 | grad 1.44 | tok/s 21173
step   1450 | loss 1.7495 | lr 3.00e-04 | grad 1.02 | tok/s 21402
step   1460 | loss 1.8108 | lr 3.00e-04 | grad 1.22 | tok/s 20779
step   1470 | loss 1.6593 | lr 3.00e-04 | grad 1.39 | tok/s 20510
step   1480 | loss 1.6837 | lr 3.00e-04 | grad 1.73 | tok/s 21240
step   1490 | loss 1.9067 | lr 3.00e-04 | grad 2.08 | tok/s 21098
step   1500 | loss 1.9497 | lr 3.00e-04 | grad 1.24 | tok/s 21405
step   1510 | loss 1.5765 | lr 3.00e-04 | grad 1.06 | tok/s 20950
step   1520 | loss 1.7213 | lr 3.00e-04 | grad 1.43 | tok/s 20887
step   1530 | loss 1.6608 | lr 3.00e-04 | grad 1.23 | tok/s 21434
step   1540 | loss 1.7534 | lr 3.00e-04 | grad 1.14 | tok/s 21448
step   1550 | loss 1.7353 | lr 3.00e-04 | grad 2.11 | tok/s 21054
step   1560 | loss 1.5586 | lr 3.00e-04 | grad 1.04 | tok/s 21753
step   1570 | loss 1.6685 | lr 3.00e-04 | grad 1.06 | tok/s 21230
step   1580 | loss 1.5514 | lr 3.00e-04 | grad 1.27 | tok/s 21356

Training complete! Final step: 1585
