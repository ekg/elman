Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_56/levelllama_100m_20260126_181457
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 269,409,792 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 6.6051 | lr 3.00e-04 | grad 2.92 | tok/s 12788
step     20 | loss 2.9456 | lr 3.00e-04 | grad 2.20 | tok/s 37619
step     30 | loss 3.2692 | lr 3.00e-04 | grad 3.03 | tok/s 39898
step     40 | loss 5.6409 | lr 3.00e-04 | grad 7.62 | tok/s 40620
step     50 | loss 4.4742 | lr 3.00e-04 | grad 4.72 | tok/s 41052
step     60 | loss 3.7116 | lr 3.00e-04 | grad 3.70 | tok/s 41033
step     70 | loss 3.2544 | lr 3.00e-04 | grad 3.91 | tok/s 40800
step     80 | loss 2.9463 | lr 3.00e-04 | grad 2.62 | tok/s 40638
step     90 | loss 2.6848 | lr 3.00e-04 | grad 2.05 | tok/s 40616
step    100 | loss 2.4952 | lr 3.00e-04 | grad 2.42 | tok/s 40517
step    110 | loss 2.6170 | lr 3.00e-04 | grad 2.31 | tok/s 40161
step    120 | loss 3.4751 | lr 3.00e-04 | grad 1.45 | tok/s 38103
step    130 | loss 2.6088 | lr 3.00e-04 | grad 2.50 | tok/s 39076
step    140 | loss 2.8896 | lr 3.00e-04 | grad 5.44 | tok/s 39044
step    150 | loss 2.7505 | lr 3.00e-04 | grad 4.97 | tok/s 40048
step    160 | loss 2.7886 | lr 3.00e-04 | grad 1.69 | tok/s 38509
step    170 | loss 2.6968 | lr 3.00e-04 | grad 0.97 | tok/s 37784
step    180 | loss 2.8263 | lr 3.00e-04 | grad 1.83 | tok/s 38676
step    190 | loss 2.4530 | lr 3.00e-04 | grad 2.14 | tok/s 37883
step    200 | loss 2.3878 | lr 3.00e-04 | grad 1.55 | tok/s 39460
step    210 | loss 2.4011 | lr 3.00e-04 | grad 2.55 | tok/s 37384
step    220 | loss 2.6481 | lr 3.00e-04 | grad 3.03 | tok/s 37777
step    230 | loss 2.4517 | lr 3.00e-04 | grad 2.69 | tok/s 37575
step    240 | loss 2.7480 | lr 3.00e-04 | grad 2.47 | tok/s 38071
step    250 | loss 2.3077 | lr 3.00e-04 | grad 2.28 | tok/s 37739
step    260 | loss 2.4312 | lr 3.00e-04 | grad 2.12 | tok/s 38768
step    270 | loss 2.2878 | lr 3.00e-04 | grad 1.62 | tok/s 37798
step    280 | loss 2.2266 | lr 3.00e-04 | grad 0.88 | tok/s 35425
step    290 | loss 2.1583 | lr 3.00e-04 | grad 1.89 | tok/s 36497
step    300 | loss 2.4174 | lr 3.00e-04 | grad 1.60 | tok/s 36699
step    310 | loss 2.1081 | lr 3.00e-04 | grad 1.52 | tok/s 36527
step    320 | loss 2.3579 | lr 3.00e-04 | grad 3.02 | tok/s 36926
step    330 | loss 2.1603 | lr 3.00e-04 | grad 1.62 | tok/s 37190
step    340 | loss 2.4648 | lr 3.00e-04 | grad 1.66 | tok/s 37077
step    350 | loss 2.4045 | lr 3.00e-04 | grad 1.38 | tok/s 38037
step    360 | loss 2.0611 | lr 3.00e-04 | grad 1.40 | tok/s 36312
step    370 | loss 2.1170 | lr 3.00e-04 | grad 1.27 | tok/s 38213
step    380 | loss 1.9136 | lr 3.00e-04 | grad 1.73 | tok/s 38525
step    390 | loss 1.8357 | lr 3.00e-04 | grad 1.51 | tok/s 38451
step    400 | loss 2.2242 | lr 3.00e-04 | grad 1.11 | tok/s 36407
step    410 | loss 2.1381 | lr 3.00e-04 | grad 1.27 | tok/s 36735
step    420 | loss 2.2634 | lr 3.00e-04 | grad 2.02 | tok/s 38268
step    430 | loss 2.1015 | lr 3.00e-04 | grad 1.77 | tok/s 37536
step    440 | loss 2.1228 | lr 3.00e-04 | grad 1.83 | tok/s 36295
step    450 | loss 2.0153 | lr 3.00e-04 | grad 1.34 | tok/s 36652
step    460 | loss 2.0844 | lr 3.00e-04 | grad 1.20 | tok/s 37274
step    470 | loss 2.0677 | lr 3.00e-04 | grad 2.02 | tok/s 36879
step    480 | loss 2.0920 | lr 3.00e-04 | grad 2.05 | tok/s 37652
step    490 | loss 2.0571 | lr 3.00e-04 | grad 1.71 | tok/s 36165
step    500 | loss 2.1931 | lr 3.00e-04 | grad 1.23 | tok/s 36705
step    510 | loss 2.0590 | lr 3.00e-04 | grad 1.23 | tok/s 34996
step    520 | loss 1.9130 | lr 3.00e-04 | grad 1.29 | tok/s 36618
step    530 | loss 2.0710 | lr 3.00e-04 | grad 1.17 | tok/s 35940
step    540 | loss 2.0256 | lr 3.00e-04 | grad 0.98 | tok/s 35202
step    550 | loss 1.7031 | lr 3.00e-04 | grad 2.22 | tok/s 36811
step    560 | loss 1.8896 | lr 3.00e-04 | grad 1.69 | tok/s 37778
step    570 | loss 1.7689 | lr 3.00e-04 | grad 1.48 | tok/s 37700
step    580 | loss 1.6912 | lr 3.00e-04 | grad 1.09 | tok/s 37712
step    590 | loss 1.7474 | lr 3.00e-04 | grad 1.20 | tok/s 37618
step    600 | loss 1.6894 | lr 3.00e-04 | grad 1.30 | tok/s 37615
step    610 | loss 1.6748 | lr 3.00e-04 | grad 1.34 | tok/s 37610
step    620 | loss 1.6571 | lr 3.00e-04 | grad 1.37 | tok/s 37430
step    630 | loss 1.9857 | lr 3.00e-04 | grad 4.09 | tok/s 35371
step    640 | loss 2.0599 | lr 3.00e-04 | grad 1.39 | tok/s 35780
step    650 | loss 1.8743 | lr 3.00e-04 | grad 1.20 | tok/s 35750
step    660 | loss 1.9314 | lr 3.00e-04 | grad 1.33 | tok/s 37033
step    670 | loss 1.9648 | lr 3.00e-04 | grad 2.83 | tok/s 35800
step    680 | loss 1.9558 | lr 3.00e-04 | grad 1.62 | tok/s 35194
step    690 | loss 1.9201 | lr 3.00e-04 | grad 1.26 | tok/s 34926
step    700 | loss 1.8163 | lr 3.00e-04 | grad 1.12 | tok/s 35621
step    710 | loss 1.9827 | lr 3.00e-04 | grad 2.48 | tok/s 35073
step    720 | loss 1.6995 | lr 3.00e-04 | grad 1.45 | tok/s 36386
step    730 | loss 1.8021 | lr 3.00e-04 | grad 1.01 | tok/s 35766
step    740 | loss 2.2338 | lr 3.00e-04 | grad 2.41 | tok/s 36768
step    750 | loss 2.0119 | lr 3.00e-04 | grad 1.33 | tok/s 37179
step    760 | loss 1.8470 | lr 3.00e-04 | grad 2.30 | tok/s 36336
step    770 | loss 1.8629 | lr 3.00e-04 | grad 1.37 | tok/s 35722
step    780 | loss 1.8009 | lr 3.00e-04 | grad 1.40 | tok/s 35985
step    790 | loss 2.0905 | lr 3.00e-04 | grad 2.69 | tok/s 36779
step    800 | loss 1.6754 | lr 3.00e-04 | grad 0.84 | tok/s 36177
step    810 | loss 1.6202 | lr 3.00e-04 | grad 2.05 | tok/s 34970
step    820 | loss 1.7664 | lr 3.00e-04 | grad 1.29 | tok/s 35609
step    830 | loss 1.8130 | lr 3.00e-04 | grad 1.32 | tok/s 35101
step    840 | loss 1.9499 | lr 3.00e-04 | grad 1.27 | tok/s 34966
step    850 | loss 1.8870 | lr 3.00e-04 | grad 1.26 | tok/s 35694
step    860 | loss 1.9477 | lr 3.00e-04 | grad 1.46 | tok/s 36210
step    870 | loss 1.9515 | lr 3.00e-04 | grad 1.38 | tok/s 36536
step    880 | loss 1.8458 | lr 3.00e-04 | grad 1.11 | tok/s 35722
step    890 | loss 1.7391 | lr 3.00e-04 | grad 1.16 | tok/s 35571
step    900 | loss 1.7988 | lr 3.00e-04 | grad 1.07 | tok/s 35398
step    910 | loss 1.8548 | lr 3.00e-04 | grad 3.61 | tok/s 34983
step    920 | loss 1.7504 | lr 3.00e-04 | grad 1.23 | tok/s 35406
step    930 | loss 1.7100 | lr 3.00e-04 | grad 1.23 | tok/s 35872
step    940 | loss 1.6694 | lr 3.00e-04 | grad 1.14 | tok/s 35027
step    950 | loss 1.7538 | lr 3.00e-04 | grad 1.65 | tok/s 34494
step    960 | loss 1.7028 | lr 3.00e-04 | grad 1.23 | tok/s 35352
step    970 | loss 1.7012 | lr 3.00e-04 | grad 1.26 | tok/s 35362
step    980 | loss 2.4098 | lr 3.00e-04 | grad 2.48 | tok/s 36854
step    990 | loss 1.8993 | lr 3.00e-04 | grad 1.27 | tok/s 35342
step   1000 | loss 1.8186 | lr 3.00e-04 | grad 1.55 | tok/s 35348
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8186.pt
step   1010 | loss 1.5582 | lr 3.00e-04 | grad 1.55 | tok/s 18970
step   1020 | loss 1.6262 | lr 3.00e-04 | grad 1.27 | tok/s 37371
step   1030 | loss 1.7938 | lr 3.00e-04 | grad 1.26 | tok/s 35412
step   1040 | loss 2.3089 | lr 3.00e-04 | grad 2.95 | tok/s 36201
step   1050 | loss 1.8815 | lr 3.00e-04 | grad 1.12 | tok/s 36447
step   1060 | loss 1.5255 | lr 3.00e-04 | grad 0.87 | tok/s 36005
step   1070 | loss 1.6325 | lr 3.00e-04 | grad 1.54 | tok/s 35924
step   1080 | loss 1.5198 | lr 3.00e-04 | grad 1.22 | tok/s 37073
step   1090 | loss 1.4997 | lr 3.00e-04 | grad 1.26 | tok/s 37067
step   1100 | loss 1.4679 | lr 3.00e-04 | grad 1.39 | tok/s 37018
step   1110 | loss 1.4111 | lr 3.00e-04 | grad 1.23 | tok/s 36976
step   1120 | loss 1.6377 | lr 3.00e-04 | grad 1.23 | tok/s 35977
step   1130 | loss 2.0420 | lr 3.00e-04 | grad 1.38 | tok/s 36371
step   1140 | loss 2.0389 | lr 3.00e-04 | grad 1.34 | tok/s 36806
step   1150 | loss 2.1542 | lr 3.00e-04 | grad 1.83 | tok/s 35923
step   1160 | loss 1.9223 | lr 3.00e-04 | grad 2.98 | tok/s 34826
step   1170 | loss 1.7735 | lr 3.00e-04 | grad 1.48 | tok/s 34685
step   1180 | loss 1.6581 | lr 3.00e-04 | grad 1.26 | tok/s 36476
step   1190 | loss 1.9559 | lr 3.00e-04 | grad 1.55 | tok/s 36515
step   1200 | loss 1.4757 | lr 3.00e-04 | grad 1.22 | tok/s 36843
step   1210 | loss 1.5659 | lr 3.00e-04 | grad 1.11 | tok/s 34747
step   1220 | loss 1.6445 | lr 3.00e-04 | grad 1.93 | tok/s 35879
step   1230 | loss 1.6838 | lr 3.00e-04 | grad 1.08 | tok/s 36088
step   1240 | loss 1.5457 | lr 3.00e-04 | grad 1.77 | tok/s 36540
step   1250 | loss 1.7871 | lr 3.00e-04 | grad 1.74 | tok/s 35650
step   1260 | loss 1.8170 | lr 3.00e-04 | grad 1.15 | tok/s 36590
step   1270 | loss 1.6332 | lr 3.00e-04 | grad 1.76 | tok/s 35494
step   1280 | loss 1.6384 | lr 3.00e-04 | grad 1.42 | tok/s 35594
step   1290 | loss 1.6365 | lr 3.00e-04 | grad 1.38 | tok/s 34839
step   1300 | loss 1.8625 | lr 3.00e-04 | grad 3.41 | tok/s 34748
step   1310 | loss 1.8642 | lr 3.00e-04 | grad 1.22 | tok/s 36145
step   1320 | loss 1.7306 | lr 3.00e-04 | grad 2.44 | tok/s 36057
step   1330 | loss 1.7616 | lr 3.00e-04 | grad 1.41 | tok/s 35977
step   1340 | loss 1.8268 | lr 3.00e-04 | grad 1.34 | tok/s 34813
step   1350 | loss 1.6998 | lr 3.00e-04 | grad 1.37 | tok/s 36187
step   1360 | loss 1.7587 | lr 3.00e-04 | grad 1.30 | tok/s 33997
step   1370 | loss 1.8626 | lr 3.00e-04 | grad 2.62 | tok/s 36317
step   1380 | loss 1.7344 | lr 3.00e-04 | grad 1.48 | tok/s 35171
step   1390 | loss 1.6818 | lr 3.00e-04 | grad 1.51 | tok/s 35880
step   1400 | loss 1.7675 | lr 3.00e-04 | grad 1.14 | tok/s 35115
step   1410 | loss 1.5865 | lr 3.00e-04 | grad 1.55 | tok/s 34421
step   1420 | loss 1.5494 | lr 3.00e-04 | grad 1.51 | tok/s 36598
step   1430 | loss 1.9819 | lr 3.00e-04 | grad 1.11 | tok/s 35409
step   1440 | loss 1.7127 | lr 3.00e-04 | grad 1.68 | tok/s 35640
step   1450 | loss 1.6940 | lr 3.00e-04 | grad 1.16 | tok/s 36028
step   1460 | loss 1.7953 | lr 3.00e-04 | grad 1.52 | tok/s 35012
step   1470 | loss 1.6186 | lr 3.00e-04 | grad 1.43 | tok/s 34498
step   1480 | loss 1.6288 | lr 3.00e-04 | grad 1.89 | tok/s 35722
step   1490 | loss 1.8512 | lr 3.00e-04 | grad 4.69 | tok/s 35429
step   1500 | loss 1.8696 | lr 3.00e-04 | grad 1.26 | tok/s 36067
step   1510 | loss 1.5183 | lr 3.00e-04 | grad 1.13 | tok/s 35338
step   1520 | loss 1.6843 | lr 3.00e-04 | grad 1.62 | tok/s 35142
step   1530 | loss 1.6187 | lr 3.00e-04 | grad 1.41 | tok/s 36087
step   1540 | loss 1.7119 | lr 3.00e-04 | grad 0.94 | tok/s 36175
step   1550 | loss 1.6983 | lr 3.00e-04 | grad 2.50 | tok/s 35503
step   1560 | loss 1.4591 | lr 3.00e-04 | grad 1.29 | tok/s 36628
step   1570 | loss 1.5735 | lr 3.00e-04 | grad 1.05 | tok/s 35883
step   1580 | loss 1.4877 | lr 3.00e-04 | grad 1.50 | tok/s 36100
step   1590 | loss 1.6926 | lr 3.00e-04 | grad 2.64 | tok/s 34714
step   1600 | loss 1.4610 | lr 3.00e-04 | grad 4.34 | tok/s 36684
step   1610 | loss 2.1155 | lr 3.00e-04 | grad 2.66 | tok/s 35702
step   1620 | loss 2.2610 | lr 3.00e-04 | grad 1.79 | tok/s 36862
step   1630 | loss 2.0087 | lr 3.00e-04 | grad 1.81 | tok/s 36809
step   1640 | loss 1.8442 | lr 3.00e-04 | grad 1.52 | tok/s 36857
step   1650 | loss 1.7643 | lr 3.00e-04 | grad 1.52 | tok/s 36863
step   1660 | loss 1.7092 | lr 3.00e-04 | grad 1.45 | tok/s 36902
step   1670 | loss 1.8362 | lr 3.00e-04 | grad 1.59 | tok/s 35759
step   1680 | loss 1.6470 | lr 3.00e-04 | grad 1.34 | tok/s 35411
step   1690 | loss 1.6629 | lr 3.00e-04 | grad 1.49 | tok/s 34634
step   1700 | loss 1.5334 | lr 3.00e-04 | grad 1.50 | tok/s 35962
step   1710 | loss 1.4910 | lr 3.00e-04 | grad 1.68 | tok/s 36066
step   1720 | loss 1.6485 | lr 3.00e-04 | grad 1.12 | tok/s 34950
step   1730 | loss 1.6992 | lr 3.00e-04 | grad 2.05 | tok/s 36043
step   1740 | loss 1.6206 | lr 3.00e-04 | grad 1.33 | tok/s 36202
step   1750 | loss 1.5039 | lr 3.00e-04 | grad 1.29 | tok/s 35465
step   1760 | loss 1.6052 | lr 3.00e-04 | grad 1.30 | tok/s 35086
step   1770 | loss 1.8852 | lr 3.00e-04 | grad 1.34 | tok/s 35957
step   1780 | loss 1.9210 | lr 3.00e-04 | grad 1.30 | tok/s 33966
step   1790 | loss 1.5060 | lr 3.00e-04 | grad 1.23 | tok/s 34520
step   1800 | loss 1.5378 | lr 3.00e-04 | grad 1.20 | tok/s 35349
step   1810 | loss 1.6110 | lr 3.00e-04 | grad 1.43 | tok/s 35693
step   1820 | loss 1.7188 | lr 3.00e-04 | grad 1.92 | tok/s 35195
step   1830 | loss 1.5685 | lr 3.00e-04 | grad 1.15 | tok/s 34216
step   1840 | loss 1.6164 | lr 3.00e-04 | grad 1.15 | tok/s 35484
step   1850 | loss 1.6451 | lr 3.00e-04 | grad 1.44 | tok/s 35189
step   1860 | loss 1.6176 | lr 3.00e-04 | grad 1.45 | tok/s 35070
step   1870 | loss 1.6202 | lr 3.00e-04 | grad 2.23 | tok/s 35699
step   1880 | loss 1.6787 | lr 3.00e-04 | grad 1.32 | tok/s 35926
step   1890 | loss 1.4683 | lr 3.00e-04 | grad 1.20 | tok/s 36912
step   1900 | loss 1.4043 | lr 3.00e-04 | grad 1.16 | tok/s 36893
step   1910 | loss 1.3798 | lr 3.00e-04 | grad 1.12 | tok/s 36899
step   1920 | loss 1.3656 | lr 3.00e-04 | grad 1.09 | tok/s 36915
step   1930 | loss 1.3931 | lr 3.00e-04 | grad 2.05 | tok/s 36807
step   1940 | loss 1.7330 | lr 3.00e-04 | grad 1.59 | tok/s 35012
step   1950 | loss 1.6156 | lr 3.00e-04 | grad 1.06 | tok/s 34741
step   1960 | loss 1.7171 | lr 3.00e-04 | grad 1.66 | tok/s 35134
step   1970 | loss 1.7508 | lr 3.00e-04 | grad 1.46 | tok/s 35838
step   1980 | loss 1.6346 | lr 3.00e-04 | grad 1.63 | tok/s 34773
step   1990 | loss 1.8283 | lr 3.00e-04 | grad 2.30 | tok/s 35734
step   2000 | loss 1.3725 | lr 3.00e-04 | grad 1.17 | tok/s 36958
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3725.pt
step   2010 | loss 1.5724 | lr 3.00e-04 | grad 1.85 | tok/s 17740
step   2020 | loss 1.5606 | lr 3.00e-04 | grad 1.36 | tok/s 35195
step   2030 | loss 1.8941 | lr 3.00e-04 | grad 10.81 | tok/s 34813
step   2040 | loss 1.6444 | lr 3.00e-04 | grad 1.49 | tok/s 35453
step   2050 | loss 1.5234 | lr 3.00e-04 | grad 1.14 | tok/s 35998
step   2060 | loss 1.7591 | lr 3.00e-04 | grad 2.27 | tok/s 35992
step   2070 | loss 1.4702 | lr 3.00e-04 | grad 1.56 | tok/s 36095
step   2080 | loss 1.5039 | lr 3.00e-04 | grad 1.19 | tok/s 34347
step   2090 | loss 1.5690 | lr 3.00e-04 | grad 3.03 | tok/s 36653
step   2100 | loss 1.9941 | lr 3.00e-04 | grad 1.94 | tok/s 37025
step   2110 | loss 1.6224 | lr 3.00e-04 | grad 1.51 | tok/s 35353
step   2120 | loss 2.4182 | lr 3.00e-04 | grad 1.75 | tok/s 36150
step   2130 | loss 1.6347 | lr 3.00e-04 | grad 1.46 | tok/s 34392
step   2140 | loss 1.8104 | lr 3.00e-04 | grad 1.33 | tok/s 36293
step   2150 | loss 1.8950 | lr 3.00e-04 | grad 1.33 | tok/s 35583
step   2160 | loss 1.6733 | lr 3.00e-04 | grad 1.36 | tok/s 35656
step   2170 | loss 1.6022 | lr 3.00e-04 | grad 3.66 | tok/s 36115
step   2180 | loss 1.3439 | lr 3.00e-04 | grad 1.86 | tok/s 36749
step   2190 | loss 1.7192 | lr 3.00e-04 | grad 1.38 | tok/s 35575
step   2200 | loss 1.5445 | lr 3.00e-04 | grad 2.64 | tok/s 36812
step   2210 | loss 1.6766 | lr 3.00e-04 | grad 1.46 | tok/s 36314
step   2220 | loss 1.5595 | lr 3.00e-04 | grad 1.27 | tok/s 34878
step   2230 | loss 1.7759 | lr 3.00e-04 | grad 1.69 | tok/s 35065
step   2240 | loss 1.5049 | lr 3.00e-04 | grad 1.36 | tok/s 35854
step   2250 | loss 1.5543 | lr 3.00e-04 | grad 1.36 | tok/s 35114
step   2260 | loss 1.7055 | lr 3.00e-04 | grad 2.78 | tok/s 36604
step   2270 | loss 1.7953 | lr 3.00e-04 | grad 1.73 | tok/s 34142
step   2280 | loss 1.6147 | lr 3.00e-04 | grad 1.36 | tok/s 35626
step   2290 | loss 1.4529 | lr 3.00e-04 | grad 1.20 | tok/s 35166
step   2300 | loss 1.7363 | lr 3.00e-04 | grad 1.12 | tok/s 34926
step   2310 | loss 1.5757 | lr 3.00e-04 | grad 1.27 | tok/s 35539
step   2320 | loss 1.6007 | lr 3.00e-04 | grad 1.28 | tok/s 36982
step   2330 | loss 1.5330 | lr 3.00e-04 | grad 1.27 | tok/s 37040
step   2340 | loss 1.4839 | lr 3.00e-04 | grad 1.16 | tok/s 37029
step   2350 | loss 1.4393 | lr 3.00e-04 | grad 1.16 | tok/s 36998
step   2360 | loss 1.4403 | lr 3.00e-04 | grad 1.36 | tok/s 37033
step   2370 | loss 1.3899 | lr 3.00e-04 | grad 1.16 | tok/s 37080
step   2380 | loss 1.3626 | lr 3.00e-04 | grad 1.31 | tok/s 36979
step   2390 | loss 1.3783 | lr 3.00e-04 | grad 1.44 | tok/s 36926
step   2400 | loss 1.3670 | lr 3.00e-04 | grad 1.20 | tok/s 36964
step   2410 | loss 1.4188 | lr 3.00e-04 | grad 1.26 | tok/s 36995
step   2420 | loss 1.7106 | lr 3.00e-04 | grad 1.05 | tok/s 35792
step   2430 | loss 1.2045 | lr 3.00e-04 | grad 1.40 | tok/s 35937
step   2440 | loss 1.5634 | lr 3.00e-04 | grad 1.27 | tok/s 33844
step   2450 | loss 1.5198 | lr 3.00e-04 | grad 1.29 | tok/s 35555
step   2460 | loss 1.7017 | lr 3.00e-04 | grad 1.70 | tok/s 36310
step   2470 | loss 1.5048 | lr 3.00e-04 | grad 3.70 | tok/s 35870
step   2480 | loss 1.6357 | lr 3.00e-04 | grad 2.53 | tok/s 35843
step   2490 | loss 1.6422 | lr 3.00e-04 | grad 2.92 | tok/s 36274
step   2500 | loss 1.7618 | lr 3.00e-04 | grad 2.12 | tok/s 35905
step   2510 | loss 1.6868 | lr 3.00e-04 | grad 1.27 | tok/s 35579
step   2520 | loss 1.5150 | lr 3.00e-04 | grad 1.58 | tok/s 35227
step   2530 | loss 1.7138 | lr 3.00e-04 | grad 1.24 | tok/s 35960
step   2540 | loss 1.6107 | lr 3.00e-04 | grad 1.80 | tok/s 34426
step   2550 | loss 1.4955 | lr 3.00e-04 | grad 1.09 | tok/s 35725
step   2560 | loss 1.7187 | lr 3.00e-04 | grad 1.11 | tok/s 35007
step   2570 | loss 1.5379 | lr 3.00e-04 | grad 1.07 | tok/s 35307
step   2580 | loss 1.7855 | lr 3.00e-04 | grad 1.43 | tok/s 35425
step   2590 | loss 1.5258 | lr 3.00e-04 | grad 1.65 | tok/s 34995
step   2600 | loss 1.6361 | lr 3.00e-04 | grad 0.93 | tok/s 34592
step   2610 | loss 1.6269 | lr 3.00e-04 | grad 1.25 | tok/s 36374
step   2620 | loss 1.6585 | lr 3.00e-04 | grad 1.23 | tok/s 35181
step   2630 | loss 1.4306 | lr 3.00e-04 | grad 1.09 | tok/s 36727
step   2640 | loss 1.5197 | lr 3.00e-04 | grad 1.30 | tok/s 35211
step   2650 | loss 1.4367 | lr 3.00e-04 | grad 1.30 | tok/s 35739
step   2660 | loss 1.4740 | lr 3.00e-04 | grad 0.96 | tok/s 35726
step   2670 | loss 1.4192 | lr 3.00e-04 | grad 1.42 | tok/s 36750
step   2680 | loss 1.5453 | lr 3.00e-04 | grad 1.26 | tok/s 34598

Training complete! Final step: 2682
