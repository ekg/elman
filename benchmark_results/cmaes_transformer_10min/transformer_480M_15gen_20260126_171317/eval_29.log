Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_29/levelllama_100m_20260126_174411
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 265,779,072 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.4446 | lr 3.00e-04 | grad 2.02 | tok/s 11447
step     20 | loss 2.9252 | lr 3.00e-04 | grad 1.49 | tok/s 26794
step     30 | loss 3.3085 | lr 3.00e-04 | grad 1.50 | tok/s 28322
step     40 | loss 5.6100 | lr 3.00e-04 | grad 6.75 | tok/s 28851
step     50 | loss 5.0966 | lr 3.00e-04 | grad 5.38 | tok/s 29261
step     60 | loss 3.9409 | lr 3.00e-04 | grad 3.20 | tok/s 29237
step     70 | loss 3.4450 | lr 3.00e-04 | grad 2.98 | tok/s 29155
step     80 | loss 3.2006 | lr 3.00e-04 | grad 2.05 | tok/s 29082
step     90 | loss 2.8905 | lr 3.00e-04 | grad 1.51 | tok/s 29016
step    100 | loss 2.6715 | lr 3.00e-04 | grad 2.12 | tok/s 28921
step    110 | loss 2.6839 | lr 3.00e-04 | grad 1.88 | tok/s 28675
step    120 | loss 3.4362 | lr 3.00e-04 | grad 1.13 | tok/s 27297
step    130 | loss 2.6280 | lr 3.00e-04 | grad 2.17 | tok/s 27936
step    140 | loss 2.9573 | lr 3.00e-04 | grad 5.50 | tok/s 27996
step    150 | loss 2.8124 | lr 3.00e-04 | grad 4.31 | tok/s 28689
step    160 | loss 2.8721 | lr 3.00e-04 | grad 1.48 | tok/s 27710
step    170 | loss 2.7154 | lr 3.00e-04 | grad 0.80 | tok/s 27257
step    180 | loss 2.8902 | lr 3.00e-04 | grad 1.46 | tok/s 27912
step    190 | loss 2.4518 | lr 3.00e-04 | grad 1.41 | tok/s 27365
step    200 | loss 2.3787 | lr 3.00e-04 | grad 0.83 | tok/s 28589
step    210 | loss 2.4018 | lr 3.00e-04 | grad 2.78 | tok/s 27105
step    220 | loss 2.6611 | lr 3.00e-04 | grad 3.84 | tok/s 27366
step    230 | loss 2.4280 | lr 3.00e-04 | grad 1.68 | tok/s 27333
step    240 | loss 2.7478 | lr 3.00e-04 | grad 1.96 | tok/s 27698
step    250 | loss 2.2979 | lr 3.00e-04 | grad 1.38 | tok/s 27497
step    260 | loss 2.4355 | lr 3.00e-04 | grad 1.66 | tok/s 28270
step    270 | loss 2.2935 | lr 3.00e-04 | grad 1.84 | tok/s 27625
step    280 | loss 2.2280 | lr 3.00e-04 | grad 0.79 | tok/s 25962
step    290 | loss 2.1624 | lr 3.00e-04 | grad 1.47 | tok/s 26820
step    300 | loss 2.4194 | lr 3.00e-04 | grad 1.04 | tok/s 27036
step    310 | loss 2.1130 | lr 3.00e-04 | grad 0.99 | tok/s 26875
step    320 | loss 2.3745 | lr 3.00e-04 | grad 2.70 | tok/s 27142
step    330 | loss 2.1732 | lr 3.00e-04 | grad 1.10 | tok/s 27415
step    340 | loss 2.4668 | lr 3.00e-04 | grad 1.55 | tok/s 27306
step    350 | loss 2.4444 | lr 3.00e-04 | grad 1.16 | tok/s 28130
step    360 | loss 2.0864 | lr 3.00e-04 | grad 1.08 | tok/s 26916
step    370 | loss 2.1390 | lr 3.00e-04 | grad 0.90 | tok/s 28340
step    380 | loss 1.9578 | lr 3.00e-04 | grad 1.23 | tok/s 28569
step    390 | loss 1.8836 | lr 3.00e-04 | grad 1.41 | tok/s 28546
step    400 | loss 2.2473 | lr 3.00e-04 | grad 0.99 | tok/s 27066
step    410 | loss 2.1451 | lr 3.00e-04 | grad 1.09 | tok/s 27321
step    420 | loss 2.3164 | lr 3.00e-04 | grad 1.62 | tok/s 28475
step    430 | loss 2.1296 | lr 3.00e-04 | grad 1.27 | tok/s 28003
step    440 | loss 2.1473 | lr 3.00e-04 | grad 1.23 | tok/s 27129
step    450 | loss 2.0434 | lr 3.00e-04 | grad 0.91 | tok/s 27453
step    460 | loss 2.1327 | lr 3.00e-04 | grad 0.91 | tok/s 27881
step    470 | loss 2.1137 | lr 3.00e-04 | grad 1.72 | tok/s 27639
step    480 | loss 2.1404 | lr 3.00e-04 | grad 1.73 | tok/s 28236
step    490 | loss 2.0818 | lr 3.00e-04 | grad 1.52 | tok/s 27097
step    500 | loss 2.2209 | lr 3.00e-04 | grad 0.96 | tok/s 27565
step    510 | loss 2.0734 | lr 3.00e-04 | grad 1.14 | tok/s 26321
step    520 | loss 1.9399 | lr 3.00e-04 | grad 0.94 | tok/s 27582
step    530 | loss 2.0962 | lr 3.00e-04 | grad 1.09 | tok/s 27121
step    540 | loss 2.0635 | lr 3.00e-04 | grad 0.83 | tok/s 26549
step    550 | loss 1.7432 | lr 3.00e-04 | grad 1.95 | tok/s 27762
step    560 | loss 1.9303 | lr 3.00e-04 | grad 1.13 | tok/s 28551
step    570 | loss 1.8183 | lr 3.00e-04 | grad 1.33 | tok/s 28511
step    580 | loss 1.7432 | lr 3.00e-04 | grad 1.09 | tok/s 28551
step    590 | loss 1.7921 | lr 3.00e-04 | grad 1.23 | tok/s 28531
step    600 | loss 1.7493 | lr 3.00e-04 | grad 1.35 | tok/s 28530
step    610 | loss 1.7175 | lr 3.00e-04 | grad 0.88 | tok/s 28587
step    620 | loss 1.6973 | lr 3.00e-04 | grad 1.09 | tok/s 28429
step    630 | loss 1.9679 | lr 3.00e-04 | grad 2.52 | tok/s 26900
step    640 | loss 2.0865 | lr 3.00e-04 | grad 1.34 | tok/s 27226
step    650 | loss 1.9043 | lr 3.00e-04 | grad 0.98 | tok/s 27219
step    660 | loss 1.9705 | lr 3.00e-04 | grad 1.14 | tok/s 28232
step    670 | loss 1.9976 | lr 3.00e-04 | grad 2.48 | tok/s 27296
step    680 | loss 1.9846 | lr 3.00e-04 | grad 1.35 | tok/s 26867
step    690 | loss 1.9420 | lr 3.00e-04 | grad 1.10 | tok/s 26640
step    700 | loss 1.8586 | lr 3.00e-04 | grad 1.09 | tok/s 27228
step    710 | loss 1.9998 | lr 3.00e-04 | grad 2.25 | tok/s 26801
step    720 | loss 1.7542 | lr 3.00e-04 | grad 1.23 | tok/s 27844
step    730 | loss 1.8429 | lr 3.00e-04 | grad 0.91 | tok/s 27404
step    740 | loss 2.2796 | lr 3.00e-04 | grad 2.02 | tok/s 28177
step    750 | loss 2.0701 | lr 3.00e-04 | grad 1.30 | tok/s 28501
step    760 | loss 1.8845 | lr 3.00e-04 | grad 2.42 | tok/s 27873
step    770 | loss 1.8829 | lr 3.00e-04 | grad 1.08 | tok/s 27436
step    780 | loss 1.8288 | lr 3.00e-04 | grad 1.39 | tok/s 27600
step    790 | loss 2.1490 | lr 3.00e-04 | grad 2.12 | tok/s 28205
step    800 | loss 1.7222 | lr 3.00e-04 | grad 1.00 | tok/s 27708
step    810 | loss 1.6515 | lr 3.00e-04 | grad 1.79 | tok/s 26799
step    820 | loss 1.8019 | lr 3.00e-04 | grad 1.14 | tok/s 27283
step    830 | loss 1.8422 | lr 3.00e-04 | grad 1.05 | tok/s 26942
step    840 | loss 1.9620 | lr 3.00e-04 | grad 1.06 | tok/s 26837
step    850 | loss 1.9314 | lr 3.00e-04 | grad 1.09 | tok/s 27392
step    860 | loss 1.9834 | lr 3.00e-04 | grad 1.28 | tok/s 27812
step    870 | loss 2.0225 | lr 3.00e-04 | grad 1.12 | tok/s 28043
step    880 | loss 1.8812 | lr 3.00e-04 | grad 0.98 | tok/s 27471
step    890 | loss 1.7652 | lr 3.00e-04 | grad 0.95 | tok/s 27358
step    900 | loss 1.8213 | lr 3.00e-04 | grad 0.97 | tok/s 27230
step    910 | loss 1.8792 | lr 3.00e-04 | grad 3.34 | tok/s 27010
step    920 | loss 1.7755 | lr 3.00e-04 | grad 1.15 | tok/s 27295
step    930 | loss 1.7439 | lr 3.00e-04 | grad 1.02 | tok/s 27659
step    940 | loss 1.7080 | lr 3.00e-04 | grad 1.13 | tok/s 27055
step    950 | loss 1.7883 | lr 3.00e-04 | grad 1.41 | tok/s 26565
step    960 | loss 1.7354 | lr 3.00e-04 | grad 1.26 | tok/s 27306
step    970 | loss 1.7287 | lr 3.00e-04 | grad 1.06 | tok/s 27315
step    980 | loss 2.4871 | lr 3.00e-04 | grad 2.45 | tok/s 28402
step    990 | loss 1.9349 | lr 3.00e-04 | grad 1.34 | tok/s 27239
step   1000 | loss 1.8395 | lr 3.00e-04 | grad 1.34 | tok/s 27317
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8395.pt
step   1010 | loss 1.5854 | lr 3.00e-04 | grad 1.27 | tok/s 16642
step   1020 | loss 1.6773 | lr 3.00e-04 | grad 1.16 | tok/s 28713
step   1030 | loss 1.8354 | lr 3.00e-04 | grad 1.22 | tok/s 27258
step   1040 | loss 2.3321 | lr 3.00e-04 | grad 1.68 | tok/s 27889
step   1050 | loss 1.9332 | lr 3.00e-04 | grad 0.96 | tok/s 28086
step   1060 | loss 1.5718 | lr 3.00e-04 | grad 0.85 | tok/s 27737
step   1070 | loss 1.6673 | lr 3.00e-04 | grad 1.25 | tok/s 27677
step   1080 | loss 1.5570 | lr 3.00e-04 | grad 1.14 | tok/s 28594
step   1090 | loss 1.5292 | lr 3.00e-04 | grad 0.89 | tok/s 28597
step   1100 | loss 1.5023 | lr 3.00e-04 | grad 1.34 | tok/s 28581
step   1110 | loss 1.4528 | lr 3.00e-04 | grad 1.19 | tok/s 28573
step   1120 | loss 1.6738 | lr 3.00e-04 | grad 1.02 | tok/s 27786
step   1130 | loss 2.0954 | lr 3.00e-04 | grad 1.16 | tok/s 28091
step   1140 | loss 2.0606 | lr 3.00e-04 | grad 1.12 | tok/s 28412
step   1150 | loss 2.2192 | lr 3.00e-04 | grad 1.77 | tok/s 27787
step   1160 | loss 1.9489 | lr 3.00e-04 | grad 2.95 | tok/s 26889
step   1170 | loss 1.7875 | lr 3.00e-04 | grad 1.30 | tok/s 26820
step   1180 | loss 1.6939 | lr 3.00e-04 | grad 1.14 | tok/s 28222
step   1190 | loss 2.0216 | lr 3.00e-04 | grad 1.45 | tok/s 28278
step   1200 | loss 1.5371 | lr 3.00e-04 | grad 1.23 | tok/s 28551
step   1210 | loss 1.5975 | lr 3.00e-04 | grad 1.08 | tok/s 26898
step   1220 | loss 1.6795 | lr 3.00e-04 | grad 1.31 | tok/s 27777
step   1230 | loss 1.7221 | lr 3.00e-04 | grad 1.02 | tok/s 28003
step   1240 | loss 1.6036 | lr 3.00e-04 | grad 1.71 | tok/s 28270
step   1250 | loss 1.8252 | lr 3.00e-04 | grad 1.42 | tok/s 27606
step   1260 | loss 1.8968 | lr 3.00e-04 | grad 1.09 | tok/s 28364
step   1270 | loss 1.6631 | lr 3.00e-04 | grad 1.60 | tok/s 27489
step   1280 | loss 1.6770 | lr 3.00e-04 | grad 1.34 | tok/s 27555
step   1290 | loss 1.6716 | lr 3.00e-04 | grad 1.35 | tok/s 26929
step   1300 | loss 1.9084 | lr 3.00e-04 | grad 3.33 | tok/s 26839
step   1310 | loss 1.9223 | lr 3.00e-04 | grad 1.12 | tok/s 27964
step   1320 | loss 1.7691 | lr 3.00e-04 | grad 2.25 | tok/s 27866
step   1330 | loss 1.7974 | lr 3.00e-04 | grad 1.32 | tok/s 27792
step   1340 | loss 1.8539 | lr 3.00e-04 | grad 1.03 | tok/s 26927
step   1350 | loss 1.7371 | lr 3.00e-04 | grad 1.14 | tok/s 28009
step   1360 | loss 1.7901 | lr 3.00e-04 | grad 1.19 | tok/s 26323
step   1370 | loss 1.9082 | lr 3.00e-04 | grad 2.39 | tok/s 28093
step   1380 | loss 1.7592 | lr 3.00e-04 | grad 1.27 | tok/s 27194
step   1390 | loss 1.7382 | lr 3.00e-04 | grad 1.38 | tok/s 27718
step   1400 | loss 1.7777 | lr 3.00e-04 | grad 1.15 | tok/s 27100
step   1410 | loss 1.6151 | lr 3.00e-04 | grad 1.38 | tok/s 26620
step   1420 | loss 1.6262 | lr 3.00e-04 | grad 1.34 | tok/s 28236
step   1430 | loss 2.0322 | lr 3.00e-04 | grad 1.05 | tok/s 27343
step   1440 | loss 1.7290 | lr 3.00e-04 | grad 1.56 | tok/s 27599
step   1450 | loss 1.7347 | lr 3.00e-04 | grad 1.12 | tok/s 27892
step   1460 | loss 1.8026 | lr 3.00e-04 | grad 1.38 | tok/s 27091
step   1470 | loss 1.6509 | lr 3.00e-04 | grad 1.42 | tok/s 26716
step   1480 | loss 1.6706 | lr 3.00e-04 | grad 1.88 | tok/s 27670
step   1490 | loss 1.8725 | lr 3.00e-04 | grad 2.14 | tok/s 27462
step   1500 | loss 1.8849 | lr 3.00e-04 | grad 1.23 | tok/s 27839
step   1510 | loss 1.5637 | lr 3.00e-04 | grad 1.05 | tok/s 27331
step   1520 | loss 1.7071 | lr 3.00e-04 | grad 1.53 | tok/s 27224
step   1530 | loss 1.6474 | lr 3.00e-04 | grad 1.30 | tok/s 27967
step   1540 | loss 1.7376 | lr 3.00e-04 | grad 0.95 | tok/s 27988
step   1550 | loss 1.7210 | lr 3.00e-04 | grad 2.34 | tok/s 27453
step   1560 | loss 1.5255 | lr 3.00e-04 | grad 1.18 | tok/s 28371
step   1570 | loss 1.6274 | lr 3.00e-04 | grad 1.05 | tok/s 27743
step   1580 | loss 1.5295 | lr 3.00e-04 | grad 1.52 | tok/s 27884
step   1590 | loss 1.7065 | lr 3.00e-04 | grad 1.96 | tok/s 26824
step   1600 | loss 1.5001 | lr 3.00e-04 | grad 3.91 | tok/s 28302
step   1610 | loss 2.1425 | lr 3.00e-04 | grad 2.55 | tok/s 27600
step   1620 | loss 2.3169 | lr 3.00e-04 | grad 2.03 | tok/s 28480
step   1630 | loss 2.0634 | lr 3.00e-04 | grad 1.33 | tok/s 28500
step   1640 | loss 1.9011 | lr 3.00e-04 | grad 1.46 | tok/s 28496
step   1650 | loss 1.8232 | lr 3.00e-04 | grad 1.37 | tok/s 28477
step   1660 | loss 1.7754 | lr 3.00e-04 | grad 1.18 | tok/s 28469
step   1670 | loss 1.8730 | lr 3.00e-04 | grad 1.52 | tok/s 27602
step   1680 | loss 1.6687 | lr 3.00e-04 | grad 1.20 | tok/s 27343
step   1690 | loss 1.6841 | lr 3.00e-04 | grad 1.46 | tok/s 26717
step   1700 | loss 1.5608 | lr 3.00e-04 | grad 1.27 | tok/s 27735
step   1710 | loss 1.5324 | lr 3.00e-04 | grad 1.55 | tok/s 27839
step   1720 | loss 1.6637 | lr 3.00e-04 | grad 1.02 | tok/s 26961
step   1730 | loss 1.7303 | lr 3.00e-04 | grad 2.22 | tok/s 27838
step   1740 | loss 1.6544 | lr 3.00e-04 | grad 1.28 | tok/s 27971
step   1750 | loss 1.5404 | lr 3.00e-04 | grad 1.23 | tok/s 27366
step   1760 | loss 1.6288 | lr 3.00e-04 | grad 1.25 | tok/s 27110
step   1770 | loss 1.9348 | lr 3.00e-04 | grad 1.30 | tok/s 27775
step   1780 | loss 1.9382 | lr 3.00e-04 | grad 1.30 | tok/s 26215
step   1790 | loss 1.5330 | lr 3.00e-04 | grad 1.13 | tok/s 26656
step   1800 | loss 1.5745 | lr 3.00e-04 | grad 1.22 | tok/s 27302
step   1810 | loss 1.6430 | lr 3.00e-04 | grad 1.44 | tok/s 27508
step   1820 | loss 1.7457 | lr 3.00e-04 | grad 1.82 | tok/s 27138
step   1830 | loss 1.5881 | lr 3.00e-04 | grad 1.02 | tok/s 26362
step   1840 | loss 1.6621 | lr 3.00e-04 | grad 1.12 | tok/s 27314
step   1850 | loss 1.6875 | lr 3.00e-04 | grad 1.38 | tok/s 27135
step   1860 | loss 1.6423 | lr 3.00e-04 | grad 1.41 | tok/s 27033
step   1870 | loss 1.6443 | lr 3.00e-04 | grad 1.52 | tok/s 27508
step   1880 | loss 1.7022 | lr 3.00e-04 | grad 1.23 | tok/s 27722
step   1890 | loss 1.5042 | lr 3.00e-04 | grad 1.31 | tok/s 28471
step   1900 | loss 1.4398 | lr 3.00e-04 | grad 1.04 | tok/s 28465
step   1910 | loss 1.4115 | lr 3.00e-04 | grad 0.84 | tok/s 28461
step   1920 | loss 1.3956 | lr 3.00e-04 | grad 1.16 | tok/s 28468
step   1930 | loss 1.4284 | lr 3.00e-04 | grad 1.96 | tok/s 28383
step   1940 | loss 1.7584 | lr 3.00e-04 | grad 1.41 | tok/s 26971
step   1950 | loss 1.6443 | lr 3.00e-04 | grad 0.98 | tok/s 26773
step   1960 | loss 1.7543 | lr 3.00e-04 | grad 1.58 | tok/s 27118
step   1970 | loss 1.7854 | lr 3.00e-04 | grad 1.40 | tok/s 27691
step   1980 | loss 1.6743 | lr 3.00e-04 | grad 1.45 | tok/s 26854
step   1990 | loss 1.8790 | lr 3.00e-04 | grad 2.11 | tok/s 27614
step   2000 | loss 1.4070 | lr 3.00e-04 | grad 1.17 | tok/s 28519
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4070.pt
step   2010 | loss 1.6114 | lr 3.00e-04 | grad 1.70 | tok/s 16135
step   2020 | loss 1.5934 | lr 3.00e-04 | grad 1.36 | tok/s 27084
step   2030 | loss 1.9148 | lr 3.00e-04 | grad 6.47 | tok/s 26770
step   2040 | loss 1.6671 | lr 3.00e-04 | grad 1.40 | tok/s 27258
step   2050 | loss 1.5424 | lr 3.00e-04 | grad 1.09 | tok/s 27611

Training complete! Final step: 2052
