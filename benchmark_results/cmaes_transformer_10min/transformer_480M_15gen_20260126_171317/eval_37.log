Using device: cuda
Output directory: benchmark_results/cmaes_transformer_10min/transformer_480M_15gen_20260126_171317/eval_37/levelllama_100m_20260126_175426
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level llama, 286,131,456 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.3120 | lr 3.00e-04 | grad 2.47 | tok/s 10928
step     20 | loss 2.9382 | lr 3.00e-04 | grad 0.89 | tok/s 24217
step     30 | loss 3.3342 | lr 3.00e-04 | grad 1.70 | tok/s 25653
step     40 | loss 5.5418 | lr 3.00e-04 | grad 6.75 | tok/s 26110
step     50 | loss 4.7555 | lr 3.00e-04 | grad 3.89 | tok/s 26411
step     60 | loss 3.8615 | lr 3.00e-04 | grad 3.14 | tok/s 26363
step     70 | loss 3.3602 | lr 3.00e-04 | grad 3.00 | tok/s 26294
step     80 | loss 3.1261 | lr 3.00e-04 | grad 1.50 | tok/s 26274
step     90 | loss 2.7972 | lr 3.00e-04 | grad 1.81 | tok/s 26217
step    100 | loss 2.5933 | lr 3.00e-04 | grad 2.00 | tok/s 26186
step    110 | loss 2.6442 | lr 3.00e-04 | grad 1.95 | tok/s 25948
step    120 | loss 3.4142 | lr 3.00e-04 | grad 1.04 | tok/s 24684
step    130 | loss 2.6197 | lr 3.00e-04 | grad 2.22 | tok/s 25242
step    140 | loss 2.9257 | lr 3.00e-04 | grad 5.09 | tok/s 25292
step    150 | loss 2.7583 | lr 3.00e-04 | grad 4.50 | tok/s 25909
step    160 | loss 2.8562 | lr 3.00e-04 | grad 1.45 | tok/s 25034
step    170 | loss 2.7106 | lr 3.00e-04 | grad 0.86 | tok/s 24639
step    180 | loss 2.9024 | lr 3.00e-04 | grad 1.35 | tok/s 25216
step    190 | loss 2.4513 | lr 3.00e-04 | grad 1.55 | tok/s 24730
step    200 | loss 2.3821 | lr 3.00e-04 | grad 1.05 | tok/s 25883
step    210 | loss 2.4034 | lr 3.00e-04 | grad 2.62 | tok/s 24521
step    220 | loss 2.6628 | lr 3.00e-04 | grad 3.45 | tok/s 24800
step    230 | loss 2.4576 | lr 3.00e-04 | grad 2.11 | tok/s 24777
step    240 | loss 2.7622 | lr 3.00e-04 | grad 2.08 | tok/s 25096
step    250 | loss 2.2995 | lr 3.00e-04 | grad 1.30 | tok/s 24918
step    260 | loss 2.4408 | lr 3.00e-04 | grad 1.73 | tok/s 25599
step    270 | loss 2.2981 | lr 3.00e-04 | grad 2.08 | tok/s 25014
step    280 | loss 2.2373 | lr 3.00e-04 | grad 0.73 | tok/s 23491
step    290 | loss 2.1665 | lr 3.00e-04 | grad 1.55 | tok/s 24248
step    300 | loss 2.4308 | lr 3.00e-04 | grad 0.98 | tok/s 24435
step    310 | loss 2.1161 | lr 3.00e-04 | grad 1.11 | tok/s 24347
step    320 | loss 2.3796 | lr 3.00e-04 | grad 2.77 | tok/s 24649
step    330 | loss 2.1746 | lr 3.00e-04 | grad 1.14 | tok/s 24891
step    340 | loss 2.4698 | lr 3.00e-04 | grad 1.52 | tok/s 24773
step    350 | loss 2.4425 | lr 3.00e-04 | grad 1.27 | tok/s 25485
step    360 | loss 2.0917 | lr 3.00e-04 | grad 1.13 | tok/s 24369
step    370 | loss 2.1428 | lr 3.00e-04 | grad 0.92 | tok/s 25689
step    380 | loss 1.9663 | lr 3.00e-04 | grad 1.48 | tok/s 25907
step    390 | loss 1.8984 | lr 3.00e-04 | grad 1.29 | tok/s 25915
step    400 | loss 2.2476 | lr 3.00e-04 | grad 0.97 | tok/s 24551
step    410 | loss 2.1472 | lr 3.00e-04 | grad 1.27 | tok/s 24774
step    420 | loss 2.3116 | lr 3.00e-04 | grad 1.33 | tok/s 25846
step    430 | loss 2.1406 | lr 3.00e-04 | grad 1.26 | tok/s 25440
step    440 | loss 2.1494 | lr 3.00e-04 | grad 1.35 | tok/s 24666
step    450 | loss 2.0437 | lr 3.00e-04 | grad 0.82 | tok/s 24898
step    460 | loss 2.1233 | lr 3.00e-04 | grad 0.99 | tok/s 25280
step    470 | loss 2.1179 | lr 3.00e-04 | grad 1.82 | tok/s 25101
step    480 | loss 2.1437 | lr 3.00e-04 | grad 1.49 | tok/s 25659
step    490 | loss 2.0807 | lr 3.00e-04 | grad 1.51 | tok/s 24655
step    500 | loss 2.2133 | lr 3.00e-04 | grad 0.99 | tok/s 25063
step    510 | loss 2.0850 | lr 3.00e-04 | grad 0.89 | tok/s 23917
step    520 | loss 1.9437 | lr 3.00e-04 | grad 0.97 | tok/s 25044
step    530 | loss 2.0882 | lr 3.00e-04 | grad 1.08 | tok/s 24627
step    540 | loss 2.0586 | lr 3.00e-04 | grad 0.86 | tok/s 24118
step    550 | loss 1.7396 | lr 3.00e-04 | grad 1.74 | tok/s 25206
step    560 | loss 1.9294 | lr 3.00e-04 | grad 1.36 | tok/s 25966
step    570 | loss 1.8108 | lr 3.00e-04 | grad 1.40 | tok/s 25993
step    580 | loss 1.7441 | lr 3.00e-04 | grad 0.78 | tok/s 25962
step    590 | loss 1.7923 | lr 3.00e-04 | grad 1.06 | tok/s 25966
step    600 | loss 1.7439 | lr 3.00e-04 | grad 1.23 | tok/s 25971
step    610 | loss 1.7214 | lr 3.00e-04 | grad 1.16 | tok/s 25958
step    620 | loss 1.7030 | lr 3.00e-04 | grad 1.16 | tok/s 25856
step    630 | loss 1.9468 | lr 3.00e-04 | grad 2.38 | tok/s 24425
step    640 | loss 2.0811 | lr 3.00e-04 | grad 1.34 | tok/s 24726
step    650 | loss 1.8989 | lr 3.00e-04 | grad 0.99 | tok/s 24757
step    660 | loss 1.9652 | lr 3.00e-04 | grad 1.20 | tok/s 25659
step    670 | loss 1.9984 | lr 3.00e-04 | grad 2.48 | tok/s 24837
step    680 | loss 1.9822 | lr 3.00e-04 | grad 1.24 | tok/s 24427
step    690 | loss 1.9474 | lr 3.00e-04 | grad 0.98 | tok/s 24216
step    700 | loss 1.8546 | lr 3.00e-04 | grad 0.99 | tok/s 24719
step    710 | loss 1.9933 | lr 3.00e-04 | grad 2.25 | tok/s 24328
step    720 | loss 1.7554 | lr 3.00e-04 | grad 1.34 | tok/s 25291
step    730 | loss 1.8391 | lr 3.00e-04 | grad 0.84 | tok/s 24855
step    740 | loss 2.2742 | lr 3.00e-04 | grad 1.96 | tok/s 25531
step    750 | loss 2.0631 | lr 3.00e-04 | grad 1.22 | tok/s 25886
step    760 | loss 1.8769 | lr 3.00e-04 | grad 2.03 | tok/s 25293
step    770 | loss 1.8840 | lr 3.00e-04 | grad 1.12 | tok/s 24866
step    780 | loss 1.8268 | lr 3.00e-04 | grad 1.33 | tok/s 25050
step    790 | loss 2.1355 | lr 3.00e-04 | grad 2.11 | tok/s 25629
step    800 | loss 1.7137 | lr 3.00e-04 | grad 0.99 | tok/s 25148
step    810 | loss 1.6546 | lr 3.00e-04 | grad 1.74 | tok/s 24330
step    820 | loss 1.7935 | lr 3.00e-04 | grad 1.18 | tok/s 24840
step    830 | loss 1.8454 | lr 3.00e-04 | grad 1.11 | tok/s 24510
step    840 | loss 1.9647 | lr 3.00e-04 | grad 1.09 | tok/s 24395
step    850 | loss 1.9212 | lr 3.00e-04 | grad 1.01 | tok/s 24892
step    860 | loss 1.9775 | lr 3.00e-04 | grad 1.23 | tok/s 25309
step    870 | loss 2.0086 | lr 3.00e-04 | grad 1.09 | tok/s 25533
step    880 | loss 1.8756 | lr 3.00e-04 | grad 0.91 | tok/s 24998
step    890 | loss 1.7624 | lr 3.00e-04 | grad 1.04 | tok/s 24921
step    900 | loss 1.8205 | lr 3.00e-04 | grad 0.99 | tok/s 24827
step    910 | loss 1.8858 | lr 3.00e-04 | grad 3.34 | tok/s 24595
step    920 | loss 1.7719 | lr 3.00e-04 | grad 1.12 | tok/s 24836
step    930 | loss 1.7361 | lr 3.00e-04 | grad 1.05 | tok/s 25146
step    940 | loss 1.7002 | lr 3.00e-04 | grad 1.05 | tok/s 24575
step    950 | loss 1.7879 | lr 3.00e-04 | grad 1.34 | tok/s 24213
step    960 | loss 1.7345 | lr 3.00e-04 | grad 1.09 | tok/s 24840
step    970 | loss 1.7271 | lr 3.00e-04 | grad 1.08 | tok/s 24820
step    980 | loss 2.4789 | lr 3.00e-04 | grad 1.79 | tok/s 25809
step    990 | loss 1.9263 | lr 3.00e-04 | grad 1.30 | tok/s 24762
step   1000 | loss 1.8429 | lr 3.00e-04 | grad 1.21 | tok/s 24837
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8429.pt
step   1010 | loss 1.5828 | lr 3.00e-04 | grad 1.25 | tok/s 15167
step   1020 | loss 1.6685 | lr 3.00e-04 | grad 1.14 | tok/s 26082
step   1030 | loss 1.8263 | lr 3.00e-04 | grad 1.17 | tok/s 24761
step   1040 | loss 2.3017 | lr 3.00e-04 | grad 1.56 | tok/s 25354
step   1050 | loss 1.9127 | lr 3.00e-04 | grad 1.04 | tok/s 25515
step   1060 | loss 1.5660 | lr 3.00e-04 | grad 1.12 | tok/s 25171
step   1070 | loss 1.6599 | lr 3.00e-04 | grad 1.24 | tok/s 25131
step   1080 | loss 1.5630 | lr 3.00e-04 | grad 1.38 | tok/s 25947
step   1090 | loss 1.5386 | lr 3.00e-04 | grad 1.23 | tok/s 25984
step   1100 | loss 1.5087 | lr 3.00e-04 | grad 1.20 | tok/s 26003
step   1110 | loss 1.4483 | lr 3.00e-04 | grad 1.12 | tok/s 25996
step   1120 | loss 1.6648 | lr 3.00e-04 | grad 1.05 | tok/s 25245
step   1130 | loss 2.0902 | lr 3.00e-04 | grad 1.09 | tok/s 25507
step   1140 | loss 2.0549 | lr 3.00e-04 | grad 1.16 | tok/s 25816
step   1150 | loss 2.2203 | lr 3.00e-04 | grad 1.73 | tok/s 25238
step   1160 | loss 1.9379 | lr 3.00e-04 | grad 2.77 | tok/s 24422
step   1170 | loss 1.7887 | lr 3.00e-04 | grad 1.29 | tok/s 24366
step   1180 | loss 1.6913 | lr 3.00e-04 | grad 1.28 | tok/s 25638
step   1190 | loss 1.9973 | lr 3.00e-04 | grad 1.41 | tok/s 25690
step   1200 | loss 1.5305 | lr 3.00e-04 | grad 1.27 | tok/s 25910
step   1210 | loss 1.5935 | lr 3.00e-04 | grad 1.09 | tok/s 24413
step   1220 | loss 1.6756 | lr 3.00e-04 | grad 1.35 | tok/s 25183
step   1230 | loss 1.7193 | lr 3.00e-04 | grad 0.99 | tok/s 25382
step   1240 | loss 1.6016 | lr 3.00e-04 | grad 1.70 | tok/s 25725
step   1250 | loss 1.8194 | lr 3.00e-04 | grad 1.41 | tok/s 25116
step   1260 | loss 1.8969 | lr 3.00e-04 | grad 1.05 | tok/s 25787
step   1270 | loss 1.6597 | lr 3.00e-04 | grad 1.62 | tok/s 25011
step   1280 | loss 1.6707 | lr 3.00e-04 | grad 1.34 | tok/s 25092
step   1290 | loss 1.6653 | lr 3.00e-04 | grad 1.30 | tok/s 24512
step   1300 | loss 1.8943 | lr 3.00e-04 | grad 3.28 | tok/s 24425
step   1310 | loss 1.9136 | lr 3.00e-04 | grad 1.19 | tok/s 25448
step   1320 | loss 1.7584 | lr 3.00e-04 | grad 2.05 | tok/s 25401
step   1330 | loss 1.7944 | lr 3.00e-04 | grad 1.29 | tok/s 25323
step   1340 | loss 1.8523 | lr 3.00e-04 | grad 1.04 | tok/s 24487
step   1350 | loss 1.7294 | lr 3.00e-04 | grad 1.06 | tok/s 25417
step   1360 | loss 1.7748 | lr 3.00e-04 | grad 1.16 | tok/s 23945
step   1370 | loss 1.9011 | lr 3.00e-04 | grad 2.41 | tok/s 25583
step   1380 | loss 1.7555 | lr 3.00e-04 | grad 1.43 | tok/s 24747
step   1390 | loss 1.7284 | lr 3.00e-04 | grad 1.39 | tok/s 25228
step   1400 | loss 1.7628 | lr 3.00e-04 | grad 1.27 | tok/s 24669
step   1410 | loss 1.6162 | lr 3.00e-04 | grad 1.60 | tok/s 24238
step   1420 | loss 1.6158 | lr 3.00e-04 | grad 1.36 | tok/s 25761
step   1430 | loss 2.0230 | lr 3.00e-04 | grad 1.01 | tok/s 24909
step   1440 | loss 1.7274 | lr 3.00e-04 | grad 1.58 | tok/s 25116
step   1450 | loss 1.7293 | lr 3.00e-04 | grad 1.11 | tok/s 25412
step   1460 | loss 1.8039 | lr 3.00e-04 | grad 1.42 | tok/s 24634
step   1470 | loss 1.6445 | lr 3.00e-04 | grad 1.44 | tok/s 24297
step   1480 | loss 1.6649 | lr 3.00e-04 | grad 1.73 | tok/s 25233
step   1490 | loss 1.8690 | lr 3.00e-04 | grad 3.98 | tok/s 25030
step   1500 | loss 1.8936 | lr 3.00e-04 | grad 1.34 | tok/s 25409
step   1510 | loss 1.5716 | lr 3.00e-04 | grad 1.01 | tok/s 24867
step   1520 | loss 1.7006 | lr 3.00e-04 | grad 1.41 | tok/s 24788
step   1530 | loss 1.6442 | lr 3.00e-04 | grad 1.38 | tok/s 25460
step   1540 | loss 1.7329 | lr 3.00e-04 | grad 0.98 | tok/s 25483
step   1550 | loss 1.7172 | lr 3.00e-04 | grad 2.23 | tok/s 25010
step   1560 | loss 1.5154 | lr 3.00e-04 | grad 1.36 | tok/s 25849
step   1570 | loss 1.6112 | lr 3.00e-04 | grad 1.14 | tok/s 25231
step   1580 | loss 1.5279 | lr 3.00e-04 | grad 1.33 | tok/s 25377
step   1590 | loss 1.7071 | lr 3.00e-04 | grad 1.88 | tok/s 24403
step   1600 | loss 1.4831 | lr 3.00e-04 | grad 4.12 | tok/s 25769
step   1610 | loss 2.1150 | lr 3.00e-04 | grad 2.48 | tok/s 25128
step   1620 | loss 2.2965 | lr 3.00e-04 | grad 1.67 | tok/s 25952
step   1630 | loss 2.0854 | lr 3.00e-04 | grad 1.48 | tok/s 25919
step   1640 | loss 1.9344 | lr 3.00e-04 | grad 1.35 | tok/s 25943
step   1650 | loss 1.8599 | lr 3.00e-04 | grad 1.42 | tok/s 25942
step   1660 | loss 1.8024 | lr 3.00e-04 | grad 1.27 | tok/s 25953
step   1670 | loss 1.8585 | lr 3.00e-04 | grad 1.43 | tok/s 25132
step   1680 | loss 1.6579 | lr 3.00e-04 | grad 1.22 | tok/s 24900
step   1690 | loss 1.6778 | lr 3.00e-04 | grad 1.38 | tok/s 24355
step   1700 | loss 1.5551 | lr 3.00e-04 | grad 1.23 | tok/s 25300
step   1710 | loss 1.5280 | lr 3.00e-04 | grad 1.46 | tok/s 25425
step   1720 | loss 1.6600 | lr 3.00e-04 | grad 1.09 | tok/s 24592
step   1730 | loss 1.7221 | lr 3.00e-04 | grad 2.03 | tok/s 25408
step   1740 | loss 1.6516 | lr 3.00e-04 | grad 1.31 | tok/s 25512
step   1750 | loss 1.5383 | lr 3.00e-04 | grad 1.29 | tok/s 24972
step   1760 | loss 1.6230 | lr 3.00e-04 | grad 1.23 | tok/s 24724
step   1770 | loss 1.9104 | lr 3.00e-04 | grad 1.20 | tok/s 25320
step   1780 | loss 1.9364 | lr 3.00e-04 | grad 1.20 | tok/s 23923
step   1790 | loss 1.5311 | lr 3.00e-04 | grad 1.14 | tok/s 24289
step   1800 | loss 1.5728 | lr 3.00e-04 | grad 1.09 | tok/s 24923
step   1810 | loss 1.6350 | lr 3.00e-04 | grad 1.27 | tok/s 25077
step   1820 | loss 1.7418 | lr 3.00e-04 | grad 1.79 | tok/s 24732
step   1830 | loss 1.5853 | lr 3.00e-04 | grad 1.02 | tok/s 24062
step   1840 | loss 1.6527 | lr 3.00e-04 | grad 1.14 | tok/s 24942
step   1850 | loss 1.6777 | lr 3.00e-04 | grad 1.25 | tok/s 24781
step   1860 | loss 1.6320 | lr 3.00e-04 | grad 1.34 | tok/s 24682
step   1870 | loss 1.6436 | lr 3.00e-04 | grad 1.54 | tok/s 25063

Training complete! Final step: 1871
