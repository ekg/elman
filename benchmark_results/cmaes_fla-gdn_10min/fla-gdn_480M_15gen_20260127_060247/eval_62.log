Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_62/levelfla-gdn_100m_20260127_071446
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 467,110,690 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1250 | lr 3.00e-04 | grad 19.00 | tok/s 10717
step     20 | loss 2.8429 | lr 3.00e-04 | grad 6.00 | tok/s 23157
step     30 | loss 3.1237 | lr 3.00e-04 | grad 9.94 | tok/s 24379
step     40 | loss 4.2112 | lr 3.00e-04 | grad 30.38 | tok/s 24811
step     50 | loss 4.3492 | lr 3.00e-04 | grad 16.00 | tok/s 25034
step     60 | loss 3.3665 | lr 3.00e-04 | grad 15.19 | tok/s 24916
step     70 | loss 2.8681 | lr 3.00e-04 | grad 8.81 | tok/s 24787
step     80 | loss 2.5531 | lr 3.00e-04 | grad 7.25 | tok/s 24680
step     90 | loss 2.4227 | lr 3.00e-04 | grad 5.84 | tok/s 24614
step    100 | loss 2.2226 | lr 3.00e-04 | grad 4.56 | tok/s 24510
step    110 | loss 2.2883 | lr 3.00e-04 | grad 4.44 | tok/s 24191
step    120 | loss 2.8525 | lr 3.00e-04 | grad 3.72 | tok/s 22997
step    130 | loss 2.1177 | lr 3.00e-04 | grad 5.25 | tok/s 23463
step    140 | loss 2.3663 | lr 3.00e-04 | grad 8.75 | tok/s 23458
step    150 | loss 1.5557 | lr 3.00e-04 | grad 6.00 | tok/s 24005
step    160 | loss 2.2799 | lr 3.00e-04 | grad 2.34 | tok/s 23126
step    170 | loss 2.2048 | lr 3.00e-04 | grad 2.09 | tok/s 22738
step    180 | loss 1.9774 | lr 3.00e-04 | grad 3.16 | tok/s 23261
step    190 | loss 1.8537 | lr 3.00e-04 | grad 2.28 | tok/s 22765
step    200 | loss 1.5829 | lr 3.00e-04 | grad 1.93 | tok/s 23734
step    210 | loss 1.7845 | lr 3.00e-04 | grad 4.16 | tok/s 22556
step    220 | loss 2.1578 | lr 3.00e-04 | grad 4.59 | tok/s 22715
step    230 | loss 1.9131 | lr 3.00e-04 | grad 2.66 | tok/s 22673
step    240 | loss 2.2019 | lr 3.00e-04 | grad 5.66 | tok/s 22901
step    250 | loss 1.6721 | lr 3.00e-04 | grad 1.50 | tok/s 22722
step    260 | loss 1.8068 | lr 3.00e-04 | grad 3.03 | tok/s 23292
step    270 | loss 1.7360 | lr 3.00e-04 | grad 1.80 | tok/s 22780
step    280 | loss 1.6858 | lr 3.00e-04 | grad 1.75 | tok/s 21394
step    290 | loss 1.5762 | lr 3.00e-04 | grad 2.20 | tok/s 22082
step    300 | loss 1.8692 | lr 3.00e-04 | grad 2.39 | tok/s 22323
step    310 | loss 1.5871 | lr 3.00e-04 | grad 1.72 | tok/s 22158
step    320 | loss 1.7958 | lr 3.00e-04 | grad 4.09 | tok/s 22376
step    330 | loss 1.6335 | lr 3.00e-04 | grad 1.80 | tok/s 22567
step    340 | loss 1.9609 | lr 3.00e-04 | grad 2.25 | tok/s 22475
step    350 | loss 1.6723 | lr 3.00e-04 | grad 1.85 | tok/s 23071
step    360 | loss 1.4893 | lr 3.00e-04 | grad 1.95 | tok/s 22089
step    370 | loss 1.4179 | lr 3.00e-04 | grad 1.61 | tok/s 23247
step    380 | loss 1.1476 | lr 3.00e-04 | grad 1.67 | tok/s 23461
step    390 | loss 1.0430 | lr 3.00e-04 | grad 1.27 | tok/s 23435
step    400 | loss 1.6727 | lr 3.00e-04 | grad 1.56 | tok/s 22214
step    410 | loss 1.6726 | lr 3.00e-04 | grad 2.47 | tok/s 22382
step    420 | loss 1.5793 | lr 3.00e-04 | grad 2.58 | tok/s 23327
step    430 | loss 1.4957 | lr 3.00e-04 | grad 1.71 | tok/s 22935
step    440 | loss 1.6170 | lr 3.00e-04 | grad 2.19 | tok/s 22198
step    450 | loss 1.5501 | lr 3.00e-04 | grad 1.35 | tok/s 22447
step    460 | loss 1.5375 | lr 3.00e-04 | grad 1.74 | tok/s 22755
step    470 | loss 1.5026 | lr 3.00e-04 | grad 3.16 | tok/s 22619
step    480 | loss 1.5450 | lr 3.00e-04 | grad 2.59 | tok/s 23084
step    490 | loss 1.6068 | lr 3.00e-04 | grad 2.22 | tok/s 22205
step    500 | loss 1.7261 | lr 3.00e-04 | grad 1.48 | tok/s 22595
step    510 | loss 1.6168 | lr 3.00e-04 | grad 1.28 | tok/s 21559
step    520 | loss 1.4637 | lr 3.00e-04 | grad 5.34 | tok/s 22553
step    530 | loss 1.6398 | lr 3.00e-04 | grad 2.02 | tok/s 22160
step    540 | loss 1.5320 | lr 3.00e-04 | grad 1.49 | tok/s 21708
step    550 | loss 1.2783 | lr 3.00e-04 | grad 2.80 | tok/s 22754
step    560 | loss 1.3802 | lr 3.00e-04 | grad 1.57 | tok/s 23332
step    570 | loss 1.2813 | lr 3.00e-04 | grad 1.58 | tok/s 23323
step    580 | loss 1.2419 | lr 3.00e-04 | grad 1.20 | tok/s 23294
step    590 | loss 1.2811 | lr 3.00e-04 | grad 1.22 | tok/s 23304
step    600 | loss 1.2068 | lr 3.00e-04 | grad 1.38 | tok/s 23243
step    610 | loss 1.2498 | lr 3.00e-04 | grad 1.53 | tok/s 23297
step    620 | loss 1.2274 | lr 3.00e-04 | grad 1.58 | tok/s 23207
step    630 | loss 1.5684 | lr 3.00e-04 | grad 5.91 | tok/s 21938
step    640 | loss 1.6607 | lr 3.00e-04 | grad 1.68 | tok/s 22222
step    650 | loss 1.4807 | lr 3.00e-04 | grad 1.55 | tok/s 22228
step    660 | loss 1.5335 | lr 3.00e-04 | grad 1.81 | tok/s 23032
step    670 | loss 1.5440 | lr 3.00e-04 | grad 4.50 | tok/s 22280
step    680 | loss 1.5609 | lr 3.00e-04 | grad 1.98 | tok/s 21944
step    690 | loss 1.4904 | lr 3.00e-04 | grad 1.80 | tok/s 21791
step    700 | loss 1.4157 | lr 3.00e-04 | grad 1.31 | tok/s 22233
step    710 | loss 1.5323 | lr 3.00e-04 | grad 3.27 | tok/s 21862
step    720 | loss 1.2461 | lr 3.00e-04 | grad 1.41 | tok/s 22735
step    730 | loss 1.3755 | lr 3.00e-04 | grad 1.27 | tok/s 22356
step    740 | loss 1.7029 | lr 3.00e-04 | grad 3.94 | tok/s 22936
step    750 | loss 1.4765 | lr 3.00e-04 | grad 1.66 | tok/s 23191
step    760 | loss 1.4434 | lr 3.00e-04 | grad 3.42 | tok/s 22747
step    770 | loss 1.5037 | lr 3.00e-04 | grad 1.69 | tok/s 22361
step    780 | loss 1.4180 | lr 3.00e-04 | grad 1.65 | tok/s 22510
step    790 | loss 1.5631 | lr 3.00e-04 | grad 5.03 | tok/s 23012
step    800 | loss 1.2675 | lr 3.00e-04 | grad 1.74 | tok/s 22654
step    810 | loss 1.2710 | lr 3.00e-04 | grad 2.64 | tok/s 21916
step    820 | loss 1.3561 | lr 3.00e-04 | grad 1.59 | tok/s 22338
step    830 | loss 1.4343 | lr 3.00e-04 | grad 1.21 | tok/s 22014
step    840 | loss 1.5378 | lr 3.00e-04 | grad 1.48 | tok/s 21951
step    850 | loss 1.4576 | lr 3.00e-04 | grad 1.33 | tok/s 22377
step    860 | loss 1.5016 | lr 3.00e-04 | grad 2.22 | tok/s 22751
step    870 | loss 1.3244 | lr 3.00e-04 | grad 1.59 | tok/s 22902
step    880 | loss 1.5275 | lr 3.00e-04 | grad 1.54 | tok/s 22473
step    890 | loss 1.4292 | lr 3.00e-04 | grad 1.19 | tok/s 22347
step    900 | loss 1.4678 | lr 3.00e-04 | grad 1.43 | tok/s 22289
step    910 | loss 1.4471 | lr 3.00e-04 | grad 6.72 | tok/s 22040
step    920 | loss 1.4036 | lr 3.00e-04 | grad 1.51 | tok/s 22254
step    930 | loss 1.3280 | lr 3.00e-04 | grad 1.61 | tok/s 22531
step    940 | loss 1.2932 | lr 3.00e-04 | grad 1.48 | tok/s 22062
step    950 | loss 1.4382 | lr 3.00e-04 | grad 2.08 | tok/s 21677
step    960 | loss 1.3847 | lr 3.00e-04 | grad 1.34 | tok/s 22268
step    970 | loss 1.4205 | lr 3.00e-04 | grad 1.50 | tok/s 22302
step    980 | loss 1.8394 | lr 3.00e-04 | grad 3.50 | tok/s 23192
step    990 | loss 1.5069 | lr 3.00e-04 | grad 1.55 | tok/s 22279
step   1000 | loss 1.5020 | lr 3.00e-04 | grad 2.00 | tok/s 22310
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5020.pt
step   1010 | loss 1.2996 | lr 3.00e-04 | grad 2.44 | tok/s 12573
step   1020 | loss 1.1687 | lr 3.00e-04 | grad 1.20 | tok/s 23568
step   1030 | loss 1.4796 | lr 3.00e-04 | grad 3.27 | tok/s 22288
step   1040 | loss 2.0295 | lr 3.00e-04 | grad 3.88 | tok/s 22810
step   1050 | loss 1.4000 | lr 3.00e-04 | grad 2.41 | tok/s 22974
step   1060 | loss 1.0981 | lr 3.00e-04 | grad 3.45 | tok/s 22818
step   1070 | loss 1.3631 | lr 3.00e-04 | grad 1.71 | tok/s 22627
step   1080 | loss 1.2190 | lr 3.00e-04 | grad 1.18 | tok/s 23374
step   1090 | loss 1.1804 | lr 3.00e-04 | grad 1.14 | tok/s 23381
step   1100 | loss 1.1656 | lr 3.00e-04 | grad 1.19 | tok/s 23357
step   1110 | loss 1.1084 | lr 3.00e-04 | grad 1.22 | tok/s 23356
step   1120 | loss 1.3896 | lr 3.00e-04 | grad 3.25 | tok/s 22730
step   1130 | loss 1.5511 | lr 3.00e-04 | grad 1.37 | tok/s 22978
step   1140 | loss 1.6880 | lr 3.00e-04 | grad 1.68 | tok/s 23229
step   1150 | loss 1.5840 | lr 3.00e-04 | grad 1.73 | tok/s 22481
step   1160 | loss 1.6592 | lr 3.00e-04 | grad 1.92 | tok/s 22166
step   1170 | loss 1.4318 | lr 3.00e-04 | grad 1.72 | tok/s 21888
step   1180 | loss 1.2867 | lr 3.00e-04 | grad 2.50 | tok/s 22990
step   1190 | loss 1.5177 | lr 3.00e-04 | grad 2.25 | tok/s 23186
step   1200 | loss 1.0654 | lr 3.00e-04 | grad 1.90 | tok/s 23269
step   1210 | loss 1.3251 | lr 3.00e-04 | grad 1.55 | tok/s 21749
step   1220 | loss 1.3173 | lr 3.00e-04 | grad 1.75 | tok/s 22792
step   1230 | loss 1.2803 | lr 3.00e-04 | grad 1.24 | tok/s 22842
step   1240 | loss 1.2539 | lr 3.00e-04 | grad 1.38 | tok/s 22914
step   1250 | loss 1.4066 | lr 3.00e-04 | grad 2.31 | tok/s 22539
step   1260 | loss 1.3236 | lr 3.00e-04 | grad 1.81 | tok/s 23061
step   1270 | loss 1.3105 | lr 3.00e-04 | grad 1.62 | tok/s 22394
step   1280 | loss 1.3300 | lr 3.00e-04 | grad 1.53 | tok/s 22141
step   1290 | loss 1.2840 | lr 3.00e-04 | grad 1.81 | tok/s 22235
step   1300 | loss 1.5733 | lr 3.00e-04 | grad 4.78 | tok/s 21894
step   1310 | loss 1.4120 | lr 3.00e-04 | grad 1.46 | tok/s 22742
step   1320 | loss 1.4114 | lr 3.00e-04 | grad 1.66 | tok/s 22759
step   1330 | loss 1.3712 | lr 3.00e-04 | grad 1.36 | tok/s 22577
step   1340 | loss 1.4652 | lr 3.00e-04 | grad 1.92 | tok/s 22102
step   1350 | loss 1.3485 | lr 3.00e-04 | grad 1.23 | tok/s 22562
step   1360 | loss 1.4038 | lr 3.00e-04 | grad 1.45 | tok/s 21731
step   1370 | loss 1.5018 | lr 3.00e-04 | grad 2.28 | tok/s 22879
step   1380 | loss 1.3799 | lr 3.00e-04 | grad 1.40 | tok/s 21916
step   1390 | loss 1.2585 | lr 3.00e-04 | grad 2.19 | tok/s 22933
step   1400 | loss 1.4401 | lr 3.00e-04 | grad 1.52 | tok/s 22161
step   1410 | loss 1.3573 | lr 3.00e-04 | grad 3.06 | tok/s 21645
step   1420 | loss 1.0553 | lr 3.00e-04 | grad 3.62 | tok/s 23082
step   1430 | loss 1.5926 | lr 3.00e-04 | grad 1.61 | tok/s 22279
step   1440 | loss 1.3715 | lr 3.00e-04 | grad 1.83 | tok/s 22841
step   1450 | loss 1.3593 | lr 3.00e-04 | grad 5.44 | tok/s 22799
step   1460 | loss 1.4971 | lr 3.00e-04 | grad 3.42 | tok/s 22190
step   1470 | loss 1.2935 | lr 3.00e-04 | grad 1.42 | tok/s 21638
step   1480 | loss 1.2983 | lr 3.00e-04 | grad 1.26 | tok/s 22812
step   1490 | loss 1.7756 | lr 3.00e-04 | grad 9.44 | tok/s 22467
step   1500 | loss 1.3921 | lr 3.00e-04 | grad 1.58 | tok/s 22491
step   1510 | loss 1.2016 | lr 3.00e-04 | grad 1.48 | tok/s 22541
step   1520 | loss 1.4077 | lr 3.00e-04 | grad 1.41 | tok/s 22375
step   1530 | loss 1.3270 | lr 3.00e-04 | grad 1.46 | tok/s 21864
step   1540 | loss 1.4400 | lr 3.00e-04 | grad 1.27 | tok/s 22899
step   1550 | loss 1.4100 | lr 3.00e-04 | grad 1.72 | tok/s 22445
step   1560 | loss 1.0735 | lr 3.00e-04 | grad 1.62 | tok/s 23305
step   1570 | loss 1.2242 | lr 3.00e-04 | grad 1.34 | tok/s 22677
step   1580 | loss 1.2219 | lr 3.00e-04 | grad 1.87 | tok/s 22535
step   1590 | loss 1.3815 | lr 3.00e-04 | grad 1.51 | tok/s 22103
step   1600 | loss 1.2545 | lr 3.00e-04 | grad 2.25 | tok/s 22975
step   1610 | loss 1.8645 | lr 3.00e-04 | grad 2.67 | tok/s 22761
step   1620 | loss 1.9184 | lr 3.00e-04 | grad 1.84 | tok/s 23292
step   1630 | loss 1.6023 | lr 3.00e-04 | grad 2.09 | tok/s 23272
step   1640 | loss 1.4330 | lr 3.00e-04 | grad 2.34 | tok/s 23309
step   1650 | loss 1.3346 | lr 3.00e-04 | grad 1.95 | tok/s 23298
step   1660 | loss 1.2941 | lr 3.00e-04 | grad 1.71 | tok/s 23330
step   1670 | loss 1.4408 | lr 3.00e-04 | grad 2.14 | tok/s 22599
step   1680 | loss 1.3522 | lr 3.00e-04 | grad 1.36 | tok/s 22415

Training complete! Final step: 1684
