Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_104/levelfla-gdn_100m_20260127_080607
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 581,086,496 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2783 | lr 3.00e-04 | grad 14.62 | tok/s 9590
step     20 | loss 2.7629 | lr 3.00e-04 | grad 5.56 | tok/s 18761
step     30 | loss 3.0496 | lr 3.00e-04 | grad 7.94 | tok/s 19724
step     40 | loss 4.1814 | lr 3.00e-04 | grad 25.62 | tok/s 20017
step     50 | loss 3.9833 | lr 3.00e-04 | grad 11.06 | tok/s 20188
step     60 | loss 3.2954 | lr 3.00e-04 | grad 11.38 | tok/s 20119
step     70 | loss 2.7421 | lr 3.00e-04 | grad 7.91 | tok/s 20030
step     80 | loss 2.4366 | lr 3.00e-04 | grad 7.66 | tok/s 19965
step     90 | loss 2.2947 | lr 3.00e-04 | grad 4.62 | tok/s 19947
step    100 | loss 2.0774 | lr 3.00e-04 | grad 3.72 | tok/s 19903
step    110 | loss 2.0904 | lr 3.00e-04 | grad 4.78 | tok/s 19792
step    120 | loss 2.7004 | lr 3.00e-04 | grad 4.19 | tok/s 18853
step    130 | loss 1.9868 | lr 3.00e-04 | grad 5.88 | tok/s 19253
step    140 | loss 2.2589 | lr 3.00e-04 | grad 8.12 | tok/s 19317
step    150 | loss 1.4011 | lr 3.00e-04 | grad 7.44 | tok/s 19762
step    160 | loss 2.1967 | lr 3.00e-04 | grad 3.03 | tok/s 19095
step    170 | loss 2.1535 | lr 3.00e-04 | grad 2.70 | tok/s 18797
step    180 | loss 1.6798 | lr 3.00e-04 | grad 3.86 | tok/s 19263
step    190 | loss 1.7824 | lr 3.00e-04 | grad 3.02 | tok/s 18898
step    200 | loss 1.5234 | lr 3.00e-04 | grad 2.27 | tok/s 19746
step    210 | loss 1.7301 | lr 3.00e-04 | grad 4.62 | tok/s 18744
step    220 | loss 2.1049 | lr 3.00e-04 | grad 5.66 | tok/s 18973
step    230 | loss 1.8154 | lr 3.00e-04 | grad 3.33 | tok/s 18927
step    240 | loss 2.1553 | lr 3.00e-04 | grad 6.25 | tok/s 19141
step    250 | loss 1.6381 | lr 3.00e-04 | grad 1.89 | tok/s 19028
step    260 | loss 1.7716 | lr 3.00e-04 | grad 3.31 | tok/s 19546
step    270 | loss 1.7054 | lr 3.00e-04 | grad 2.55 | tok/s 19127
step    280 | loss 1.6582 | lr 3.00e-04 | grad 2.36 | tok/s 17946
step    290 | loss 1.5558 | lr 3.00e-04 | grad 2.61 | tok/s 18568
step    300 | loss 1.8374 | lr 3.00e-04 | grad 2.47 | tok/s 18719
step    310 | loss 1.5654 | lr 3.00e-04 | grad 2.09 | tok/s 18611
step    320 | loss 1.7659 | lr 3.00e-04 | grad 4.19 | tok/s 18850
step    330 | loss 1.6109 | lr 3.00e-04 | grad 2.31 | tok/s 19052
step    340 | loss 1.9092 | lr 3.00e-04 | grad 2.94 | tok/s 18992
step    350 | loss 1.6079 | lr 3.00e-04 | grad 2.28 | tok/s 19513
step    360 | loss 1.4771 | lr 3.00e-04 | grad 2.14 | tok/s 18727
step    370 | loss 1.3878 | lr 3.00e-04 | grad 1.78 | tok/s 19693
step    380 | loss 1.1164 | lr 3.00e-04 | grad 1.84 | tok/s 19858
step    390 | loss 1.0226 | lr 3.00e-04 | grad 1.58 | tok/s 19863
step    400 | loss 1.6543 | lr 3.00e-04 | grad 1.94 | tok/s 18826
step    410 | loss 1.6541 | lr 3.00e-04 | grad 2.83 | tok/s 18977
step    420 | loss 1.5238 | lr 3.00e-04 | grad 2.70 | tok/s 19761
step    430 | loss 1.4710 | lr 3.00e-04 | grad 2.23 | tok/s 19403
step    440 | loss 1.5982 | lr 3.00e-04 | grad 2.83 | tok/s 18859
step    450 | loss 1.5307 | lr 3.00e-04 | grad 1.66 | tok/s 19058
step    460 | loss 1.5200 | lr 3.00e-04 | grad 1.89 | tok/s 19286
step    470 | loss 1.4829 | lr 3.00e-04 | grad 3.97 | tok/s 19183
step    480 | loss 1.5390 | lr 3.00e-04 | grad 3.23 | tok/s 19604
step    490 | loss 1.5930 | lr 3.00e-04 | grad 2.69 | tok/s 18843
step    500 | loss 1.6970 | lr 3.00e-04 | grad 1.66 | tok/s 19127
step    510 | loss 1.6051 | lr 3.00e-04 | grad 1.58 | tok/s 18310
step    520 | loss 1.4517 | lr 3.00e-04 | grad 2.88 | tok/s 19171
step    530 | loss 1.6190 | lr 3.00e-04 | grad 2.08 | tok/s 18871
step    540 | loss 1.5025 | lr 3.00e-04 | grad 1.74 | tok/s 18500
step    550 | loss 1.2693 | lr 3.00e-04 | grad 3.84 | tok/s 19384
step    560 | loss 1.3736 | lr 3.00e-04 | grad 2.30 | tok/s 19889
step    570 | loss 1.2726 | lr 3.00e-04 | grad 2.08 | tok/s 19896
step    580 | loss 1.2331 | lr 3.00e-04 | grad 1.56 | tok/s 19909
step    590 | loss 1.2679 | lr 3.00e-04 | grad 1.68 | tok/s 19923
step    600 | loss 1.1935 | lr 3.00e-04 | grad 1.75 | tok/s 19909
step    610 | loss 1.2377 | lr 3.00e-04 | grad 1.81 | tok/s 19906
step    620 | loss 1.2175 | lr 3.00e-04 | grad 2.28 | tok/s 19805
step    630 | loss 1.5248 | lr 3.00e-04 | grad 6.66 | tok/s 18756
step    640 | loss 1.6489 | lr 3.00e-04 | grad 2.12 | tok/s 18979
step    650 | loss 1.4645 | lr 3.00e-04 | grad 1.91 | tok/s 18968
step    660 | loss 1.5238 | lr 3.00e-04 | grad 2.23 | tok/s 19666
step    670 | loss 1.5318 | lr 3.00e-04 | grad 5.19 | tok/s 19063
step    680 | loss 1.5408 | lr 3.00e-04 | grad 2.64 | tok/s 18716
step    690 | loss 1.4801 | lr 3.00e-04 | grad 1.91 | tok/s 18630
step    700 | loss 1.3926 | lr 3.00e-04 | grad 1.47 | tok/s 19014
step    710 | loss 1.5281 | lr 3.00e-04 | grad 4.34 | tok/s 18739
step    720 | loss 1.2355 | lr 3.00e-04 | grad 1.88 | tok/s 19417
step    730 | loss 1.3632 | lr 3.00e-04 | grad 1.69 | tok/s 19104
step    740 | loss 1.6693 | lr 3.00e-04 | grad 4.78 | tok/s 19620
step    750 | loss 1.4380 | lr 3.00e-04 | grad 1.94 | tok/s 19821
step    760 | loss 1.4291 | lr 3.00e-04 | grad 4.53 | tok/s 19473
step    770 | loss 1.4882 | lr 3.00e-04 | grad 2.08 | tok/s 19101
step    780 | loss 1.4079 | lr 3.00e-04 | grad 1.87 | tok/s 19236
step    790 | loss 1.5281 | lr 3.00e-04 | grad 5.78 | tok/s 19662
step    800 | loss 1.2488 | lr 3.00e-04 | grad 2.16 | tok/s 19336
step    810 | loss 1.2354 | lr 3.00e-04 | grad 2.58 | tok/s 18715
step    820 | loss 1.3364 | lr 3.00e-04 | grad 1.84 | tok/s 19061
step    830 | loss 1.4118 | lr 3.00e-04 | grad 1.59 | tok/s 18828
step    840 | loss 1.5209 | lr 3.00e-04 | grad 2.05 | tok/s 18729
step    850 | loss 1.4416 | lr 3.00e-04 | grad 1.55 | tok/s 19094
step    860 | loss 1.4761 | lr 3.00e-04 | grad 2.72 | tok/s 19407
step    870 | loss 1.2829 | lr 3.00e-04 | grad 1.91 | tok/s 19559
step    880 | loss 1.5109 | lr 3.00e-04 | grad 2.03 | tok/s 19206
step    890 | loss 1.4181 | lr 3.00e-04 | grad 1.60 | tok/s 19092
step    900 | loss 1.4576 | lr 3.00e-04 | grad 1.59 | tok/s 19017
step    910 | loss 1.4160 | lr 3.00e-04 | grad 7.88 | tok/s 18811
step    920 | loss 1.3900 | lr 3.00e-04 | grad 2.02 | tok/s 19027
step    930 | loss 1.3190 | lr 3.00e-04 | grad 1.94 | tok/s 19266
step    940 | loss 1.2758 | lr 3.00e-04 | grad 1.62 | tok/s 18847
step    950 | loss 1.4244 | lr 3.00e-04 | grad 2.67 | tok/s 18573
step    960 | loss 1.3687 | lr 3.00e-04 | grad 1.91 | tok/s 19018
step    970 | loss 1.4101 | lr 3.00e-04 | grad 1.80 | tok/s 19043
step    980 | loss 1.7921 | lr 3.00e-04 | grad 4.56 | tok/s 19769
step    990 | loss 1.4835 | lr 3.00e-04 | grad 1.78 | tok/s 18976
step   1000 | loss 1.4844 | lr 3.00e-04 | grad 2.88 | tok/s 19052
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4844.pt
step   1010 | loss 1.2867 | lr 3.00e-04 | grad 1.91 | tok/s 9605
step   1020 | loss 1.1382 | lr 3.00e-04 | grad 1.51 | tok/s 20206
step   1030 | loss 1.5180 | lr 3.00e-04 | grad 1.91 | tok/s 19029
step   1040 | loss 2.0136 | lr 3.00e-04 | grad 3.34 | tok/s 19609
step   1050 | loss 1.3674 | lr 3.00e-04 | grad 2.03 | tok/s 19345
step   1060 | loss 1.0322 | lr 3.00e-04 | grad 1.45 | tok/s 19734
step   1070 | loss 1.3863 | lr 3.00e-04 | grad 1.74 | tok/s 19282
step   1080 | loss 1.1986 | lr 3.00e-04 | grad 2.12 | tok/s 19928
step   1090 | loss 1.1792 | lr 3.00e-04 | grad 1.72 | tok/s 19935
step   1100 | loss 1.1358 | lr 3.00e-04 | grad 2.02 | tok/s 19919
step   1110 | loss 1.1214 | lr 3.00e-04 | grad 1.91 | tok/s 19878
step   1120 | loss 1.4073 | lr 3.00e-04 | grad 3.42 | tok/s 19364
step   1130 | loss 1.5264 | lr 3.00e-04 | grad 2.58 | tok/s 19534
step   1140 | loss 1.6179 | lr 3.00e-04 | grad 1.45 | tok/s 19772
step   1150 | loss 1.5561 | lr 3.00e-04 | grad 3.42 | tok/s 18809
step   1160 | loss 1.6610 | lr 3.00e-04 | grad 7.34 | tok/s 19080
step   1170 | loss 1.3548 | lr 3.00e-04 | grad 2.16 | tok/s 18807
step   1180 | loss 1.2741 | lr 3.00e-04 | grad 2.14 | tok/s 19498
step   1190 | loss 1.4332 | lr 3.00e-04 | grad 7.41 | tok/s 19890
step   1200 | loss 1.0642 | lr 3.00e-04 | grad 3.59 | tok/s 19920
step   1210 | loss 1.3745 | lr 3.00e-04 | grad 2.44 | tok/s 18568
step   1220 | loss 1.2788 | lr 3.00e-04 | grad 1.58 | tok/s 19285
step   1230 | loss 1.2441 | lr 3.00e-04 | grad 1.32 | tok/s 19750
step   1240 | loss 1.2577 | lr 3.00e-04 | grad 1.48 | tok/s 19457
step   1250 | loss 1.3835 | lr 3.00e-04 | grad 4.06 | tok/s 19476
step   1260 | loss 1.2936 | lr 3.00e-04 | grad 2.25 | tok/s 19617
step   1270 | loss 1.2926 | lr 3.00e-04 | grad 1.63 | tok/s 19275
step   1280 | loss 1.3417 | lr 3.00e-04 | grad 2.05 | tok/s 18972
step   1290 | loss 1.2486 | lr 3.00e-04 | grad 1.70 | tok/s 19048
step   1300 | loss 1.5100 | lr 3.00e-04 | grad 5.03 | tok/s 18727
step   1310 | loss 1.4018 | lr 3.00e-04 | grad 1.80 | tok/s 19384
step   1320 | loss 1.4291 | lr 3.00e-04 | grad 3.34 | tok/s 19526
step   1330 | loss 1.3206 | lr 3.00e-04 | grad 2.25 | tok/s 19267
step   1340 | loss 1.4557 | lr 3.00e-04 | grad 1.91 | tok/s 18941
step   1350 | loss 1.3770 | lr 3.00e-04 | grad 4.75 | tok/s 19172
step   1360 | loss 1.3177 | lr 3.00e-04 | grad 1.84 | tok/s 18700
step   1370 | loss 1.5542 | lr 3.00e-04 | grad 2.78 | tok/s 19523
step   1380 | loss 1.3338 | lr 3.00e-04 | grad 2.58 | tok/s 18649
step   1390 | loss 1.2320 | lr 3.00e-04 | grad 2.50 | tok/s 19629
step   1400 | loss 1.3723 | lr 3.00e-04 | grad 1.45 | tok/s 18993
step   1410 | loss 1.3520 | lr 3.00e-04 | grad 6.41 | tok/s 18569
step   1420 | loss 1.1521 | lr 3.00e-04 | grad 7.03 | tok/s 19710

Training complete! Final step: 1424
