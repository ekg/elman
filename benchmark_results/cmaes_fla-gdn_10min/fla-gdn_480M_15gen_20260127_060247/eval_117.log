Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_117/levelfla-gdn_100m_20260127_082641
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,801,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1618 | lr 3.00e-04 | grad 15.69 | tok/s 10512
step     20 | loss 2.8090 | lr 3.00e-04 | grad 6.84 | tok/s 22483
step     30 | loss 3.0810 | lr 3.00e-04 | grad 5.44 | tok/s 23688
step     40 | loss 4.2199 | lr 3.00e-04 | grad 24.88 | tok/s 24176
step     50 | loss 4.1098 | lr 3.00e-04 | grad 12.38 | tok/s 24398
step     60 | loss 3.2965 | lr 3.00e-04 | grad 12.12 | tok/s 24327
step     70 | loss 2.7678 | lr 3.00e-04 | grad 7.97 | tok/s 24180
step     80 | loss 2.4646 | lr 3.00e-04 | grad 6.97 | tok/s 24132
step     90 | loss 2.3421 | lr 3.00e-04 | grad 4.78 | tok/s 24056
step    100 | loss 2.1629 | lr 3.00e-04 | grad 4.38 | tok/s 23971
step    110 | loss 2.1752 | lr 3.00e-04 | grad 4.84 | tok/s 23774
step    120 | loss 2.8103 | lr 3.00e-04 | grad 3.28 | tok/s 22677
step    130 | loss 2.0476 | lr 3.00e-04 | grad 5.97 | tok/s 23176
step    140 | loss 2.3081 | lr 3.00e-04 | grad 8.75 | tok/s 23223
step    150 | loss 1.4667 | lr 3.00e-04 | grad 6.47 | tok/s 23789
step    160 | loss 2.2458 | lr 3.00e-04 | grad 2.77 | tok/s 22986
step    170 | loss 2.1690 | lr 3.00e-04 | grad 2.30 | tok/s 22666
step    180 | loss 1.8032 | lr 3.00e-04 | grad 3.69 | tok/s 23182
step    190 | loss 1.8150 | lr 3.00e-04 | grad 2.50 | tok/s 22732
step    200 | loss 1.5424 | lr 3.00e-04 | grad 2.03 | tok/s 23757
step    210 | loss 1.7547 | lr 3.00e-04 | grad 4.12 | tok/s 22566
step    220 | loss 2.1332 | lr 3.00e-04 | grad 6.47 | tok/s 22777
step    230 | loss 1.8678 | lr 3.00e-04 | grad 2.75 | tok/s 22735
step    240 | loss 2.1853 | lr 3.00e-04 | grad 5.66 | tok/s 23038
step    250 | loss 1.6512 | lr 3.00e-04 | grad 1.67 | tok/s 22845
step    260 | loss 1.7844 | lr 3.00e-04 | grad 3.17 | tok/s 23490
step    270 | loss 1.7181 | lr 3.00e-04 | grad 2.19 | tok/s 22955
step    280 | loss 1.6691 | lr 3.00e-04 | grad 2.06 | tok/s 21577
step    290 | loss 1.5637 | lr 3.00e-04 | grad 2.52 | tok/s 22316
step    300 | loss 1.8517 | lr 3.00e-04 | grad 2.64 | tok/s 22518
step    310 | loss 1.5718 | lr 3.00e-04 | grad 1.94 | tok/s 22445
step    320 | loss 1.7751 | lr 3.00e-04 | grad 4.41 | tok/s 22688
step    330 | loss 1.6119 | lr 3.00e-04 | grad 2.08 | tok/s 22895
step    340 | loss 1.9548 | lr 3.00e-04 | grad 2.36 | tok/s 22816
step    350 | loss 1.6381 | lr 3.00e-04 | grad 2.02 | tok/s 23428
step    360 | loss 1.4833 | lr 3.00e-04 | grad 2.08 | tok/s 22413
step    370 | loss 1.3950 | lr 3.00e-04 | grad 1.71 | tok/s 23626
step    380 | loss 1.1247 | lr 3.00e-04 | grad 1.71 | tok/s 23837
step    390 | loss 1.0284 | lr 3.00e-04 | grad 1.42 | tok/s 23831
step    400 | loss 1.6590 | lr 3.00e-04 | grad 1.72 | tok/s 22581
step    410 | loss 1.6530 | lr 3.00e-04 | grad 2.61 | tok/s 22788
step    420 | loss 1.5461 | lr 3.00e-04 | grad 2.81 | tok/s 23723
step    430 | loss 1.4981 | lr 3.00e-04 | grad 1.95 | tok/s 23392
step    440 | loss 1.6048 | lr 3.00e-04 | grad 2.47 | tok/s 22653
step    450 | loss 1.5366 | lr 3.00e-04 | grad 1.50 | tok/s 22890
step    460 | loss 1.5263 | lr 3.00e-04 | grad 1.91 | tok/s 23226
step    470 | loss 1.4885 | lr 3.00e-04 | grad 3.75 | tok/s 23059
step    480 | loss 1.5402 | lr 3.00e-04 | grad 2.84 | tok/s 23571
step    490 | loss 1.5933 | lr 3.00e-04 | grad 2.44 | tok/s 22657
step    500 | loss 1.7065 | lr 3.00e-04 | grad 1.57 | tok/s 23033
step    510 | loss 1.6034 | lr 3.00e-04 | grad 1.38 | tok/s 22042
step    520 | loss 1.4574 | lr 3.00e-04 | grad 2.48 | tok/s 23057
step    530 | loss 1.6293 | lr 3.00e-04 | grad 2.12 | tok/s 22705
step    540 | loss 1.5159 | lr 3.00e-04 | grad 1.66 | tok/s 22231
step    550 | loss 1.2785 | lr 3.00e-04 | grad 3.28 | tok/s 23284
step    560 | loss 1.3745 | lr 3.00e-04 | grad 1.94 | tok/s 23845
step    570 | loss 1.2757 | lr 3.00e-04 | grad 1.80 | tok/s 23812
step    580 | loss 1.2361 | lr 3.00e-04 | grad 1.35 | tok/s 23832
step    590 | loss 1.2699 | lr 3.00e-04 | grad 1.44 | tok/s 23867
step    600 | loss 1.1984 | lr 3.00e-04 | grad 1.57 | tok/s 23856
step    610 | loss 1.2424 | lr 3.00e-04 | grad 1.92 | tok/s 23915
step    620 | loss 1.2226 | lr 3.00e-04 | grad 1.78 | tok/s 23807
step    630 | loss 1.5595 | lr 3.00e-04 | grad 7.31 | tok/s 22534
step    640 | loss 1.6595 | lr 3.00e-04 | grad 1.95 | tok/s 22759
step    650 | loss 1.4730 | lr 3.00e-04 | grad 1.72 | tok/s 22768
step    660 | loss 1.5225 | lr 3.00e-04 | grad 1.93 | tok/s 23621
step    670 | loss 1.5315 | lr 3.00e-04 | grad 4.78 | tok/s 22818
step    680 | loss 1.5475 | lr 3.00e-04 | grad 2.12 | tok/s 22453
step    690 | loss 1.5012 | lr 3.00e-04 | grad 1.91 | tok/s 22319
step    700 | loss 1.3933 | lr 3.00e-04 | grad 1.38 | tok/s 22810
step    710 | loss 1.5452 | lr 3.00e-04 | grad 4.22 | tok/s 22447
step    720 | loss 1.2305 | lr 3.00e-04 | grad 1.61 | tok/s 23286
step    730 | loss 1.3684 | lr 3.00e-04 | grad 1.42 | tok/s 22929
step    740 | loss 1.6807 | lr 3.00e-04 | grad 4.66 | tok/s 23556
step    750 | loss 1.4680 | lr 3.00e-04 | grad 1.64 | tok/s 23805
step    760 | loss 1.4343 | lr 3.00e-04 | grad 3.64 | tok/s 23312
step    770 | loss 1.4955 | lr 3.00e-04 | grad 1.88 | tok/s 22927
step    780 | loss 1.4114 | lr 3.00e-04 | grad 1.73 | tok/s 23048
step    790 | loss 1.5486 | lr 3.00e-04 | grad 5.69 | tok/s 23577
step    800 | loss 1.2617 | lr 3.00e-04 | grad 1.82 | tok/s 23225
step    810 | loss 1.2504 | lr 3.00e-04 | grad 2.64 | tok/s 22443
step    820 | loss 1.3364 | lr 3.00e-04 | grad 1.66 | tok/s 22857
step    830 | loss 1.4211 | lr 3.00e-04 | grad 1.35 | tok/s 22575
step    840 | loss 1.5282 | lr 3.00e-04 | grad 1.76 | tok/s 22503
step    850 | loss 1.4431 | lr 3.00e-04 | grad 1.45 | tok/s 22944
step    860 | loss 1.4896 | lr 3.00e-04 | grad 2.62 | tok/s 23304
step    870 | loss 1.3039 | lr 3.00e-04 | grad 1.72 | tok/s 23509
step    880 | loss 1.5148 | lr 3.00e-04 | grad 1.64 | tok/s 23042
step    890 | loss 1.4210 | lr 3.00e-04 | grad 1.42 | tok/s 22944
step    900 | loss 1.4610 | lr 3.00e-04 | grad 1.52 | tok/s 22820
step    910 | loss 1.4398 | lr 3.00e-04 | grad 8.31 | tok/s 22601
step    920 | loss 1.3952 | lr 3.00e-04 | grad 1.71 | tok/s 22874
step    930 | loss 1.3174 | lr 3.00e-04 | grad 1.75 | tok/s 23163
step    940 | loss 1.2846 | lr 3.00e-04 | grad 1.62 | tok/s 22654
step    950 | loss 1.4242 | lr 3.00e-04 | grad 2.25 | tok/s 22273
step    960 | loss 1.3737 | lr 3.00e-04 | grad 1.45 | tok/s 22864
step    970 | loss 1.4087 | lr 3.00e-04 | grad 1.55 | tok/s 22858
step    980 | loss 1.8148 | lr 3.00e-04 | grad 3.66 | tok/s 23772
step    990 | loss 1.4941 | lr 3.00e-04 | grad 1.62 | tok/s 22838
step   1000 | loss 1.4923 | lr 3.00e-04 | grad 2.23 | tok/s 22845
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4923.pt
step   1010 | loss 1.2874 | lr 3.00e-04 | grad 2.45 | tok/s 11792
step   1020 | loss 1.1529 | lr 3.00e-04 | grad 1.26 | tok/s 24193
step   1030 | loss 1.4620 | lr 3.00e-04 | grad 2.72 | tok/s 22930
step   1040 | loss 2.0126 | lr 3.00e-04 | grad 3.95 | tok/s 23400
step   1050 | loss 1.3803 | lr 3.00e-04 | grad 2.52 | tok/s 23564
step   1060 | loss 1.0868 | lr 3.00e-04 | grad 5.41 | tok/s 23433
step   1070 | loss 1.3548 | lr 3.00e-04 | grad 1.91 | tok/s 23259
step   1080 | loss 1.2092 | lr 3.00e-04 | grad 1.45 | tok/s 24027
step   1090 | loss 1.1741 | lr 3.00e-04 | grad 1.24 | tok/s 24003
step   1100 | loss 1.1586 | lr 3.00e-04 | grad 1.40 | tok/s 23984
step   1110 | loss 1.0983 | lr 3.00e-04 | grad 1.12 | tok/s 23968
step   1120 | loss 1.3850 | lr 3.00e-04 | grad 3.36 | tok/s 23303
step   1130 | loss 1.5368 | lr 3.00e-04 | grad 1.62 | tok/s 23527
step   1140 | loss 1.6796 | lr 3.00e-04 | grad 1.81 | tok/s 23818
step   1150 | loss 1.5321 | lr 3.00e-04 | grad 1.91 | tok/s 23065
step   1160 | loss 1.6538 | lr 3.00e-04 | grad 2.16 | tok/s 22703
step   1170 | loss 1.4083 | lr 3.00e-04 | grad 1.89 | tok/s 22480
step   1180 | loss 1.2725 | lr 3.00e-04 | grad 2.78 | tok/s 23626
step   1190 | loss 1.4915 | lr 3.00e-04 | grad 2.28 | tok/s 23799
step   1200 | loss 1.0619 | lr 3.00e-04 | grad 2.03 | tok/s 23902
step   1210 | loss 1.3185 | lr 3.00e-04 | grad 1.69 | tok/s 22323
step   1220 | loss 1.3003 | lr 3.00e-04 | grad 1.82 | tok/s 23444
step   1230 | loss 1.2736 | lr 3.00e-04 | grad 1.25 | tok/s 23477
step   1240 | loss 1.2435 | lr 3.00e-04 | grad 1.49 | tok/s 23568
step   1250 | loss 1.3946 | lr 3.00e-04 | grad 2.41 | tok/s 23170
step   1260 | loss 1.3109 | lr 3.00e-04 | grad 2.06 | tok/s 23712
step   1270 | loss 1.3022 | lr 3.00e-04 | grad 1.80 | tok/s 23049
step   1280 | loss 1.3134 | lr 3.00e-04 | grad 1.64 | tok/s 22791
step   1290 | loss 1.2698 | lr 3.00e-04 | grad 1.91 | tok/s 22855
step   1300 | loss 1.5573 | lr 3.00e-04 | grad 5.69 | tok/s 22515
step   1310 | loss 1.3939 | lr 3.00e-04 | grad 1.59 | tok/s 23395
step   1320 | loss 1.3988 | lr 3.00e-04 | grad 1.73 | tok/s 23432
step   1330 | loss 1.3569 | lr 3.00e-04 | grad 1.48 | tok/s 23227
step   1340 | loss 1.4521 | lr 3.00e-04 | grad 2.03 | tok/s 22762
step   1350 | loss 1.3303 | lr 3.00e-04 | grad 1.37 | tok/s 23193
step   1360 | loss 1.3899 | lr 3.00e-04 | grad 1.59 | tok/s 22386
step   1370 | loss 1.4965 | lr 3.00e-04 | grad 2.38 | tok/s 23539
step   1380 | loss 1.3683 | lr 3.00e-04 | grad 1.52 | tok/s 22542
step   1390 | loss 1.2409 | lr 3.00e-04 | grad 2.34 | tok/s 23559
step   1400 | loss 1.4184 | lr 3.00e-04 | grad 1.76 | tok/s 22776
step   1410 | loss 1.3483 | lr 3.00e-04 | grad 3.42 | tok/s 22289
step   1420 | loss 1.0374 | lr 3.00e-04 | grad 6.16 | tok/s 23684
step   1430 | loss 1.5829 | lr 3.00e-04 | grad 1.73 | tok/s 22826
step   1440 | loss 1.3575 | lr 3.00e-04 | grad 1.90 | tok/s 23416
step   1450 | loss 1.3411 | lr 3.00e-04 | grad 5.91 | tok/s 23393
step   1460 | loss 1.4870 | lr 3.00e-04 | grad 4.12 | tok/s 22758
step   1470 | loss 1.2867 | lr 3.00e-04 | grad 1.56 | tok/s 22251
step   1480 | loss 1.2840 | lr 3.00e-04 | grad 1.32 | tok/s 23418
step   1490 | loss 1.7619 | lr 3.00e-04 | grad 7.09 | tok/s 23044
step   1500 | loss 1.3750 | lr 3.00e-04 | grad 1.67 | tok/s 23065
step   1510 | loss 1.1648 | lr 3.00e-04 | grad 1.67 | tok/s 23064
step   1520 | loss 1.3941 | lr 3.00e-04 | grad 1.47 | tok/s 22894
step   1530 | loss 1.3154 | lr 3.00e-04 | grad 1.55 | tok/s 23240
step   1540 | loss 1.4418 | lr 3.00e-04 | grad 1.46 | tok/s 23460
step   1550 | loss 1.4011 | lr 3.00e-04 | grad 1.90 | tok/s 23025
step   1560 | loss 1.0580 | lr 3.00e-04 | grad 1.86 | tok/s 23894
step   1570 | loss 1.2089 | lr 3.00e-04 | grad 1.45 | tok/s 23226
step   1580 | loss 1.2114 | lr 3.00e-04 | grad 1.98 | tok/s 23164
step   1590 | loss 1.3571 | lr 3.00e-04 | grad 1.62 | tok/s 22674
step   1600 | loss 1.2180 | lr 3.00e-04 | grad 2.42 | tok/s 23580
step   1610 | loss 1.8586 | lr 3.00e-04 | grad 2.98 | tok/s 23345
step   1620 | loss 1.8910 | lr 3.00e-04 | grad 1.98 | tok/s 23904
step   1630 | loss 1.5863 | lr 3.00e-04 | grad 2.39 | tok/s 23932
step   1640 | loss 1.4222 | lr 3.00e-04 | grad 2.39 | tok/s 23911
step   1650 | loss 1.3249 | lr 3.00e-04 | grad 2.19 | tok/s 23909
step   1660 | loss 1.2837 | lr 3.00e-04 | grad 1.92 | tok/s 23891
step   1670 | loss 1.4335 | lr 3.00e-04 | grad 2.92 | tok/s 23167
step   1680 | loss 1.3490 | lr 3.00e-04 | grad 1.47 | tok/s 22979
step   1690 | loss 1.3980 | lr 3.00e-04 | grad 1.90 | tok/s 22318
step   1700 | loss 1.2431 | lr 3.00e-04 | grad 1.43 | tok/s 23449
step   1710 | loss 1.1708 | lr 3.00e-04 | grad 1.67 | tok/s 22071

Training complete! Final step: 1713
