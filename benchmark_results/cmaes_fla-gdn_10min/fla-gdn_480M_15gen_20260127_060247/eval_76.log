Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_76/levelfla-gdn_100m_20260127_073520
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 464,339,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0786 | lr 3.00e-04 | grad 17.75 | tok/s 10725
step     20 | loss 2.8459 | lr 3.00e-04 | grad 7.22 | tok/s 22922
step     30 | loss 3.0608 | lr 3.00e-04 | grad 8.69 | tok/s 24145
step     40 | loss 4.2494 | lr 3.00e-04 | grad 27.75 | tok/s 24527
step     50 | loss 4.2362 | lr 3.00e-04 | grad 14.12 | tok/s 24759
step     60 | loss 3.4065 | lr 3.00e-04 | grad 13.19 | tok/s 24632
step     70 | loss 2.8033 | lr 3.00e-04 | grad 8.62 | tok/s 24510
step     80 | loss 2.5613 | lr 3.00e-04 | grad 6.94 | tok/s 24437
step     90 | loss 2.3756 | lr 3.00e-04 | grad 5.81 | tok/s 24410
step    100 | loss 2.2246 | lr 3.00e-04 | grad 3.33 | tok/s 24328
step    110 | loss 2.2619 | lr 3.00e-04 | grad 4.59 | tok/s 24122
step    120 | loss 2.9283 | lr 3.00e-04 | grad 3.58 | tok/s 22972
step    130 | loss 2.0901 | lr 3.00e-04 | grad 5.31 | tok/s 23442
step    140 | loss 2.3511 | lr 3.00e-04 | grad 8.75 | tok/s 23501
step    150 | loss 1.5408 | lr 3.00e-04 | grad 6.22 | tok/s 24060
step    160 | loss 2.2658 | lr 3.00e-04 | grad 2.66 | tok/s 23208
step    170 | loss 2.2001 | lr 3.00e-04 | grad 2.27 | tok/s 22894
step    180 | loss 1.8523 | lr 3.00e-04 | grad 3.56 | tok/s 23411
step    190 | loss 1.8466 | lr 3.00e-04 | grad 2.64 | tok/s 22955
step    200 | loss 1.5656 | lr 3.00e-04 | grad 1.95 | tok/s 23995
step    210 | loss 1.7832 | lr 3.00e-04 | grad 3.98 | tok/s 22824
step    220 | loss 2.1300 | lr 3.00e-04 | grad 4.91 | tok/s 23028
step    230 | loss 1.8402 | lr 3.00e-04 | grad 2.62 | tok/s 23020
step    240 | loss 2.1839 | lr 3.00e-04 | grad 6.19 | tok/s 23280
step    250 | loss 1.6605 | lr 3.00e-04 | grad 1.56 | tok/s 23133
step    260 | loss 1.7927 | lr 3.00e-04 | grad 3.08 | tok/s 23778
step    270 | loss 1.7302 | lr 3.00e-04 | grad 2.02 | tok/s 23282
step    280 | loss 1.6770 | lr 3.00e-04 | grad 1.94 | tok/s 21870
step    290 | loss 1.5699 | lr 3.00e-04 | grad 2.23 | tok/s 22606
step    300 | loss 1.8628 | lr 3.00e-04 | grad 2.39 | tok/s 22765
step    310 | loss 1.5779 | lr 3.00e-04 | grad 1.76 | tok/s 22642
step    320 | loss 1.7862 | lr 3.00e-04 | grad 4.31 | tok/s 22922
step    330 | loss 1.6230 | lr 3.00e-04 | grad 1.90 | tok/s 23118
step    340 | loss 1.9599 | lr 3.00e-04 | grad 2.38 | tok/s 23045
step    350 | loss 1.6499 | lr 3.00e-04 | grad 2.03 | tok/s 23692
step    360 | loss 1.4888 | lr 3.00e-04 | grad 2.02 | tok/s 22673
step    370 | loss 1.4070 | lr 3.00e-04 | grad 1.66 | tok/s 23889
step    380 | loss 1.1367 | lr 3.00e-04 | grad 1.66 | tok/s 24071
step    390 | loss 1.0330 | lr 3.00e-04 | grad 1.41 | tok/s 24058
step    400 | loss 1.6705 | lr 3.00e-04 | grad 1.62 | tok/s 22795
step    410 | loss 1.6634 | lr 3.00e-04 | grad 2.56 | tok/s 23034
step    420 | loss 1.5520 | lr 3.00e-04 | grad 2.75 | tok/s 23997
step    430 | loss 1.4888 | lr 3.00e-04 | grad 1.88 | tok/s 23620
step    440 | loss 1.6084 | lr 3.00e-04 | grad 2.22 | tok/s 22898
step    450 | loss 1.5443 | lr 3.00e-04 | grad 1.38 | tok/s 23154
step    460 | loss 1.5320 | lr 3.00e-04 | grad 1.80 | tok/s 23476
step    470 | loss 1.4941 | lr 3.00e-04 | grad 3.41 | tok/s 23313
step    480 | loss 1.5467 | lr 3.00e-04 | grad 2.75 | tok/s 23815
step    490 | loss 1.5997 | lr 3.00e-04 | grad 2.31 | tok/s 22855
step    500 | loss 1.7242 | lr 3.00e-04 | grad 1.55 | tok/s 23238
step    510 | loss 1.6098 | lr 3.00e-04 | grad 1.35 | tok/s 22209
step    520 | loss 1.4548 | lr 3.00e-04 | grad 2.53 | tok/s 23251
step    530 | loss 1.6375 | lr 3.00e-04 | grad 2.06 | tok/s 22871
step    540 | loss 1.5251 | lr 3.00e-04 | grad 1.56 | tok/s 22397
step    550 | loss 1.2770 | lr 3.00e-04 | grad 3.02 | tok/s 23460
step    560 | loss 1.3766 | lr 3.00e-04 | grad 1.80 | tok/s 24071
step    570 | loss 1.2772 | lr 3.00e-04 | grad 1.70 | tok/s 24039
step    580 | loss 1.2381 | lr 3.00e-04 | grad 1.22 | tok/s 24054
step    590 | loss 1.2740 | lr 3.00e-04 | grad 1.33 | tok/s 24066
step    600 | loss 1.2022 | lr 3.00e-04 | grad 1.49 | tok/s 24063
step    610 | loss 1.2461 | lr 3.00e-04 | grad 1.69 | tok/s 24074
step    620 | loss 1.2227 | lr 3.00e-04 | grad 1.59 | tok/s 24021
step    630 | loss 1.5535 | lr 3.00e-04 | grad 6.69 | tok/s 22745
step    640 | loss 1.6625 | lr 3.00e-04 | grad 1.99 | tok/s 23007
step    650 | loss 1.4775 | lr 3.00e-04 | grad 1.66 | tok/s 22961
step    660 | loss 1.5253 | lr 3.00e-04 | grad 1.85 | tok/s 23807
step    670 | loss 1.5419 | lr 3.00e-04 | grad 4.56 | tok/s 23025
step    680 | loss 1.5479 | lr 3.00e-04 | grad 2.06 | tok/s 22661
step    690 | loss 1.4975 | lr 3.00e-04 | grad 1.83 | tok/s 22530
step    700 | loss 1.4006 | lr 3.00e-04 | grad 1.34 | tok/s 23009
step    710 | loss 1.5355 | lr 3.00e-04 | grad 3.59 | tok/s 22654
step    720 | loss 1.2398 | lr 3.00e-04 | grad 1.52 | tok/s 23536
step    730 | loss 1.3704 | lr 3.00e-04 | grad 1.37 | tok/s 23140
step    740 | loss 1.6969 | lr 3.00e-04 | grad 4.25 | tok/s 23729
step    750 | loss 1.4737 | lr 3.00e-04 | grad 1.66 | tok/s 24021
step    760 | loss 1.4368 | lr 3.00e-04 | grad 3.66 | tok/s 23533
step    770 | loss 1.5044 | lr 3.00e-04 | grad 1.80 | tok/s 23137
step    780 | loss 1.4190 | lr 3.00e-04 | grad 1.67 | tok/s 23311
step    790 | loss 1.5543 | lr 3.00e-04 | grad 4.84 | tok/s 23788
step    800 | loss 1.2637 | lr 3.00e-04 | grad 1.84 | tok/s 23454
step    810 | loss 1.2514 | lr 3.00e-04 | grad 2.48 | tok/s 22682
step    820 | loss 1.3447 | lr 3.00e-04 | grad 1.65 | tok/s 23083
step    830 | loss 1.4205 | lr 3.00e-04 | grad 1.34 | tok/s 22795
step    840 | loss 1.5337 | lr 3.00e-04 | grad 1.58 | tok/s 22679
step    850 | loss 1.4464 | lr 3.00e-04 | grad 1.42 | tok/s 23135
step    860 | loss 1.4935 | lr 3.00e-04 | grad 2.64 | tok/s 23501
step    870 | loss 1.3151 | lr 3.00e-04 | grad 1.69 | tok/s 23678
step    880 | loss 1.5243 | lr 3.00e-04 | grad 1.59 | tok/s 23224
step    890 | loss 1.4258 | lr 3.00e-04 | grad 1.23 | tok/s 23150
step    900 | loss 1.4655 | lr 3.00e-04 | grad 1.52 | tok/s 23055
step    910 | loss 1.4332 | lr 3.00e-04 | grad 6.56 | tok/s 22805
step    920 | loss 1.4026 | lr 3.00e-04 | grad 1.52 | tok/s 23070
step    930 | loss 1.3191 | lr 3.00e-04 | grad 1.73 | tok/s 23359
step    940 | loss 1.2869 | lr 3.00e-04 | grad 1.52 | tok/s 22856
step    950 | loss 1.4302 | lr 3.00e-04 | grad 2.11 | tok/s 22443
step    960 | loss 1.3804 | lr 3.00e-04 | grad 1.41 | tok/s 23040
step    970 | loss 1.4110 | lr 3.00e-04 | grad 1.60 | tok/s 23062
step    980 | loss 1.8281 | lr 3.00e-04 | grad 3.73 | tok/s 23987
step    990 | loss 1.4940 | lr 3.00e-04 | grad 1.62 | tok/s 23011
step   1000 | loss 1.5023 | lr 3.00e-04 | grad 2.11 | tok/s 23069
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5023.pt
step   1010 | loss 1.2906 | lr 3.00e-04 | grad 2.45 | tok/s 12127
step   1020 | loss 1.1572 | lr 3.00e-04 | grad 1.25 | tok/s 24496
step   1030 | loss 1.4678 | lr 3.00e-04 | grad 2.84 | tok/s 23168
step   1040 | loss 2.0344 | lr 3.00e-04 | grad 3.98 | tok/s 23704
step   1050 | loss 1.3850 | lr 3.00e-04 | grad 2.47 | tok/s 23852
step   1060 | loss 1.0768 | lr 3.00e-04 | grad 4.97 | tok/s 23644
step   1070 | loss 1.3602 | lr 3.00e-04 | grad 1.70 | tok/s 23453
step   1080 | loss 1.2094 | lr 3.00e-04 | grad 1.31 | tok/s 24247
step   1090 | loss 1.1737 | lr 3.00e-04 | grad 1.20 | tok/s 24215
step   1100 | loss 1.1600 | lr 3.00e-04 | grad 1.19 | tok/s 24201
step   1110 | loss 1.1008 | lr 3.00e-04 | grad 1.42 | tok/s 24210
step   1120 | loss 1.3875 | lr 3.00e-04 | grad 3.39 | tok/s 23555
step   1130 | loss 1.5376 | lr 3.00e-04 | grad 1.37 | tok/s 23786
step   1140 | loss 1.6778 | lr 3.00e-04 | grad 1.73 | tok/s 24071
step   1150 | loss 1.5626 | lr 3.00e-04 | grad 1.84 | tok/s 23298
step   1160 | loss 1.6577 | lr 3.00e-04 | grad 2.06 | tok/s 22924
step   1170 | loss 1.4169 | lr 3.00e-04 | grad 1.88 | tok/s 22685
step   1180 | loss 1.2790 | lr 3.00e-04 | grad 2.66 | tok/s 23810
step   1190 | loss 1.4985 | lr 3.00e-04 | grad 2.38 | tok/s 24047
step   1200 | loss 1.0686 | lr 3.00e-04 | grad 2.05 | tok/s 24186
step   1210 | loss 1.3214 | lr 3.00e-04 | grad 1.65 | tok/s 22597
step   1220 | loss 1.3031 | lr 3.00e-04 | grad 1.81 | tok/s 23736
step   1230 | loss 1.2705 | lr 3.00e-04 | grad 1.21 | tok/s 23747
step   1240 | loss 1.2495 | lr 3.00e-04 | grad 1.41 | tok/s 23834
step   1250 | loss 1.3963 | lr 3.00e-04 | grad 2.22 | tok/s 23463
step   1260 | loss 1.3124 | lr 3.00e-04 | grad 1.81 | tok/s 23965
step   1270 | loss 1.3073 | lr 3.00e-04 | grad 1.62 | tok/s 23277
step   1280 | loss 1.3152 | lr 3.00e-04 | grad 1.54 | tok/s 23028
step   1290 | loss 1.2772 | lr 3.00e-04 | grad 1.92 | tok/s 23075
step   1300 | loss 1.5599 | lr 3.00e-04 | grad 5.12 | tok/s 22707
step   1310 | loss 1.3969 | lr 3.00e-04 | grad 1.55 | tok/s 23563
step   1320 | loss 1.4028 | lr 3.00e-04 | grad 1.77 | tok/s 23626
step   1330 | loss 1.3649 | lr 3.00e-04 | grad 1.44 | tok/s 23387
step   1340 | loss 1.4545 | lr 3.00e-04 | grad 1.98 | tok/s 22918
step   1350 | loss 1.3395 | lr 3.00e-04 | grad 1.31 | tok/s 23393
step   1360 | loss 1.3945 | lr 3.00e-04 | grad 1.57 | tok/s 22562
step   1370 | loss 1.4947 | lr 3.00e-04 | grad 2.28 | tok/s 23713
step   1380 | loss 1.3825 | lr 3.00e-04 | grad 1.52 | tok/s 22750
step   1390 | loss 1.2529 | lr 3.00e-04 | grad 2.27 | tok/s 23784
step   1400 | loss 1.4165 | lr 3.00e-04 | grad 1.61 | tok/s 22992
step   1410 | loss 1.3497 | lr 3.00e-04 | grad 3.02 | tok/s 22477
step   1420 | loss 1.0336 | lr 3.00e-04 | grad 6.25 | tok/s 23879
step   1430 | loss 1.5972 | lr 3.00e-04 | grad 1.73 | tok/s 23037
step   1440 | loss 1.3610 | lr 3.00e-04 | grad 1.91 | tok/s 23642
step   1450 | loss 1.3453 | lr 3.00e-04 | grad 5.31 | tok/s 23601
step   1460 | loss 1.4863 | lr 3.00e-04 | grad 3.50 | tok/s 22954
step   1470 | loss 1.2874 | lr 3.00e-04 | grad 1.51 | tok/s 22408
step   1480 | loss 1.2872 | lr 3.00e-04 | grad 1.30 | tok/s 23640
step   1490 | loss 1.7800 | lr 3.00e-04 | grad 8.56 | tok/s 23269
step   1500 | loss 1.3887 | lr 3.00e-04 | grad 1.67 | tok/s 23313
step   1510 | loss 1.1838 | lr 3.00e-04 | grad 1.60 | tok/s 23338
step   1520 | loss 1.3995 | lr 3.00e-04 | grad 1.46 | tok/s 23113
step   1530 | loss 1.3190 | lr 3.00e-04 | grad 1.51 | tok/s 23513
step   1540 | loss 1.4423 | lr 3.00e-04 | grad 1.41 | tok/s 23719
step   1550 | loss 1.4080 | lr 3.00e-04 | grad 1.84 | tok/s 23268
step   1560 | loss 1.0641 | lr 3.00e-04 | grad 1.75 | tok/s 24107
step   1570 | loss 1.2119 | lr 3.00e-04 | grad 1.39 | tok/s 23443
step   1580 | loss 1.2129 | lr 3.00e-04 | grad 1.95 | tok/s 23398
step   1590 | loss 1.3744 | lr 3.00e-04 | grad 1.75 | tok/s 22914
step   1600 | loss 1.2588 | lr 3.00e-04 | grad 2.23 | tok/s 23801
step   1610 | loss 1.8591 | lr 3.00e-04 | grad 2.84 | tok/s 23504
step   1620 | loss 1.8984 | lr 3.00e-04 | grad 1.96 | tok/s 23113
step   1630 | loss 1.5882 | lr 3.00e-04 | grad 2.27 | tok/s 24135
step   1640 | loss 1.4220 | lr 3.00e-04 | grad 1.86 | tok/s 24115
step   1650 | loss 1.3277 | lr 3.00e-04 | grad 1.98 | tok/s 24112
step   1660 | loss 1.2845 | lr 3.00e-04 | grad 2.09 | tok/s 24110
step   1670 | loss 1.4383 | lr 3.00e-04 | grad 2.53 | tok/s 23398
step   1680 | loss 1.3459 | lr 3.00e-04 | grad 1.45 | tok/s 23161
step   1690 | loss 1.3960 | lr 3.00e-04 | grad 1.75 | tok/s 22526
step   1700 | loss 1.2435 | lr 3.00e-04 | grad 1.34 | tok/s 23649
step   1710 | loss 1.1717 | lr 3.00e-04 | grad 1.63 | tok/s 23398
step   1720 | loss 1.4006 | lr 3.00e-04 | grad 1.49 | tok/s 23060
step   1730 | loss 1.3499 | lr 3.00e-04 | grad 1.99 | tok/s 23400

Training complete! Final step: 1731
