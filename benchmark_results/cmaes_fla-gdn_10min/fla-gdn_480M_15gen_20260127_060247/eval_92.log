Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_92/levelfla-gdn_100m_20260127_075551
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 464,339,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0728 | lr 3.00e-04 | grad 16.62 | tok/s 10469
step     20 | loss 2.8166 | lr 3.00e-04 | grad 6.09 | tok/s 22805
step     30 | loss 3.0765 | lr 3.00e-04 | grad 8.69 | tok/s 24006
step     40 | loss 4.1607 | lr 3.00e-04 | grad 28.75 | tok/s 24381
step     50 | loss 4.2169 | lr 3.00e-04 | grad 14.75 | tok/s 24663
step     60 | loss 3.4672 | lr 3.00e-04 | grad 15.44 | tok/s 24510
step     70 | loss 2.8747 | lr 3.00e-04 | grad 8.94 | tok/s 24482
step     80 | loss 2.6018 | lr 3.00e-04 | grad 7.97 | tok/s 24392
step     90 | loss 2.4131 | lr 3.00e-04 | grad 7.12 | tok/s 24300
step    100 | loss 2.2359 | lr 3.00e-04 | grad 3.23 | tok/s 24264
step    110 | loss 2.2830 | lr 3.00e-04 | grad 5.19 | tok/s 24034
step    120 | loss 2.8711 | lr 3.00e-04 | grad 3.61 | tok/s 22859
step    130 | loss 2.0748 | lr 3.00e-04 | grad 5.31 | tok/s 23366
step    140 | loss 2.3235 | lr 3.00e-04 | grad 8.19 | tok/s 23440
step    150 | loss 1.4993 | lr 3.00e-04 | grad 6.16 | tok/s 23992
step    160 | loss 2.2727 | lr 3.00e-04 | grad 2.56 | tok/s 23154
step    170 | loss 2.1937 | lr 3.00e-04 | grad 2.23 | tok/s 22812
step    180 | loss 1.8238 | lr 3.00e-04 | grad 3.52 | tok/s 23329
step    190 | loss 1.8361 | lr 3.00e-04 | grad 2.48 | tok/s 22903
step    200 | loss 1.5648 | lr 3.00e-04 | grad 1.91 | tok/s 23935
step    210 | loss 1.7675 | lr 3.00e-04 | grad 3.67 | tok/s 22730
step    220 | loss 2.1557 | lr 3.00e-04 | grad 5.16 | tok/s 22957
step    230 | loss 1.8378 | lr 3.00e-04 | grad 2.53 | tok/s 22912
step    240 | loss 2.1797 | lr 3.00e-04 | grad 5.84 | tok/s 23197
step    250 | loss 1.6593 | lr 3.00e-04 | grad 1.55 | tok/s 23057
step    260 | loss 1.7916 | lr 3.00e-04 | grad 3.08 | tok/s 23684
step    270 | loss 1.7238 | lr 3.00e-04 | grad 1.99 | tok/s 23144
step    280 | loss 1.6793 | lr 3.00e-04 | grad 1.95 | tok/s 21772
step    290 | loss 1.5728 | lr 3.00e-04 | grad 2.23 | tok/s 22520
step    300 | loss 1.8612 | lr 3.00e-04 | grad 2.20 | tok/s 22741
step    310 | loss 1.5781 | lr 3.00e-04 | grad 1.81 | tok/s 22594
step    320 | loss 1.7875 | lr 3.00e-04 | grad 3.81 | tok/s 22843
step    330 | loss 1.6207 | lr 3.00e-04 | grad 1.84 | tok/s 23083
step    340 | loss 1.9640 | lr 3.00e-04 | grad 2.30 | tok/s 22984
step    350 | loss 1.6436 | lr 3.00e-04 | grad 1.98 | tok/s 23640
step    360 | loss 1.4857 | lr 3.00e-04 | grad 2.02 | tok/s 22661
step    370 | loss 1.4044 | lr 3.00e-04 | grad 1.66 | tok/s 23839
step    380 | loss 1.1341 | lr 3.00e-04 | grad 1.72 | tok/s 24051
step    390 | loss 1.0352 | lr 3.00e-04 | grad 1.30 | tok/s 24036
step    400 | loss 1.6794 | lr 3.00e-04 | grad 1.62 | tok/s 22795
step    410 | loss 1.6660 | lr 3.00e-04 | grad 2.56 | tok/s 22992
step    420 | loss 1.5537 | lr 3.00e-04 | grad 2.73 | tok/s 23947
step    430 | loss 1.4858 | lr 3.00e-04 | grad 1.84 | tok/s 23558
step    440 | loss 1.6103 | lr 3.00e-04 | grad 2.22 | tok/s 22824
step    450 | loss 1.5409 | lr 3.00e-04 | grad 1.41 | tok/s 23114
step    460 | loss 1.5236 | lr 3.00e-04 | grad 1.78 | tok/s 23463
step    470 | loss 1.4937 | lr 3.00e-04 | grad 3.58 | tok/s 23258
step    480 | loss 1.5392 | lr 3.00e-04 | grad 2.67 | tok/s 23781
step    490 | loss 1.5999 | lr 3.00e-04 | grad 2.41 | tok/s 22863
step    500 | loss 1.7226 | lr 3.00e-04 | grad 1.52 | tok/s 23191
step    510 | loss 1.6086 | lr 3.00e-04 | grad 1.34 | tok/s 22178
step    520 | loss 1.4628 | lr 3.00e-04 | grad 2.56 | tok/s 23222
step    530 | loss 1.6355 | lr 3.00e-04 | grad 1.99 | tok/s 22838
step    540 | loss 1.5201 | lr 3.00e-04 | grad 1.62 | tok/s 22399
step    550 | loss 1.2782 | lr 3.00e-04 | grad 3.02 | tok/s 23433
step    560 | loss 1.3746 | lr 3.00e-04 | grad 1.69 | tok/s 24013
step    570 | loss 1.2779 | lr 3.00e-04 | grad 1.70 | tok/s 24014
step    580 | loss 1.2385 | lr 3.00e-04 | grad 1.27 | tok/s 24013
step    590 | loss 1.2731 | lr 3.00e-04 | grad 1.28 | tok/s 24015
step    600 | loss 1.2013 | lr 3.00e-04 | grad 1.49 | tok/s 24023
step    610 | loss 1.2455 | lr 3.00e-04 | grad 1.62 | tok/s 24027
step    620 | loss 1.2259 | lr 3.00e-04 | grad 1.68 | tok/s 23961
step    630 | loss 1.5576 | lr 3.00e-04 | grad 6.47 | tok/s 22685
step    640 | loss 1.6636 | lr 3.00e-04 | grad 1.73 | tok/s 23000
step    650 | loss 1.4767 | lr 3.00e-04 | grad 1.63 | tok/s 22970
step    660 | loss 1.5268 | lr 3.00e-04 | grad 1.85 | tok/s 23817
step    670 | loss 1.5408 | lr 3.00e-04 | grad 4.84 | tok/s 23073
step    680 | loss 1.5474 | lr 3.00e-04 | grad 2.11 | tok/s 22695
step    690 | loss 1.4933 | lr 3.00e-04 | grad 1.86 | tok/s 22516
step    700 | loss 1.4030 | lr 3.00e-04 | grad 1.34 | tok/s 23026
step    710 | loss 1.5403 | lr 3.00e-04 | grad 3.67 | tok/s 22656
step    720 | loss 1.2403 | lr 3.00e-04 | grad 1.51 | tok/s 23496
step    730 | loss 1.3658 | lr 3.00e-04 | grad 1.38 | tok/s 23111
step    740 | loss 1.6865 | lr 3.00e-04 | grad 3.61 | tok/s 23727
step    750 | loss 1.4650 | lr 3.00e-04 | grad 1.72 | tok/s 24017
step    760 | loss 1.4395 | lr 3.00e-04 | grad 3.59 | tok/s 23515
step    770 | loss 1.5032 | lr 3.00e-04 | grad 1.76 | tok/s 23153
step    780 | loss 1.4180 | lr 3.00e-04 | grad 1.70 | tok/s 23284
step    790 | loss 1.5450 | lr 3.00e-04 | grad 5.78 | tok/s 23789
step    800 | loss 1.2650 | lr 3.00e-04 | grad 1.79 | tok/s 23449
step    810 | loss 1.2573 | lr 3.00e-04 | grad 2.62 | tok/s 22690
step    820 | loss 1.3476 | lr 3.00e-04 | grad 1.62 | tok/s 23079
step    830 | loss 1.4274 | lr 3.00e-04 | grad 1.32 | tok/s 22769
step    840 | loss 1.5297 | lr 3.00e-04 | grad 1.61 | tok/s 22674
step    850 | loss 1.4493 | lr 3.00e-04 | grad 1.42 | tok/s 23139
step    860 | loss 1.4867 | lr 3.00e-04 | grad 2.52 | tok/s 23511
step    870 | loss 1.3056 | lr 3.00e-04 | grad 1.73 | tok/s 23664
step    880 | loss 1.5239 | lr 3.00e-04 | grad 1.57 | tok/s 23232
step    890 | loss 1.4268 | lr 3.00e-04 | grad 1.22 | tok/s 23101
step    900 | loss 1.4639 | lr 3.00e-04 | grad 1.52 | tok/s 23009
step    910 | loss 1.4281 | lr 3.00e-04 | grad 7.06 | tok/s 22771
step    920 | loss 1.4032 | lr 3.00e-04 | grad 1.55 | tok/s 23033
step    930 | loss 1.3231 | lr 3.00e-04 | grad 1.77 | tok/s 23303
step    940 | loss 1.2872 | lr 3.00e-04 | grad 1.55 | tok/s 22784
step    950 | loss 1.4312 | lr 3.00e-04 | grad 2.19 | tok/s 22433
step    960 | loss 1.3756 | lr 3.00e-04 | grad 1.36 | tok/s 23022
step    970 | loss 1.4163 | lr 3.00e-04 | grad 1.51 | tok/s 23054
step    980 | loss 1.8356 | lr 3.00e-04 | grad 3.88 | tok/s 23924
step    990 | loss 1.4932 | lr 3.00e-04 | grad 1.62 | tok/s 22962
step   1000 | loss 1.5054 | lr 3.00e-04 | grad 2.19 | tok/s 23037
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5054.pt
step   1010 | loss 1.2917 | lr 3.00e-04 | grad 2.52 | tok/s 12142
step   1020 | loss 1.1660 | lr 3.00e-04 | grad 1.28 | tok/s 24427
step   1030 | loss 1.4678 | lr 3.00e-04 | grad 2.39 | tok/s 23119
step   1040 | loss 2.0338 | lr 3.00e-04 | grad 3.81 | tok/s 23629
step   1050 | loss 1.3934 | lr 3.00e-04 | grad 2.48 | tok/s 23804
step   1060 | loss 1.1313 | lr 3.00e-04 | grad 6.44 | tok/s 23629
step   1070 | loss 1.3642 | lr 3.00e-04 | grad 1.71 | tok/s 23431
step   1080 | loss 1.2125 | lr 3.00e-04 | grad 1.27 | tok/s 24195
step   1090 | loss 1.1759 | lr 3.00e-04 | grad 1.23 | tok/s 24175
step   1100 | loss 1.1614 | lr 3.00e-04 | grad 1.20 | tok/s 24186
step   1110 | loss 1.1032 | lr 3.00e-04 | grad 1.30 | tok/s 24151
step   1120 | loss 1.3915 | lr 3.00e-04 | grad 3.28 | tok/s 23513
step   1130 | loss 1.5398 | lr 3.00e-04 | grad 1.42 | tok/s 23772
step   1140 | loss 1.6830 | lr 3.00e-04 | grad 1.74 | tok/s 24028
step   1150 | loss 1.5724 | lr 3.00e-04 | grad 1.84 | tok/s 23265
step   1160 | loss 1.6611 | lr 3.00e-04 | grad 2.05 | tok/s 22930
step   1170 | loss 1.4237 | lr 3.00e-04 | grad 1.86 | tok/s 22657
step   1180 | loss 1.2823 | lr 3.00e-04 | grad 2.83 | tok/s 23790
step   1190 | loss 1.5035 | lr 3.00e-04 | grad 2.39 | tok/s 23973
step   1200 | loss 1.0632 | lr 3.00e-04 | grad 2.11 | tok/s 24102
step   1210 | loss 1.3230 | lr 3.00e-04 | grad 1.67 | tok/s 22515
step   1220 | loss 1.3115 | lr 3.00e-04 | grad 1.83 | tok/s 23631
step   1230 | loss 1.2726 | lr 3.00e-04 | grad 1.25 | tok/s 23699
step   1240 | loss 1.2481 | lr 3.00e-04 | grad 1.35 | tok/s 23782
step   1250 | loss 1.3990 | lr 3.00e-04 | grad 2.52 | tok/s 23425
step   1260 | loss 1.3087 | lr 3.00e-04 | grad 1.91 | tok/s 23945
step   1270 | loss 1.3104 | lr 3.00e-04 | grad 1.77 | tok/s 23259
step   1280 | loss 1.3177 | lr 3.00e-04 | grad 1.59 | tok/s 22954
step   1290 | loss 1.2776 | lr 3.00e-04 | grad 1.85 | tok/s 23057
step   1300 | loss 1.5706 | lr 3.00e-04 | grad 5.59 | tok/s 22680
step   1310 | loss 1.4012 | lr 3.00e-04 | grad 1.52 | tok/s 23577
step   1320 | loss 1.4054 | lr 3.00e-04 | grad 1.59 | tok/s 23588
step   1330 | loss 1.3652 | lr 3.00e-04 | grad 1.42 | tok/s 23358
step   1340 | loss 1.4520 | lr 3.00e-04 | grad 2.03 | tok/s 22878
step   1350 | loss 1.3371 | lr 3.00e-04 | grad 1.33 | tok/s 23354
step   1360 | loss 1.3946 | lr 3.00e-04 | grad 1.55 | tok/s 22539
step   1370 | loss 1.5010 | lr 3.00e-04 | grad 2.25 | tok/s 23721
step   1380 | loss 1.3716 | lr 3.00e-04 | grad 1.55 | tok/s 22689
step   1390 | loss 1.2540 | lr 3.00e-04 | grad 2.17 | tok/s 23716
step   1400 | loss 1.4195 | lr 3.00e-04 | grad 1.52 | tok/s 22898
step   1410 | loss 1.3523 | lr 3.00e-04 | grad 3.28 | tok/s 22380
step   1420 | loss 1.0436 | lr 3.00e-04 | grad 7.25 | tok/s 23821
step   1430 | loss 1.5711 | lr 3.00e-04 | grad 1.69 | tok/s 22947
step   1440 | loss 1.3613 | lr 3.00e-04 | grad 1.90 | tok/s 23542
step   1450 | loss 1.3489 | lr 3.00e-04 | grad 6.47 | tok/s 23559
step   1460 | loss 1.4891 | lr 3.00e-04 | grad 3.64 | tok/s 22886
step   1470 | loss 1.2899 | lr 3.00e-04 | grad 1.52 | tok/s 22369
step   1480 | loss 1.2909 | lr 3.00e-04 | grad 1.30 | tok/s 23607
step   1490 | loss 1.7669 | lr 3.00e-04 | grad 9.44 | tok/s 23209
step   1500 | loss 1.3971 | lr 3.00e-04 | grad 1.72 | tok/s 23263
step   1510 | loss 1.2475 | lr 3.00e-04 | grad 1.55 | tok/s 23317
step   1520 | loss 1.3992 | lr 3.00e-04 | grad 1.48 | tok/s 23095
step   1530 | loss 1.3224 | lr 3.00e-04 | grad 1.53 | tok/s 23417
step   1540 | loss 1.4440 | lr 3.00e-04 | grad 1.41 | tok/s 23670
step   1550 | loss 1.4113 | lr 3.00e-04 | grad 1.84 | tok/s 23207
step   1560 | loss 1.0649 | lr 3.00e-04 | grad 1.78 | tok/s 24050
step   1570 | loss 1.2202 | lr 3.00e-04 | grad 1.38 | tok/s 23405
step   1580 | loss 1.2159 | lr 3.00e-04 | grad 1.98 | tok/s 23309
step   1590 | loss 1.3763 | lr 3.00e-04 | grad 1.62 | tok/s 22845
step   1600 | loss 1.1947 | lr 3.00e-04 | grad 2.45 | tok/s 23738
step   1610 | loss 1.8668 | lr 3.00e-04 | grad 3.00 | tok/s 23477
step   1620 | loss 1.9087 | lr 3.00e-04 | grad 2.05 | tok/s 22017
step   1630 | loss 1.5993 | lr 3.00e-04 | grad 2.30 | tok/s 24107
step   1640 | loss 1.4309 | lr 3.00e-04 | grad 2.45 | tok/s 24105
step   1650 | loss 1.3334 | lr 3.00e-04 | grad 2.02 | tok/s 24102
step   1660 | loss 1.2909 | lr 3.00e-04 | grad 1.71 | tok/s 24080
step   1670 | loss 1.4475 | lr 3.00e-04 | grad 2.97 | tok/s 23367
step   1680 | loss 1.3469 | lr 3.00e-04 | grad 1.40 | tok/s 23131
step   1690 | loss 1.4027 | lr 3.00e-04 | grad 1.92 | tok/s 22480
step   1700 | loss 1.2453 | lr 3.00e-04 | grad 1.36 | tok/s 23628
step   1710 | loss 1.1779 | lr 3.00e-04 | grad 1.64 | tok/s 23328
step   1720 | loss 1.4021 | lr 3.00e-04 | grad 1.51 | tok/s 23019

Training complete! Final step: 1728
