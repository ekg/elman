Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_80/levelfla-gdn_100m_20260127_073520
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 464,339,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0779 | lr 3.00e-04 | grad 17.25 | tok/s 10541
step     20 | loss 2.8476 | lr 3.00e-04 | grad 6.09 | tok/s 22479
step     30 | loss 3.1281 | lr 3.00e-04 | grad 6.03 | tok/s 23686
step     40 | loss 4.1747 | lr 3.00e-04 | grad 28.25 | tok/s 24120
step     50 | loss 4.3013 | lr 3.00e-04 | grad 17.12 | tok/s 24267
step     60 | loss 3.4028 | lr 3.00e-04 | grad 14.94 | tok/s 24120
step     70 | loss 2.8956 | lr 3.00e-04 | grad 9.00 | tok/s 23958
step     80 | loss 2.5932 | lr 3.00e-04 | grad 6.94 | tok/s 23904
step     90 | loss 2.4572 | lr 3.00e-04 | grad 5.22 | tok/s 23801
step    100 | loss 2.2586 | lr 3.00e-04 | grad 3.44 | tok/s 23796
step    110 | loss 2.2718 | lr 3.00e-04 | grad 4.53 | tok/s 23512
step    120 | loss 2.7948 | lr 3.00e-04 | grad 3.73 | tok/s 22380
step    130 | loss 2.0840 | lr 3.00e-04 | grad 5.41 | tok/s 22847
step    140 | loss 2.3274 | lr 3.00e-04 | grad 8.88 | tok/s 22921
step    150 | loss 1.4653 | lr 3.00e-04 | grad 6.06 | tok/s 23476
step    160 | loss 2.2702 | lr 3.00e-04 | grad 2.58 | tok/s 22676
step    170 | loss 2.1923 | lr 3.00e-04 | grad 2.17 | tok/s 22325
step    180 | loss 1.8193 | lr 3.00e-04 | grad 3.45 | tok/s 22805
step    190 | loss 1.8322 | lr 3.00e-04 | grad 2.39 | tok/s 22354
step    200 | loss 1.5595 | lr 3.00e-04 | grad 1.91 | tok/s 23389
step    210 | loss 1.7637 | lr 3.00e-04 | grad 3.88 | tok/s 22181
step    220 | loss 2.1320 | lr 3.00e-04 | grad 4.81 | tok/s 22448
step    230 | loss 1.8278 | lr 3.00e-04 | grad 2.62 | tok/s 22370
step    240 | loss 2.1751 | lr 3.00e-04 | grad 5.53 | tok/s 22689
step    250 | loss 1.6553 | lr 3.00e-04 | grad 1.58 | tok/s 22527
step    260 | loss 1.7885 | lr 3.00e-04 | grad 2.97 | tok/s 23126
step    270 | loss 1.7177 | lr 3.00e-04 | grad 1.98 | tok/s 22578
step    280 | loss 1.6810 | lr 3.00e-04 | grad 1.97 | tok/s 21244
step    290 | loss 1.5639 | lr 3.00e-04 | grad 2.28 | tok/s 21930
step    300 | loss 1.8498 | lr 3.00e-04 | grad 2.33 | tok/s 22107
step    310 | loss 1.5773 | lr 3.00e-04 | grad 1.80 | tok/s 22035
step    320 | loss 1.7846 | lr 3.00e-04 | grad 4.50 | tok/s 22278
step    330 | loss 1.6192 | lr 3.00e-04 | grad 1.93 | tok/s 22532
step    340 | loss 1.9576 | lr 3.00e-04 | grad 2.17 | tok/s 22423
step    350 | loss 1.6349 | lr 3.00e-04 | grad 2.00 | tok/s 23069
step    360 | loss 1.4917 | lr 3.00e-04 | grad 2.02 | tok/s 22071
step    370 | loss 1.3997 | lr 3.00e-04 | grad 1.68 | tok/s 23200
step    380 | loss 1.1328 | lr 3.00e-04 | grad 1.68 | tok/s 23402
step    390 | loss 1.0316 | lr 3.00e-04 | grad 1.32 | tok/s 23418
step    400 | loss 1.6711 | lr 3.00e-04 | grad 1.69 | tok/s 22169
step    410 | loss 1.6598 | lr 3.00e-04 | grad 2.56 | tok/s 22370
step    420 | loss 1.5522 | lr 3.00e-04 | grad 2.75 | tok/s 23298
step    430 | loss 1.4856 | lr 3.00e-04 | grad 1.84 | tok/s 22882
step    440 | loss 1.6065 | lr 3.00e-04 | grad 2.27 | tok/s 22184
step    450 | loss 1.5398 | lr 3.00e-04 | grad 1.40 | tok/s 22438
step    460 | loss 1.5232 | lr 3.00e-04 | grad 1.80 | tok/s 22785
step    470 | loss 1.4898 | lr 3.00e-04 | grad 3.44 | tok/s 22649
step    480 | loss 1.5437 | lr 3.00e-04 | grad 2.73 | tok/s 23116
step    490 | loss 1.5991 | lr 3.00e-04 | grad 2.34 | tok/s 22251
step    500 | loss 1.7107 | lr 3.00e-04 | grad 1.52 | tok/s 22598
step    510 | loss 1.6054 | lr 3.00e-04 | grad 1.34 | tok/s 21617
step    520 | loss 1.4612 | lr 3.00e-04 | grad 2.88 | tok/s 22628
step    530 | loss 1.6319 | lr 3.00e-04 | grad 2.06 | tok/s 22272
step    540 | loss 1.5215 | lr 3.00e-04 | grad 1.57 | tok/s 21812
step    550 | loss 1.2705 | lr 3.00e-04 | grad 2.97 | tok/s 22777
step    560 | loss 1.3741 | lr 3.00e-04 | grad 1.79 | tok/s 23362
step    570 | loss 1.2786 | lr 3.00e-04 | grad 1.70 | tok/s 23350
step    580 | loss 1.2372 | lr 3.00e-04 | grad 1.24 | tok/s 23376
step    590 | loss 1.2721 | lr 3.00e-04 | grad 1.32 | tok/s 23377
step    600 | loss 1.2002 | lr 3.00e-04 | grad 1.55 | tok/s 23376
step    610 | loss 1.2424 | lr 3.00e-04 | grad 1.51 | tok/s 23392
step    620 | loss 1.2234 | lr 3.00e-04 | grad 1.56 | tok/s 23291
step    630 | loss 1.5577 | lr 3.00e-04 | grad 6.38 | tok/s 22070
step    640 | loss 1.6605 | lr 3.00e-04 | grad 1.70 | tok/s 22307
step    650 | loss 1.4761 | lr 3.00e-04 | grad 1.63 | tok/s 22332
step    660 | loss 1.5262 | lr 3.00e-04 | grad 1.80 | tok/s 23105
step    670 | loss 1.5443 | lr 3.00e-04 | grad 4.84 | tok/s 22398
step    680 | loss 1.5474 | lr 3.00e-04 | grad 2.16 | tok/s 22064
step    690 | loss 1.4846 | lr 3.00e-04 | grad 1.84 | tok/s 21885
step    700 | loss 1.4022 | lr 3.00e-04 | grad 1.38 | tok/s 22337
step    710 | loss 1.5453 | lr 3.00e-04 | grad 4.19 | tok/s 22030
step    720 | loss 1.2364 | lr 3.00e-04 | grad 1.59 | tok/s 22861
step    730 | loss 1.3669 | lr 3.00e-04 | grad 1.40 | tok/s 22466
step    740 | loss 1.6864 | lr 3.00e-04 | grad 4.09 | tok/s 23059
step    750 | loss 1.4755 | lr 3.00e-04 | grad 1.67 | tok/s 23337
step    760 | loss 1.4377 | lr 3.00e-04 | grad 3.61 | tok/s 22863
step    770 | loss 1.5036 | lr 3.00e-04 | grad 1.87 | tok/s 22507
step    780 | loss 1.4145 | lr 3.00e-04 | grad 1.70 | tok/s 22611
step    790 | loss 1.5525 | lr 3.00e-04 | grad 4.75 | tok/s 23111
step    800 | loss 1.2566 | lr 3.00e-04 | grad 1.71 | tok/s 22755
step    810 | loss 1.2505 | lr 3.00e-04 | grad 2.59 | tok/s 22007
step    820 | loss 1.3440 | lr 3.00e-04 | grad 1.59 | tok/s 22421
step    830 | loss 1.4187 | lr 3.00e-04 | grad 1.27 | tok/s 22153
step    840 | loss 1.5337 | lr 3.00e-04 | grad 1.63 | tok/s 22014
step    850 | loss 1.4399 | lr 3.00e-04 | grad 1.44 | tok/s 22509
step    860 | loss 1.4857 | lr 3.00e-04 | grad 2.48 | tok/s 22840
step    870 | loss 1.2981 | lr 3.00e-04 | grad 1.76 | tok/s 23016
step    880 | loss 1.5166 | lr 3.00e-04 | grad 1.63 | tok/s 22616
step    890 | loss 1.4230 | lr 3.00e-04 | grad 1.29 | tok/s 22460
step    900 | loss 1.4666 | lr 3.00e-04 | grad 1.50 | tok/s 22366
step    910 | loss 1.4254 | lr 3.00e-04 | grad 6.09 | tok/s 22104
step    920 | loss 1.3993 | lr 3.00e-04 | grad 1.57 | tok/s 22336
step    930 | loss 1.3191 | lr 3.00e-04 | grad 1.73 | tok/s 22668
step    940 | loss 1.2839 | lr 3.00e-04 | grad 1.55 | tok/s 22159
step    950 | loss 1.4283 | lr 3.00e-04 | grad 2.14 | tok/s 21811
step    960 | loss 1.3752 | lr 3.00e-04 | grad 1.33 | tok/s 22374
step    970 | loss 1.4115 | lr 3.00e-04 | grad 1.65 | tok/s 22381
step    980 | loss 1.8180 | lr 3.00e-04 | grad 4.16 | tok/s 23261
step    990 | loss 1.4955 | lr 3.00e-04 | grad 1.55 | tok/s 22358
step   1000 | loss 1.5033 | lr 3.00e-04 | grad 2.11 | tok/s 22380
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5033.pt
step   1010 | loss 1.2930 | lr 3.00e-04 | grad 2.55 | tok/s 11897
step   1020 | loss 1.1555 | lr 3.00e-04 | grad 1.30 | tok/s 23735
step   1030 | loss 1.4631 | lr 3.00e-04 | grad 2.50 | tok/s 22458
step   1040 | loss 2.0312 | lr 3.00e-04 | grad 3.38 | tok/s 22959
step   1050 | loss 1.3809 | lr 3.00e-04 | grad 2.52 | tok/s 23147
step   1060 | loss 1.0949 | lr 3.00e-04 | grad 4.41 | tok/s 22927
step   1070 | loss 1.3563 | lr 3.00e-04 | grad 1.85 | tok/s 22779
step   1080 | loss 1.2121 | lr 3.00e-04 | grad 1.32 | tok/s 23499
step   1090 | loss 1.1755 | lr 3.00e-04 | grad 1.19 | tok/s 23533
step   1100 | loss 1.1626 | lr 3.00e-04 | grad 1.23 | tok/s 23518
step   1110 | loss 1.1017 | lr 3.00e-04 | grad 1.23 | tok/s 23511
step   1120 | loss 1.3860 | lr 3.00e-04 | grad 3.25 | tok/s 22934
step   1130 | loss 1.5238 | lr 3.00e-04 | grad 1.38 | tok/s 23152
step   1140 | loss 1.6760 | lr 3.00e-04 | grad 1.75 | tok/s 23358
step   1150 | loss 1.5557 | lr 3.00e-04 | grad 1.90 | tok/s 22682
step   1160 | loss 1.6616 | lr 3.00e-04 | grad 2.14 | tok/s 22294
step   1170 | loss 1.4263 | lr 3.00e-04 | grad 1.90 | tok/s 22052
step   1180 | loss 1.2770 | lr 3.00e-04 | grad 2.67 | tok/s 23183
step   1190 | loss 1.5075 | lr 3.00e-04 | grad 2.34 | tok/s 23327
step   1200 | loss 1.0695 | lr 3.00e-04 | grad 2.06 | tok/s 23455
step   1210 | loss 1.3188 | lr 3.00e-04 | grad 1.65 | tok/s 21874
step   1220 | loss 1.3072 | lr 3.00e-04 | grad 1.85 | tok/s 23018
step   1230 | loss 1.2673 | lr 3.00e-04 | grad 1.30 | tok/s 23037
step   1240 | loss 1.2480 | lr 3.00e-04 | grad 1.41 | tok/s 23126
step   1250 | loss 1.3996 | lr 3.00e-04 | grad 2.38 | tok/s 22754
step   1260 | loss 1.3071 | lr 3.00e-04 | grad 1.91 | tok/s 23268
step   1270 | loss 1.3058 | lr 3.00e-04 | grad 1.64 | tok/s 22629
step   1280 | loss 1.3235 | lr 3.00e-04 | grad 1.55 | tok/s 22383
step   1290 | loss 1.2732 | lr 3.00e-04 | grad 1.82 | tok/s 22386
step   1300 | loss 1.5692 | lr 3.00e-04 | grad 6.09 | tok/s 22063
step   1310 | loss 1.4099 | lr 3.00e-04 | grad 1.57 | tok/s 22910
step   1320 | loss 1.4028 | lr 3.00e-04 | grad 1.87 | tok/s 22926
step   1330 | loss 1.3636 | lr 3.00e-04 | grad 1.39 | tok/s 22713
step   1340 | loss 1.4468 | lr 3.00e-04 | grad 1.97 | tok/s 22217
step   1350 | loss 1.3327 | lr 3.00e-04 | grad 1.35 | tok/s 22674
step   1360 | loss 1.3997 | lr 3.00e-04 | grad 1.57 | tok/s 21908
step   1370 | loss 1.4997 | lr 3.00e-04 | grad 2.28 | tok/s 23012
step   1380 | loss 1.3784 | lr 3.00e-04 | grad 1.50 | tok/s 22042
step   1390 | loss 1.2530 | lr 3.00e-04 | grad 2.25 | tok/s 23061
step   1400 | loss 1.4167 | lr 3.00e-04 | grad 1.59 | tok/s 22251
step   1410 | loss 1.3483 | lr 3.00e-04 | grad 3.05 | tok/s 21759
step   1420 | loss 1.0282 | lr 3.00e-04 | grad 5.56 | tok/s 23145
step   1430 | loss 1.5878 | lr 3.00e-04 | grad 1.74 | tok/s 22331
step   1440 | loss 1.3565 | lr 3.00e-04 | grad 1.83 | tok/s 22904
step   1450 | loss 1.3484 | lr 3.00e-04 | grad 6.69 | tok/s 22904
step   1460 | loss 1.4907 | lr 3.00e-04 | grad 3.62 | tok/s 22274
step   1470 | loss 1.2881 | lr 3.00e-04 | grad 1.52 | tok/s 21744
step   1480 | loss 1.2890 | lr 3.00e-04 | grad 1.28 | tok/s 22952
step   1490 | loss 1.7612 | lr 3.00e-04 | grad 7.59 | tok/s 22565
step   1500 | loss 1.3886 | lr 3.00e-04 | grad 1.63 | tok/s 22616
step   1510 | loss 1.2069 | lr 3.00e-04 | grad 1.60 | tok/s 22640
step   1520 | loss 1.3977 | lr 3.00e-04 | grad 1.45 | tok/s 22422
step   1530 | loss 1.3174 | lr 3.00e-04 | grad 1.54 | tok/s 22760
step   1540 | loss 1.4368 | lr 3.00e-04 | grad 1.38 | tok/s 23026
step   1550 | loss 1.4054 | lr 3.00e-04 | grad 1.77 | tok/s 22533
step   1560 | loss 1.0638 | lr 3.00e-04 | grad 1.80 | tok/s 23377
step   1570 | loss 1.2157 | lr 3.00e-04 | grad 1.41 | tok/s 22728
step   1580 | loss 1.2156 | lr 3.00e-04 | grad 1.96 | tok/s 22652
step   1590 | loss 1.3698 | lr 3.00e-04 | grad 1.63 | tok/s 22241
step   1600 | loss 1.2291 | lr 3.00e-04 | grad 2.31 | tok/s 23084
step   1610 | loss 1.8560 | lr 3.00e-04 | grad 2.89 | tok/s 22867
step   1620 | loss 1.8929 | lr 3.00e-04 | grad 2.00 | tok/s 22337
step   1630 | loss 1.5814 | lr 3.00e-04 | grad 2.23 | tok/s 23392
step   1640 | loss 1.4206 | lr 3.00e-04 | grad 2.06 | tok/s 23371
step   1650 | loss 1.3273 | lr 3.00e-04 | grad 1.89 | tok/s 23362
step   1660 | loss 1.2813 | lr 3.00e-04 | grad 1.97 | tok/s 23385
step   1670 | loss 1.4355 | lr 3.00e-04 | grad 3.16 | tok/s 22702
step   1680 | loss 1.3503 | lr 3.00e-04 | grad 1.44 | tok/s 22480

Training complete! Final step: 1683
