Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_82/levelfla-gdn_100m_20260127_074535
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 454,897,728 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2153 | lr 3.00e-04 | grad 22.75 | tok/s 10509
step     20 | loss 3.0052 | lr 3.00e-04 | grad 7.25 | tok/s 22531
step     30 | loss 3.3022 | lr 3.00e-04 | grad 10.31 | tok/s 23762
step     40 | loss 4.2317 | lr 3.00e-04 | grad 35.25 | tok/s 24172
step     50 | loss 4.5773 | lr 3.00e-04 | grad 23.12 | tok/s 24457
step     60 | loss 3.6815 | lr 3.00e-04 | grad 17.62 | tok/s 24391
step     70 | loss 3.0267 | lr 3.00e-04 | grad 9.81 | tok/s 24336
step     80 | loss 2.7532 | lr 3.00e-04 | grad 7.47 | tok/s 24300
step     90 | loss 2.4748 | lr 3.00e-04 | grad 6.59 | tok/s 24224
step    100 | loss 2.3439 | lr 3.00e-04 | grad 4.62 | tok/s 24202
step    110 | loss 2.4023 | lr 3.00e-04 | grad 4.69 | tok/s 24007
step    120 | loss 2.9813 | lr 3.00e-04 | grad 3.47 | tok/s 22857
step    130 | loss 2.1669 | lr 3.00e-04 | grad 4.94 | tok/s 23372
step    140 | loss 2.4061 | lr 3.00e-04 | grad 8.56 | tok/s 23465
step    150 | loss 1.5555 | lr 3.00e-04 | grad 5.38 | tok/s 23961
step    160 | loss 2.3401 | lr 3.00e-04 | grad 2.36 | tok/s 23150
step    170 | loss 2.2486 | lr 3.00e-04 | grad 2.17 | tok/s 22838
step    180 | loss 2.0082 | lr 3.00e-04 | grad 3.34 | tok/s 23386
step    190 | loss 1.8936 | lr 3.00e-04 | grad 2.34 | tok/s 22975
step    200 | loss 1.6246 | lr 3.00e-04 | grad 1.78 | tok/s 23942
step    210 | loss 1.8159 | lr 3.00e-04 | grad 3.86 | tok/s 22776
step    220 | loss 2.1709 | lr 3.00e-04 | grad 3.41 | tok/s 23006
step    230 | loss 1.8827 | lr 3.00e-04 | grad 2.50 | tok/s 22947
step    240 | loss 2.2161 | lr 3.00e-04 | grad 5.69 | tok/s 23237
step    250 | loss 1.6954 | lr 3.00e-04 | grad 1.52 | tok/s 23088
step    260 | loss 1.8265 | lr 3.00e-04 | grad 2.81 | tok/s 23742
step    270 | loss 1.7474 | lr 3.00e-04 | grad 1.77 | tok/s 23209
step    280 | loss 1.6991 | lr 3.00e-04 | grad 1.80 | tok/s 21824
step    290 | loss 1.5898 | lr 3.00e-04 | grad 2.20 | tok/s 22554
step    300 | loss 1.8851 | lr 3.00e-04 | grad 1.99 | tok/s 22732
step    310 | loss 1.5983 | lr 3.00e-04 | grad 1.65 | tok/s 22628
step    320 | loss 1.8017 | lr 3.00e-04 | grad 3.33 | tok/s 22885
step    330 | loss 1.6397 | lr 3.00e-04 | grad 1.68 | tok/s 23107
step    340 | loss 1.9694 | lr 3.00e-04 | grad 2.31 | tok/s 23017
step    350 | loss 1.6799 | lr 3.00e-04 | grad 1.78 | tok/s 23730
step    360 | loss 1.5073 | lr 3.00e-04 | grad 1.84 | tok/s 22679
step    370 | loss 1.4269 | lr 3.00e-04 | grad 1.57 | tok/s 23898
step    380 | loss 1.1590 | lr 3.00e-04 | grad 1.55 | tok/s 24049
step    390 | loss 1.0498 | lr 3.00e-04 | grad 1.28 | tok/s 24030
step    400 | loss 1.6895 | lr 3.00e-04 | grad 1.55 | tok/s 22794
step    410 | loss 1.6766 | lr 3.00e-04 | grad 2.42 | tok/s 23020
step    420 | loss 1.5836 | lr 3.00e-04 | grad 2.55 | tok/s 23995
step    430 | loss 1.5168 | lr 3.00e-04 | grad 1.61 | tok/s 23605
step    440 | loss 1.6194 | lr 3.00e-04 | grad 2.08 | tok/s 22865
step    450 | loss 1.5569 | lr 3.00e-04 | grad 1.30 | tok/s 23125
step    460 | loss 1.5445 | lr 3.00e-04 | grad 1.69 | tok/s 23440
step    470 | loss 1.5098 | lr 3.00e-04 | grad 2.95 | tok/s 23283
step    480 | loss 1.5447 | lr 3.00e-04 | grad 2.41 | tok/s 23777
step    490 | loss 1.6089 | lr 3.00e-04 | grad 2.14 | tok/s 21350
step    500 | loss 1.7214 | lr 3.00e-04 | grad 1.45 | tok/s 23214
step    510 | loss 1.6210 | lr 3.00e-04 | grad 1.31 | tok/s 22199
step    520 | loss 1.4718 | lr 3.00e-04 | grad 3.39 | tok/s 23231
step    530 | loss 1.6490 | lr 3.00e-04 | grad 1.91 | tok/s 22863
step    540 | loss 1.5402 | lr 3.00e-04 | grad 1.45 | tok/s 22373
step    550 | loss 1.2926 | lr 3.00e-04 | grad 2.55 | tok/s 23418
step    560 | loss 1.3857 | lr 3.00e-04 | grad 1.48 | tok/s 24016
step    570 | loss 1.2864 | lr 3.00e-04 | grad 1.48 | tok/s 24029
step    580 | loss 1.2461 | lr 3.00e-04 | grad 1.16 | tok/s 24035
step    590 | loss 1.2848 | lr 3.00e-04 | grad 1.14 | tok/s 24023
step    600 | loss 1.2151 | lr 3.00e-04 | grad 1.30 | tok/s 24043
step    610 | loss 1.2550 | lr 3.00e-04 | grad 1.41 | tok/s 24014
step    620 | loss 1.2346 | lr 3.00e-04 | grad 1.53 | tok/s 23934
step    630 | loss 1.5901 | lr 3.00e-04 | grad 5.09 | tok/s 22698
step    640 | loss 1.6751 | lr 3.00e-04 | grad 1.76 | tok/s 22929
step    650 | loss 1.4881 | lr 3.00e-04 | grad 1.49 | tok/s 22875
step    660 | loss 1.5411 | lr 3.00e-04 | grad 1.60 | tok/s 23733
step    670 | loss 1.5577 | lr 3.00e-04 | grad 4.47 | tok/s 22973
step    680 | loss 1.5650 | lr 3.00e-04 | grad 1.81 | tok/s 22589
step    690 | loss 1.5094 | lr 3.00e-04 | grad 1.90 | tok/s 22418
step    700 | loss 1.4208 | lr 3.00e-04 | grad 1.31 | tok/s 22940
step    710 | loss 1.5502 | lr 3.00e-04 | grad 3.27 | tok/s 22576
step    720 | loss 1.2471 | lr 3.00e-04 | grad 1.33 | tok/s 23488
step    730 | loss 1.3851 | lr 3.00e-04 | grad 1.25 | tok/s 23094
step    740 | loss 1.7071 | lr 3.00e-04 | grad 3.42 | tok/s 23691
step    750 | loss 1.4919 | lr 3.00e-04 | grad 1.42 | tok/s 23944
step    760 | loss 1.4549 | lr 3.00e-04 | grad 3.17 | tok/s 23487
step    770 | loss 1.5154 | lr 3.00e-04 | grad 1.64 | tok/s 23091
step    780 | loss 1.4261 | lr 3.00e-04 | grad 1.53 | tok/s 23245
step    790 | loss 1.5739 | lr 3.00e-04 | grad 4.09 | tok/s 23757
step    800 | loss 1.2722 | lr 3.00e-04 | grad 1.54 | tok/s 23399
step    810 | loss 1.2582 | lr 3.00e-04 | grad 2.31 | tok/s 22612
step    820 | loss 1.3550 | lr 3.00e-04 | grad 1.50 | tok/s 23021
step    830 | loss 1.4402 | lr 3.00e-04 | grad 1.14 | tok/s 22716
step    840 | loss 1.5417 | lr 3.00e-04 | grad 1.41 | tok/s 22633
step    850 | loss 1.4615 | lr 3.00e-04 | grad 1.30 | tok/s 23073
step    860 | loss 1.5058 | lr 3.00e-04 | grad 2.05 | tok/s 23477
step    870 | loss 1.3383 | lr 3.00e-04 | grad 1.69 | tok/s 23636
step    880 | loss 1.5314 | lr 3.00e-04 | grad 1.41 | tok/s 23179
step    890 | loss 1.4351 | lr 3.00e-04 | grad 1.12 | tok/s 23070
step    900 | loss 1.4747 | lr 3.00e-04 | grad 1.39 | tok/s 23007
step    910 | loss 1.4464 | lr 3.00e-04 | grad 6.03 | tok/s 22738
step    920 | loss 1.4088 | lr 3.00e-04 | grad 1.42 | tok/s 23019
step    930 | loss 1.3341 | lr 3.00e-04 | grad 1.55 | tok/s 23322
step    940 | loss 1.3012 | lr 3.00e-04 | grad 1.51 | tok/s 22807
step    950 | loss 1.4390 | lr 3.00e-04 | grad 1.94 | tok/s 22448
step    960 | loss 1.3880 | lr 3.00e-04 | grad 1.18 | tok/s 22991
step    970 | loss 1.4215 | lr 3.00e-04 | grad 1.33 | tok/s 23011
step    980 | loss 1.8641 | lr 3.00e-04 | grad 3.47 | tok/s 23952
step    990 | loss 1.5118 | lr 3.00e-04 | grad 1.47 | tok/s 22993
step   1000 | loss 1.5117 | lr 3.00e-04 | grad 1.77 | tok/s 23052
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5117.pt
step   1010 | loss 1.3007 | lr 3.00e-04 | grad 2.27 | tok/s 12530
step   1020 | loss 1.1768 | lr 3.00e-04 | grad 1.20 | tok/s 24329
step   1030 | loss 1.4843 | lr 3.00e-04 | grad 2.39 | tok/s 23067
step   1040 | loss 2.0523 | lr 3.00e-04 | grad 3.98 | tok/s 23560
step   1050 | loss 1.4065 | lr 3.00e-04 | grad 2.38 | tok/s 23784
step   1060 | loss 1.0963 | lr 3.00e-04 | grad 1.73 | tok/s 23571
step   1070 | loss 1.3708 | lr 3.00e-04 | grad 1.60 | tok/s 22204
step   1080 | loss 1.2195 | lr 3.00e-04 | grad 1.15 | tok/s 24191
step   1090 | loss 1.1826 | lr 3.00e-04 | grad 1.08 | tok/s 24196
step   1100 | loss 1.1690 | lr 3.00e-04 | grad 1.09 | tok/s 24184
step   1110 | loss 1.1110 | lr 3.00e-04 | grad 1.05 | tok/s 24166
step   1120 | loss 1.3928 | lr 3.00e-04 | grad 3.25 | tok/s 23531
step   1130 | loss 1.5700 | lr 3.00e-04 | grad 1.30 | tok/s 23769
step   1140 | loss 1.7031 | lr 3.00e-04 | grad 1.62 | tok/s 24032
step   1150 | loss 1.5645 | lr 3.00e-04 | grad 1.76 | tok/s 23274
step   1160 | loss 1.6752 | lr 3.00e-04 | grad 1.97 | tok/s 22942
step   1170 | loss 1.4338 | lr 3.00e-04 | grad 1.69 | tok/s 22685
step   1180 | loss 1.2874 | lr 3.00e-04 | grad 2.36 | tok/s 23824
step   1190 | loss 1.5256 | lr 3.00e-04 | grad 2.17 | tok/s 23993
step   1200 | loss 1.0739 | lr 3.00e-04 | grad 1.77 | tok/s 24156
step   1210 | loss 1.3261 | lr 3.00e-04 | grad 1.50 | tok/s 22550
step   1220 | loss 1.3215 | lr 3.00e-04 | grad 1.69 | tok/s 23657
step   1230 | loss 1.2850 | lr 3.00e-04 | grad 1.17 | tok/s 23708
step   1240 | loss 1.2672 | lr 3.00e-04 | grad 1.28 | tok/s 23725
step   1250 | loss 1.4067 | lr 3.00e-04 | grad 2.11 | tok/s 23408
step   1260 | loss 1.3355 | lr 3.00e-04 | grad 1.65 | tok/s 23896
step   1270 | loss 1.3141 | lr 3.00e-04 | grad 1.55 | tok/s 23232
step   1280 | loss 1.3357 | lr 3.00e-04 | grad 1.45 | tok/s 22992
step   1290 | loss 1.2879 | lr 3.00e-04 | grad 1.77 | tok/s 23078
step   1300 | loss 1.5773 | lr 3.00e-04 | grad 4.53 | tok/s 22713
step   1310 | loss 1.4083 | lr 3.00e-04 | grad 1.45 | tok/s 23542
step   1320 | loss 1.4128 | lr 3.00e-04 | grad 1.59 | tok/s 23574
step   1330 | loss 1.3769 | lr 3.00e-04 | grad 1.35 | tok/s 23350
step   1340 | loss 1.4664 | lr 3.00e-04 | grad 1.84 | tok/s 22898
step   1350 | loss 1.3476 | lr 3.00e-04 | grad 1.23 | tok/s 23362
step   1360 | loss 1.4008 | lr 3.00e-04 | grad 1.46 | tok/s 22538
step   1370 | loss 1.5291 | lr 3.00e-04 | grad 2.16 | tok/s 23724
step   1380 | loss 1.3842 | lr 3.00e-04 | grad 1.49 | tok/s 22692
step   1390 | loss 1.2707 | lr 3.00e-04 | grad 2.03 | tok/s 23752
step   1400 | loss 1.4240 | lr 3.00e-04 | grad 1.37 | tok/s 22930
step   1410 | loss 1.3554 | lr 3.00e-04 | grad 2.94 | tok/s 22435
step   1420 | loss 1.0449 | lr 3.00e-04 | grad 4.06 | tok/s 23856
step   1430 | loss 1.5931 | lr 3.00e-04 | grad 1.59 | tok/s 23023
step   1440 | loss 1.3790 | lr 3.00e-04 | grad 1.78 | tok/s 23628
step   1450 | loss 1.3566 | lr 3.00e-04 | grad 5.78 | tok/s 23595
step   1460 | loss 1.4986 | lr 3.00e-04 | grad 3.19 | tok/s 22936
step   1470 | loss 1.2943 | lr 3.00e-04 | grad 1.34 | tok/s 22424
step   1480 | loss 1.3052 | lr 3.00e-04 | grad 1.26 | tok/s 23652
step   1490 | loss 1.7764 | lr 3.00e-04 | grad 6.81 | tok/s 23269
step   1500 | loss 1.4008 | lr 3.00e-04 | grad 1.55 | tok/s 23332
step   1510 | loss 1.1756 | lr 3.00e-04 | grad 1.47 | tok/s 23353
step   1520 | loss 1.4081 | lr 3.00e-04 | grad 1.34 | tok/s 23127
step   1530 | loss 1.3338 | lr 3.00e-04 | grad 1.40 | tok/s 23529
step   1540 | loss 1.4557 | lr 3.00e-04 | grad 1.28 | tok/s 23716
step   1550 | loss 1.4134 | lr 3.00e-04 | grad 1.70 | tok/s 23239
step   1560 | loss 1.0787 | lr 3.00e-04 | grad 1.48 | tok/s 24123
step   1570 | loss 1.2335 | lr 3.00e-04 | grad 1.25 | tok/s 23472
step   1580 | loss 1.2249 | lr 3.00e-04 | grad 1.84 | tok/s 23396
step   1590 | loss 1.3862 | lr 3.00e-04 | grad 1.50 | tok/s 22953
step   1600 | loss 1.1873 | lr 3.00e-04 | grad 2.22 | tok/s 23817
step   1610 | loss 1.8779 | lr 3.00e-04 | grad 2.52 | tok/s 23584
step   1620 | loss 1.9174 | lr 3.00e-04 | grad 1.69 | tok/s 24089
step   1630 | loss 1.6082 | lr 3.00e-04 | grad 2.05 | tok/s 24094
step   1640 | loss 1.4400 | lr 3.00e-04 | grad 1.74 | tok/s 24129
step   1650 | loss 1.3423 | lr 3.00e-04 | grad 1.72 | tok/s 23089
step   1660 | loss 1.2981 | lr 3.00e-04 | grad 1.60 | tok/s 24114
step   1670 | loss 1.4614 | lr 3.00e-04 | grad 2.34 | tok/s 23385
step   1680 | loss 1.3590 | lr 3.00e-04 | grad 1.37 | tok/s 23185
step   1690 | loss 1.4140 | lr 3.00e-04 | grad 1.70 | tok/s 22491
step   1700 | loss 1.2606 | lr 3.00e-04 | grad 1.24 | tok/s 23652
step   1710 | loss 1.1890 | lr 3.00e-04 | grad 1.55 | tok/s 23371
step   1720 | loss 1.4122 | lr 3.00e-04 | grad 1.45 | tok/s 23108

Training complete! Final step: 1728
