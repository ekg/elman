Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_115/levelfla-gdn_100m_20260127_082641
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,801,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1659 | lr 3.00e-04 | grad 16.12 | tok/s 10866
step     20 | loss 2.8144 | lr 3.00e-04 | grad 6.56 | tok/s 24613
step     30 | loss 3.0707 | lr 3.00e-04 | grad 5.50 | tok/s 25893
step     40 | loss 4.1756 | lr 3.00e-04 | grad 26.38 | tok/s 26284
step     50 | loss 4.1107 | lr 3.00e-04 | grad 13.31 | tok/s 26552
step     60 | loss 3.3454 | lr 3.00e-04 | grad 11.75 | tok/s 26361
step     70 | loss 2.7777 | lr 3.00e-04 | grad 7.16 | tok/s 26192
step     80 | loss 2.4656 | lr 3.00e-04 | grad 6.34 | tok/s 26086
step     90 | loss 2.3494 | lr 3.00e-04 | grad 4.88 | tok/s 25894
step    100 | loss 2.1144 | lr 3.00e-04 | grad 3.36 | tok/s 25771
step    110 | loss 2.1649 | lr 3.00e-04 | grad 4.50 | tok/s 25448
step    120 | loss 2.7537 | lr 3.00e-04 | grad 3.42 | tok/s 24187
step    130 | loss 2.0372 | lr 3.00e-04 | grad 5.84 | tok/s 24654
step    140 | loss 2.3068 | lr 3.00e-04 | grad 8.62 | tok/s 24635
step    150 | loss 1.4742 | lr 3.00e-04 | grad 6.50 | tok/s 25159
step    160 | loss 2.2354 | lr 3.00e-04 | grad 2.70 | tok/s 24216
step    170 | loss 2.1705 | lr 3.00e-04 | grad 2.20 | tok/s 23789
step    180 | loss 1.8039 | lr 3.00e-04 | grad 3.33 | tok/s 24276
step    190 | loss 1.8025 | lr 3.00e-04 | grad 2.77 | tok/s 23744
step    200 | loss 1.5349 | lr 3.00e-04 | grad 1.95 | tok/s 24736
step    210 | loss 1.7513 | lr 3.00e-04 | grad 3.98 | tok/s 23420
step    220 | loss 2.0979 | lr 3.00e-04 | grad 3.34 | tok/s 23607
step    230 | loss 1.8388 | lr 3.00e-04 | grad 2.98 | tok/s 23557
step    240 | loss 2.1655 | lr 3.00e-04 | grad 5.91 | tok/s 23770
step    250 | loss 1.6432 | lr 3.00e-04 | grad 1.70 | tok/s 23571
step    260 | loss 1.7766 | lr 3.00e-04 | grad 3.16 | tok/s 24183
step    270 | loss 1.7108 | lr 3.00e-04 | grad 2.06 | tok/s 23584
step    280 | loss 1.6665 | lr 3.00e-04 | grad 2.03 | tok/s 22153
step    290 | loss 1.5604 | lr 3.00e-04 | grad 2.42 | tok/s 22861
step    300 | loss 1.8522 | lr 3.00e-04 | grad 2.62 | tok/s 22994
step    310 | loss 1.5688 | lr 3.00e-04 | grad 1.85 | tok/s 22870
step    320 | loss 1.7787 | lr 3.00e-04 | grad 4.44 | tok/s 23119
step    330 | loss 1.6105 | lr 3.00e-04 | grad 2.06 | tok/s 23325
step    340 | loss 1.9251 | lr 3.00e-04 | grad 2.34 | tok/s 23206
step    350 | loss 1.6248 | lr 3.00e-04 | grad 2.06 | tok/s 23813
step    360 | loss 1.4797 | lr 3.00e-04 | grad 2.02 | tok/s 22763
step    370 | loss 1.3959 | lr 3.00e-04 | grad 1.72 | tok/s 23957
step    380 | loss 1.1264 | lr 3.00e-04 | grad 1.74 | tok/s 24094
step    390 | loss 1.0282 | lr 3.00e-04 | grad 1.37 | tok/s 24089
step    400 | loss 1.6577 | lr 3.00e-04 | grad 1.70 | tok/s 22809
step    410 | loss 1.6533 | lr 3.00e-04 | grad 2.62 | tok/s 23013
step    420 | loss 1.5405 | lr 3.00e-04 | grad 2.95 | tok/s 23929
step    430 | loss 1.4735 | lr 3.00e-04 | grad 1.82 | tok/s 23547
step    440 | loss 1.5979 | lr 3.00e-04 | grad 2.47 | tok/s 22799
step    450 | loss 1.5350 | lr 3.00e-04 | grad 1.45 | tok/s 23039
step    460 | loss 1.5212 | lr 3.00e-04 | grad 1.84 | tok/s 23407
step    470 | loss 1.4856 | lr 3.00e-04 | grad 3.59 | tok/s 23194
step    480 | loss 1.5418 | lr 3.00e-04 | grad 2.86 | tok/s 23670
step    490 | loss 1.5902 | lr 3.00e-04 | grad 2.39 | tok/s 22743
step    500 | loss 1.7067 | lr 3.00e-04 | grad 1.58 | tok/s 23102
step    510 | loss 1.5999 | lr 3.00e-04 | grad 1.39 | tok/s 22075
step    520 | loss 1.4589 | lr 3.00e-04 | grad 3.86 | tok/s 23074
step    530 | loss 1.6210 | lr 3.00e-04 | grad 2.03 | tok/s 22696
step    540 | loss 1.5168 | lr 3.00e-04 | grad 1.63 | tok/s 22201
step    550 | loss 1.2751 | lr 3.00e-04 | grad 3.17 | tok/s 23269
step    560 | loss 1.3716 | lr 3.00e-04 | grad 1.86 | tok/s 23806
step    570 | loss 1.2747 | lr 3.00e-04 | grad 1.81 | tok/s 23826
step    580 | loss 1.2342 | lr 3.00e-04 | grad 1.31 | tok/s 23768
step    590 | loss 1.2696 | lr 3.00e-04 | grad 1.40 | tok/s 23779
step    600 | loss 1.1970 | lr 3.00e-04 | grad 1.58 | tok/s 23775
step    610 | loss 1.2398 | lr 3.00e-04 | grad 1.74 | tok/s 23793
step    620 | loss 1.2209 | lr 3.00e-04 | grad 1.84 | tok/s 23685
step    630 | loss 1.5361 | lr 3.00e-04 | grad 6.53 | tok/s 22459
step    640 | loss 1.6546 | lr 3.00e-04 | grad 1.77 | tok/s 22675
step    650 | loss 1.4677 | lr 3.00e-04 | grad 1.74 | tok/s 22658
step    660 | loss 1.5223 | lr 3.00e-04 | grad 1.89 | tok/s 23498
step    670 | loss 1.5403 | lr 3.00e-04 | grad 4.84 | tok/s 22703
step    680 | loss 1.5428 | lr 3.00e-04 | grad 2.14 | tok/s 22364
step    690 | loss 1.4836 | lr 3.00e-04 | grad 1.88 | tok/s 22192
step    700 | loss 1.3950 | lr 3.00e-04 | grad 1.36 | tok/s 22681
step    710 | loss 1.5414 | lr 3.00e-04 | grad 3.97 | tok/s 22332
step    720 | loss 1.2353 | lr 3.00e-04 | grad 1.59 | tok/s 23177
step    730 | loss 1.3677 | lr 3.00e-04 | grad 1.47 | tok/s 22824
step    740 | loss 1.6813 | lr 3.00e-04 | grad 4.97 | tok/s 23424
step    750 | loss 1.4650 | lr 3.00e-04 | grad 1.66 | tok/s 23698
step    760 | loss 1.4354 | lr 3.00e-04 | grad 3.77 | tok/s 23173
step    770 | loss 1.4976 | lr 3.00e-04 | grad 1.88 | tok/s 22801
step    780 | loss 1.4119 | lr 3.00e-04 | grad 1.73 | tok/s 22951
step    790 | loss 1.5324 | lr 3.00e-04 | grad 5.38 | tok/s 23454
step    800 | loss 1.2566 | lr 3.00e-04 | grad 1.90 | tok/s 23115
step    810 | loss 1.2528 | lr 3.00e-04 | grad 2.56 | tok/s 22336
step    820 | loss 1.3434 | lr 3.00e-04 | grad 1.68 | tok/s 22743
step    830 | loss 1.4240 | lr 3.00e-04 | grad 1.27 | tok/s 22424
step    840 | loss 1.5281 | lr 3.00e-04 | grad 1.68 | tok/s 22329
step    850 | loss 1.4432 | lr 3.00e-04 | grad 1.45 | tok/s 22760
step    860 | loss 1.4826 | lr 3.00e-04 | grad 2.47 | tok/s 23147
step    870 | loss 1.2987 | lr 3.00e-04 | grad 1.70 | tok/s 23322
step    880 | loss 1.5115 | lr 3.00e-04 | grad 1.63 | tok/s 22876
step    890 | loss 1.4200 | lr 3.00e-04 | grad 1.32 | tok/s 22788
step    900 | loss 1.4595 | lr 3.00e-04 | grad 1.54 | tok/s 22678
step    910 | loss 1.4214 | lr 3.00e-04 | grad 6.75 | tok/s 22442
step    920 | loss 1.3937 | lr 3.00e-04 | grad 1.69 | tok/s 22682
step    930 | loss 1.3228 | lr 3.00e-04 | grad 1.77 | tok/s 22989
step    940 | loss 1.2782 | lr 3.00e-04 | grad 1.59 | tok/s 22473
step    950 | loss 1.4226 | lr 3.00e-04 | grad 2.17 | tok/s 22075
step    960 | loss 1.3752 | lr 3.00e-04 | grad 1.41 | tok/s 22670
step    970 | loss 1.4108 | lr 3.00e-04 | grad 1.55 | tok/s 22673
step    980 | loss 1.8121 | lr 3.00e-04 | grad 3.33 | tok/s 23575
step    990 | loss 1.4876 | lr 3.00e-04 | grad 1.62 | tok/s 22630
step   1000 | loss 1.4876 | lr 3.00e-04 | grad 2.25 | tok/s 22719
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4876.pt
step   1010 | loss 1.2891 | lr 3.00e-04 | grad 1.74 | tok/s 11134
step   1020 | loss 1.1481 | lr 3.00e-04 | grad 1.37 | tok/s 24153
step   1030 | loss 1.5237 | lr 3.00e-04 | grad 1.66 | tok/s 22728
step   1040 | loss 1.9965 | lr 3.00e-04 | grad 2.64 | tok/s 23424
step   1050 | loss 1.3663 | lr 3.00e-04 | grad 1.74 | tok/s 23144
step   1060 | loss 1.0405 | lr 3.00e-04 | grad 1.34 | tok/s 23672
step   1070 | loss 1.3918 | lr 3.00e-04 | grad 1.63 | tok/s 23123
step   1080 | loss 1.2006 | lr 3.00e-04 | grad 1.64 | tok/s 23893
step   1090 | loss 1.1816 | lr 3.00e-04 | grad 1.37 | tok/s 23858
step   1100 | loss 1.1389 | lr 3.00e-04 | grad 1.54 | tok/s 23834
step   1110 | loss 1.1254 | lr 3.00e-04 | grad 1.44 | tok/s 23847
step   1120 | loss 1.4082 | lr 3.00e-04 | grad 3.00 | tok/s 23214
step   1130 | loss 1.5198 | lr 3.00e-04 | grad 2.39 | tok/s 23398
step   1140 | loss 1.6293 | lr 3.00e-04 | grad 1.41 | tok/s 23696
step   1150 | loss 1.5611 | lr 3.00e-04 | grad 2.20 | tok/s 22542
step   1160 | loss 1.6868 | lr 3.00e-04 | grad 6.62 | tok/s 22824
step   1170 | loss 1.3624 | lr 3.00e-04 | grad 1.80 | tok/s 22482
step   1180 | loss 1.2778 | lr 3.00e-04 | grad 1.84 | tok/s 23301
step   1190 | loss 1.4368 | lr 3.00e-04 | grad 5.28 | tok/s 23750
step   1200 | loss 1.0673 | lr 3.00e-04 | grad 3.22 | tok/s 23781
step   1210 | loss 1.3742 | lr 3.00e-04 | grad 2.20 | tok/s 22165
step   1220 | loss 1.2826 | lr 3.00e-04 | grad 1.41 | tok/s 23010
step   1230 | loss 1.2454 | lr 3.00e-04 | grad 1.21 | tok/s 23614
step   1240 | loss 1.2559 | lr 3.00e-04 | grad 1.31 | tok/s 23208
step   1250 | loss 1.3897 | lr 3.00e-04 | grad 4.38 | tok/s 23254
step   1260 | loss 1.3065 | lr 3.00e-04 | grad 1.81 | tok/s 23433
step   1270 | loss 1.2981 | lr 3.00e-04 | grad 1.36 | tok/s 23001
step   1280 | loss 1.3420 | lr 3.00e-04 | grad 1.75 | tok/s 22635
step   1290 | loss 1.2516 | lr 3.00e-04 | grad 1.60 | tok/s 22707
step   1300 | loss 1.5373 | lr 3.00e-04 | grad 4.66 | tok/s 22303
step   1310 | loss 1.4073 | lr 3.00e-04 | grad 1.62 | tok/s 23129
step   1320 | loss 1.4276 | lr 3.00e-04 | grad 3.33 | tok/s 23306
step   1330 | loss 1.3227 | lr 3.00e-04 | grad 1.74 | tok/s 22983
step   1340 | loss 1.4655 | lr 3.00e-04 | grad 1.70 | tok/s 22581
step   1350 | loss 1.3791 | lr 3.00e-04 | grad 4.25 | tok/s 22875
step   1360 | loss 1.3164 | lr 3.00e-04 | grad 1.64 | tok/s 22304
step   1370 | loss 1.5677 | lr 3.00e-04 | grad 2.72 | tok/s 23352
step   1380 | loss 1.3357 | lr 3.00e-04 | grad 2.44 | tok/s 22251
step   1390 | loss 1.2520 | lr 3.00e-04 | grad 2.12 | tok/s 23401
step   1400 | loss 1.3741 | lr 3.00e-04 | grad 1.35 | tok/s 22664
step   1410 | loss 1.3618 | lr 3.00e-04 | grad 5.12 | tok/s 22102
step   1420 | loss 1.1575 | lr 3.00e-04 | grad 7.03 | tok/s 23469
step   1430 | loss 1.4734 | lr 3.00e-04 | grad 1.57 | tok/s 22602
step   1440 | loss 1.3514 | lr 3.00e-04 | grad 1.96 | tok/s 23298
step   1450 | loss 1.3451 | lr 3.00e-04 | grad 1.82 | tok/s 23183
step   1460 | loss 1.5061 | lr 3.00e-04 | grad 3.91 | tok/s 22636
step   1470 | loss 1.2581 | lr 3.00e-04 | grad 1.45 | tok/s 21871
step   1480 | loss 1.2925 | lr 3.00e-04 | grad 2.06 | tok/s 23415
step   1490 | loss 1.7938 | lr 3.00e-04 | grad 2.77 | tok/s 22799
step   1500 | loss 1.3451 | lr 3.00e-04 | grad 1.67 | tok/s 22993
step   1510 | loss 1.1884 | lr 3.00e-04 | grad 1.27 | tok/s 22831
step   1520 | loss 1.4014 | lr 3.00e-04 | grad 1.70 | tok/s 22873
step   1530 | loss 1.3258 | lr 3.00e-04 | grad 2.36 | tok/s 23087
step   1540 | loss 1.4437 | lr 3.00e-04 | grad 1.50 | tok/s 23294
step   1550 | loss 1.3849 | lr 3.00e-04 | grad 2.70 | tok/s 22823
step   1560 | loss 1.0483 | lr 3.00e-04 | grad 1.52 | tok/s 23705
step   1570 | loss 1.1847 | lr 3.00e-04 | grad 1.27 | tok/s 23039
step   1580 | loss 1.3040 | lr 3.00e-04 | grad 6.97 | tok/s 22904
step   1590 | loss 1.3074 | lr 3.00e-04 | grad 1.56 | tok/s 22545
step   1600 | loss 1.2199 | lr 3.00e-04 | grad 2.08 | tok/s 23118
step   1610 | loss 1.9299 | lr 3.00e-04 | grad 2.45 | tok/s 23381
step   1620 | loss 1.8259 | lr 3.00e-04 | grad 2.45 | tok/s 23704
step   1630 | loss 1.5648 | lr 3.00e-04 | grad 1.91 | tok/s 23658
step   1640 | loss 1.4070 | lr 3.00e-04 | grad 2.00 | tok/s 23685
step   1650 | loss 1.3322 | lr 3.00e-04 | grad 2.23 | tok/s 23661
step   1660 | loss 1.2748 | lr 3.00e-04 | grad 1.97 | tok/s 23725
step   1670 | loss 1.4300 | lr 3.00e-04 | grad 1.98 | tok/s 22941
step   1680 | loss 1.3401 | lr 3.00e-04 | grad 1.47 | tok/s 22338
step   1690 | loss 1.4054 | lr 3.00e-04 | grad 1.27 | tok/s 22471
step   1700 | loss 1.2174 | lr 3.00e-04 | grad 1.39 | tok/s 23346
step   1710 | loss 1.2106 | lr 3.00e-04 | grad 1.59 | tok/s 21791
step   1720 | loss 1.3821 | lr 3.00e-04 | grad 1.79 | tok/s 22759

Training complete! Final step: 1722
