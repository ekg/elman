Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_1/levelfla-gdn_100m_20260127_060254
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,835,352 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1066 | lr 3.00e-04 | grad 16.50 | tok/s 10271
step     20 | loss 2.9181 | lr 3.00e-04 | grad 6.47 | tok/s 21974
step     30 | loss 3.2443 | lr 3.00e-04 | grad 8.00 | tok/s 23091
step     40 | loss 4.2973 | lr 3.00e-04 | grad 35.75 | tok/s 23452
step     50 | loss 4.4394 | lr 3.00e-04 | grad 22.00 | tok/s 23666
step     60 | loss 3.5396 | lr 3.00e-04 | grad 16.00 | tok/s 23548
step     70 | loss 2.9747 | lr 3.00e-04 | grad 9.38 | tok/s 23366
step     80 | loss 2.6972 | lr 3.00e-04 | grad 9.25 | tok/s 23260
step     90 | loss 2.5171 | lr 3.00e-04 | grad 6.72 | tok/s 23114
step    100 | loss 2.3094 | lr 3.00e-04 | grad 3.69 | tok/s 22968
step    110 | loss 2.3684 | lr 3.00e-04 | grad 4.81 | tok/s 22620
step    120 | loss 2.9277 | lr 3.00e-04 | grad 3.33 | tok/s 21453
step    130 | loss 2.1659 | lr 3.00e-04 | grad 4.88 | tok/s 21897
step    140 | loss 2.4131 | lr 3.00e-04 | grad 8.38 | tok/s 21825
step    150 | loss 1.5957 | lr 3.00e-04 | grad 5.50 | tok/s 22253
step    160 | loss 2.3133 | lr 3.00e-04 | grad 2.19 | tok/s 21391
step    170 | loss 2.2358 | lr 3.00e-04 | grad 2.06 | tok/s 20975
step    180 | loss 1.9715 | lr 3.00e-04 | grad 3.27 | tok/s 21371
step    190 | loss 1.8793 | lr 3.00e-04 | grad 2.22 | tok/s 20904
step    200 | loss 1.6118 | lr 3.00e-04 | grad 1.74 | tok/s 21713
step    210 | loss 1.8083 | lr 3.00e-04 | grad 5.66 | tok/s 20557
step    220 | loss 2.1933 | lr 3.00e-04 | grad 11.19 | tok/s 20659
step    230 | loss 1.8761 | lr 3.00e-04 | grad 2.88 | tok/s 20579
step    240 | loss 2.2052 | lr 3.00e-04 | grad 5.72 | tok/s 20788
step    250 | loss 1.6851 | lr 3.00e-04 | grad 1.41 | tok/s 20622
step    260 | loss 1.8229 | lr 3.00e-04 | grad 2.83 | tok/s 21086
step    270 | loss 1.7466 | lr 3.00e-04 | grad 1.71 | tok/s 20570
step    280 | loss 1.6913 | lr 3.00e-04 | grad 1.68 | tok/s 19324
step    290 | loss 1.5867 | lr 3.00e-04 | grad 2.08 | tok/s 19913
step    300 | loss 1.8775 | lr 3.00e-04 | grad 2.14 | tok/s 20044
step    310 | loss 1.5924 | lr 3.00e-04 | grad 1.59 | tok/s 19881
step    320 | loss 1.8052 | lr 3.00e-04 | grad 3.72 | tok/s 20083
step    330 | loss 1.6367 | lr 3.00e-04 | grad 1.68 | tok/s 20268
step    340 | loss 1.9617 | lr 3.00e-04 | grad 2.11 | tok/s 20156
step    350 | loss 1.6871 | lr 3.00e-04 | grad 1.83 | tok/s 20684
step    360 | loss 1.5013 | lr 3.00e-04 | grad 1.84 | tok/s 19815
step    370 | loss 1.4217 | lr 3.00e-04 | grad 1.52 | tok/s 20832
step    380 | loss 1.1529 | lr 3.00e-04 | grad 1.57 | tok/s 20996
step    390 | loss 1.0442 | lr 3.00e-04 | grad 1.21 | tok/s 20986
step    400 | loss 1.6884 | lr 3.00e-04 | grad 1.53 | tok/s 19875
step    410 | loss 1.6746 | lr 3.00e-04 | grad 2.38 | tok/s 19993
step    420 | loss 1.5805 | lr 3.00e-04 | grad 2.66 | tok/s 20848
step    430 | loss 1.5073 | lr 3.00e-04 | grad 1.55 | tok/s 20471
step    440 | loss 1.6233 | lr 3.00e-04 | grad 2.03 | tok/s 19859
step    450 | loss 1.5526 | lr 3.00e-04 | grad 1.28 | tok/s 19188
step    460 | loss 1.5372 | lr 3.00e-04 | grad 1.71 | tok/s 20402
step    470 | loss 1.5012 | lr 3.00e-04 | grad 3.12 | tok/s 20226
step    480 | loss 1.5373 | lr 3.00e-04 | grad 2.45 | tok/s 20683
step    490 | loss 1.6067 | lr 3.00e-04 | grad 2.05 | tok/s 19836
step    500 | loss 1.7339 | lr 3.00e-04 | grad 1.41 | tok/s 20157
step    510 | loss 1.6156 | lr 3.00e-04 | grad 1.26 | tok/s 19285
step    520 | loss 1.4692 | lr 3.00e-04 | grad 2.78 | tok/s 20178
step    530 | loss 1.6468 | lr 3.00e-04 | grad 1.90 | tok/s 19817
step    540 | loss 1.5349 | lr 3.00e-04 | grad 1.47 | tok/s 19403
step    550 | loss 1.2877 | lr 3.00e-04 | grad 2.66 | tok/s 20292
step    560 | loss 1.3860 | lr 3.00e-04 | grad 1.44 | tok/s 20848
step    570 | loss 1.2844 | lr 3.00e-04 | grad 1.47 | tok/s 20804
step    580 | loss 1.2452 | lr 3.00e-04 | grad 1.14 | tok/s 20841
step    590 | loss 1.2821 | lr 3.00e-04 | grad 1.18 | tok/s 20864
step    600 | loss 1.2145 | lr 3.00e-04 | grad 1.38 | tok/s 20859
step    610 | loss 1.2536 | lr 3.00e-04 | grad 1.37 | tok/s 20840
step    620 | loss 1.2316 | lr 3.00e-04 | grad 1.39 | tok/s 20756
step    630 | loss 1.6046 | lr 3.00e-04 | grad 5.81 | tok/s 19646
step    640 | loss 1.6725 | lr 3.00e-04 | grad 1.67 | tok/s 19902
step    650 | loss 1.4863 | lr 3.00e-04 | grad 1.46 | tok/s 19930
step    660 | loss 1.5376 | lr 3.00e-04 | grad 1.63 | tok/s 20637
step    670 | loss 1.5535 | lr 3.00e-04 | grad 4.38 | tok/s 19972
step    680 | loss 1.5583 | lr 3.00e-04 | grad 1.79 | tok/s 19683
step    690 | loss 1.5170 | lr 3.00e-04 | grad 1.78 | tok/s 19506
step    700 | loss 1.4175 | lr 3.00e-04 | grad 1.28 | tok/s 19919
step    710 | loss 1.5513 | lr 3.00e-04 | grad 3.25 | tok/s 19595
step    720 | loss 1.2490 | lr 3.00e-04 | grad 1.30 | tok/s 20362
step    730 | loss 1.3842 | lr 3.00e-04 | grad 1.23 | tok/s 20033
step    740 | loss 1.7178 | lr 3.00e-04 | grad 3.94 | tok/s 20558
step    750 | loss 1.4913 | lr 3.00e-04 | grad 1.41 | tok/s 20805
step    760 | loss 1.4495 | lr 3.00e-04 | grad 3.14 | tok/s 20376
step    770 | loss 1.5139 | lr 3.00e-04 | grad 1.64 | tok/s 20044
step    780 | loss 1.4253 | lr 3.00e-04 | grad 1.50 | tok/s 20173
step    790 | loss 1.5757 | lr 3.00e-04 | grad 4.94 | tok/s 20630
step    800 | loss 1.2764 | lr 3.00e-04 | grad 1.57 | tok/s 20302
step    810 | loss 1.2663 | lr 3.00e-04 | grad 2.47 | tok/s 19646
step    820 | loss 1.3601 | lr 3.00e-04 | grad 1.49 | tok/s 19976
step    830 | loss 1.4389 | lr 3.00e-04 | grad 1.17 | tok/s 19737
step    840 | loss 1.5502 | lr 3.00e-04 | grad 1.41 | tok/s 19632
step    850 | loss 1.4616 | lr 3.00e-04 | grad 1.30 | tok/s 20044
step    860 | loss 1.5050 | lr 3.00e-04 | grad 2.12 | tok/s 20351
step    870 | loss 1.3440 | lr 3.00e-04 | grad 1.60 | tok/s 20490
step    880 | loss 1.5318 | lr 3.00e-04 | grad 1.42 | tok/s 20093
step    890 | loss 1.4356 | lr 3.00e-04 | grad 1.12 | tok/s 19991
step    900 | loss 1.4727 | lr 3.00e-04 | grad 1.41 | tok/s 19920
step    910 | loss 1.4463 | lr 3.00e-04 | grad 5.94 | tok/s 19709
step    920 | loss 1.4095 | lr 3.00e-04 | grad 1.41 | tok/s 19922
step    930 | loss 1.3321 | lr 3.00e-04 | grad 1.52 | tok/s 20178
step    940 | loss 1.3003 | lr 3.00e-04 | grad 1.48 | tok/s 19737
step    950 | loss 1.4381 | lr 3.00e-04 | grad 1.95 | tok/s 19389
step    960 | loss 1.3880 | lr 3.00e-04 | grad 1.17 | tok/s 19910
step    970 | loss 1.4226 | lr 3.00e-04 | grad 1.40 | tok/s 19929
step    980 | loss 1.8666 | lr 3.00e-04 | grad 3.20 | tok/s 20011
step    990 | loss 1.5063 | lr 3.00e-04 | grad 1.45 | tok/s 19869
step   1000 | loss 1.5170 | lr 3.00e-04 | grad 1.76 | tok/s 19952
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5170.pt
step   1010 | loss 1.3023 | lr 3.00e-04 | grad 1.58 | tok/s 10142
step   1020 | loss 1.1657 | lr 3.00e-04 | grad 1.13 | tok/s 21217
step   1030 | loss 1.5441 | lr 3.00e-04 | grad 1.51 | tok/s 19950
step   1040 | loss 2.0188 | lr 3.00e-04 | grad 2.30 | tok/s 20578
step   1050 | loss 1.3953 | lr 3.00e-04 | grad 1.51 | tok/s 20332
step   1060 | loss 1.0757 | lr 3.00e-04 | grad 1.09 | tok/s 20807
step   1070 | loss 1.4038 | lr 3.00e-04 | grad 1.31 | tok/s 20323
step   1080 | loss 1.2076 | lr 3.00e-04 | grad 1.26 | tok/s 20991
step   1090 | loss 1.1880 | lr 3.00e-04 | grad 1.21 | tok/s 20973
step   1100 | loss 1.1465 | lr 3.00e-04 | grad 1.27 | tok/s 20951
step   1110 | loss 1.1338 | lr 3.00e-04 | grad 1.27 | tok/s 20899
step   1120 | loss 1.4311 | lr 3.00e-04 | grad 2.67 | tok/s 20335
step   1130 | loss 1.5671 | lr 3.00e-04 | grad 2.20 | tok/s 20530
step   1140 | loss 1.6434 | lr 3.00e-04 | grad 1.25 | tok/s 20770
step   1150 | loss 1.5971 | lr 3.00e-04 | grad 3.05 | tok/s 19771
step   1160 | loss 1.6982 | lr 3.00e-04 | grad 5.62 | tok/s 20044
step   1170 | loss 1.3836 | lr 3.00e-04 | grad 1.52 | tok/s 19740
step   1180 | loss 1.2939 | lr 3.00e-04 | grad 1.63 | tok/s 20454
step   1190 | loss 1.4846 | lr 3.00e-04 | grad 5.03 | tok/s 20880
step   1200 | loss 1.0841 | lr 3.00e-04 | grad 2.88 | tok/s 20900
step   1210 | loss 1.3873 | lr 3.00e-04 | grad 1.96 | tok/s 19479
step   1220 | loss 1.3000 | lr 3.00e-04 | grad 1.27 | tok/s 20240
step   1230 | loss 1.2548 | lr 3.00e-04 | grad 1.15 | tok/s 20718
step   1240 | loss 1.2729 | lr 3.00e-04 | grad 1.14 | tok/s 20370
step   1250 | loss 1.4101 | lr 3.00e-04 | grad 3.39 | tok/s 20423
step   1260 | loss 1.3302 | lr 3.00e-04 | grad 1.58 | tok/s 20579
step   1270 | loss 1.3102 | lr 3.00e-04 | grad 1.19 | tok/s 20194
step   1280 | loss 1.3666 | lr 3.00e-04 | grad 1.45 | tok/s 19906
step   1290 | loss 1.2704 | lr 3.00e-04 | grad 1.45 | tok/s 19941
step   1300 | loss 1.5520 | lr 3.00e-04 | grad 4.34 | tok/s 19594
step   1310 | loss 1.4210 | lr 3.00e-04 | grad 1.45 | tok/s 20323
step   1320 | loss 1.4467 | lr 3.00e-04 | grad 2.62 | tok/s 20447
step   1330 | loss 1.3389 | lr 3.00e-04 | grad 1.36 | tok/s 20153
step   1340 | loss 1.4725 | lr 3.00e-04 | grad 1.48 | tok/s 19800
step   1350 | loss 1.3974 | lr 3.00e-04 | grad 3.81 | tok/s 20052
step   1360 | loss 1.3378 | lr 3.00e-04 | grad 1.53 | tok/s 19580
step   1370 | loss 1.5958 | lr 3.00e-04 | grad 2.47 | tok/s 20470
step   1380 | loss 1.3585 | lr 3.00e-04 | grad 2.34 | tok/s 19542
step   1390 | loss 1.2628 | lr 3.00e-04 | grad 1.76 | tok/s 20518
step   1400 | loss 1.3898 | lr 3.00e-04 | grad 1.17 | tok/s 19860
step   1410 | loss 1.3756 | lr 3.00e-04 | grad 4.00 | tok/s 19370
step   1420 | loss 1.1872 | lr 3.00e-04 | grad 6.16 | tok/s 20586
step   1430 | loss 1.4816 | lr 3.00e-04 | grad 1.44 | tok/s 19817
step   1440 | loss 1.3766 | lr 3.00e-04 | grad 1.73 | tok/s 20437
step   1450 | loss 1.3592 | lr 3.00e-04 | grad 1.56 | tok/s 20314
step   1460 | loss 1.5254 | lr 3.00e-04 | grad 3.41 | tok/s 19854
step   1470 | loss 1.2675 | lr 3.00e-04 | grad 1.27 | tok/s 19195
step   1480 | loss 1.3157 | lr 3.00e-04 | grad 1.82 | tok/s 20505
step   1490 | loss 1.8316 | lr 3.00e-04 | grad 4.00 | tok/s 19988
step   1500 | loss 1.3609 | lr 3.00e-04 | grad 1.58 | tok/s 19798
step   1510 | loss 1.1738 | lr 3.00e-04 | grad 1.09 | tok/s 20024

Training complete! Final step: 1516
