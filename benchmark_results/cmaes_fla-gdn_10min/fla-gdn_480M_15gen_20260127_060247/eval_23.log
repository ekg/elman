Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_23/levelfla-gdn_100m_20260127_062323
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 467,110,690 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0883 | lr 3.00e-04 | grad 15.38 | tok/s 10368
step     20 | loss 2.8271 | lr 3.00e-04 | grad 7.44 | tok/s 21325
step     30 | loss 3.1468 | lr 3.00e-04 | grad 9.31 | tok/s 22467
step     40 | loss 4.2147 | lr 3.00e-04 | grad 30.12 | tok/s 22877
step     50 | loss 4.3627 | lr 3.00e-04 | grad 16.38 | tok/s 23116
step     60 | loss 3.4221 | lr 3.00e-04 | grad 15.06 | tok/s 23038
step     70 | loss 2.8373 | lr 3.00e-04 | grad 8.81 | tok/s 22908
step     80 | loss 2.6250 | lr 3.00e-04 | grad 8.81 | tok/s 22852
step     90 | loss 2.4042 | lr 3.00e-04 | grad 5.84 | tok/s 22767
step    100 | loss 2.2548 | lr 3.00e-04 | grad 3.33 | tok/s 22733
step    110 | loss 2.3141 | lr 3.00e-04 | grad 4.50 | tok/s 22481
step    120 | loss 2.8935 | lr 3.00e-04 | grad 3.59 | tok/s 21394
step    130 | loss 2.1331 | lr 3.00e-04 | grad 5.41 | tok/s 21847
step    140 | loss 2.3631 | lr 3.00e-04 | grad 8.69 | tok/s 21941
step    150 | loss 1.7099 | lr 3.00e-04 | grad 5.97 | tok/s 22445
step    160 | loss 2.3017 | lr 3.00e-04 | grad 2.44 | tok/s 21674
step    170 | loss 2.2240 | lr 3.00e-04 | grad 2.11 | tok/s 21319
step    180 | loss 1.9574 | lr 3.00e-04 | grad 3.27 | tok/s 21849
step    190 | loss 1.8686 | lr 3.00e-04 | grad 2.50 | tok/s 21406
step    200 | loss 1.6044 | lr 3.00e-04 | grad 1.96 | tok/s 22356
step    210 | loss 1.7975 | lr 3.00e-04 | grad 4.66 | tok/s 21260
step    220 | loss 2.1816 | lr 3.00e-04 | grad 3.97 | tok/s 21484
step    230 | loss 1.9871 | lr 3.00e-04 | grad 2.59 | tok/s 21411
step    240 | loss 2.2141 | lr 3.00e-04 | grad 5.66 | tok/s 21669
step    250 | loss 1.6820 | lr 3.00e-04 | grad 1.52 | tok/s 21527
step    260 | loss 1.8132 | lr 3.00e-04 | grad 3.08 | tok/s 22119
step    270 | loss 1.7455 | lr 3.00e-04 | grad 1.85 | tok/s 21594
step    280 | loss 1.6953 | lr 3.00e-04 | grad 1.91 | tok/s 20299
step    290 | loss 1.5821 | lr 3.00e-04 | grad 2.28 | tok/s 20974
step    300 | loss 1.8745 | lr 3.00e-04 | grad 2.36 | tok/s 21149
step    310 | loss 1.5923 | lr 3.00e-04 | grad 1.75 | tok/s 21058
step    320 | loss 1.7969 | lr 3.00e-04 | grad 3.94 | tok/s 21315
step    330 | loss 1.6373 | lr 3.00e-04 | grad 1.80 | tok/s 21527
step    340 | loss 1.9569 | lr 3.00e-04 | grad 2.12 | tok/s 21442
step    350 | loss 1.6765 | lr 3.00e-04 | grad 1.87 | tok/s 22029
step    360 | loss 1.5009 | lr 3.00e-04 | grad 2.00 | tok/s 21098
step    370 | loss 1.4151 | lr 3.00e-04 | grad 1.54 | tok/s 22198
step    380 | loss 1.1507 | lr 3.00e-04 | grad 1.65 | tok/s 22419
step    390 | loss 1.0455 | lr 3.00e-04 | grad 1.32 | tok/s 22379
step    400 | loss 1.6817 | lr 3.00e-04 | grad 1.58 | tok/s 21191
step    410 | loss 1.6695 | lr 3.00e-04 | grad 2.33 | tok/s 21414
step    420 | loss 1.5709 | lr 3.00e-04 | grad 2.53 | tok/s 22313
step    430 | loss 1.5045 | lr 3.00e-04 | grad 1.73 | tok/s 21969
step    440 | loss 1.6211 | lr 3.00e-04 | grad 2.09 | tok/s 21272
step    450 | loss 1.5508 | lr 3.00e-04 | grad 1.38 | tok/s 21530
step    460 | loss 1.5361 | lr 3.00e-04 | grad 1.77 | tok/s 21815
step    470 | loss 1.5011 | lr 3.00e-04 | grad 3.44 | tok/s 21636
step    480 | loss 1.5499 | lr 3.00e-04 | grad 2.47 | tok/s 22132
step    490 | loss 1.6039 | lr 3.00e-04 | grad 2.16 | tok/s 21246
step    500 | loss 1.7348 | lr 3.00e-04 | grad 1.48 | tok/s 21605
step    510 | loss 1.6215 | lr 3.00e-04 | grad 1.28 | tok/s 20641
step    520 | loss 1.4686 | lr 3.00e-04 | grad 3.89 | tok/s 21619
step    530 | loss 1.6401 | lr 3.00e-04 | grad 1.92 | tok/s 21221
step    540 | loss 1.5283 | lr 3.00e-04 | grad 1.49 | tok/s 20793
step    550 | loss 1.2893 | lr 3.00e-04 | grad 2.81 | tok/s 21793
step    560 | loss 1.3838 | lr 3.00e-04 | grad 1.52 | tok/s 22337
step    570 | loss 1.2846 | lr 3.00e-04 | grad 1.60 | tok/s 22339
step    580 | loss 1.2456 | lr 3.00e-04 | grad 1.17 | tok/s 22313
step    590 | loss 1.2794 | lr 3.00e-04 | grad 1.29 | tok/s 22317
step    600 | loss 1.2074 | lr 3.00e-04 | grad 1.38 | tok/s 22323
step    610 | loss 1.2510 | lr 3.00e-04 | grad 1.51 | tok/s 22289
step    620 | loss 1.2310 | lr 3.00e-04 | grad 1.46 | tok/s 22249
step    630 | loss 1.5567 | lr 3.00e-04 | grad 5.38 | tok/s 21065
step    640 | loss 1.6682 | lr 3.00e-04 | grad 1.66 | tok/s 21327
step    650 | loss 1.4859 | lr 3.00e-04 | grad 1.57 | tok/s 21309
step    660 | loss 1.5383 | lr 3.00e-04 | grad 1.77 | tok/s 22129
step    670 | loss 1.5538 | lr 3.00e-04 | grad 4.56 | tok/s 21372
step    680 | loss 1.5621 | lr 3.00e-04 | grad 1.98 | tok/s 21077
step    690 | loss 1.4994 | lr 3.00e-04 | grad 1.82 | tok/s 20896
step    700 | loss 1.4111 | lr 3.00e-04 | grad 1.30 | tok/s 21324
step    710 | loss 1.5447 | lr 3.00e-04 | grad 3.27 | tok/s 21011
step    720 | loss 1.2473 | lr 3.00e-04 | grad 1.41 | tok/s 21785
step    730 | loss 1.3743 | lr 3.00e-04 | grad 1.29 | tok/s 21423
step    740 | loss 1.7073 | lr 3.00e-04 | grad 3.98 | tok/s 21982
step    750 | loss 1.4848 | lr 3.00e-04 | grad 1.56 | tok/s 22227
step    760 | loss 1.4503 | lr 3.00e-04 | grad 3.36 | tok/s 21790
step    770 | loss 1.5067 | lr 3.00e-04 | grad 1.73 | tok/s 21429
step    780 | loss 1.4246 | lr 3.00e-04 | grad 1.57 | tok/s 21586
step    790 | loss 1.5730 | lr 3.00e-04 | grad 4.75 | tok/s 22049
step    800 | loss 1.2702 | lr 3.00e-04 | grad 1.60 | tok/s 21751
step    810 | loss 1.2679 | lr 3.00e-04 | grad 2.38 | tok/s 21051
step    820 | loss 1.3578 | lr 3.00e-04 | grad 1.57 | tok/s 21394
step    830 | loss 1.4365 | lr 3.00e-04 | grad 1.21 | tok/s 21085
step    840 | loss 1.5410 | lr 3.00e-04 | grad 1.48 | tok/s 21029
step    850 | loss 1.4572 | lr 3.00e-04 | grad 1.33 | tok/s 21442
step    860 | loss 1.5003 | lr 3.00e-04 | grad 2.25 | tok/s 21820
step    870 | loss 1.3243 | lr 3.00e-04 | grad 1.66 | tok/s 21939
step    880 | loss 1.5307 | lr 3.00e-04 | grad 1.48 | tok/s 21618
step    890 | loss 1.4358 | lr 3.00e-04 | grad 1.19 | tok/s 21489
step    900 | loss 1.4713 | lr 3.00e-04 | grad 1.40 | tok/s 21412
step    910 | loss 1.4354 | lr 3.00e-04 | grad 5.59 | tok/s 21168
step    920 | loss 1.4058 | lr 3.00e-04 | grad 1.55 | tok/s 21407
step    930 | loss 1.3313 | lr 3.00e-04 | grad 1.62 | tok/s 21658
step    940 | loss 1.2896 | lr 3.00e-04 | grad 1.47 | tok/s 21229
step    950 | loss 1.4418 | lr 3.00e-04 | grad 2.09 | tok/s 20876
step    960 | loss 1.3874 | lr 3.00e-04 | grad 1.23 | tok/s 21416
step    970 | loss 1.4181 | lr 3.00e-04 | grad 1.45 | tok/s 21454
step    980 | loss 1.8501 | lr 3.00e-04 | grad 3.42 | tok/s 22307
step    990 | loss 1.5065 | lr 3.00e-04 | grad 1.52 | tok/s 21382
step   1000 | loss 1.5088 | lr 3.00e-04 | grad 1.98 | tok/s 21429
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5088.pt
step   1010 | loss 1.2973 | lr 3.00e-04 | grad 2.44 | tok/s 12038
step   1020 | loss 1.1745 | lr 3.00e-04 | grad 1.19 | tok/s 22699
step   1030 | loss 1.4759 | lr 3.00e-04 | grad 5.12 | tok/s 21507
step   1040 | loss 2.0206 | lr 3.00e-04 | grad 3.27 | tok/s 21996
step   1050 | loss 1.3953 | lr 3.00e-04 | grad 2.38 | tok/s 22143
step   1060 | loss 1.1551 | lr 3.00e-04 | grad 5.03 | tok/s 21963
step   1070 | loss 1.3672 | lr 3.00e-04 | grad 1.67 | tok/s 21754
step   1080 | loss 1.2159 | lr 3.00e-04 | grad 1.17 | tok/s 22473
step   1090 | loss 1.1808 | lr 3.00e-04 | grad 1.12 | tok/s 22448
step   1100 | loss 1.1677 | lr 3.00e-04 | grad 1.13 | tok/s 22463
step   1110 | loss 1.1080 | lr 3.00e-04 | grad 1.11 | tok/s 22430
step   1120 | loss 1.3939 | lr 3.00e-04 | grad 3.08 | tok/s 21866
step   1130 | loss 1.5399 | lr 3.00e-04 | grad 1.30 | tok/s 22069
step   1140 | loss 1.6877 | lr 3.00e-04 | grad 1.63 | tok/s 22340
step   1150 | loss 1.5704 | lr 3.00e-04 | grad 1.73 | tok/s 21678
step   1160 | loss 1.6665 | lr 3.00e-04 | grad 2.00 | tok/s 21329
step   1170 | loss 1.4309 | lr 3.00e-04 | grad 1.70 | tok/s 21115
step   1180 | loss 1.2872 | lr 3.00e-04 | grad 2.53 | tok/s 22153
step   1190 | loss 1.5216 | lr 3.00e-04 | grad 2.17 | tok/s 22336
step   1200 | loss 1.0658 | lr 3.00e-04 | grad 1.86 | tok/s 22458
step   1210 | loss 1.3192 | lr 3.00e-04 | grad 1.59 | tok/s 20955
step   1220 | loss 1.3166 | lr 3.00e-04 | grad 1.72 | tok/s 21987
step   1230 | loss 1.2768 | lr 3.00e-04 | grad 1.20 | tok/s 22005
step   1240 | loss 1.2517 | lr 3.00e-04 | grad 1.35 | tok/s 22069
step   1250 | loss 1.4058 | lr 3.00e-04 | grad 2.28 | tok/s 21788
step   1260 | loss 1.3203 | lr 3.00e-04 | grad 1.80 | tok/s 22228
step   1270 | loss 1.3092 | lr 3.00e-04 | grad 1.64 | tok/s 21655
step   1280 | loss 1.3260 | lr 3.00e-04 | grad 1.48 | tok/s 21378
step   1290 | loss 1.2817 | lr 3.00e-04 | grad 1.81 | tok/s 21424
step   1300 | loss 1.5684 | lr 3.00e-04 | grad 5.19 | tok/s 21092
step   1310 | loss 1.4059 | lr 3.00e-04 | grad 1.49 | tok/s 21911
step   1320 | loss 1.4114 | lr 3.00e-04 | grad 1.68 | tok/s 21935
step   1330 | loss 1.3766 | lr 3.00e-04 | grad 1.39 | tok/s 21737
step   1340 | loss 1.4550 | lr 3.00e-04 | grad 1.77 | tok/s 21282
step   1350 | loss 1.3394 | lr 3.00e-04 | grad 1.27 | tok/s 21721
step   1360 | loss 1.3853 | lr 3.00e-04 | grad 1.51 | tok/s 20974
step   1370 | loss 1.5184 | lr 3.00e-04 | grad 2.12 | tok/s 22038
step   1380 | loss 1.3764 | lr 3.00e-04 | grad 1.45 | tok/s 21101
step   1390 | loss 1.2578 | lr 3.00e-04 | grad 2.06 | tok/s 22042
step   1400 | loss 1.4362 | lr 3.00e-04 | grad 1.45 | tok/s 21321
step   1410 | loss 1.3557 | lr 3.00e-04 | grad 3.22 | tok/s 20831
step   1420 | loss 1.0634 | lr 3.00e-04 | grad 6.12 | tok/s 22131
step   1430 | loss 1.5968 | lr 3.00e-04 | grad 1.60 | tok/s 21353
step   1440 | loss 1.3682 | lr 3.00e-04 | grad 1.84 | tok/s 21931
step   1450 | loss 1.3522 | lr 3.00e-04 | grad 5.28 | tok/s 21935
step   1460 | loss 1.4943 | lr 3.00e-04 | grad 3.39 | tok/s 21304
step   1470 | loss 1.2983 | lr 3.00e-04 | grad 1.43 | tok/s 20826
step   1480 | loss 1.2929 | lr 3.00e-04 | grad 1.23 | tok/s 21919
step   1490 | loss 1.7830 | lr 3.00e-04 | grad 7.38 | tok/s 21563
step   1500 | loss 1.4053 | lr 3.00e-04 | grad 1.56 | tok/s 21659
step   1510 | loss 1.2060 | lr 3.00e-04 | grad 1.55 | tok/s 21698
step   1520 | loss 1.4057 | lr 3.00e-04 | grad 1.41 | tok/s 21453
step   1530 | loss 1.3254 | lr 3.00e-04 | grad 1.43 | tok/s 21039
step   1540 | loss 1.4470 | lr 3.00e-04 | grad 1.33 | tok/s 21988
step   1550 | loss 1.4115 | lr 3.00e-04 | grad 1.75 | tok/s 21590
step   1560 | loss 1.0710 | lr 3.00e-04 | grad 1.61 | tok/s 22368
step   1570 | loss 1.2388 | lr 3.00e-04 | grad 1.35 | tok/s 21787
step   1580 | loss 1.2214 | lr 3.00e-04 | grad 1.89 | tok/s 21720
step   1590 | loss 1.3817 | lr 3.00e-04 | grad 1.62 | tok/s 21243
step   1600 | loss 1.2280 | lr 3.00e-04 | grad 2.33 | tok/s 22078
step   1610 | loss 1.8707 | lr 3.00e-04 | grad 2.58 | tok/s 21833

Training complete! Final step: 1610
