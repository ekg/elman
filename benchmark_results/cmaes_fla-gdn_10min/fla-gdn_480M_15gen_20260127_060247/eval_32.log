Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_32/levelfla-gdn_100m_20260127_063338
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,801,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.1892 | lr 3.00e-04 | grad 27.00 | tok/s 1075
step     20 | loss 3.7982 | lr 3.00e-04 | grad 12.94 | tok/s 26218
step     30 | loss 3.2103 | lr 3.00e-04 | grad 9.00 | tok/s 26063
step     40 | loss 3.0013 | lr 3.00e-04 | grad 6.62 | tok/s 25800
step     50 | loss 3.4388 | lr 3.00e-04 | grad 6.53 | tok/s 24611
step     60 | loss 2.3656 | lr 3.00e-04 | grad 6.97 | tok/s 24920
step     70 | loss 2.5674 | lr 3.00e-04 | grad 10.31 | tok/s 24667
step     80 | loss 1.4562 | lr 3.00e-04 | grad 4.44 | tok/s 25441
step     90 | loss 2.5087 | lr 3.00e-04 | grad 3.72 | tok/s 24789
step    100 | loss 2.3513 | lr 3.00e-04 | grad 3.11 | tok/s 24038
step    110 | loss 1.9209 | lr 3.00e-04 | grad 19.75 | tok/s 24634
step    120 | loss 1.9733 | lr 3.00e-04 | grad 3.42 | tok/s 23613
step    130 | loss 1.6516 | lr 3.00e-04 | grad 3.72 | tok/s 25011
step    140 | loss 1.7575 | lr 3.00e-04 | grad 4.03 | tok/s 23704
step    150 | loss 2.1835 | lr 3.00e-04 | grad 5.12 | tok/s 24082
step    160 | loss 1.9473 | lr 3.00e-04 | grad 2.64 | tok/s 23716
step    170 | loss 2.1566 | lr 3.00e-04 | grad 2.83 | tok/s 24078
step    180 | loss 1.7775 | lr 3.00e-04 | grad 2.05 | tok/s 23938
step    190 | loss 1.7660 | lr 3.00e-04 | grad 2.81 | tok/s 24468
step    200 | loss 1.7883 | lr 3.00e-04 | grad 2.53 | tok/s 23988
step    210 | loss 1.6567 | lr 3.00e-04 | grad 2.17 | tok/s 22681
step    220 | loss 1.5727 | lr 3.00e-04 | grad 2.05 | tok/s 23465
step    230 | loss 1.8695 | lr 3.00e-04 | grad 1.91 | tok/s 23401
step    240 | loss 1.5897 | lr 3.00e-04 | grad 1.68 | tok/s 23246
step    250 | loss 1.7714 | lr 3.00e-04 | grad 4.81 | tok/s 23331
step    260 | loss 1.6529 | lr 3.00e-04 | grad 2.72 | tok/s 23876
step    270 | loss 1.9399 | lr 3.00e-04 | grad 5.91 | tok/s 23601
step    280 | loss 1.6714 | lr 3.00e-04 | grad 6.28 | tok/s 24533
step    290 | loss 1.5087 | lr 3.00e-04 | grad 2.23 | tok/s 22919
step    300 | loss 1.4093 | lr 3.00e-04 | grad 1.58 | tok/s 24450
step    310 | loss 1.1760 | lr 3.00e-04 | grad 1.59 | tok/s 24695
step    320 | loss 1.0341 | lr 3.00e-04 | grad 1.72 | tok/s 24706
step    330 | loss 1.6127 | lr 3.00e-04 | grad 2.84 | tok/s 23571
step    340 | loss 1.6402 | lr 3.00e-04 | grad 7.41 | tok/s 23555
step    350 | loss 1.5586 | lr 3.00e-04 | grad 2.48 | tok/s 24515
step    360 | loss 1.5184 | lr 3.00e-04 | grad 1.70 | tok/s 24381
step    370 | loss 1.5909 | lr 3.00e-04 | grad 2.36 | tok/s 23651
step    380 | loss 1.5780 | lr 3.00e-04 | grad 1.72 | tok/s 23615
step    390 | loss 1.5262 | lr 3.00e-04 | grad 2.03 | tok/s 24070
step    400 | loss 1.4219 | lr 3.00e-04 | grad 1.56 | tok/s 22680
step    410 | loss 1.5597 | lr 3.00e-04 | grad 1.92 | tok/s 24591
step    420 | loss 1.6294 | lr 3.00e-04 | grad 1.80 | tok/s 23426
step    430 | loss 1.7269 | lr 3.00e-04 | grad 3.39 | tok/s 23887
step    440 | loss 1.5950 | lr 3.00e-04 | grad 1.45 | tok/s 22817
step    450 | loss 1.4590 | lr 3.00e-04 | grad 7.56 | tok/s 23893
step    460 | loss 1.6169 | lr 3.00e-04 | grad 2.81 | tok/s 23402
step    470 | loss 1.5535 | lr 3.00e-04 | grad 1.57 | tok/s 23428
step    480 | loss 1.2307 | lr 3.00e-04 | grad 1.68 | tok/s 23598
step    490 | loss 1.4181 | lr 3.00e-04 | grad 1.54 | tok/s 24670
step    500 | loss 1.2744 | lr 3.00e-04 | grad 1.54 | tok/s 24709
step    510 | loss 1.2458 | lr 3.00e-04 | grad 1.83 | tok/s 24721
step    520 | loss 1.2800 | lr 3.00e-04 | grad 1.45 | tok/s 24663
step    530 | loss 1.1934 | lr 3.00e-04 | grad 1.36 | tok/s 24721
step    540 | loss 1.2522 | lr 3.00e-04 | grad 1.72 | tok/s 24700
step    550 | loss 1.2023 | lr 3.00e-04 | grad 1.10 | tok/s 24728
step    560 | loss 1.6502 | lr 3.00e-04 | grad 6.59 | tok/s 23305
step    570 | loss 1.6124 | lr 3.00e-04 | grad 2.16 | tok/s 23493
step    580 | loss 1.4712 | lr 3.00e-04 | grad 1.77 | tok/s 23957
step    590 | loss 1.5233 | lr 3.00e-04 | grad 1.98 | tok/s 24085
step    600 | loss 1.4879 | lr 3.00e-04 | grad 1.97 | tok/s 23810
step    610 | loss 1.6037 | lr 3.00e-04 | grad 1.38 | tok/s 23154
step    620 | loss 1.5011 | lr 3.00e-04 | grad 1.76 | tok/s 23303
step    630 | loss 1.4443 | lr 3.00e-04 | grad 1.91 | tok/s 23485
step    640 | loss 1.5055 | lr 3.00e-04 | grad 2.91 | tok/s 23341
step    650 | loss 1.2616 | lr 3.00e-04 | grad 1.62 | tok/s 24084
step    660 | loss 1.3429 | lr 3.00e-04 | grad 1.81 | tok/s 24090
step    670 | loss 1.6427 | lr 3.00e-04 | grad 5.84 | tok/s 24165
step    680 | loss 1.5224 | lr 3.00e-04 | grad 1.57 | tok/s 24738
step    690 | loss 1.4031 | lr 3.00e-04 | grad 1.45 | tok/s 24238
step    700 | loss 1.4978 | lr 3.00e-04 | grad 1.44 | tok/s 23853
step    710 | loss 1.4340 | lr 3.00e-04 | grad 2.17 | tok/s 23910
step    720 | loss 1.5055 | lr 3.00e-04 | grad 4.19 | tok/s 24516
step    730 | loss 1.3895 | lr 3.00e-04 | grad 1.61 | tok/s 24048
step    740 | loss 1.1713 | lr 3.00e-04 | grad 3.33 | tok/s 23400
step    750 | loss 1.3502 | lr 3.00e-04 | grad 2.22 | tok/s 24132
step    760 | loss 1.4382 | lr 3.00e-04 | grad 1.41 | tok/s 23118
step    770 | loss 1.5396 | lr 3.00e-04 | grad 1.94 | tok/s 23605
step    780 | loss 1.4446 | lr 3.00e-04 | grad 2.17 | tok/s 23815
step    790 | loss 1.4658 | lr 3.00e-04 | grad 2.67 | tok/s 23997
step    800 | loss 1.2839 | lr 3.00e-04 | grad 1.88 | tok/s 24635
step    810 | loss 1.5086 | lr 3.00e-04 | grad 2.02 | tok/s 23724
step    820 | loss 1.4396 | lr 3.00e-04 | grad 1.64 | tok/s 23941
step    830 | loss 1.4686 | lr 3.00e-04 | grad 5.03 | tok/s 23869
step    840 | loss 1.4258 | lr 3.00e-04 | grad 1.90 | tok/s 23292
step    850 | loss 1.4443 | lr 3.00e-04 | grad 1.81 | tok/s 23812
step    860 | loss 1.3105 | lr 3.00e-04 | grad 1.53 | tok/s 23976
step    870 | loss 1.2990 | lr 3.00e-04 | grad 2.11 | tok/s 23554
step    880 | loss 1.3991 | lr 3.00e-04 | grad 1.40 | tok/s 22987
step    890 | loss 1.4000 | lr 3.00e-04 | grad 1.48 | tok/s 23735
step    900 | loss 1.3966 | lr 3.00e-04 | grad 1.41 | tok/s 23745
step    910 | loss 1.8017 | lr 3.00e-04 | grad 3.36 | tok/s 24666
step    920 | loss 1.5259 | lr 3.00e-04 | grad 1.53 | tok/s 23727
step    930 | loss 1.5314 | lr 3.00e-04 | grad 1.45 | tok/s 23973
step    940 | loss 1.1512 | lr 3.00e-04 | grad 1.66 | tok/s 24205
step    950 | loss 1.1774 | lr 3.00e-04 | grad 1.75 | tok/s 24725
step    960 | loss 1.2983 | lr 3.00e-04 | grad 2.19 | tok/s 24047
step    970 | loss 1.8441 | lr 3.00e-04 | grad 6.31 | tok/s 23716
step    980 | loss 1.7131 | lr 3.00e-04 | grad 2.06 | tok/s 24367
step    990 | loss 1.3076 | lr 3.00e-04 | grad 2.41 | tok/s 24002
step   1000 | loss 1.1361 | lr 3.00e-04 | grad 2.02 | tok/s 24225
  >>> saved checkpoint: checkpoint_step_001000_loss_1.1361.pt
step   1010 | loss 1.2362 | lr 3.00e-04 | grad 1.52 | tok/s 12615
step   1020 | loss 1.1951 | lr 3.00e-04 | grad 1.51 | tok/s 25010
step   1030 | loss 1.1639 | lr 3.00e-04 | grad 1.45 | tok/s 25015
step   1040 | loss 1.1091 | lr 3.00e-04 | grad 1.36 | tok/s 24983
step   1050 | loss 1.3276 | lr 3.00e-04 | grad 1.67 | tok/s 24316
step   1060 | loss 1.5505 | lr 3.00e-04 | grad 1.36 | tok/s 24554
step   1070 | loss 1.6842 | lr 3.00e-04 | grad 1.54 | tok/s 24888
step   1080 | loss 1.5526 | lr 3.00e-04 | grad 4.19 | tok/s 24298
step   1090 | loss 1.5843 | lr 3.00e-04 | grad 3.67 | tok/s 23584
step   1100 | loss 1.4993 | lr 3.00e-04 | grad 1.83 | tok/s 23499
step   1110 | loss 1.2950 | lr 3.00e-04 | grad 1.41 | tok/s 24705
step   1120 | loss 1.4807 | lr 3.00e-04 | grad 2.56 | tok/s 24693
step   1130 | loss 1.0909 | lr 3.00e-04 | grad 1.29 | tok/s 24932
step   1140 | loss 1.2612 | lr 3.00e-04 | grad 1.52 | tok/s 23523
step   1150 | loss 1.2939 | lr 3.00e-04 | grad 1.64 | tok/s 24245
step   1160 | loss 1.3358 | lr 3.00e-04 | grad 1.62 | tok/s 24401
step   1170 | loss 1.1971 | lr 3.00e-04 | grad 2.14 | tok/s 24608
step   1180 | loss 1.4126 | lr 3.00e-04 | grad 2.42 | tok/s 24076
step   1190 | loss 1.2839 | lr 3.00e-04 | grad 1.46 | tok/s 24704
step   1200 | loss 1.3051 | lr 3.00e-04 | grad 2.17 | tok/s 23989
step   1210 | loss 1.3034 | lr 3.00e-04 | grad 2.02 | tok/s 23987
step   1220 | loss 1.2878 | lr 3.00e-04 | grad 1.97 | tok/s 23485
step   1230 | loss 1.5118 | lr 3.00e-04 | grad 7.50 | tok/s 23352
step   1240 | loss 1.4125 | lr 3.00e-04 | grad 1.75 | tok/s 24317
step   1250 | loss 1.4204 | lr 3.00e-04 | grad 3.00 | tok/s 24254
step   1260 | loss 1.3692 | lr 3.00e-04 | grad 1.55 | tok/s 24186
step   1270 | loss 1.4440 | lr 3.00e-04 | grad 3.14 | tok/s 23437
step   1280 | loss 1.3524 | lr 3.00e-04 | grad 1.42 | tok/s 24351
step   1290 | loss 1.3932 | lr 3.00e-04 | grad 1.57 | tok/s 22910
step   1300 | loss 1.4599 | lr 3.00e-04 | grad 4.09 | tok/s 24413
step   1310 | loss 1.4007 | lr 3.00e-04 | grad 1.83 | tok/s 23693
step   1320 | loss 1.2447 | lr 3.00e-04 | grad 2.38 | tok/s 24111
step   1330 | loss 1.4577 | lr 3.00e-04 | grad 1.29 | tok/s 23572
step   1340 | loss 1.2920 | lr 3.00e-04 | grad 1.44 | tok/s 23152
step   1350 | loss 1.0436 | lr 3.00e-04 | grad 1.68 | tok/s 24523
step   1360 | loss 1.6282 | lr 3.00e-04 | grad 1.24 | tok/s 23772
step   1370 | loss 1.3780 | lr 3.00e-04 | grad 2.03 | tok/s 23937
step   1380 | loss 1.3277 | lr 3.00e-04 | grad 1.38 | tok/s 24211
step   1390 | loss 1.4266 | lr 3.00e-04 | grad 2.27 | tok/s 23508
step   1400 | loss 1.3261 | lr 3.00e-04 | grad 1.62 | tok/s 23207
step   1410 | loss 1.3005 | lr 3.00e-04 | grad 2.47 | tok/s 23992
step   1420 | loss 1.5905 | lr 3.00e-04 | grad 6.75 | tok/s 23828
step   1430 | loss 1.5521 | lr 3.00e-04 | grad 1.37 | tok/s 24226
step   1440 | loss 1.1931 | lr 3.00e-04 | grad 1.48 | tok/s 23772
step   1450 | loss 1.4016 | lr 3.00e-04 | grad 1.95 | tok/s 23665
step   1460 | loss 1.3176 | lr 3.00e-04 | grad 1.64 | tok/s 24349
step   1470 | loss 1.4290 | lr 3.00e-04 | grad 1.73 | tok/s 24353
step   1480 | loss 1.3991 | lr 3.00e-04 | grad 4.50 | tok/s 23894
step   1490 | loss 1.0760 | lr 3.00e-04 | grad 1.28 | tok/s 24634
step   1500 | loss 1.2702 | lr 3.00e-04 | grad 1.41 | tok/s 24125
step   1510 | loss 1.1585 | lr 3.00e-04 | grad 1.62 | tok/s 24244
step   1520 | loss 1.3938 | lr 3.00e-04 | grad 3.95 | tok/s 23329
step   1530 | loss 1.2166 | lr 3.00e-04 | grad 5.47 | tok/s 24610
step   1540 | loss 1.7785 | lr 3.00e-04 | grad 3.27 | tok/s 23958
step   1550 | loss 1.9596 | lr 3.00e-04 | grad 2.22 | tok/s 24725
step   1560 | loss 1.5945 | lr 3.00e-04 | grad 2.39 | tok/s 24773
step   1570 | loss 1.4315 | lr 3.00e-04 | grad 1.97 | tok/s 24713

Training complete! Final step: 1574
