Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_6/levelfla-gdn_100m_20260127_060254
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 464,339,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.2392 | lr 3.00e-04 | grad 27.88 | tok/s 1105
step     20 | loss 3.8522 | lr 3.00e-04 | grad 17.50 | tok/s 25063
step     30 | loss 3.2407 | lr 3.00e-04 | grad 7.88 | tok/s 24957
step     40 | loss 2.9328 | lr 3.00e-04 | grad 11.69 | tok/s 24662
step     50 | loss 3.5213 | lr 3.00e-04 | grad 5.59 | tok/s 23562
step     60 | loss 2.5607 | lr 3.00e-04 | grad 6.78 | tok/s 23945
step     70 | loss 2.5397 | lr 3.00e-04 | grad 5.81 | tok/s 23674
step     80 | loss 1.8593 | lr 3.00e-04 | grad 7.25 | tok/s 24090
step     90 | loss 2.3906 | lr 3.00e-04 | grad 5.31 | tok/s 24021
step    100 | loss 2.4706 | lr 3.00e-04 | grad 8.19 | tok/s 22539
step    110 | loss 2.0256 | lr 3.00e-04 | grad 6.19 | tok/s 23573
step    120 | loss 2.0406 | lr 3.00e-04 | grad 2.55 | tok/s 22209
step    130 | loss 1.7126 | lr 3.00e-04 | grad 2.81 | tok/s 23802
step    140 | loss 1.6642 | lr 3.00e-04 | grad 1.95 | tok/s 22756
step    150 | loss 2.2317 | lr 3.00e-04 | grad 2.12 | tok/s 22789
step    160 | loss 2.0786 | lr 3.00e-04 | grad 2.30 | tok/s 22517
step    170 | loss 2.0258 | lr 3.00e-04 | grad 6.31 | tok/s 23046
step    180 | loss 1.9143 | lr 3.00e-04 | grad 2.59 | tok/s 22545
step    190 | loss 1.7897 | lr 3.00e-04 | grad 1.95 | tok/s 23371
step    200 | loss 1.8673 | lr 3.00e-04 | grad 1.74 | tok/s 22508
step    210 | loss 1.6520 | lr 3.00e-04 | grad 1.88 | tok/s 21687
step    220 | loss 1.6226 | lr 3.00e-04 | grad 2.02 | tok/s 22079
step    230 | loss 1.8478 | lr 3.00e-04 | grad 1.88 | tok/s 21879
step    240 | loss 1.6196 | lr 3.00e-04 | grad 2.58 | tok/s 22368
step    250 | loss 1.7366 | lr 3.00e-04 | grad 2.48 | tok/s 22040
step    260 | loss 1.7166 | lr 3.00e-04 | grad 2.05 | tok/s 22437
step    270 | loss 1.8450 | lr 3.00e-04 | grad 1.74 | tok/s 22436
step    280 | loss 1.7817 | lr 3.00e-04 | grad 1.60 | tok/s 22954
step    290 | loss 1.5361 | lr 3.00e-04 | grad 3.47 | tok/s 21679
step    300 | loss 1.4904 | lr 3.00e-04 | grad 2.02 | tok/s 23156
step    310 | loss 1.2271 | lr 3.00e-04 | grad 1.62 | tok/s 23315
step    320 | loss 1.0740 | lr 3.00e-04 | grad 1.52 | tok/s 23315
step    330 | loss 1.4072 | lr 3.00e-04 | grad 2.89 | tok/s 22567
step    340 | loss 1.7494 | lr 3.00e-04 | grad 2.06 | tok/s 22062
step    350 | loss 1.5369 | lr 3.00e-04 | grad 1.55 | tok/s 23059
step    360 | loss 1.5527 | lr 3.00e-04 | grad 1.78 | tok/s 23137
step    370 | loss 1.5902 | lr 3.00e-04 | grad 2.75 | tok/s 22236
step    380 | loss 1.6132 | lr 3.00e-04 | grad 2.08 | tok/s 22537
step    390 | loss 1.5010 | lr 3.00e-04 | grad 2.28 | tok/s 22611
step    400 | loss 1.5232 | lr 3.00e-04 | grad 2.30 | tok/s 22298
step    410 | loss 1.4937 | lr 3.00e-04 | grad 2.70 | tok/s 23252
step    420 | loss 1.6442 | lr 3.00e-04 | grad 1.51 | tok/s 22332
step    430 | loss 1.5574 | lr 3.00e-04 | grad 1.79 | tok/s 22208
step    440 | loss 1.7977 | lr 3.00e-04 | grad 2.34 | tok/s 22012
step    450 | loss 1.4423 | lr 3.00e-04 | grad 2.45 | tok/s 22134
step    460 | loss 1.5691 | lr 3.00e-04 | grad 2.00 | tok/s 22131
step    470 | loss 1.6364 | lr 3.00e-04 | grad 1.57 | tok/s 22448
step    480 | loss 1.2509 | lr 3.00e-04 | grad 2.34 | tok/s 22219
step    490 | loss 1.4315 | lr 3.00e-04 | grad 1.41 | tok/s 22998
step    500 | loss 1.2922 | lr 3.00e-04 | grad 1.82 | tok/s 23346
step    510 | loss 1.2676 | lr 3.00e-04 | grad 1.52 | tok/s 23326
step    520 | loss 1.2602 | lr 3.00e-04 | grad 1.39 | tok/s 23312
step    530 | loss 1.2155 | lr 3.00e-04 | grad 1.67 | tok/s 21716
step    540 | loss 1.2492 | lr 3.00e-04 | grad 1.41 | tok/s 23334
step    550 | loss 1.2211 | lr 3.00e-04 | grad 1.40 | tok/s 23339
step    560 | loss 1.6108 | lr 3.00e-04 | grad 2.06 | tok/s 22181
step    570 | loss 1.5429 | lr 3.00e-04 | grad 3.11 | tok/s 22055
step    580 | loss 1.5099 | lr 3.00e-04 | grad 1.52 | tok/s 22852
step    590 | loss 1.4852 | lr 3.00e-04 | grad 1.96 | tok/s 22555
step    600 | loss 1.5215 | lr 3.00e-04 | grad 1.55 | tok/s 22828
step    610 | loss 1.6562 | lr 3.00e-04 | grad 2.09 | tok/s 21908
step    620 | loss 1.4721 | lr 3.00e-04 | grad 2.19 | tok/s 22428
step    630 | loss 1.4403 | lr 3.00e-04 | grad 1.91 | tok/s 21707
step    640 | loss 1.4640 | lr 3.00e-04 | grad 1.65 | tok/s 22101
step    650 | loss 1.4114 | lr 3.00e-04 | grad 1.62 | tok/s 22626
step    660 | loss 1.2627 | lr 3.00e-04 | grad 2.03 | tok/s 23085
step    670 | loss 1.5280 | lr 3.00e-04 | grad 2.28 | tok/s 22542
step    680 | loss 1.7020 | lr 3.00e-04 | grad 1.85 | tok/s 23347
step    690 | loss 1.4088 | lr 3.00e-04 | grad 1.39 | tok/s 23255
step    700 | loss 1.4785 | lr 3.00e-04 | grad 6.03 | tok/s 22517
step    710 | loss 1.3900 | lr 3.00e-04 | grad 1.91 | tok/s 22893
step    720 | loss 1.4563 | lr 3.00e-04 | grad 1.71 | tok/s 22517
step    730 | loss 1.6182 | lr 3.00e-04 | grad 1.07 | tok/s 22689
step    740 | loss 1.0597 | lr 3.00e-04 | grad 1.38 | tok/s 22273
step    750 | loss 1.3923 | lr 3.00e-04 | grad 1.78 | tok/s 22914
step    760 | loss 1.4899 | lr 3.00e-04 | grad 1.27 | tok/s 21509
step    770 | loss 1.5062 | lr 3.00e-04 | grad 2.44 | tok/s 22425
step    780 | loss 1.4342 | lr 3.00e-04 | grad 2.72 | tok/s 22562
step    790 | loss 1.4123 | lr 3.00e-04 | grad 1.57 | tok/s 22452
step    800 | loss 1.3664 | lr 3.00e-04 | grad 1.68 | tok/s 23439
step    810 | loss 1.4732 | lr 3.00e-04 | grad 1.97 | tok/s 22454
step    820 | loss 1.4901 | lr 3.00e-04 | grad 1.98 | tok/s 22369
step    830 | loss 1.4003 | lr 3.00e-04 | grad 1.66 | tok/s 22564
step    840 | loss 1.4813 | lr 3.00e-04 | grad 1.30 | tok/s 21944
step    850 | loss 1.4683 | lr 3.00e-04 | grad 4.62 | tok/s 22697
step    860 | loss 1.3152 | lr 3.00e-04 | grad 1.61 | tok/s 22469
step    870 | loss 1.2801 | lr 3.00e-04 | grad 1.20 | tok/s 22351
step    880 | loss 1.4322 | lr 3.00e-04 | grad 1.40 | tok/s 21698
step    890 | loss 1.3685 | lr 3.00e-04 | grad 1.22 | tok/s 21950
step    900 | loss 1.3852 | lr 3.00e-04 | grad 1.36 | tok/s 22453
step    910 | loss 1.8243 | lr 3.00e-04 | grad 3.97 | tok/s 23192
step    920 | loss 1.5362 | lr 3.00e-04 | grad 2.34 | tok/s 22746
step    930 | loss 1.4890 | lr 3.00e-04 | grad 1.49 | tok/s 22291
step    940 | loss 1.2023 | lr 3.00e-04 | grad 1.73 | tok/s 22853
step    950 | loss 1.1433 | lr 3.00e-04 | grad 2.64 | tok/s 23204
step    960 | loss 1.3133 | lr 3.00e-04 | grad 1.48 | tok/s 23012
step    970 | loss 1.6725 | lr 3.00e-04 | grad 7.66 | tok/s 22070
step    980 | loss 1.8906 | lr 3.00e-04 | grad 1.27 | tok/s 23186
step    990 | loss 1.3909 | lr 3.00e-04 | grad 2.86 | tok/s 22436
step   1000 | loss 1.0363 | lr 3.00e-04 | grad 2.48 | tok/s 23113
  >>> saved checkpoint: checkpoint_step_001000_loss_1.0363.pt
step   1010 | loss 1.2943 | lr 3.00e-04 | grad 1.62 | tok/s 11267
step   1020 | loss 1.2003 | lr 3.00e-04 | grad 1.28 | tok/s 23599
step   1030 | loss 1.1755 | lr 3.00e-04 | grad 1.55 | tok/s 23600
step   1040 | loss 1.1140 | lr 3.00e-04 | grad 1.21 | tok/s 23568
step   1050 | loss 1.3162 | lr 3.00e-04 | grad 1.27 | tok/s 23345
step   1060 | loss 1.5433 | lr 3.00e-04 | grad 1.73 | tok/s 22817
step   1070 | loss 1.6992 | lr 3.00e-04 | grad 1.69 | tok/s 23334
step   1080 | loss 1.5566 | lr 3.00e-04 | grad 7.03 | tok/s 23000
step   1090 | loss 1.4865 | lr 3.00e-04 | grad 2.59 | tok/s 22121
step   1100 | loss 1.6019 | lr 3.00e-04 | grad 6.97 | tok/s 22201
step   1110 | loss 1.3106 | lr 3.00e-04 | grad 1.20 | tok/s 23146
step   1120 | loss 1.4802 | lr 3.00e-04 | grad 2.17 | tok/s 23338
step   1130 | loss 1.1556 | lr 3.00e-04 | grad 1.34 | tok/s 23524
step   1140 | loss 1.2253 | lr 3.00e-04 | grad 2.25 | tok/s 22406
step   1150 | loss 1.2911 | lr 3.00e-04 | grad 2.66 | tok/s 22709
step   1160 | loss 1.3666 | lr 3.00e-04 | grad 1.49 | tok/s 22865
step   1170 | loss 1.1900 | lr 3.00e-04 | grad 2.12 | tok/s 23350
step   1180 | loss 1.3950 | lr 3.00e-04 | grad 2.47 | tok/s 22612
step   1190 | loss 1.3033 | lr 3.00e-04 | grad 3.08 | tok/s 23419
step   1200 | loss 1.3278 | lr 3.00e-04 | grad 2.84 | tok/s 22521
step   1210 | loss 1.2855 | lr 3.00e-04 | grad 5.56 | tok/s 22633
step   1220 | loss 1.3207 | lr 3.00e-04 | grad 1.70 | tok/s 22133
step   1230 | loss 1.4225 | lr 3.00e-04 | grad 3.53 | tok/s 22160
step   1240 | loss 1.5116 | lr 3.00e-04 | grad 1.48 | tok/s 22953
step   1250 | loss 1.3907 | lr 3.00e-04 | grad 1.28 | tok/s 22915
step   1260 | loss 1.4239 | lr 3.00e-04 | grad 1.62 | tok/s 23082
step   1270 | loss 1.4212 | lr 3.00e-04 | grad 6.62 | tok/s 22046
step   1280 | loss 1.3597 | lr 3.00e-04 | grad 2.64 | tok/s 22983
step   1290 | loss 1.4127 | lr 3.00e-04 | grad 1.37 | tok/s 21898
step   1300 | loss 1.4213 | lr 3.00e-04 | grad 4.91 | tok/s 22845
step   1310 | loss 1.4404 | lr 3.00e-04 | grad 1.52 | tok/s 22460
step   1320 | loss 1.2744 | lr 3.00e-04 | grad 1.25 | tok/s 22911
step   1330 | loss 1.4879 | lr 3.00e-04 | grad 1.37 | tok/s 22221
step   1340 | loss 1.2730 | lr 3.00e-04 | grad 2.45 | tok/s 22037
step   1350 | loss 1.0157 | lr 3.00e-04 | grad 1.92 | tok/s 23263
step   1360 | loss 1.6344 | lr 3.00e-04 | grad 1.79 | tok/s 22673
step   1370 | loss 1.3782 | lr 3.00e-04 | grad 2.95 | tok/s 22519
step   1380 | loss 1.3308 | lr 3.00e-04 | grad 1.86 | tok/s 23012
step   1390 | loss 1.3917 | lr 3.00e-04 | grad 1.99 | tok/s 22232
step   1400 | loss 1.3792 | lr 3.00e-04 | grad 1.82 | tok/s 22003
step   1410 | loss 1.3123 | lr 3.00e-04 | grad 2.27 | tok/s 22758
step   1420 | loss 1.4774 | lr 3.00e-04 | grad 1.63 | tok/s 22627
step   1430 | loss 1.7100 | lr 3.00e-04 | grad 3.20 | tok/s 22881
step   1440 | loss 1.1878 | lr 3.00e-04 | grad 1.61 | tok/s 22587
step   1450 | loss 1.3882 | lr 3.00e-04 | grad 1.37 | tok/s 22312
step   1460 | loss 1.3406 | lr 3.00e-04 | grad 2.17 | tok/s 22984
step   1470 | loss 1.4450 | lr 3.00e-04 | grad 1.58 | tok/s 23206
step   1480 | loss 1.3415 | lr 3.00e-04 | grad 2.30 | tok/s 22361
step   1490 | loss 1.1832 | lr 3.00e-04 | grad 1.37 | tok/s 23271

Training complete! Final step: 1495
