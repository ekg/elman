Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_34/levelfla-gdn_100m_20260127_064356
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,801,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.1194 | lr 3.00e-04 | grad 15.31 | tok/s 10711
step     20 | loss 2.7693 | lr 3.00e-04 | grad 5.00 | tok/s 24364
step     30 | loss 3.0634 | lr 3.00e-04 | grad 5.66 | tok/s 25692
step     40 | loss 4.2269 | lr 3.00e-04 | grad 24.50 | tok/s 26088
step     50 | loss 4.0565 | lr 3.00e-04 | grad 11.81 | tok/s 26401
step     60 | loss 3.2712 | lr 3.00e-04 | grad 11.62 | tok/s 26213
step     70 | loss 2.7481 | lr 3.00e-04 | grad 6.69 | tok/s 26159
step     80 | loss 2.3956 | lr 3.00e-04 | grad 6.62 | tok/s 26054
step     90 | loss 2.3644 | lr 3.00e-04 | grad 4.84 | tok/s 25976
step    100 | loss 2.1055 | lr 3.00e-04 | grad 5.28 | tok/s 25963
step    110 | loss 2.1444 | lr 3.00e-04 | grad 4.62 | tok/s 25684
step    120 | loss 2.7787 | lr 3.00e-04 | grad 3.47 | tok/s 24502
step    130 | loss 2.0233 | lr 3.00e-04 | grad 5.56 | tok/s 25012
step    140 | loss 2.2951 | lr 3.00e-04 | grad 8.62 | tok/s 25070
step    150 | loss 1.6079 | lr 3.00e-04 | grad 6.41 | tok/s 25663
step    160 | loss 2.2444 | lr 3.00e-04 | grad 2.69 | tok/s 24745
step    170 | loss 2.1602 | lr 3.00e-04 | grad 2.27 | tok/s 24426
step    180 | loss 1.7689 | lr 3.00e-04 | grad 3.61 | tok/s 25010
step    190 | loss 1.8120 | lr 3.00e-04 | grad 2.64 | tok/s 24555
step    200 | loss 1.5394 | lr 3.00e-04 | grad 2.02 | tok/s 25595
step    210 | loss 1.7470 | lr 3.00e-04 | grad 4.41 | tok/s 24340
step    220 | loss 2.0928 | lr 3.00e-04 | grad 3.05 | tok/s 24597
step    230 | loss 1.8010 | lr 3.00e-04 | grad 2.73 | tok/s 24530
step    240 | loss 2.1464 | lr 3.00e-04 | grad 5.81 | tok/s 24855
step    250 | loss 1.6396 | lr 3.00e-04 | grad 1.73 | tok/s 24688
step    260 | loss 1.7669 | lr 3.00e-04 | grad 3.05 | tok/s 25327
step    270 | loss 1.7075 | lr 3.00e-04 | grad 2.06 | tok/s 24799
step    280 | loss 1.6606 | lr 3.00e-04 | grad 1.97 | tok/s 23284
step    290 | loss 1.5600 | lr 3.00e-04 | grad 2.44 | tok/s 24107
step    300 | loss 1.8392 | lr 3.00e-04 | grad 2.58 | tok/s 24267
step    310 | loss 1.5661 | lr 3.00e-04 | grad 1.80 | tok/s 24146
step    320 | loss 1.7781 | lr 3.00e-04 | grad 4.22 | tok/s 24481
step    330 | loss 1.6132 | lr 3.00e-04 | grad 2.05 | tok/s 24719
step    340 | loss 1.9202 | lr 3.00e-04 | grad 2.50 | tok/s 24590
step    350 | loss 1.6312 | lr 3.00e-04 | grad 2.09 | tok/s 25260
step    360 | loss 1.4821 | lr 3.00e-04 | grad 2.14 | tok/s 24253
step    370 | loss 1.3931 | lr 3.00e-04 | grad 1.65 | tok/s 25521
step    380 | loss 1.1247 | lr 3.00e-04 | grad 1.71 | tok/s 25747
step    390 | loss 1.0267 | lr 3.00e-04 | grad 1.50 | tok/s 25790
step    400 | loss 1.6567 | lr 3.00e-04 | grad 1.66 | tok/s 24454
step    410 | loss 1.6465 | lr 3.00e-04 | grad 2.64 | tok/s 24646
step    420 | loss 1.5383 | lr 3.00e-04 | grad 2.86 | tok/s 25712
step    430 | loss 1.4781 | lr 3.00e-04 | grad 1.89 | tok/s 25297
step    440 | loss 1.5966 | lr 3.00e-04 | grad 2.38 | tok/s 24601
step    450 | loss 1.5346 | lr 3.00e-04 | grad 1.41 | tok/s 24845
step    460 | loss 1.5178 | lr 3.00e-04 | grad 1.91 | tok/s 25192
step    470 | loss 1.4839 | lr 3.00e-04 | grad 3.66 | tok/s 25014
step    480 | loss 1.5410 | lr 3.00e-04 | grad 2.91 | tok/s 25547
step    490 | loss 1.5931 | lr 3.00e-04 | grad 2.39 | tok/s 24524
step    500 | loss 1.7121 | lr 3.00e-04 | grad 1.55 | tok/s 24958
step    510 | loss 1.5988 | lr 3.00e-04 | grad 1.41 | tok/s 23865
step    520 | loss 1.4549 | lr 3.00e-04 | grad 2.48 | tok/s 25003
step    530 | loss 1.6281 | lr 3.00e-04 | grad 2.14 | tok/s 24582
step    540 | loss 1.5158 | lr 3.00e-04 | grad 1.61 | tok/s 24092
step    550 | loss 1.2796 | lr 3.00e-04 | grad 3.30 | tok/s 25206
step    560 | loss 1.3745 | lr 3.00e-04 | grad 1.84 | tok/s 25820
step    570 | loss 1.2728 | lr 3.00e-04 | grad 1.78 | tok/s 25874
step    580 | loss 1.2388 | lr 3.00e-04 | grad 1.34 | tok/s 25866
step    590 | loss 1.2703 | lr 3.00e-04 | grad 1.53 | tok/s 25926
step    600 | loss 1.2018 | lr 3.00e-04 | grad 1.53 | tok/s 25915
step    610 | loss 1.2415 | lr 3.00e-04 | grad 1.68 | tok/s 25887
step    620 | loss 1.2228 | lr 3.00e-04 | grad 1.90 | tok/s 25814
step    630 | loss 1.5493 | lr 3.00e-04 | grad 5.97 | tok/s 24434
step    640 | loss 1.6577 | lr 3.00e-04 | grad 1.80 | tok/s 24696
step    650 | loss 1.4698 | lr 3.00e-04 | grad 1.70 | tok/s 24686
step    660 | loss 1.5236 | lr 3.00e-04 | grad 1.95 | tok/s 25585
step    670 | loss 1.5406 | lr 3.00e-04 | grad 5.09 | tok/s 24756
step    680 | loss 1.5402 | lr 3.00e-04 | grad 2.19 | tok/s 24389
step    690 | loss 1.4931 | lr 3.00e-04 | grad 1.92 | tok/s 24188
step    700 | loss 1.3967 | lr 3.00e-04 | grad 1.34 | tok/s 24712
step    710 | loss 1.5442 | lr 3.00e-04 | grad 4.50 | tok/s 24338
step    720 | loss 1.2321 | lr 3.00e-04 | grad 1.61 | tok/s 25275
step    730 | loss 1.3682 | lr 3.00e-04 | grad 1.45 | tok/s 24863
step    740 | loss 1.6774 | lr 3.00e-04 | grad 3.75 | tok/s 25544
step    750 | loss 1.4668 | lr 3.00e-04 | grad 1.76 | tok/s 25839
step    760 | loss 1.4398 | lr 3.00e-04 | grad 3.67 | tok/s 25336
step    770 | loss 1.4950 | lr 3.00e-04 | grad 1.91 | tok/s 24924
step    780 | loss 1.4130 | lr 3.00e-04 | grad 1.70 | tok/s 25075
step    790 | loss 1.5536 | lr 3.00e-04 | grad 5.09 | tok/s 25639
step    800 | loss 1.2568 | lr 3.00e-04 | grad 1.70 | tok/s 25244
step    810 | loss 1.2517 | lr 3.00e-04 | grad 2.70 | tok/s 24426
step    820 | loss 1.3449 | lr 3.00e-04 | grad 1.70 | tok/s 24834
step    830 | loss 1.4182 | lr 3.00e-04 | grad 1.29 | tok/s 24552
step    840 | loss 1.5319 | lr 3.00e-04 | grad 1.70 | tok/s 24431
step    850 | loss 1.4424 | lr 3.00e-04 | grad 1.45 | tok/s 24938
step    860 | loss 1.4951 | lr 3.00e-04 | grad 2.61 | tok/s 25278
step    870 | loss 1.3003 | lr 3.00e-04 | grad 1.69 | tok/s 25484
step    880 | loss 1.5149 | lr 3.00e-04 | grad 1.67 | tok/s 25041
step    890 | loss 1.4194 | lr 3.00e-04 | grad 1.39 | tok/s 24948
step    900 | loss 1.4576 | lr 3.00e-04 | grad 1.62 | tok/s 24844
step    910 | loss 1.4254 | lr 3.00e-04 | grad 7.25 | tok/s 24612
step    920 | loss 1.3988 | lr 3.00e-04 | grad 1.71 | tok/s 24870
step    930 | loss 1.3207 | lr 3.00e-04 | grad 1.80 | tok/s 25161
step    940 | loss 1.2829 | lr 3.00e-04 | grad 1.59 | tok/s 24624
step    950 | loss 1.4223 | lr 3.00e-04 | grad 2.30 | tok/s 24235
step    960 | loss 1.3729 | lr 3.00e-04 | grad 1.36 | tok/s 24838
step    970 | loss 1.4106 | lr 3.00e-04 | grad 1.63 | tok/s 24845
step    980 | loss 1.8257 | lr 3.00e-04 | grad 3.64 | tok/s 25827
step    990 | loss 1.4920 | lr 3.00e-04 | grad 1.67 | tok/s 24755
step   1000 | loss 1.4963 | lr 3.00e-04 | grad 2.22 | tok/s 24837
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4963.pt
step   1010 | loss 1.2900 | lr 3.00e-04 | grad 2.50 | tok/s 12891
step   1020 | loss 1.1637 | lr 3.00e-04 | grad 1.27 | tok/s 26254
step   1030 | loss 1.4670 | lr 3.00e-04 | grad 2.52 | tok/s 24855
step   1040 | loss 2.0205 | lr 3.00e-04 | grad 4.19 | tok/s 25426
step   1050 | loss 1.3878 | lr 3.00e-04 | grad 2.48 | tok/s 25608
step   1060 | loss 1.1275 | lr 3.00e-04 | grad 4.31 | tok/s 25479
step   1070 | loss 1.3583 | lr 3.00e-04 | grad 1.95 | tok/s 25233
step   1080 | loss 1.2120 | lr 3.00e-04 | grad 1.35 | tok/s 26063
step   1090 | loss 1.1752 | lr 3.00e-04 | grad 1.27 | tok/s 26015
step   1100 | loss 1.1606 | lr 3.00e-04 | grad 1.41 | tok/s 26043
step   1110 | loss 1.1006 | lr 3.00e-04 | grad 1.14 | tok/s 26021
step   1120 | loss 1.3850 | lr 3.00e-04 | grad 3.38 | tok/s 25293
step   1130 | loss 1.5401 | lr 3.00e-04 | grad 1.50 | tok/s 25564
step   1140 | loss 1.6834 | lr 3.00e-04 | grad 1.80 | tok/s 25837
step   1150 | loss 1.5296 | lr 3.00e-04 | grad 1.84 | tok/s 25026
step   1160 | loss 1.6472 | lr 3.00e-04 | grad 2.25 | tok/s 24690
step   1170 | loss 1.4141 | lr 3.00e-04 | grad 1.88 | tok/s 24404
step   1180 | loss 1.2785 | lr 3.00e-04 | grad 2.83 | tok/s 25651
step   1190 | loss 1.4963 | lr 3.00e-04 | grad 2.31 | tok/s 25893
step   1200 | loss 1.0654 | lr 3.00e-04 | grad 2.08 | tok/s 26012
step   1210 | loss 1.3152 | lr 3.00e-04 | grad 1.61 | tok/s 24319
step   1220 | loss 1.3018 | lr 3.00e-04 | grad 1.85 | tok/s 25479
step   1230 | loss 1.2708 | lr 3.00e-04 | grad 1.31 | tok/s 25521
step   1240 | loss 1.2488 | lr 3.00e-04 | grad 1.48 | tok/s 25580
step   1250 | loss 1.3915 | lr 3.00e-04 | grad 2.36 | tok/s 25235
step   1260 | loss 1.3139 | lr 3.00e-04 | grad 1.90 | tok/s 25777
step   1270 | loss 1.3031 | lr 3.00e-04 | grad 1.73 | tok/s 25064
step   1280 | loss 1.3128 | lr 3.00e-04 | grad 1.55 | tok/s 24778
step   1290 | loss 1.2707 | lr 3.00e-04 | grad 1.87 | tok/s 24881
step   1300 | loss 1.5479 | lr 3.00e-04 | grad 5.41 | tok/s 24510
step   1310 | loss 1.3909 | lr 3.00e-04 | grad 1.54 | tok/s 25424
step   1320 | loss 1.4028 | lr 3.00e-04 | grad 1.66 | tok/s 25459
step   1330 | loss 1.3642 | lr 3.00e-04 | grad 1.45 | tok/s 25215
step   1340 | loss 1.4467 | lr 3.00e-04 | grad 2.05 | tok/s 24697
step   1350 | loss 1.3301 | lr 3.00e-04 | grad 1.39 | tok/s 25220
step   1360 | loss 1.3881 | lr 3.00e-04 | grad 1.58 | tok/s 24296
step   1370 | loss 1.4950 | lr 3.00e-04 | grad 2.28 | tok/s 25599
step   1380 | loss 1.3712 | lr 3.00e-04 | grad 1.56 | tok/s 24499
step   1390 | loss 1.2416 | lr 3.00e-04 | grad 2.33 | tok/s 25599
step   1400 | loss 1.4161 | lr 3.00e-04 | grad 1.73 | tok/s 24720
step   1410 | loss 1.3446 | lr 3.00e-04 | grad 3.19 | tok/s 24198
step   1420 | loss 1.0368 | lr 3.00e-04 | grad 6.34 | tok/s 25690
step   1430 | loss 1.5794 | lr 3.00e-04 | grad 1.76 | tok/s 24798
step   1440 | loss 1.3536 | lr 3.00e-04 | grad 1.84 | tok/s 25441
step   1450 | loss 1.3443 | lr 3.00e-04 | grad 6.66 | tok/s 25420
step   1460 | loss 1.4832 | lr 3.00e-04 | grad 3.86 | tok/s 24687
step   1470 | loss 1.2823 | lr 3.00e-04 | grad 1.61 | tok/s 24099
step   1480 | loss 1.2926 | lr 3.00e-04 | grad 1.30 | tok/s 25426
step   1490 | loss 1.7670 | lr 3.00e-04 | grad 7.59 | tok/s 25019
step   1500 | loss 1.3822 | lr 3.00e-04 | grad 1.67 | tok/s 25069
step   1510 | loss 1.2023 | lr 3.00e-04 | grad 1.56 | tok/s 25098
step   1520 | loss 1.3910 | lr 3.00e-04 | grad 1.51 | tok/s 24869
step   1530 | loss 1.3205 | lr 3.00e-04 | grad 1.49 | tok/s 25245
step   1540 | loss 1.4364 | lr 3.00e-04 | grad 1.41 | tok/s 25500
step   1550 | loss 1.4047 | lr 3.00e-04 | grad 1.81 | tok/s 24991
step   1560 | loss 1.0621 | lr 3.00e-04 | grad 1.84 | tok/s 25939
step   1570 | loss 1.2126 | lr 3.00e-04 | grad 1.37 | tok/s 25245
step   1580 | loss 1.2144 | lr 3.00e-04 | grad 2.06 | tok/s 25145
step   1590 | loss 1.3901 | lr 3.00e-04 | grad 1.73 | tok/s 24645
step   1600 | loss 1.2390 | lr 3.00e-04 | grad 2.30 | tok/s 25636
step   1610 | loss 1.8506 | lr 3.00e-04 | grad 2.97 | tok/s 25310
step   1620 | loss 1.8734 | lr 3.00e-04 | grad 2.38 | tok/s 25896
step   1630 | loss 1.5790 | lr 3.00e-04 | grad 2.44 | tok/s 25914
step   1640 | loss 1.4193 | lr 3.00e-04 | grad 1.87 | tok/s 25942
step   1650 | loss 1.3231 | lr 3.00e-04 | grad 2.05 | tok/s 25966
step   1660 | loss 1.2839 | lr 3.00e-04 | grad 1.97 | tok/s 25926
step   1670 | loss 1.4328 | lr 3.00e-04 | grad 3.08 | tok/s 25138
step   1680 | loss 1.3412 | lr 3.00e-04 | grad 1.42 | tok/s 24923
step   1690 | loss 1.4007 | lr 3.00e-04 | grad 1.88 | tok/s 24214
step   1700 | loss 1.2480 | lr 3.00e-04 | grad 1.46 | tok/s 25410
step   1710 | loss 1.1746 | lr 3.00e-04 | grad 1.71 | tok/s 23476
step   1720 | loss 1.4011 | lr 3.00e-04 | grad 1.55 | tok/s 24841
step   1730 | loss 1.3535 | lr 3.00e-04 | grad 2.02 | tok/s 25180
step   1740 | loss 1.3327 | lr 3.00e-04 | grad 1.69 | tok/s 25701
step   1750 | loss 1.1556 | lr 3.00e-04 | grad 1.21 | tok/s 24905
step   1760 | loss 1.3897 | lr 3.00e-04 | grad 1.62 | tok/s 24581
step   1770 | loss 1.4975 | lr 3.00e-04 | grad 1.48 | tok/s 25486
step   1780 | loss 1.6514 | lr 3.00e-04 | grad 1.55 | tok/s 23719
step   1790 | loss 1.2493 | lr 3.00e-04 | grad 1.67 | tok/s 24668
step   1800 | loss 1.1920 | lr 3.00e-04 | grad 1.37 | tok/s 24807
step   1810 | loss 1.3267 | lr 3.00e-04 | grad 1.37 | tok/s 24963
step   1820 | loss 1.4664 | lr 3.00e-04 | grad 1.55 | tok/s 24846
step   1830 | loss 1.3175 | lr 3.00e-04 | grad 1.24 | tok/s 24035
step   1840 | loss 1.2397 | lr 3.00e-04 | grad 1.47 | tok/s 25054
step   1850 | loss 1.3304 | lr 3.00e-04 | grad 1.38 | tok/s 24484

Training complete! Final step: 1857
