Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_56/levelfla-gdn_100m_20260127_070427
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 464,339,192 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0728 | lr 3.00e-04 | grad 16.62 | tok/s 10477
step     20 | loss 2.8166 | lr 3.00e-04 | grad 6.09 | tok/s 22198
step     30 | loss 3.0765 | lr 3.00e-04 | grad 8.69 | tok/s 23357
step     40 | loss 4.1607 | lr 3.00e-04 | grad 28.75 | tok/s 23756
step     50 | loss 4.2169 | lr 3.00e-04 | grad 14.75 | tok/s 23996
step     60 | loss 3.4672 | lr 3.00e-04 | grad 15.44 | tok/s 23939
step     70 | loss 2.8747 | lr 3.00e-04 | grad 8.94 | tok/s 23828
step     80 | loss 2.6018 | lr 3.00e-04 | grad 7.97 | tok/s 23691
step     90 | loss 2.4131 | lr 3.00e-04 | grad 7.12 | tok/s 23632
step    100 | loss 2.2359 | lr 3.00e-04 | grad 3.23 | tok/s 23568
step    110 | loss 2.2830 | lr 3.00e-04 | grad 5.19 | tok/s 23335
step    120 | loss 2.8711 | lr 3.00e-04 | grad 3.61 | tok/s 22317
step    130 | loss 2.0748 | lr 3.00e-04 | grad 5.31 | tok/s 22817
step    140 | loss 2.3235 | lr 3.00e-04 | grad 8.19 | tok/s 22836
step    150 | loss 1.4993 | lr 3.00e-04 | grad 6.16 | tok/s 23308
step    160 | loss 2.2727 | lr 3.00e-04 | grad 2.56 | tok/s 22537
step    170 | loss 2.1937 | lr 3.00e-04 | grad 2.23 | tok/s 22217
step    180 | loss 1.8238 | lr 3.00e-04 | grad 3.52 | tok/s 22756
step    190 | loss 1.8361 | lr 3.00e-04 | grad 2.48 | tok/s 22284
step    200 | loss 1.5648 | lr 3.00e-04 | grad 1.91 | tok/s 23314
step    210 | loss 1.7675 | lr 3.00e-04 | grad 3.67 | tok/s 22157
step    220 | loss 2.1557 | lr 3.00e-04 | grad 5.16 | tok/s 22336
step    230 | loss 1.8378 | lr 3.00e-04 | grad 2.53 | tok/s 22295
step    240 | loss 2.1797 | lr 3.00e-04 | grad 5.84 | tok/s 22582
step    250 | loss 1.6593 | lr 3.00e-04 | grad 1.55 | tok/s 22487
step    260 | loss 1.7916 | lr 3.00e-04 | grad 3.08 | tok/s 23076
step    270 | loss 1.7238 | lr 3.00e-04 | grad 1.99 | tok/s 22582
step    280 | loss 1.6793 | lr 3.00e-04 | grad 1.95 | tok/s 21202
step    290 | loss 1.5728 | lr 3.00e-04 | grad 2.23 | tok/s 21894
step    300 | loss 1.8612 | lr 3.00e-04 | grad 2.20 | tok/s 22067
step    310 | loss 1.5781 | lr 3.00e-04 | grad 1.81 | tok/s 21968
step    320 | loss 1.7875 | lr 3.00e-04 | grad 3.81 | tok/s 22208
step    330 | loss 1.6207 | lr 3.00e-04 | grad 1.84 | tok/s 22454
step    340 | loss 1.9640 | lr 3.00e-04 | grad 2.30 | tok/s 22375
step    350 | loss 1.6436 | lr 3.00e-04 | grad 1.98 | tok/s 23021
step    360 | loss 1.4857 | lr 3.00e-04 | grad 2.02 | tok/s 22029
step    370 | loss 1.4044 | lr 3.00e-04 | grad 1.66 | tok/s 23217
step    380 | loss 1.1341 | lr 3.00e-04 | grad 1.72 | tok/s 23421
step    390 | loss 1.0352 | lr 3.00e-04 | grad 1.30 | tok/s 23419
step    400 | loss 1.6794 | lr 3.00e-04 | grad 1.62 | tok/s 22180
step    410 | loss 1.6660 | lr 3.00e-04 | grad 2.56 | tok/s 22381
step    420 | loss 1.5537 | lr 3.00e-04 | grad 2.73 | tok/s 23292
step    430 | loss 1.4858 | lr 3.00e-04 | grad 1.84 | tok/s 22942
step    440 | loss 1.6103 | lr 3.00e-04 | grad 2.22 | tok/s 22210
step    450 | loss 1.5409 | lr 3.00e-04 | grad 1.41 | tok/s 22503
step    460 | loss 1.5236 | lr 3.00e-04 | grad 1.78 | tok/s 22843
step    470 | loss 1.4937 | lr 3.00e-04 | grad 3.58 | tok/s 22681
step    480 | loss 1.5392 | lr 3.00e-04 | grad 2.67 | tok/s 23226
step    490 | loss 1.5999 | lr 3.00e-04 | grad 2.41 | tok/s 22216
step    500 | loss 1.7226 | lr 3.00e-04 | grad 1.52 | tok/s 22616
step    510 | loss 1.6086 | lr 3.00e-04 | grad 1.34 | tok/s 21641
step    520 | loss 1.4628 | lr 3.00e-04 | grad 2.56 | tok/s 22618
step    530 | loss 1.6355 | lr 3.00e-04 | grad 1.99 | tok/s 22292
step    540 | loss 1.5201 | lr 3.00e-04 | grad 1.62 | tok/s 21799
step    550 | loss 1.2782 | lr 3.00e-04 | grad 3.02 | tok/s 22858
step    560 | loss 1.3746 | lr 3.00e-04 | grad 1.69 | tok/s 23445
step    570 | loss 1.2779 | lr 3.00e-04 | grad 1.70 | tok/s 23433
step    580 | loss 1.2385 | lr 3.00e-04 | grad 1.27 | tok/s 23400
step    590 | loss 1.2731 | lr 3.00e-04 | grad 1.28 | tok/s 23383
step    600 | loss 1.2013 | lr 3.00e-04 | grad 1.49 | tok/s 23375
step    610 | loss 1.2455 | lr 3.00e-04 | grad 1.62 | tok/s 23398
step    620 | loss 1.2259 | lr 3.00e-04 | grad 1.68 | tok/s 23305
step    630 | loss 1.5576 | lr 3.00e-04 | grad 6.47 | tok/s 22108
step    640 | loss 1.6636 | lr 3.00e-04 | grad 1.73 | tok/s 22335
step    650 | loss 1.4767 | lr 3.00e-04 | grad 1.63 | tok/s 22320
step    660 | loss 1.5268 | lr 3.00e-04 | grad 1.85 | tok/s 23186
step    670 | loss 1.5408 | lr 3.00e-04 | grad 4.84 | tok/s 22412
step    680 | loss 1.5474 | lr 3.00e-04 | grad 2.11 | tok/s 22066
step    690 | loss 1.4933 | lr 3.00e-04 | grad 1.86 | tok/s 21876
step    700 | loss 1.4030 | lr 3.00e-04 | grad 1.34 | tok/s 22407
step    710 | loss 1.5403 | lr 3.00e-04 | grad 3.67 | tok/s 22016
step    720 | loss 1.2403 | lr 3.00e-04 | grad 1.51 | tok/s 22879
step    730 | loss 1.3658 | lr 3.00e-04 | grad 1.38 | tok/s 22506
step    740 | loss 1.6865 | lr 3.00e-04 | grad 3.61 | tok/s 23109
step    750 | loss 1.4650 | lr 3.00e-04 | grad 1.72 | tok/s 23436
step    760 | loss 1.4395 | lr 3.00e-04 | grad 3.59 | tok/s 22867
step    770 | loss 1.5032 | lr 3.00e-04 | grad 1.76 | tok/s 22492
step    780 | loss 1.4180 | lr 3.00e-04 | grad 1.70 | tok/s 22660
step    790 | loss 1.5450 | lr 3.00e-04 | grad 5.78 | tok/s 23131
step    800 | loss 1.2650 | lr 3.00e-04 | grad 1.79 | tok/s 22803
step    810 | loss 1.2573 | lr 3.00e-04 | grad 2.62 | tok/s 22106
step    820 | loss 1.3476 | lr 3.00e-04 | grad 1.62 | tok/s 22463
step    830 | loss 1.4274 | lr 3.00e-04 | grad 1.32 | tok/s 22213
step    840 | loss 1.5297 | lr 3.00e-04 | grad 1.61 | tok/s 22050
step    850 | loss 1.4493 | lr 3.00e-04 | grad 1.42 | tok/s 22541
step    860 | loss 1.4867 | lr 3.00e-04 | grad 2.52 | tok/s 22873
step    870 | loss 1.3056 | lr 3.00e-04 | grad 1.73 | tok/s 23089
step    880 | loss 1.5239 | lr 3.00e-04 | grad 1.57 | tok/s 22599
step    890 | loss 1.4268 | lr 3.00e-04 | grad 1.22 | tok/s 22518
step    900 | loss 1.4639 | lr 3.00e-04 | grad 1.52 | tok/s 22389
step    910 | loss 1.4281 | lr 3.00e-04 | grad 7.06 | tok/s 22201
step    920 | loss 1.4032 | lr 3.00e-04 | grad 1.55 | tok/s 22462
step    930 | loss 1.3231 | lr 3.00e-04 | grad 1.77 | tok/s 22707
step    940 | loss 1.2872 | lr 3.00e-04 | grad 1.55 | tok/s 22210
step    950 | loss 1.4312 | lr 3.00e-04 | grad 2.19 | tok/s 21860
step    960 | loss 1.3756 | lr 3.00e-04 | grad 1.36 | tok/s 22389
step    970 | loss 1.4163 | lr 3.00e-04 | grad 1.51 | tok/s 22419
step    980 | loss 1.8356 | lr 3.00e-04 | grad 3.88 | tok/s 23352
step    990 | loss 1.4932 | lr 3.00e-04 | grad 1.62 | tok/s 22376
step   1000 | loss 1.5054 | lr 3.00e-04 | grad 2.19 | tok/s 22529
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5054.pt
step   1010 | loss 1.2917 | lr 3.00e-04 | grad 2.52 | tok/s 11845
step   1020 | loss 1.1660 | lr 3.00e-04 | grad 1.28 | tok/s 23842
step   1030 | loss 1.4678 | lr 3.00e-04 | grad 2.39 | tok/s 22555
step   1040 | loss 2.0338 | lr 3.00e-04 | grad 3.81 | tok/s 23044
step   1050 | loss 1.3934 | lr 3.00e-04 | grad 2.48 | tok/s 23207
step   1060 | loss 1.1313 | lr 3.00e-04 | grad 6.44 | tok/s 23033
step   1070 | loss 1.3642 | lr 3.00e-04 | grad 1.71 | tok/s 22884
step   1080 | loss 1.2125 | lr 3.00e-04 | grad 1.27 | tok/s 23594
step   1090 | loss 1.1759 | lr 3.00e-04 | grad 1.23 | tok/s 23539
step   1100 | loss 1.1614 | lr 3.00e-04 | grad 1.20 | tok/s 23495
step   1110 | loss 1.1032 | lr 3.00e-04 | grad 1.30 | tok/s 23510
step   1120 | loss 1.3915 | lr 3.00e-04 | grad 3.28 | tok/s 22854
step   1130 | loss 1.5398 | lr 3.00e-04 | grad 1.42 | tok/s 23127
step   1140 | loss 1.6830 | lr 3.00e-04 | grad 1.74 | tok/s 23387
step   1150 | loss 1.5724 | lr 3.00e-04 | grad 1.84 | tok/s 22662
step   1160 | loss 1.6611 | lr 3.00e-04 | grad 2.05 | tok/s 22358
step   1170 | loss 1.4237 | lr 3.00e-04 | grad 1.86 | tok/s 22105
step   1180 | loss 1.2823 | lr 3.00e-04 | grad 2.83 | tok/s 23173
step   1190 | loss 1.5035 | lr 3.00e-04 | grad 2.39 | tok/s 23422
step   1200 | loss 1.0632 | lr 3.00e-04 | grad 2.11 | tok/s 23585
step   1210 | loss 1.3230 | lr 3.00e-04 | grad 1.67 | tok/s 21991
step   1220 | loss 1.3115 | lr 3.00e-04 | grad 1.83 | tok/s 23073
step   1230 | loss 1.2726 | lr 3.00e-04 | grad 1.25 | tok/s 23123
step   1240 | loss 1.2481 | lr 3.00e-04 | grad 1.35 | tok/s 23160
step   1250 | loss 1.3990 | lr 3.00e-04 | grad 2.52 | tok/s 22853
step   1260 | loss 1.3087 | lr 3.00e-04 | grad 1.91 | tok/s 23342
step   1270 | loss 1.3104 | lr 3.00e-04 | grad 1.77 | tok/s 22673
step   1280 | loss 1.3177 | lr 3.00e-04 | grad 1.59 | tok/s 22477
step   1290 | loss 1.2776 | lr 3.00e-04 | grad 1.85 | tok/s 22516
step   1300 | loss 1.5706 | lr 3.00e-04 | grad 5.59 | tok/s 22100
step   1310 | loss 1.4012 | lr 3.00e-04 | grad 1.52 | tok/s 22966
step   1320 | loss 1.4054 | lr 3.00e-04 | grad 1.59 | tok/s 23041
step   1330 | loss 1.3652 | lr 3.00e-04 | grad 1.42 | tok/s 22799
step   1340 | loss 1.4520 | lr 3.00e-04 | grad 2.03 | tok/s 22334
step   1350 | loss 1.3371 | lr 3.00e-04 | grad 1.33 | tok/s 22821
step   1360 | loss 1.3946 | lr 3.00e-04 | grad 1.55 | tok/s 22034
step   1370 | loss 1.5010 | lr 3.00e-04 | grad 2.25 | tok/s 23142
step   1380 | loss 1.3716 | lr 3.00e-04 | grad 1.55 | tok/s 22124
step   1390 | loss 1.2540 | lr 3.00e-04 | grad 2.17 | tok/s 23199
step   1400 | loss 1.4195 | lr 3.00e-04 | grad 1.52 | tok/s 22386
step   1410 | loss 1.3523 | lr 3.00e-04 | grad 3.28 | tok/s 21877
step   1420 | loss 1.0436 | lr 3.00e-04 | grad 7.25 | tok/s 23316
step   1430 | loss 1.5711 | lr 3.00e-04 | grad 1.69 | tok/s 22476
step   1440 | loss 1.3613 | lr 3.00e-04 | grad 1.90 | tok/s 23054
step   1450 | loss 1.3489 | lr 3.00e-04 | grad 6.47 | tok/s 23004
step   1460 | loss 1.4891 | lr 3.00e-04 | grad 3.64 | tok/s 22398
step   1470 | loss 1.2899 | lr 3.00e-04 | grad 1.52 | tok/s 21896
step   1480 | loss 1.2909 | lr 3.00e-04 | grad 1.30 | tok/s 23101
step   1490 | loss 1.7669 | lr 3.00e-04 | grad 9.44 | tok/s 22665
step   1500 | loss 1.3971 | lr 3.00e-04 | grad 1.72 | tok/s 22707
step   1510 | loss 1.2475 | lr 3.00e-04 | grad 1.55 | tok/s 22768
step   1520 | loss 1.3992 | lr 3.00e-04 | grad 1.48 | tok/s 22515
step   1530 | loss 1.3224 | lr 3.00e-04 | grad 1.53 | tok/s 22866
step   1540 | loss 1.4440 | lr 3.00e-04 | grad 1.41 | tok/s 23129
step   1550 | loss 1.4113 | lr 3.00e-04 | grad 1.84 | tok/s 22596
step   1560 | loss 1.0649 | lr 3.00e-04 | grad 1.78 | tok/s 23537
step   1570 | loss 1.2202 | lr 3.00e-04 | grad 1.38 | tok/s 22873
step   1580 | loss 1.2159 | lr 3.00e-04 | grad 1.98 | tok/s 22798
step   1590 | loss 1.3763 | lr 3.00e-04 | grad 1.62 | tok/s 22351
step   1600 | loss 1.1947 | lr 3.00e-04 | grad 2.45 | tok/s 23204
step   1610 | loss 1.8668 | lr 3.00e-04 | grad 3.00 | tok/s 22922
step   1620 | loss 1.9087 | lr 3.00e-04 | grad 2.05 | tok/s 22075
step   1630 | loss 1.5993 | lr 3.00e-04 | grad 2.30 | tok/s 23510
step   1640 | loss 1.4309 | lr 3.00e-04 | grad 2.45 | tok/s 23484
step   1650 | loss 1.3334 | lr 3.00e-04 | grad 2.02 | tok/s 23484
step   1660 | loss 1.2909 | lr 3.00e-04 | grad 1.71 | tok/s 23459
step   1670 | loss 1.4475 | lr 3.00e-04 | grad 2.97 | tok/s 22764
step   1680 | loss 1.3469 | lr 3.00e-04 | grad 1.40 | tok/s 22585

Training complete! Final step: 1685
