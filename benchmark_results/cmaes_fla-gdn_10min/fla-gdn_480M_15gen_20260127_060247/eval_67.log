Using device: cuda
Output directory: benchmark_results/cmaes_fla-gdn_10min/fla-gdn_480M_15gen_20260127_060247/eval_67/levelfla-gdn_100m_20260127_072502
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 473,801,056 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.0689 | lr 3.00e-04 | grad 15.19 | tok/s 10615
step     20 | loss 2.7756 | lr 3.00e-04 | grad 5.50 | tok/s 22592
step     30 | loss 3.1131 | lr 3.00e-04 | grad 5.78 | tok/s 23820
step     40 | loss 4.2398 | lr 3.00e-04 | grad 24.88 | tok/s 24216
step     50 | loss 4.0719 | lr 3.00e-04 | grad 11.88 | tok/s 24444
step     60 | loss 3.2977 | lr 3.00e-04 | grad 13.00 | tok/s 24338
step     70 | loss 2.7882 | lr 3.00e-04 | grad 8.06 | tok/s 24225
step     80 | loss 2.5051 | lr 3.00e-04 | grad 7.09 | tok/s 24159
step     90 | loss 2.3559 | lr 3.00e-04 | grad 4.94 | tok/s 24077
step    100 | loss 2.1551 | lr 3.00e-04 | grad 5.34 | tok/s 24046
step    110 | loss 2.1780 | lr 3.00e-04 | grad 4.53 | tok/s 23810
step    120 | loss 2.8855 | lr 3.00e-04 | grad 3.56 | tok/s 22710
step    130 | loss 2.0335 | lr 3.00e-04 | grad 5.59 | tok/s 23175
step    140 | loss 2.3099 | lr 3.00e-04 | grad 9.00 | tok/s 23238
step    150 | loss 1.4919 | lr 3.00e-04 | grad 6.56 | tok/s 23809
step    160 | loss 2.2504 | lr 3.00e-04 | grad 2.73 | tok/s 22974
step    170 | loss 2.1666 | lr 3.00e-04 | grad 2.27 | tok/s 22632
step    180 | loss 1.7861 | lr 3.00e-04 | grad 3.48 | tok/s 23161
step    190 | loss 1.8069 | lr 3.00e-04 | grad 2.77 | tok/s 22726
step    200 | loss 1.5424 | lr 3.00e-04 | grad 2.00 | tok/s 23730
step    210 | loss 1.7525 | lr 3.00e-04 | grad 4.38 | tok/s 22530
step    220 | loss 2.1269 | lr 3.00e-04 | grad 6.78 | tok/s 22750
step    230 | loss 1.8224 | lr 3.00e-04 | grad 2.91 | tok/s 22699
step    240 | loss 2.1776 | lr 3.00e-04 | grad 5.94 | tok/s 22960
step    250 | loss 1.6428 | lr 3.00e-04 | grad 1.70 | tok/s 22805
step    260 | loss 1.7775 | lr 3.00e-04 | grad 3.16 | tok/s 23413
step    270 | loss 1.7165 | lr 3.00e-04 | grad 2.12 | tok/s 22875
step    280 | loss 1.6670 | lr 3.00e-04 | grad 2.05 | tok/s 21500
step    290 | loss 1.5577 | lr 3.00e-04 | grad 2.42 | tok/s 22243
step    300 | loss 1.8433 | lr 3.00e-04 | grad 2.58 | tok/s 22378
step    310 | loss 1.5735 | lr 3.00e-04 | grad 1.86 | tok/s 22279
step    320 | loss 1.7745 | lr 3.00e-04 | grad 4.28 | tok/s 22540
step    330 | loss 1.6138 | lr 3.00e-04 | grad 2.11 | tok/s 22774
step    340 | loss 1.9345 | lr 3.00e-04 | grad 2.31 | tok/s 22657
step    350 | loss 1.6314 | lr 3.00e-04 | grad 2.03 | tok/s 23298
step    360 | loss 1.4833 | lr 3.00e-04 | grad 2.25 | tok/s 22330
step    370 | loss 1.3983 | lr 3.00e-04 | grad 1.77 | tok/s 23495
step    380 | loss 1.1285 | lr 3.00e-04 | grad 1.84 | tok/s 23685
step    390 | loss 1.0272 | lr 3.00e-04 | grad 1.55 | tok/s 23708
step    400 | loss 1.6561 | lr 3.00e-04 | grad 1.69 | tok/s 22495
step    410 | loss 1.6555 | lr 3.00e-04 | grad 2.70 | tok/s 22715
step    420 | loss 1.5479 | lr 3.00e-04 | grad 2.66 | tok/s 23653
step    430 | loss 1.4849 | lr 3.00e-04 | grad 1.89 | tok/s 23250
step    440 | loss 1.6033 | lr 3.00e-04 | grad 2.48 | tok/s 22560
step    450 | loss 1.5379 | lr 3.00e-04 | grad 1.47 | tok/s 22787
step    460 | loss 1.5281 | lr 3.00e-04 | grad 1.88 | tok/s 23136
step    470 | loss 1.4857 | lr 3.00e-04 | grad 3.48 | tok/s 22982
step    480 | loss 1.5440 | lr 3.00e-04 | grad 2.91 | tok/s 23445
step    490 | loss 1.5936 | lr 3.00e-04 | grad 2.42 | tok/s 22502
step    500 | loss 1.7204 | lr 3.00e-04 | grad 1.59 | tok/s 22896
step    510 | loss 1.6003 | lr 3.00e-04 | grad 1.38 | tok/s 21879
step    520 | loss 1.4569 | lr 3.00e-04 | grad 2.70 | tok/s 22925
step    530 | loss 1.6274 | lr 3.00e-04 | grad 2.09 | tok/s 22533
step    540 | loss 1.5186 | lr 3.00e-04 | grad 1.68 | tok/s 22080
step    550 | loss 1.2756 | lr 3.00e-04 | grad 3.36 | tok/s 23112
step    560 | loss 1.3746 | lr 3.00e-04 | grad 1.91 | tok/s 23700
step    570 | loss 1.2745 | lr 3.00e-04 | grad 1.91 | tok/s 23717
step    580 | loss 1.2350 | lr 3.00e-04 | grad 1.33 | tok/s 23729
step    590 | loss 1.2691 | lr 3.00e-04 | grad 1.48 | tok/s 23729
step    600 | loss 1.1971 | lr 3.00e-04 | grad 1.55 | tok/s 23741
step    610 | loss 1.2440 | lr 3.00e-04 | grad 1.85 | tok/s 23715
step    620 | loss 1.2239 | lr 3.00e-04 | grad 1.76 | tok/s 23637
step    630 | loss 1.5555 | lr 3.00e-04 | grad 7.34 | tok/s 22405
step    640 | loss 1.6545 | lr 3.00e-04 | grad 1.81 | tok/s 22668
step    650 | loss 1.4705 | lr 3.00e-04 | grad 1.71 | tok/s 22656
step    660 | loss 1.5232 | lr 3.00e-04 | grad 2.00 | tok/s 23500
step    670 | loss 1.5411 | lr 3.00e-04 | grad 4.78 | tok/s 22755
step    680 | loss 1.5458 | lr 3.00e-04 | grad 2.23 | tok/s 22432
step    690 | loss 1.4933 | lr 3.00e-04 | grad 1.95 | tok/s 22241
step    700 | loss 1.4007 | lr 3.00e-04 | grad 1.36 | tok/s 22713
step    710 | loss 1.5439 | lr 3.00e-04 | grad 4.09 | tok/s 22367
step    720 | loss 1.2394 | lr 3.00e-04 | grad 1.63 | tok/s 23200
step    730 | loss 1.3681 | lr 3.00e-04 | grad 1.51 | tok/s 22851
step    740 | loss 1.6843 | lr 3.00e-04 | grad 4.56 | tok/s 23467
step    750 | loss 1.4722 | lr 3.00e-04 | grad 1.77 | tok/s 23749
step    760 | loss 1.4366 | lr 3.00e-04 | grad 3.70 | tok/s 23267
step    770 | loss 1.4964 | lr 3.00e-04 | grad 1.89 | tok/s 22875
step    780 | loss 1.4123 | lr 3.00e-04 | grad 1.80 | tok/s 23048
step    790 | loss 1.5505 | lr 3.00e-04 | grad 6.25 | tok/s 23527
step    800 | loss 1.2614 | lr 3.00e-04 | grad 2.11 | tok/s 23211
step    810 | loss 1.2538 | lr 3.00e-04 | grad 2.69 | tok/s 22435
step    820 | loss 1.3400 | lr 3.00e-04 | grad 1.71 | tok/s 22830
step    830 | loss 1.4183 | lr 3.00e-04 | grad 1.38 | tok/s 22555
step    840 | loss 1.5356 | lr 3.00e-04 | grad 1.73 | tok/s 22451
step    850 | loss 1.4423 | lr 3.00e-04 | grad 1.45 | tok/s 22913
step    860 | loss 1.4841 | lr 3.00e-04 | grad 2.72 | tok/s 23257
step    870 | loss 1.3025 | lr 3.00e-04 | grad 1.73 | tok/s 23461
step    880 | loss 1.5128 | lr 3.00e-04 | grad 1.64 | tok/s 22980
step    890 | loss 1.4230 | lr 3.00e-04 | grad 1.38 | tok/s 22921
step    900 | loss 1.4613 | lr 3.00e-04 | grad 1.53 | tok/s 22786
step    910 | loss 1.4220 | lr 3.00e-04 | grad 6.66 | tok/s 22564
step    920 | loss 1.3998 | lr 3.00e-04 | grad 1.77 | tok/s 22849
step    930 | loss 1.3166 | lr 3.00e-04 | grad 1.76 | tok/s 23137
step    940 | loss 1.2843 | lr 3.00e-04 | grad 1.62 | tok/s 22632
step    950 | loss 1.4261 | lr 3.00e-04 | grad 2.36 | tok/s 22250
step    960 | loss 1.3751 | lr 3.00e-04 | grad 1.48 | tok/s 22825
step    970 | loss 1.4085 | lr 3.00e-04 | grad 1.64 | tok/s 22844
step    980 | loss 1.8051 | lr 3.00e-04 | grad 3.48 | tok/s 23762
step    990 | loss 1.4867 | lr 3.00e-04 | grad 1.63 | tok/s 22796
step   1000 | loss 1.4945 | lr 3.00e-04 | grad 2.28 | tok/s 22857
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4945.pt
step   1010 | loss 1.2885 | lr 3.00e-04 | grad 2.62 | tok/s 11519
step   1020 | loss 1.1533 | lr 3.00e-04 | grad 1.25 | tok/s 24281
step   1030 | loss 1.4650 | lr 3.00e-04 | grad 2.86 | tok/s 22980
step   1040 | loss 2.0204 | lr 3.00e-04 | grad 5.59 | tok/s 23465
step   1050 | loss 1.3907 | lr 3.00e-04 | grad 2.55 | tok/s 23635
step   1060 | loss 1.0832 | lr 3.00e-04 | grad 6.50 | tok/s 23466
step   1070 | loss 1.3552 | lr 3.00e-04 | grad 1.91 | tok/s 23271
step   1080 | loss 1.2101 | lr 3.00e-04 | grad 1.37 | tok/s 24067
step   1090 | loss 1.1735 | lr 3.00e-04 | grad 1.20 | tok/s 24030
step   1100 | loss 1.1608 | lr 3.00e-04 | grad 1.42 | tok/s 24015
step   1110 | loss 1.1008 | lr 3.00e-04 | grad 1.23 | tok/s 24023
step   1120 | loss 1.3838 | lr 3.00e-04 | grad 3.36 | tok/s 23395
step   1130 | loss 1.5346 | lr 3.00e-04 | grad 1.51 | tok/s 23632
step   1140 | loss 1.6717 | lr 3.00e-04 | grad 1.82 | tok/s 23906
step   1150 | loss 1.5442 | lr 3.00e-04 | grad 1.94 | tok/s 23170
step   1160 | loss 1.6490 | lr 3.00e-04 | grad 2.20 | tok/s 22832
step   1170 | loss 1.4084 | lr 3.00e-04 | grad 2.02 | tok/s 22565
step   1180 | loss 1.2758 | lr 3.00e-04 | grad 2.77 | tok/s 23706
step   1190 | loss 1.4988 | lr 3.00e-04 | grad 2.39 | tok/s 23884
step   1200 | loss 1.0751 | lr 3.00e-04 | grad 2.05 | tok/s 24031
step   1210 | loss 1.3130 | lr 3.00e-04 | grad 1.66 | tok/s 22456
step   1220 | loss 1.3062 | lr 3.00e-04 | grad 1.90 | tok/s 23522
step   1230 | loss 1.2680 | lr 3.00e-04 | grad 1.31 | tok/s 23616
step   1240 | loss 1.2448 | lr 3.00e-04 | grad 1.52 | tok/s 23674
step   1250 | loss 1.3944 | lr 3.00e-04 | grad 2.33 | tok/s 23323
step   1260 | loss 1.3065 | lr 3.00e-04 | grad 1.95 | tok/s 23822
step   1270 | loss 1.3055 | lr 3.00e-04 | grad 1.83 | tok/s 23149
step   1280 | loss 1.3193 | lr 3.00e-04 | grad 1.62 | tok/s 22903
step   1290 | loss 1.2668 | lr 3.00e-04 | grad 1.84 | tok/s 22944
step   1300 | loss 1.5460 | lr 3.00e-04 | grad 5.25 | tok/s 22610
step   1310 | loss 1.3985 | lr 3.00e-04 | grad 1.59 | tok/s 23485
step   1320 | loss 1.3998 | lr 3.00e-04 | grad 1.82 | tok/s 23516
step   1330 | loss 1.3605 | lr 3.00e-04 | grad 1.50 | tok/s 23287
step   1340 | loss 1.4514 | lr 3.00e-04 | grad 2.05 | tok/s 22821
step   1350 | loss 1.3327 | lr 3.00e-04 | grad 1.39 | tok/s 23285
step   1360 | loss 1.3860 | lr 3.00e-04 | grad 1.57 | tok/s 22458
step   1370 | loss 1.5002 | lr 3.00e-04 | grad 2.55 | tok/s 23649
step   1380 | loss 1.3689 | lr 3.00e-04 | grad 1.58 | tok/s 22617
step   1390 | loss 1.2526 | lr 3.00e-04 | grad 2.36 | tok/s 23657
step   1400 | loss 1.4201 | lr 3.00e-04 | grad 1.69 | tok/s 22841
step   1410 | loss 1.3493 | lr 3.00e-04 | grad 3.50 | tok/s 22350
step   1420 | loss 1.0476 | lr 3.00e-04 | grad 7.09 | tok/s 23760
step   1430 | loss 1.5891 | lr 3.00e-04 | grad 1.74 | tok/s 22921
step   1440 | loss 1.3565 | lr 3.00e-04 | grad 1.98 | tok/s 23516
step   1450 | loss 1.3413 | lr 3.00e-04 | grad 6.06 | tok/s 23488
step   1460 | loss 1.4857 | lr 3.00e-04 | grad 3.98 | tok/s 22866
step   1470 | loss 1.2944 | lr 3.00e-04 | grad 1.59 | tok/s 22327
step   1480 | loss 1.2827 | lr 3.00e-04 | grad 1.34 | tok/s 23529
step   1490 | loss 1.7737 | lr 3.00e-04 | grad 7.34 | tok/s 23170
step   1500 | loss 1.3784 | lr 3.00e-04 | grad 1.76 | tok/s 23221
step   1510 | loss 1.1810 | lr 3.00e-04 | grad 1.64 | tok/s 23269
step   1520 | loss 1.3943 | lr 3.00e-04 | grad 1.50 | tok/s 23064
step   1530 | loss 1.3175 | lr 3.00e-04 | grad 1.58 | tok/s 23438
step   1540 | loss 1.4421 | lr 3.00e-04 | grad 1.51 | tok/s 23641
step   1550 | loss 1.4009 | lr 3.00e-04 | grad 1.87 | tok/s 23164
step   1560 | loss 1.0591 | lr 3.00e-04 | grad 1.93 | tok/s 24062
step   1570 | loss 1.2192 | lr 3.00e-04 | grad 1.45 | tok/s 23388
step   1580 | loss 1.2119 | lr 3.00e-04 | grad 1.91 | tok/s 23308
step   1590 | loss 1.3754 | lr 3.00e-04 | grad 1.69 | tok/s 22796
step   1600 | loss 1.2300 | lr 3.00e-04 | grad 2.41 | tok/s 23690
step   1610 | loss 1.8482 | lr 3.00e-04 | grad 2.94 | tok/s 23460
step   1620 | loss 1.8911 | lr 3.00e-04 | grad 2.09 | tok/s 24043
step   1630 | loss 1.5984 | lr 3.00e-04 | grad 2.38 | tok/s 24022
step   1640 | loss 1.4275 | lr 3.00e-04 | grad 2.02 | tok/s 24024
step   1650 | loss 1.3278 | lr 3.00e-04 | grad 2.06 | tok/s 24013
step   1660 | loss 1.2852 | lr 3.00e-04 | grad 2.05 | tok/s 24016
step   1670 | loss 1.4387 | lr 3.00e-04 | grad 3.19 | tok/s 23301
step   1680 | loss 1.3445 | lr 3.00e-04 | grad 1.45 | tok/s 23078
step   1690 | loss 1.3933 | lr 3.00e-04 | grad 1.91 | tok/s 22400
step   1700 | loss 1.2441 | lr 3.00e-04 | grad 1.48 | tok/s 23540
step   1710 | loss 1.1742 | lr 3.00e-04 | grad 1.74 | tok/s 22331

Training complete! Final step: 1713
