Using device: cuda
Output directory: /home/erikg/elman/benchmark_results/learned_radii/E44_auto/level44_100m_20260113_204008
Auto r_h_mode: none (level 44 has bounded/no W_h)
Model: Level 44, 99,873,120 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.1258 | lr 4.90e-06 | grad 16.72 | tok/s 22829
step    100 | loss 3.9407 | lr 9.90e-06 | grad 3.58 | tok/s 29618
step    150 | loss 3.3694 | lr 1.49e-05 | grad 7.18 | tok/s 28364
step    200 | loss 3.1525 | lr 1.99e-05 | grad 1.78 | tok/s 28046
step    250 | loss 2.9417 | lr 2.49e-05 | grad 4.88 | tok/s 27119
step    300 | loss 2.6121 | lr 2.99e-05 | grad 4.28 | tok/s 26953
step    350 | loss 2.4797 | lr 3.49e-05 | grad 3.17 | tok/s 26748
step    400 | loss 2.1466 | lr 3.99e-05 | grad 1.87 | tok/s 27808
step    450 | loss 2.3664 | lr 4.49e-05 | grad 2.24 | tok/s 27387
step    500 | loss 2.1452 | lr 4.99e-05 | grad 2.07 | tok/s 27264
step    550 | loss 2.1266 | lr 5.49e-05 | grad 2.03 | tok/s 26489
step    600 | loss 1.7570 | lr 5.99e-05 | grad 1.90 | tok/s 28206
step    650 | loss 1.8185 | lr 6.49e-05 | grad 2.34 | tok/s 27573
step    700 | loss 2.0157 | lr 6.99e-05 | grad 2.04 | tok/s 26820
step    750 | loss 1.9588 | lr 7.49e-05 | grad 3.08 | tok/s 27117
step    800 | loss 1.9967 | lr 7.99e-05 | grad 3.79 | tok/s 27215
step    850 | loss 1.8963 | lr 8.49e-05 | grad 1.78 | tok/s 26543
step    900 | loss 1.9688 | lr 8.99e-05 | grad 1.55 | tok/s 26906
step    950 | loss 1.8220 | lr 9.49e-05 | grad 1.34 | tok/s 26775
step   1000 | loss 1.9897 | lr 9.99e-05 | grad 1.72 | tok/s 26872
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9897.pt
step   1050 | loss 1.9187 | lr 1.59e-06 | grad 5.14 | tok/s 22563
step   1100 | loss 1.9131 | lr 3.37e-06 | grad 1.07 | tok/s 27012
step   1150 | loss 1.8226 | lr 6.32e-06 | grad 3.07 | tok/s 27241
step   1200 | loss 1.9731 | lr 1.04e-05 | grad 2.30 | tok/s 26180
step   1250 | loss 1.7534 | lr 1.54e-05 | grad 1.04 | tok/s 26802
step   1300 | loss 1.7790 | lr 2.13e-05 | grad 1.63 | tok/s 26758
step   1350 | loss 1.8595 | lr 2.79e-05 | grad 1.33 | tok/s 26490
step   1400 | loss 1.8635 | lr 3.51e-05 | grad 2.84 | tok/s 26368
step   1450 | loss 1.8527 | lr 4.26e-05 | grad 2.22 | tok/s 26610
step   1500 | loss 1.7628 | lr 5.03e-05 | grad 1.65 | tok/s 26738
step   1550 | loss 1.8052 | lr 5.81e-05 | grad 1.28 | tok/s 26780
step   1600 | loss 1.6729 | lr 6.56e-05 | grad 1.36 | tok/s 27137
step   1650 | loss 2.0539 | lr 7.28e-05 | grad 1.42 | tok/s 27306
step   1700 | loss 1.7231 | lr 7.95e-05 | grad 1.32 | tok/s 27233
step   1750 | loss 1.7001 | lr 8.54e-05 | grad 1.76 | tok/s 26447
step   1800 | loss 1.8621 | lr 9.05e-05 | grad 1.42 | tok/s 26216
step   1850 | loss 1.7133 | lr 9.45e-05 | grad 1.48 | tok/s 26221
step   1900 | loss 1.7773 | lr 9.75e-05 | grad 1.32 | tok/s 26278
step   1950 | loss 1.3537 | lr 9.94e-05 | grad 0.83 | tok/s 27745
step   2000 | loss 1.8095 | lr 1.00e-04 | grad 1.05 | tok/s 26200
  >>> saved checkpoint: checkpoint_step_002000_loss_1.8095.pt

Training complete! Final step: 2035
