Using device: cuda
Output directory: benchmark_results/learned_radii/Mamba2/levelmamba2_100m_20260113_204629
Model: Level mamba2, 102,140,275 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     50 | loss 5.2192 | lr 4.90e-06 | grad 8.57 | tok/s 13823
step    100 | loss 4.2276 | lr 9.90e-06 | grad 3.94 | tok/s 24361
step    150 | loss 3.1664 | lr 1.49e-05 | grad 5.09 | tok/s 23036
step    200 | loss 2.7871 | lr 1.99e-05 | grad 1.91 | tok/s 22452
step    250 | loss 2.4841 | lr 2.49e-05 | grad 2.32 | tok/s 21624
step    300 | loss 2.0982 | lr 2.99e-05 | grad 3.33 | tok/s 21399
step    350 | loss 2.0484 | lr 3.49e-05 | grad 3.15 | tok/s 21110
step    400 | loss 1.7086 | lr 3.99e-05 | grad 2.02 | tok/s 21843
step    450 | loss 1.9989 | lr 4.49e-05 | grad 1.72 | tok/s 21382
step    500 | loss 1.8053 | lr 4.99e-05 | grad 1.98 | tok/s 21198
step    550 | loss 1.8410 | lr 5.49e-05 | grad 1.80 | tok/s 20599
step    600 | loss 1.4828 | lr 5.99e-05 | grad 1.80 | tok/s 21922
step    650 | loss 1.5925 | lr 6.49e-05 | grad 2.00 | tok/s 21371
step    700 | loss 1.7754 | lr 6.99e-05 | grad 2.04 | tok/s 20675
step    750 | loss 1.7481 | lr 7.49e-05 | grad 3.52 | tok/s 20904
step    800 | loss 1.7869 | lr 7.99e-05 | grad 3.91 | tok/s 21043
step    850 | loss 1.6617 | lr 8.49e-05 | grad 1.37 | tok/s 20418
step    900 | loss 1.7378 | lr 8.99e-05 | grad 1.24 | tok/s 20728
step    950 | loss 1.6085 | lr 9.49e-05 | grad 1.21 | tok/s 20601
step   1000 | loss 1.7644 | lr 9.99e-05 | grad 1.25 | tok/s 20614
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7644.pt
step   1050 | loss 1.7238 | lr 1.59e-06 | grad 5.21 | tok/s 18376
step   1100 | loss 1.7429 | lr 3.37e-06 | grad 1.04 | tok/s 20801
step   1150 | loss 1.6540 | lr 6.32e-06 | grad 2.37 | tok/s 20953
step   1200 | loss 1.8007 | lr 1.04e-05 | grad 2.21 | tok/s 20164
step   1250 | loss 1.6004 | lr 1.54e-05 | grad 1.05 | tok/s 20708
step   1300 | loss 1.6070 | lr 2.13e-05 | grad 1.49 | tok/s 20658
step   1350 | loss 1.6768 | lr 2.79e-05 | grad 1.23 | tok/s 20532
step   1400 | loss 1.6821 | lr 3.51e-05 | grad 3.23 | tok/s 20344
step   1450 | loss 1.6361 | lr 4.26e-05 | grad 1.70 | tok/s 20494
step   1500 | loss 1.5793 | lr 5.03e-05 | grad 1.52 | tok/s 20635
step   1550 | loss 1.6070 | lr 5.81e-05 | grad 1.16 | tok/s 20565

Training complete! Final step: 1576
