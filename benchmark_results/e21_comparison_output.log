E21 10-minute comparison: 600s per model
Batch size: 64, Seq len: 512
============================================================
Launching E1_d1280x6 on GPU 0...
Launching Mamba2_50M on GPU 1...
Launching minLSTM on GPU 2...
Launching E18A on GPU 3...
Launching E21 on GPU 4...
Launching E21S on GPU 5...
Launching E21T on GPU 6...
Launching E21L on GPU 7...

All 8 experiments launched! Running for 600s each...
Monitoring progress...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  0/8 completed...
  8/8 completed...

============================================================
FINAL RESULTS - 10 minutes training time
============================================================
FINAL E1_d1280x6: steps=3950, tokens=129.4M, loss=1.3700, tok/s=215.7K
FINAL Mamba2_50M: steps=2145, tokens=70.3M, loss=1.2891, tok/s=117.1K
FINAL minLSTM: steps=3104, tokens=101.7M, loss=1.4465, tok/s=169.5K
FINAL E18A: steps=4093, tokens=134.1M, loss=1.3759, tok/s=223.5K
FINAL E21: steps=49, tokens=1.6M, loss=2.8871, tok/s=2.7K
FINAL E21S: steps=79, tokens=2.6M, loss=2.7154, tok/s=4.3K
FINAL E21T: steps=49, tokens=1.6M, loss=2.7876, tok/s=2.7K
FINAL E21L: steps=50, tokens=1.6M, loss=2.7544, tok/s=2.7K

============================================================
SUMMARY TABLE
============================================================
Model               Loss      Tok/s       Params
--------------------------------------------------
E1_d1280x6        1.3700     215.7K   49,505,280
Mamba2_50M        1.2891     117.1K   50,928,750
minLSTM           1.4465     169.5K   29,836,800
E18A              1.3759     223.5K   49,505,280
E21               2.8871       2.7K   75,333,984
E21S              2.7154       4.3K   56,997,984
E21T              2.7876       2.7K   75,333,984
E21L              2.7544       2.7K   75,333,984
============================================================
