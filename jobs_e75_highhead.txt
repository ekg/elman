# E75 high-head configs (staying within shared memory limits)
# Target: maximize state while keeping n_state ≤ 48

# E75h32n48: 32×48×48 = 73,728 state/layer (39% of mamba2)
python train.py --level E75h32n48 --dim 1920 --expansion 2.0 --n_state 48 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_highhead/E75h32n48

# E75h64n32: 64×32×32 = 65,536 state/layer (35% of mamba2)
python train.py --level E75h64n32 --dim 2048 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_highhead/E75h64n32

# E75h64n48: 64×48×48 = 147,456 state/layer (78% of mamba2) ← BEST
python train.py --level E75h64n48 --dim 1792 --expansion 2.0 --n_state 48 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_highhead/E75h64n48

# Reference: mamba2 at 1B
python train.py --level mamba2 --dim 2944 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_highhead/mamba2
