Using device: cuda
Output directory: benchmark_results/mega_100m_20260122_155533/E88_h8n64/levelE88c_h8n64_100m_20260122_155742
Auto r_h_mode: none (level 0 has bounded/no W_h)
Traceback (most recent call last):
  File "/home/erikg/elman/train.py", line 584, in <module>
    train(args)
  File "/home/erikg/elman/train.py", line 359, in train
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.38 GiB of which 15.44 MiB is free. Process 3293830 has 11.64 GiB memory in use. Process 3294430 has 11.64 GiB memory in use. Process 3298587 has 11.64 GiB memory in use. Process 3299280 has 11.64 GiB memory in use. Including non-PyTorch memory, this process has 810.00 MiB memory in use. Of the allocated memory 357.44 MiB is allocated by PyTorch, and 28.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
