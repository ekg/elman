# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 896 --depth 20 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10.0 --output benchmark_results/e75_100m_20260119_175828/mamba2
# Started: 2026-01-19T17:58:36.704665
============================================================

Using device: cuda
Output directory: benchmark_results/e75_100m_20260119_175828/mamba2/levelmamba2_100m_20260119_175842
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4576 | lr 2.70e-05 | grad 5.00 | tok/s 9592
step     20 | loss 4.3084 | lr 5.70e-05 | grad 6.47 | tok/s 66182
step     30 | loss 4.5681 | lr 8.70e-05 | grad 3.27 | tok/s 69650
step     40 | loss 3.1655 | lr 1.17e-04 | grad 1.87 | tok/s 69389
step     50 | loss 2.3871 | lr 1.47e-04 | grad 1.48 | tok/s 69037
step     60 | loss 2.7603 | lr 1.77e-04 | grad 3.27 | tok/s 67389
step     70 | loss 2.4720 | lr 2.07e-04 | grad 1.88 | tok/s 64801
step     80 | loss 2.8039 | lr 2.37e-04 | grad 3.19 | tok/s 66985
step     90 | loss 2.7422 | lr 2.67e-04 | grad 4.06 | tok/s 64333
step    100 | loss 2.2439 | lr 2.97e-04 | grad 1.20 | tok/s 64703
step    110 | loss 2.2788 | lr 6.94e-06 | grad 1.96 | tok/s 61485
step    120 | loss 2.4864 | lr 2.69e-05 | grad 1.52 | tok/s 61539
step    130 | loss 2.3552 | lr 5.89e-05 | grad 1.16 | tok/s 62641
step    140 | loss 2.0977 | lr 9.99e-05 | grad 1.00 | tok/s 62542
step    150 | loss 1.9593 | lr 1.46e-04 | grad 1.89 | tok/s 59467
step    160 | loss 1.8687 | lr 1.92e-04 | grad 1.62 | tok/s 59760
step    170 | loss 2.0567 | lr 2.35e-04 | grad 3.33 | tok/s 61634
step    180 | loss 2.0397 | lr 2.69e-04 | grad 1.27 | tok/s 61635
step    190 | loss 1.7564 | lr 2.91e-04 | grad 1.27 | tok/s 62399
step    200 | loss 1.3557 | lr 3.00e-04 | grad 1.05 | tok/s 63317
step    210 | loss 2.1281 | lr 2.94e-04 | grad 1.71 | tok/s 60629
step    220 | loss 1.9227 | lr 2.74e-04 | grad 1.29 | tok/s 62364
step    230 | loss 1.8169 | lr 2.42e-04 | grad 1.48 | tok/s 59803
step    240 | loss 1.7887 | lr 2.01e-04 | grad 1.52 | tok/s 61071
step    250 | loss 1.7934 | lr 1.55e-04 | grad 1.30 | tok/s 60181
step    260 | loss 1.8788 | lr 1.09e-04 | grad 0.85 | tok/s 57745
step    270 | loss 1.7246 | lr 6.65e-05 | grad 0.95 | tok/s 59651
step    280 | loss 1.6076 | lr 3.24e-05 | grad 1.73 | tok/s 59501
step    290 | loss 1.6131 | lr 9.84e-06 | grad 0.76 | tok/s 62460
step    300 | loss 1.5958 | lr 1.07e-06 | grad 0.86 | tok/s 62115
step    310 | loss 1.5799 | lr 6.94e-06 | grad 0.68 | tok/s 62329
step    320 | loss 1.6803 | lr 2.69e-05 | grad 1.67 | tok/s 59799
step    330 | loss 1.7104 | lr 5.89e-05 | grad 0.79 | tok/s 58321
step    340 | loss 1.7104 | lr 9.99e-05 | grad 2.16 | tok/s 59451
step    350 | loss 1.7265 | lr 1.46e-04 | grad 0.92 | tok/s 57617
step    360 | loss 1.6734 | lr 1.92e-04 | grad 2.34 | tok/s 58276
step    370 | loss 1.5130 | lr 2.35e-04 | grad 1.00 | tok/s 59294
step    380 | loss 1.9881 | lr 2.69e-04 | grad 1.07 | tok/s 60930
step    390 | loss 1.7114 | lr 2.91e-04 | grad 1.45 | tok/s 58544
step    400 | loss 1.8062 | lr 3.00e-04 | grad 2.44 | tok/s 60012
step    410 | loss 1.5786 | lr 2.94e-04 | grad 2.33 | tok/s 58244
step    420 | loss 1.7147 | lr 2.74e-04 | grad 1.22 | tok/s 57712
step    430 | loss 1.8063 | lr 2.42e-04 | grad 1.23 | tok/s 57448
step    440 | loss 1.8216 | lr 2.01e-04 | grad 0.89 | tok/s 59686
step    450 | loss 1.6807 | lr 1.55e-04 | grad 0.77 | tok/s 58109
step    460 | loss 1.6676 | lr 1.09e-04 | grad 0.67 | tok/s 58264
step    470 | loss 1.6157 | lr 6.65e-05 | grad 1.02 | tok/s 58901
step    480 | loss 1.5195 | lr 3.24e-05 | grad 0.61 | tok/s 56787
step    490 | loss 1.5151 | lr 9.84e-06 | grad 0.64 | tok/s 57763
step    500 | loss 2.4362 | lr 1.07e-06 | grad 0.97 | tok/s 59625
step    510 | loss 1.5563 | lr 6.94e-06 | grad 0.70 | tok/s 58209
step    520 | loss 1.6087 | lr 2.69e-05 | grad 0.62 | tok/s 60057
step    530 | loss 2.1002 | lr 5.89e-05 | grad 0.67 | tok/s 58666
step    540 | loss 1.4955 | lr 9.99e-05 | grad 1.06 | tok/s 58534
step    550 | loss 1.4277 | lr 1.46e-04 | grad 0.59 | tok/s 60182
step    560 | loss 1.3017 | lr 1.92e-04 | grad 0.66 | tok/s 61087
step    570 | loss 1.5541 | lr 2.35e-04 | grad 1.98 | tok/s 59606
step    580 | loss 1.8484 | lr 2.69e-04 | grad 0.88 | tok/s 58820
step    590 | loss 2.1055 | lr 2.91e-04 | grad 1.01 | tok/s 57699
step    600 | loss 1.6771 | lr 3.00e-04 | grad 1.40 | tok/s 57773
step    610 | loss 1.6291 | lr 2.94e-04 | grad 1.26 | tok/s 60608
step    620 | loss 1.6193 | lr 2.74e-04 | grad 0.89 | tok/s 57119
step    630 | loss 1.5242 | lr 2.42e-04 | grad 0.84 | tok/s 59144
step    640 | loss 1.7530 | lr 2.01e-04 | grad 0.96 | tok/s 59263
step    650 | loss 1.5442 | lr 1.55e-04 | grad 0.89 | tok/s 58096
step    660 | loss 1.8185 | lr 1.09e-04 | grad 5.69 | tok/s 57395
step    670 | loss 1.6860 | lr 6.65e-05 | grad 1.87 | tok/s 59403
step    680 | loss 1.6206 | lr 3.24e-05 | grad 0.98 | tok/s 57141
step    690 | loss 1.6438 | lr 9.84e-06 | grad 1.49 | tok/s 57523
step    700 | loss 1.7597 | lr 1.07e-06 | grad 1.62 | tok/s 57983
step    710 | loss 1.6786 | lr 6.94e-06 | grad 1.13 | tok/s 58444
step    720 | loss 1.7474 | lr 2.68e-05 | grad 1.59 | tok/s 58151
step    730 | loss 1.7251 | lr 5.89e-05 | grad 1.20 | tok/s 58802
step    740 | loss 1.6568 | lr 9.99e-05 | grad 1.66 | tok/s 58267
step    750 | loss 1.4784 | lr 1.46e-04 | grad 1.25 | tok/s 57568
step    760 | loss 1.7380 | lr 1.92e-04 | grad 0.67 | tok/s 58426
step    770 | loss 1.5300 | lr 2.35e-04 | grad 0.97 | tok/s 57990
step    780 | loss 1.5764 | lr 2.69e-04 | grad 0.79 | tok/s 58637
step    790 | loss 1.4855 | lr 2.91e-04 | grad 0.56 | tok/s 58957
step    800 | loss 1.4801 | lr 3.00e-04 | grad 0.93 | tok/s 59059
step    810 | loss 1.5828 | lr 2.94e-04 | grad 1.75 | tok/s 58491
step    820 | loss 2.3069 | lr 2.74e-04 | grad 1.40 | tok/s 60033
step    830 | loss 1.7664 | lr 2.42e-04 | grad 0.66 | tok/s 60851
step    840 | loss 1.4284 | lr 2.01e-04 | grad 0.65 | tok/s 61045
step    850 | loss 1.8549 | lr 1.55e-04 | grad 1.09 | tok/s 57929
step    860 | loss 1.6220 | lr 1.09e-04 | grad 0.89 | tok/s 56842
step    870 | loss 1.5403 | lr 6.65e-05 | grad 0.89 | tok/s 58545
step    880 | loss 1.6022 | lr 3.24e-05 | grad 1.11 | tok/s 58400
step    890 | loss 1.5469 | lr 9.84e-06 | grad 0.93 | tok/s 58186
step    900 | loss 1.9449 | lr 1.07e-06 | grad 0.90 | tok/s 56812
step    910 | loss 1.5618 | lr 6.94e-06 | grad 0.75 | tok/s 57838
step    920 | loss 1.5787 | lr 2.68e-05 | grad 0.77 | tok/s 57490
step    930 | loss 1.6658 | lr 5.89e-05 | grad 1.49 | tok/s 57489
step    940 | loss 1.5838 | lr 9.99e-05 | grad 1.63 | tok/s 56988
step    950 | loss 1.6254 | lr 1.46e-04 | grad 1.02 | tok/s 58225
step    960 | loss 1.4071 | lr 1.92e-04 | grad 0.63 | tok/s 61187
step    970 | loss 1.2511 | lr 2.35e-04 | grad 0.45 | tok/s 61162
step    980 | loss 1.3994 | lr 2.69e-04 | grad 2.67 | tok/s 59523
step    990 | loss 1.6720 | lr 2.91e-04 | grad 0.71 | tok/s 57866
step   1000 | loss 1.5888 | lr 3.00e-04 | grad 0.62 | tok/s 56524
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5888.pt
step   1010 | loss 1.7136 | lr 2.94e-04 | grad 0.93 | tok/s 46387
step   1020 | loss 1.4145 | lr 2.74e-04 | grad 0.82 | tok/s 58171
step   1030 | loss 1.8876 | lr 2.42e-04 | grad 0.83 | tok/s 57153
step   1040 | loss 1.5014 | lr 2.01e-04 | grad 1.09 | tok/s 58436
step   1050 | loss 1.5157 | lr 1.55e-04 | grad 0.66 | tok/s 58696
step   1060 | loss 1.6237 | lr 1.09e-04 | grad 1.36 | tok/s 58618
step   1070 | loss 1.7626 | lr 6.65e-05 | grad 0.84 | tok/s 58864
step   1080 | loss 2.1859 | lr 3.24e-05 | grad 0.99 | tok/s 58261
step   1090 | loss 1.8855 | lr 9.84e-06 | grad 0.86 | tok/s 58623
step   1100 | loss 1.5488 | lr 1.07e-06 | grad 0.65 | tok/s 58321
step   1110 | loss 1.4936 | lr 6.93e-06 | grad 0.80 | tok/s 59187
step   1120 | loss 1.7171 | lr 2.68e-05 | grad 0.78 | tok/s 59936
step   1130 | loss 1.5759 | lr 5.89e-05 | grad 0.68 | tok/s 56979
step   1140 | loss 1.4458 | lr 9.99e-05 | grad 0.58 | tok/s 58524
step   1150 | loss 1.6887 | lr 1.46e-04 | grad 1.05 | tok/s 58314
step   1160 | loss 1.3979 | lr 1.92e-04 | grad 0.55 | tok/s 57641
step   1170 | loss 1.7122 | lr 2.35e-04 | grad 0.80 | tok/s 58379
step   1180 | loss 1.4568 | lr 2.69e-04 | grad 0.67 | tok/s 61270
step   1190 | loss 1.3060 | lr 2.91e-04 | grad 0.55 | tok/s 61519
step   1200 | loss 1.2292 | lr 3.00e-04 | grad 0.55 | tok/s 61271
step   1210 | loss 1.2075 | lr 2.94e-04 | grad 0.64 | tok/s 61397
step   1220 | loss 1.2540 | lr 2.74e-04 | grad 0.86 | tok/s 60779
step   1230 | loss 1.4447 | lr 2.42e-04 | grad 0.71 | tok/s 58759
step   1240 | loss 1.5054 | lr 2.01e-04 | grad 0.70 | tok/s 57475
step   1250 | loss 1.5996 | lr 1.55e-04 | grad 3.06 | tok/s 59395
step   1260 | loss 1.6213 | lr 1.09e-04 | grad 2.31 | tok/s 59371
step   1270 | loss 1.6905 | lr 6.65e-05 | grad 1.22 | tok/s 58690
step   1280 | loss 1.5377 | lr 3.24e-05 | grad 0.79 | tok/s 58046
step   1290 | loss 1.4954 | lr 9.84e-06 | grad 0.71 | tok/s 57668
step   1300 | loss 1.5401 | lr 1.07e-06 | grad 0.69 | tok/s 57260
step   1310 | loss 1.6131 | lr 6.93e-06 | grad 0.68 | tok/s 57292
step   1320 | loss 1.6018 | lr 2.68e-05 | grad 1.09 | tok/s 58322
step   1330 | loss 1.5118 | lr 5.89e-05 | grad 0.54 | tok/s 58357
step   1340 | loss 1.4233 | lr 9.99e-05 | grad 0.83 | tok/s 58526
step   1350 | loss 1.4362 | lr 1.46e-04 | grad 1.34 | tok/s 59919
step   1360 | loss 1.4258 | lr 1.92e-04 | grad 0.66 | tok/s 56949
step   1370 | loss 1.5153 | lr 2.35e-04 | grad 0.67 | tok/s 57775
step   1380 | loss 1.5830 | lr 2.69e-04 | grad 0.76 | tok/s 58302
step   1390 | loss 1.5172 | lr 2.91e-04 | grad 1.26 | tok/s 56822
step   1400 | loss 1.5327 | lr 3.00e-04 | grad 7.84 | tok/s 58978
step   1410 | loss 1.5203 | lr 2.94e-04 | grad 0.91 | tok/s 59248
step   1420 | loss 1.5963 | lr 2.74e-04 | grad 0.73 | tok/s 56765
step   1430 | loss 1.4237 | lr 2.42e-04 | grad 0.67 | tok/s 55352
step   1440 | loss 1.3519 | lr 2.01e-04 | grad 0.56 | tok/s 58511
step   1450 | loss 1.3535 | lr 1.55e-04 | grad 1.48 | tok/s 59689
step   1460 | loss 1.4690 | lr 1.09e-04 | grad 0.53 | tok/s 55732
step   1470 | loss 1.5593 | lr 6.65e-05 | grad 2.03 | tok/s 57993
step   1480 | loss 1.4316 | lr 3.24e-05 | grad 1.54 | tok/s 58588
step   1490 | loss 1.5719 | lr 9.84e-06 | grad 1.99 | tok/s 58422
step   1500 | loss 1.6738 | lr 1.07e-06 | grad 1.98 | tok/s 56868
step   1510 | loss 1.5583 | lr 6.93e-06 | grad 0.99 | tok/s 59815
step   1520 | loss 1.5080 | lr 2.68e-05 | grad 1.09 | tok/s 59248
step   1530 | loss 1.4816 | lr 5.89e-05 | grad 0.52 | tok/s 58869
step   1540 | loss 1.4485 | lr 9.99e-05 | grad 0.48 | tok/s 57718
step   1550 | loss 1.4299 | lr 1.46e-04 | grad 2.41 | tok/s 59983
step   1560 | loss 1.8814 | lr 1.92e-04 | grad 1.04 | tok/s 58503
step   1570 | loss 1.4631 | lr 2.35e-04 | grad 0.94 | tok/s 57491
step   1580 | loss 1.5956 | lr 2.69e-04 | grad 1.00 | tok/s 59285
step   1590 | loss 1.4219 | lr 2.91e-04 | grad 0.59 | tok/s 58021
step   1600 | loss 1.5492 | lr 3.00e-04 | grad 0.71 | tok/s 56797
step   1610 | loss 1.3541 | lr 2.94e-04 | grad 0.68 | tok/s 60313
step   1620 | loss 1.5282 | lr 2.74e-04 | grad 0.54 | tok/s 59216
step   1630 | loss 1.4877 | lr 2.42e-04 | grad 0.70 | tok/s 59526
step   1640 | loss 1.4426 | lr 2.01e-04 | grad 0.61 | tok/s 57598
step   1650 | loss 1.4636 | lr 1.55e-04 | grad 0.86 | tok/s 56833
step   1660 | loss 1.4588 | lr 1.09e-04 | grad 0.55 | tok/s 57231
step   1670 | loss 1.4895 | lr 6.65e-05 | grad 1.78 | tok/s 59561
step   1680 | loss 1.9136 | lr 3.24e-05 | grad 0.53 | tok/s 59642
step   1690 | loss 1.4412 | lr 9.84e-06 | grad 0.80 | tok/s 58125
step   1700 | loss 1.6820 | lr 1.07e-06 | grad 0.49 | tok/s 59417
step   1710 | loss 1.4884 | lr 6.93e-06 | grad 0.85 | tok/s 57713
step   1720 | loss 1.4473 | lr 2.68e-05 | grad 0.63 | tok/s 58014
step   1730 | loss 1.5720 | lr 5.89e-05 | grad 0.78 | tok/s 58074
step   1740 | loss 1.4848 | lr 9.99e-05 | grad 0.58 | tok/s 58959
step   1750 | loss 1.4128 | lr 1.46e-04 | grad 0.50 | tok/s 56935
step   1760 | loss 1.5877 | lr 1.92e-04 | grad 0.65 | tok/s 57663
step   1770 | loss 1.5602 | lr 2.35e-04 | grad 0.51 | tok/s 59101
step   1780 | loss 1.4490 | lr 2.69e-04 | grad 0.93 | tok/s 56656
step   1790 | loss 1.6053 | lr 2.91e-04 | grad 0.71 | tok/s 57830
step   1800 | loss 1.4007 | lr 3.00e-04 | grad 0.61 | tok/s 58754
step   1810 | loss 1.4974 | lr 2.94e-04 | grad 0.80 | tok/s 58589
step   1820 | loss 1.4067 | lr 2.74e-04 | grad 0.59 | tok/s 58017
step   1830 | loss 1.4479 | lr 2.42e-04 | grad 0.63 | tok/s 57768
step   1840 | loss 1.4499 | lr 2.01e-04 | grad 0.74 | tok/s 57292
step   1850 | loss 1.6478 | lr 1.55e-04 | grad 0.91 | tok/s 57738
step   1860 | loss 1.4221 | lr 1.09e-04 | grad 0.49 | tok/s 57601
step   1870 | loss 1.4485 | lr 6.65e-05 | grad 1.09 | tok/s 58959
step   1880 | loss 1.4322 | lr 3.24e-05 | grad 0.55 | tok/s 59094
step   1890 | loss 1.4991 | lr 9.84e-06 | grad 0.52 | tok/s 57927
step   1900 | loss 1.4758 | lr 1.07e-06 | grad 0.68 | tok/s 58540
step   1910 | loss 1.5348 | lr 6.93e-06 | grad 1.23 | tok/s 57960
step   1920 | loss 1.4223 | lr 2.68e-05 | grad 0.80 | tok/s 59583
step   1930 | loss 1.4037 | lr 5.89e-05 | grad 0.85 | tok/s 59052
step   1940 | loss 1.3896 | lr 9.99e-05 | grad 0.59 | tok/s 60218
step   1950 | loss 1.4315 | lr 1.46e-04 | grad 0.71 | tok/s 58844
step   1960 | loss 1.7120 | lr 1.92e-04 | grad 3.36 | tok/s 59929
step   1970 | loss 1.3786 | lr 2.35e-04 | grad 1.00 | tok/s 57845
step   1980 | loss 1.4949 | lr 2.69e-04 | grad 1.93 | tok/s 57840
step   1990 | loss 1.5328 | lr 2.91e-04 | grad 0.95 | tok/s 59113
step   2000 | loss 1.4407 | lr 3.00e-04 | grad 0.90 | tok/s 59664
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4407.pt
step   2010 | loss 1.2805 | lr 2.94e-04 | grad 0.58 | tok/s 46852
step   2020 | loss 1.1531 | lr 2.74e-04 | grad 0.48 | tok/s 61540
step   2030 | loss 1.4071 | lr 2.42e-04 | grad 0.71 | tok/s 60856
step   2040 | loss 1.2939 | lr 2.01e-04 | grad 0.48 | tok/s 61536
step   2050 | loss 1.2382 | lr 1.55e-04 | grad 0.75 | tok/s 60584
step   2060 | loss 1.5587 | lr 1.09e-04 | grad 0.59 | tok/s 57906
step   2070 | loss 1.5168 | lr 6.65e-05 | grad 1.06 | tok/s 60788
step   2080 | loss 1.5453 | lr 3.24e-05 | grad 2.19 | tok/s 57501
step   2090 | loss 1.5216 | lr 9.84e-06 | grad 0.75 | tok/s 59415
step   2100 | loss 1.4764 | lr 1.07e-06 | grad 0.81 | tok/s 57816
step   2110 | loss 1.3545 | lr 6.93e-06 | grad 0.58 | tok/s 59780
step   2120 | loss 1.3408 | lr 2.68e-05 | grad 1.00 | tok/s 59201
step   2130 | loss 1.4642 | lr 5.89e-05 | grad 1.73 | tok/s 57481
step   2140 | loss 1.5178 | lr 9.99e-05 | grad 2.34 | tok/s 57510
step   2150 | loss 1.5087 | lr 1.46e-04 | grad 0.47 | tok/s 58243
step   2160 | loss 1.4787 | lr 1.92e-04 | grad 0.54 | tok/s 57662
step   2170 | loss 1.5427 | lr 2.35e-04 | grad 0.68 | tok/s 58321
step   2180 | loss 1.4300 | lr 2.69e-04 | grad 0.69 | tok/s 59532
step   2190 | loss 1.6873 | lr 2.91e-04 | grad 0.66 | tok/s 59273
step   2200 | loss 1.2253 | lr 3.00e-04 | grad 0.45 | tok/s 61438

Training complete! Final step: 2204
