Using device: cuda
Output directory: benchmark_results/mega_500m_20260122_161201/fla-gdn/levelfla-gdn_100m_20260122_161216
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 533,694,928 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.1778 | lr 3.00e-04 | grad 16.12 | tok/s 2853
step     20 | loss 3.2807 | lr 3.00e-04 | grad 29.75 | tok/s 21795
step     30 | loss 4.7341 | lr 3.00e-04 | grad 15.00 | tok/s 22846
step     40 | loss 3.4014 | lr 3.00e-04 | grad 10.19 | tok/s 22622
step     50 | loss 2.7162 | lr 3.00e-04 | grad 7.75 | tok/s 22449
step     60 | loss 2.8982 | lr 3.00e-04 | grad 4.00 | tok/s 21854
step     70 | loss 2.3956 | lr 3.00e-04 | grad 6.84 | tok/s 21025
step     80 | loss 2.4115 | lr 3.00e-04 | grad 4.56 | tok/s 21780
step     90 | loss 2.4453 | lr 3.00e-04 | grad 8.50 | tok/s 20957
step    100 | loss 2.0429 | lr 3.00e-04 | grad 3.39 | tok/s 21065
step    110 | loss 2.0257 | lr 3.00e-04 | grad 3.33 | tok/s 20738
step    120 | loss 2.0972 | lr 3.00e-04 | grad 2.03 | tok/s 20445
step    130 | loss 2.0439 | lr 3.00e-04 | grad 2.33 | tok/s 20854
step    140 | loss 1.8744 | lr 3.00e-04 | grad 2.69 | tok/s 20887
step    150 | loss 1.7462 | lr 3.00e-04 | grad 3.14 | tok/s 19922
step    160 | loss 1.7186 | lr 3.00e-04 | grad 1.90 | tok/s 20066
step    170 | loss 1.8676 | lr 3.00e-04 | grad 7.22 | tok/s 20743
step    180 | loss 1.8210 | lr 3.00e-04 | grad 1.33 | tok/s 20795
step    190 | loss 1.5375 | lr 3.00e-04 | grad 1.77 | tok/s 21149
step    200 | loss 1.1868 | lr 3.00e-04 | grad 1.84 | tok/s 21593
step    210 | loss 1.8078 | lr 3.00e-04 | grad 2.44 | tok/s 20673
step    220 | loss 1.6229 | lr 3.00e-04 | grad 1.80 | tok/s 21355
step    230 | loss 1.6344 | lr 3.00e-04 | grad 3.02 | tok/s 20642
step    240 | loss 1.6181 | lr 3.00e-04 | grad 2.53 | tok/s 21057
step    250 | loss 1.6070 | lr 3.00e-04 | grad 1.93 | tok/s 20764
step    260 | loss 1.6967 | lr 3.00e-04 | grad 1.70 | tok/s 19967
step    270 | loss 1.6089 | lr 3.00e-04 | grad 1.55 | tok/s 20683
step    280 | loss 1.4834 | lr 3.00e-04 | grad 2.89 | tok/s 20692
step    290 | loss 1.3849 | lr 3.00e-04 | grad 1.79 | tok/s 21743
step    300 | loss 1.3298 | lr 3.00e-04 | grad 1.51 | tok/s 21733
step    310 | loss 1.2841 | lr 3.00e-04 | grad 1.78 | tok/s 21733
step    320 | loss 1.4645 | lr 3.00e-04 | grad 2.69 | tok/s 20966
step    330 | loss 1.5815 | lr 3.00e-04 | grad 1.49 | tok/s 20455
step    340 | loss 1.5878 | lr 3.00e-04 | grad 3.66 | tok/s 20859
step    350 | loss 1.5882 | lr 3.00e-04 | grad 1.67 | tok/s 20297
step    360 | loss 1.5498 | lr 3.00e-04 | grad 4.56 | tok/s 20542
step    370 | loss 1.3611 | lr 3.00e-04 | grad 1.39 | tok/s 20917
step    380 | loss 1.7192 | lr 3.00e-04 | grad 1.59 | tok/s 21450
step    390 | loss 1.4980 | lr 3.00e-04 | grad 1.28 | tok/s 20703
step    400 | loss 1.5628 | lr 3.00e-04 | grad 3.86 | tok/s 21227
step    410 | loss 1.3386 | lr 3.00e-04 | grad 3.42 | tok/s 20627
step    420 | loss 1.4547 | lr 3.00e-04 | grad 1.23 | tok/s 20422
step    430 | loss 1.5582 | lr 3.00e-04 | grad 2.02 | tok/s 20368
step    440 | loss 1.5114 | lr 3.00e-04 | grad 1.09 | tok/s 21155
step    450 | loss 1.5118 | lr 3.00e-04 | grad 1.57 | tok/s 20559
step    460 | loss 1.4900 | lr 3.00e-04 | grad 1.26 | tok/s 20649
step    470 | loss 1.4505 | lr 3.00e-04 | grad 1.95 | tok/s 20881
step    480 | loss 1.3945 | lr 3.00e-04 | grad 1.38 | tok/s 20167
step    490 | loss 1.4071 | lr 3.00e-04 | grad 1.16 | tok/s 20507
step    500 | loss 1.8241 | lr 3.00e-04 | grad 1.91 | tok/s 21161
step    510 | loss 1.3730 | lr 3.00e-04 | grad 1.10 | tok/s 20713
step    520 | loss 1.3069 | lr 3.00e-04 | grad 1.56 | tok/s 21384
step    530 | loss 1.8657 | lr 3.00e-04 | grad 1.32 | tok/s 20895
step    540 | loss 1.2974 | lr 3.00e-04 | grad 2.31 | tok/s 20937
step    550 | loss 1.2947 | lr 3.00e-04 | grad 1.56 | tok/s 21448
step    560 | loss 1.1989 | lr 3.00e-04 | grad 1.26 | tok/s 21731
step    570 | loss 1.3859 | lr 3.00e-04 | grad 2.83 | tok/s 21237
step    580 | loss 1.6025 | lr 3.00e-04 | grad 1.52 | tok/s 20972
step    590 | loss 1.7670 | lr 3.00e-04 | grad 1.30 | tok/s 20561
step    600 | loss 1.4052 | lr 3.00e-04 | grad 2.31 | tok/s 20618
step    610 | loss 1.3293 | lr 3.00e-04 | grad 1.72 | tok/s 21614
step    620 | loss 1.3737 | lr 3.00e-04 | grad 1.32 | tok/s 20510
step    630 | loss 1.3077 | lr 3.00e-04 | grad 1.36 | tok/s 21162
step    640 | loss 1.4612 | lr 3.00e-04 | grad 1.47 | tok/s 21137
step    650 | loss 1.3502 | lr 3.00e-04 | grad 1.52 | tok/s 20763
step    660 | loss 1.5371 | lr 3.00e-04 | grad 5.25 | tok/s 20488
step    670 | loss 1.5016 | lr 3.00e-04 | grad 2.08 | tok/s 21195
step    680 | loss 1.4323 | lr 3.00e-04 | grad 1.76 | tok/s 20449
step    690 | loss 1.4438 | lr 3.00e-04 | grad 1.81 | tok/s 20611
step    700 | loss 1.5447 | lr 3.00e-04 | grad 2.42 | tok/s 20721
step    710 | loss 1.3985 | lr 3.00e-04 | grad 1.65 | tok/s 20833
step    720 | loss 1.3186 | lr 3.00e-04 | grad 6.16 | tok/s 20699
step    730 | loss 1.5262 | lr 3.00e-04 | grad 1.73 | tok/s 20943

Training complete! Final step: 734
