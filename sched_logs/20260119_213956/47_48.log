# Job 47: 48
# GPU: 4
# Command: python train.py --level 48 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/48
# Started: 2026-01-19T22:31:13.565634
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/48/level48_100m_20260119_223119
Auto r_h_mode: none (level 48 has bounded/no W_h)
Model: Level 48, 8,382,080 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6231 | lr 2.70e-05 | grad 14.31 | tok/s 18448
step     20 | loss 5.2896 | lr 5.70e-05 | grad 8.44 | tok/s 49150
step     30 | loss 5.3496 | lr 8.70e-05 | grad 33.75 | tok/s 51530
step     40 | loss 5.2176 | lr 1.17e-04 | grad 32.25 | tok/s 52411
step     50 | loss 4.8744 | lr 1.47e-04 | grad 7.03 | tok/s 52233
step     60 | loss 4.7048 | lr 1.77e-04 | grad 11.25 | tok/s 50594
step     70 | loss 4.0905 | lr 2.07e-04 | grad 3.88 | tok/s 48942
step     80 | loss 3.8614 | lr 2.37e-04 | grad 3.80 | tok/s 50952
step     90 | loss 3.5511 | lr 2.67e-04 | grad 3.73 | tok/s 49060
step    100 | loss 3.3010 | lr 2.97e-04 | grad 1.04 | tok/s 49597
step    110 | loss 3.2101 | lr 6.94e-06 | grad 1.62 | tok/s 47849
step    120 | loss 3.5941 | lr 2.69e-05 | grad 1.16 | tok/s 47508
step    130 | loss 3.3764 | lr 5.89e-05 | grad 1.01 | tok/s 48295
step    140 | loss 3.1645 | lr 9.99e-05 | grad 0.93 | tok/s 48710
step    150 | loss 3.0560 | lr 1.46e-04 | grad 1.91 | tok/s 46784
step    160 | loss 2.9548 | lr 1.92e-04 | grad 1.41 | tok/s 47327
step    170 | loss 3.0481 | lr 2.35e-04 | grad 3.44 | tok/s 48774
step    180 | loss 3.0412 | lr 2.69e-04 | grad 1.00 | tok/s 47851
step    190 | loss 2.7023 | lr 2.91e-04 | grad 1.04 | tok/s 49759
step    200 | loss 2.4891 | lr 3.00e-04 | grad 0.71 | tok/s 50582
step    210 | loss 2.6640 | lr 2.94e-04 | grad 1.70 | tok/s 48209
step    220 | loss 2.6044 | lr 2.74e-04 | grad 1.39 | tok/s 50359
step    230 | loss 2.3826 | lr 2.42e-04 | grad 1.26 | tok/s 48502
step    240 | loss 2.4588 | lr 2.01e-04 | grad 1.51 | tok/s 48637
step    250 | loss 2.3701 | lr 1.55e-04 | grad 1.27 | tok/s 49101
step    260 | loss 2.3795 | lr 1.09e-04 | grad 0.63 | tok/s 47034
step    270 | loss 2.2760 | lr 6.65e-05 | grad 0.93 | tok/s 48556
step    280 | loss 2.1898 | lr 3.24e-05 | grad 1.49 | tok/s 48928
step    290 | loss 2.2214 | lr 9.84e-06 | grad 0.92 | tok/s 51567
step    300 | loss 2.1990 | lr 1.07e-06 | grad 0.76 | tok/s 51271
step    310 | loss 2.2075 | lr 6.94e-06 | grad 0.77 | tok/s 51118
step    320 | loss 2.2535 | lr 2.69e-05 | grad 1.71 | tok/s 49219
step    330 | loss 2.3113 | lr 5.89e-05 | grad 0.62 | tok/s 47903
step    340 | loss 2.3229 | lr 9.99e-05 | grad 1.54 | tok/s 49375
step    350 | loss 2.3381 | lr 1.46e-04 | grad 1.05 | tok/s 48361
step    360 | loss 2.2853 | lr 1.92e-04 | grad 2.11 | tok/s 48866
step    370 | loss 2.1654 | lr 2.35e-04 | grad 1.13 | tok/s 49798
step    380 | loss 2.6276 | lr 2.69e-04 | grad 1.17 | tok/s 51428
step    390 | loss 2.1854 | lr 2.91e-04 | grad 1.20 | tok/s 49222
step    400 | loss 2.2834 | lr 3.00e-04 | grad 2.64 | tok/s 50248
step    410 | loss 2.0250 | lr 2.94e-04 | grad 1.84 | tok/s 49342
step    420 | loss 2.1543 | lr 2.74e-04 | grad 1.12 | tok/s 48778
step    430 | loss 2.3078 | lr 2.42e-04 | grad 1.12 | tok/s 48604
step    440 | loss 2.4019 | lr 2.01e-04 | grad 0.97 | tok/s 50817
step    450 | loss 2.0652 | lr 1.55e-04 | grad 0.72 | tok/s 49452
step    460 | loss 2.0911 | lr 1.09e-04 | grad 0.77 | tok/s 49031
step    470 | loss 2.0785 | lr 6.65e-05 | grad 1.25 | tok/s 50169
step    480 | loss 1.9866 | lr 3.24e-05 | grad 0.66 | tok/s 47770
step    490 | loss 1.9481 | lr 9.84e-06 | grad 0.64 | tok/s 49133
step    500 | loss 2.9147 | lr 1.07e-06 | grad 0.95 | tok/s 50729
step    510 | loss 2.0606 | lr 6.94e-06 | grad 0.64 | tok/s 49327
step    520 | loss 2.0233 | lr 2.69e-05 | grad 0.57 | tok/s 50854
step    530 | loss 2.5202 | lr 5.89e-05 | grad 0.58 | tok/s 50131
step    540 | loss 1.9828 | lr 9.99e-05 | grad 0.91 | tok/s 49964
step    550 | loss 1.9074 | lr 1.46e-04 | grad 0.89 | tok/s 51475
step    560 | loss 1.8043 | lr 1.92e-04 | grad 0.72 | tok/s 51744
step    570 | loss 2.0548 | lr 2.35e-04 | grad 1.99 | tok/s 50913
step    580 | loss 2.3369 | lr 2.69e-04 | grad 0.95 | tok/s 49858
step    590 | loss 2.5832 | lr 2.91e-04 | grad 1.72 | tok/s 48753
step    600 | loss 2.1196 | lr 3.00e-04 | grad 1.38 | tok/s 49533
step    610 | loss 2.1230 | lr 2.94e-04 | grad 1.16 | tok/s 52063
step    620 | loss 1.9711 | lr 2.74e-04 | grad 0.98 | tok/s 49103
step    630 | loss 1.9451 | lr 2.42e-04 | grad 0.89 | tok/s 50883
step    640 | loss 2.2900 | lr 2.01e-04 | grad 1.35 | tok/s 50913
step    650 | loss 1.9333 | lr 1.55e-04 | grad 1.34 | tok/s 49425
step    660 | loss 2.2822 | lr 1.09e-04 | grad 5.25 | tok/s 49117
step    670 | loss 2.0784 | lr 6.65e-05 | grad 1.70 | tok/s 50678
step    680 | loss 2.0170 | lr 3.24e-05 | grad 1.05 | tok/s 48999
step    690 | loss 2.0589 | lr 9.84e-06 | grad 1.18 | tok/s 50257
step    700 | loss 2.1617 | lr 1.07e-06 | grad 1.12 | tok/s 50641
step    710 | loss 2.1027 | lr 6.94e-06 | grad 1.09 | tok/s 51038
step    720 | loss 2.1810 | lr 2.68e-05 | grad 1.38 | tok/s 50326
step    730 | loss 2.1177 | lr 5.89e-05 | grad 1.25 | tok/s 51272
step    740 | loss 2.0585 | lr 9.99e-05 | grad 2.20 | tok/s 50631
step    750 | loss 1.8808 | lr 1.46e-04 | grad 1.48 | tok/s 50162
step    760 | loss 2.5456 | lr 1.92e-04 | grad 0.80 | tok/s 50217
step    770 | loss 1.9053 | lr 2.35e-04 | grad 1.35 | tok/s 50549
step    780 | loss 1.9424 | lr 2.69e-04 | grad 1.41 | tok/s 50970
step    790 | loss 1.9331 | lr 2.91e-04 | grad 1.20 | tok/s 51686
step    800 | loss 1.9234 | lr 3.00e-04 | grad 1.25 | tok/s 51342
step    810 | loss 1.9702 | lr 2.94e-04 | grad 1.97 | tok/s 51165
step    820 | loss 2.6812 | lr 2.74e-04 | grad 1.62 | tok/s 51299
step    830 | loss 2.4208 | lr 2.42e-04 | grad 1.00 | tok/s 53292
step    840 | loss 2.0744 | lr 2.01e-04 | grad 0.65 | tok/s 53327
step    850 | loss 2.2453 | lr 1.55e-04 | grad 1.61 | tok/s 50161
step    860 | loss 2.0422 | lr 1.09e-04 | grad 1.06 | tok/s 49164
step    870 | loss 1.9665 | lr 6.65e-05 | grad 0.82 | tok/s 51240
step    880 | loss 2.0299 | lr 3.24e-05 | grad 1.14 | tok/s 49930
step    890 | loss 1.9251 | lr 9.84e-06 | grad 0.89 | tok/s 50770
step    900 | loss 2.3929 | lr 1.07e-06 | grad 1.07 | tok/s 49666
step    910 | loss 1.9918 | lr 6.94e-06 | grad 0.78 | tok/s 50131
step    920 | loss 1.9469 | lr 2.68e-05 | grad 0.70 | tok/s 50232
step    930 | loss 2.0794 | lr 5.89e-05 | grad 1.49 | tok/s 50163
step    940 | loss 1.9763 | lr 9.99e-05 | grad 1.45 | tok/s 49711
step    950 | loss 2.0206 | lr 1.46e-04 | grad 1.40 | tok/s 50762
step    960 | loss 1.8524 | lr 1.92e-04 | grad 0.78 | tok/s 53375
step    970 | loss 1.6675 | lr 2.35e-04 | grad 0.72 | tok/s 53417
step    980 | loss 1.7828 | lr 2.69e-04 | grad 2.28 | tok/s 50691
step    990 | loss 2.1049 | lr 2.91e-04 | grad 1.05 | tok/s 50449
step   1000 | loss 1.9199 | lr 3.00e-04 | grad 0.86 | tok/s 49245
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9199.pt
step   1010 | loss 2.1958 | lr 2.94e-04 | grad 1.66 | tok/s 50912
step   1020 | loss 1.8167 | lr 2.74e-04 | grad 0.97 | tok/s 50574
step   1030 | loss 2.1721 | lr 2.42e-04 | grad 1.02 | tok/s 49807
step   1040 | loss 1.7980 | lr 2.01e-04 | grad 1.62 | tok/s 50624
step   1050 | loss 1.8121 | lr 1.55e-04 | grad 0.93 | tok/s 50693
step   1060 | loss 2.1004 | lr 1.09e-04 | grad 1.50 | tok/s 51071
step   1070 | loss 2.1893 | lr 6.65e-05 | grad 0.91 | tok/s 51349
step   1080 | loss 2.4526 | lr 3.24e-05 | grad 1.00 | tok/s 50476
step   1090 | loss 2.1800 | lr 9.84e-06 | grad 0.93 | tok/s 51134
step   1100 | loss 1.8324 | lr 1.07e-06 | grad 0.92 | tok/s 50558
step   1110 | loss 1.9472 | lr 6.93e-06 | grad 0.82 | tok/s 51183
step   1120 | loss 2.1559 | lr 2.68e-05 | grad 0.88 | tok/s 52099
step   1130 | loss 1.8569 | lr 5.89e-05 | grad 0.77 | tok/s 48860
step   1140 | loss 1.7553 | lr 9.99e-05 | grad 0.82 | tok/s 50494
step   1150 | loss 2.0613 | lr 1.46e-04 | grad 1.16 | tok/s 50770
step   1160 | loss 1.6946 | lr 1.92e-04 | grad 0.72 | tok/s 50232
step   1170 | loss 2.1848 | lr 2.35e-04 | grad 1.02 | tok/s 50834
step   1180 | loss 1.7458 | lr 2.69e-04 | grad 1.02 | tok/s 52774
step   1190 | loss 1.6399 | lr 2.91e-04 | grad 0.77 | tok/s 53518
step   1200 | loss 1.5489 | lr 3.00e-04 | grad 0.78 | tok/s 52695
step   1210 | loss 1.5118 | lr 2.94e-04 | grad 0.77 | tok/s 53191
step   1220 | loss 1.5405 | lr 2.74e-04 | grad 1.14 | tok/s 52312
step   1230 | loss 1.7009 | lr 2.42e-04 | grad 1.20 | tok/s 50536
step   1240 | loss 1.8036 | lr 2.01e-04 | grad 0.93 | tok/s 49886
step   1250 | loss 1.8913 | lr 1.55e-04 | grad 3.08 | tok/s 51284
step   1260 | loss 1.9577 | lr 1.09e-04 | grad 2.42 | tok/s 51795
step   1270 | loss 2.0130 | lr 6.65e-05 | grad 1.41 | tok/s 50850
step   1280 | loss 1.8482 | lr 3.24e-05 | grad 0.99 | tok/s 50407
step   1290 | loss 1.7800 | lr 9.84e-06 | grad 1.06 | tok/s 49566
step   1300 | loss 1.8464 | lr 1.07e-06 | grad 0.88 | tok/s 49841
step   1310 | loss 1.9886 | lr 6.93e-06 | grad 0.95 | tok/s 49568
step   1320 | loss 1.9002 | lr 2.68e-05 | grad 1.34 | tok/s 50732
step   1330 | loss 1.8342 | lr 5.89e-05 | grad 0.75 | tok/s 50748
step   1340 | loss 1.7823 | lr 9.99e-05 | grad 1.10 | tok/s 50540
step   1350 | loss 1.8427 | lr 1.46e-04 | grad 2.05 | tok/s 52203
step   1360 | loss 1.7310 | lr 1.92e-04 | grad 0.83 | tok/s 49745
step   1370 | loss 1.8619 | lr 2.35e-04 | grad 1.08 | tok/s 49925
step   1380 | loss 1.9362 | lr 2.69e-04 | grad 1.09 | tok/s 50715
step   1390 | loss 1.8261 | lr 2.91e-04 | grad 1.86 | tok/s 49685
step   1400 | loss 1.9280 | lr 3.00e-04 | grad 6.81 | tok/s 50737
step   1410 | loss 2.0123 | lr 2.94e-04 | grad 1.45 | tok/s 50948
step   1420 | loss 1.9666 | lr 2.74e-04 | grad 1.41 | tok/s 47665
step   1430 | loss 1.6993 | lr 2.42e-04 | grad 0.98 | tok/s 45909
step   1440 | loss 1.6317 | lr 2.01e-04 | grad 1.02 | tok/s 48227
step   1450 | loss 1.7006 | lr 1.55e-04 | grad 2.02 | tok/s 49827
step   1460 | loss 1.7332 | lr 1.09e-04 | grad 0.75 | tok/s 46576
step   1470 | loss 1.8564 | lr 6.65e-05 | grad 2.27 | tok/s 48268
step   1480 | loss 1.7502 | lr 3.24e-05 | grad 1.84 | tok/s 48826
step   1490 | loss 1.9070 | lr 9.84e-06 | grad 2.61 | tok/s 48723
step   1500 | loss 1.9755 | lr 1.07e-06 | grad 1.81 | tok/s 47140
step   1510 | loss 1.8749 | lr 6.93e-06 | grad 1.22 | tok/s 50243
step   1520 | loss 1.8349 | lr 2.68e-05 | grad 1.38 | tok/s 49233
step   1530 | loss 1.7775 | lr 5.89e-05 | grad 0.71 | tok/s 48986
step   1540 | loss 1.7591 | lr 9.99e-05 | grad 0.65 | tok/s 48177
step   1550 | loss 1.7938 | lr 1.46e-04 | grad 2.78 | tok/s 50037
step   1560 | loss 2.4083 | lr 1.92e-04 | grad 1.43 | tok/s 48719
step   1570 | loss 1.7672 | lr 2.35e-04 | grad 1.34 | tok/s 48002
step   1580 | loss 1.9594 | lr 2.69e-04 | grad 1.41 | tok/s 49932
step   1590 | loss 1.7049 | lr 2.91e-04 | grad 0.93 | tok/s 48304
step   1600 | loss 1.7779 | lr 3.00e-04 | grad 1.01 | tok/s 47454
step   1610 | loss 1.6360 | lr 2.94e-04 | grad 0.85 | tok/s 50123
step   1620 | loss 1.7956 | lr 2.74e-04 | grad 0.81 | tok/s 49277
step   1630 | loss 1.8124 | lr 2.42e-04 | grad 1.38 | tok/s 49333
step   1640 | loss 1.7056 | lr 2.01e-04 | grad 0.81 | tok/s 48164
step   1650 | loss 1.7403 | lr 1.55e-04 | grad 1.56 | tok/s 47417
step   1660 | loss 1.7322 | lr 1.09e-04 | grad 0.82 | tok/s 47621
step   1670 | loss 1.8397 | lr 6.65e-05 | grad 2.83 | tok/s 49341
step   1680 | loss 2.3883 | lr 3.24e-05 | grad 0.73 | tok/s 49632
step   1690 | loss 1.7357 | lr 9.84e-06 | grad 1.09 | tok/s 48141
step   1700 | loss 2.1576 | lr 1.07e-06 | grad 0.81 | tok/s 49649
step   1710 | loss 1.7501 | lr 6.93e-06 | grad 1.06 | tok/s 48269
step   1720 | loss 1.7965 | lr 2.68e-05 | grad 1.05 | tok/s 48219
step   1730 | loss 1.9245 | lr 5.89e-05 | grad 0.95 | tok/s 48334
step   1740 | loss 1.8131 | lr 9.99e-05 | grad 0.77 | tok/s 49111
step   1750 | loss 1.6725 | lr 1.46e-04 | grad 0.75 | tok/s 46733
step   1760 | loss 1.9800 | lr 1.92e-04 | grad 0.88 | tok/s 47445
step   1770 | loss 1.8717 | lr 2.35e-04 | grad 0.84 | tok/s 48867
step   1780 | loss 1.7431 | lr 2.69e-04 | grad 1.23 | tok/s 46588
step   1790 | loss 1.9537 | lr 2.91e-04 | grad 1.02 | tok/s 47473
step   1800 | loss 1.6396 | lr 3.00e-04 | grad 0.95 | tok/s 44684
step   1810 | loss 1.7512 | lr 2.94e-04 | grad 0.91 | tok/s 49978
step   1820 | loss 1.7042 | lr 2.74e-04 | grad 1.02 | tok/s 50286
step   1830 | loss 1.7241 | lr 2.42e-04 | grad 0.94 | tok/s 49677
step   1840 | loss 1.7393 | lr 2.01e-04 | grad 1.17 | tok/s 49250
step   1850 | loss 1.9419 | lr 1.55e-04 | grad 1.12 | tok/s 49975
step   1860 | loss 1.6973 | lr 1.09e-04 | grad 0.67 | tok/s 50013
step   1870 | loss 1.7682 | lr 6.65e-05 | grad 1.28 | tok/s 51682
step   1880 | loss 1.6826 | lr 3.24e-05 | grad 0.78 | tok/s 51718
step   1890 | loss 1.8168 | lr 9.84e-06 | grad 0.67 | tok/s 51044

Training complete! Final step: 1890
