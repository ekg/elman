# Job 46: 46
# GPU: 3
# Command: python train.py --level 46 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/46
# Started: 2026-01-19T22:31:02.115455
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/46/level46_100m_20260119_223107
Auto r_h_mode: none (level 46 has bounded/no W_h)
Model: Level 46, 16,574,080 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.5663 | lr 2.70e-05 | grad 20.88 | tok/s 19505
step     20 | loss 5.2886 | lr 5.70e-05 | grad 32.50 | tok/s 35705
step     30 | loss 5.3624 | lr 8.70e-05 | grad 8.75 | tok/s 33642
step     40 | loss 5.3142 | lr 1.17e-04 | grad 6.25 | tok/s 44237
step     50 | loss 5.1353 | lr 1.47e-04 | grad 14.88 | tok/s 48995
step     60 | loss 4.5944 | lr 1.77e-04 | grad 18.25 | tok/s 47548
step     70 | loss 3.6871 | lr 2.07e-04 | grad 3.61 | tok/s 46157
step     80 | loss 3.6144 | lr 2.37e-04 | grad 2.42 | tok/s 47798
step     90 | loss 3.4654 | lr 2.67e-04 | grad 3.55 | tok/s 45663
step    100 | loss 3.3059 | lr 2.97e-04 | grad 1.03 | tok/s 46651
step    110 | loss 3.2162 | lr 6.94e-06 | grad 1.59 | tok/s 44500
step    120 | loss 3.6028 | lr 2.69e-05 | grad 1.06 | tok/s 44403
step    130 | loss 3.3831 | lr 5.89e-05 | grad 1.09 | tok/s 45439
step    140 | loss 3.1708 | lr 9.99e-05 | grad 0.66 | tok/s 45378
step    150 | loss 3.0466 | lr 1.46e-04 | grad 2.22 | tok/s 43203
step    160 | loss 2.8479 | lr 1.92e-04 | grad 1.22 | tok/s 43803
step    170 | loss 2.8665 | lr 2.35e-04 | grad 4.56 | tok/s 45251
step    180 | loss 2.7859 | lr 2.69e-04 | grad 1.22 | tok/s 45490
step    190 | loss 2.4262 | lr 2.91e-04 | grad 1.41 | tok/s 46235
step    200 | loss 2.0559 | lr 3.00e-04 | grad 1.08 | tok/s 47005
step    210 | loss 2.4417 | lr 2.94e-04 | grad 1.85 | tok/s 45404
step    220 | loss 2.3242 | lr 2.74e-04 | grad 1.03 | tok/s 46287
step    230 | loss 2.1426 | lr 2.42e-04 | grad 1.59 | tok/s 45223
step    240 | loss 2.1527 | lr 2.01e-04 | grad 1.77 | tok/s 45920
step    250 | loss 2.1108 | lr 1.55e-04 | grad 1.34 | tok/s 44899
step    260 | loss 2.1647 | lr 1.09e-04 | grad 0.78 | tok/s 43584
step    270 | loss 2.0125 | lr 6.65e-05 | grad 1.02 | tok/s 45268
step    280 | loss 1.9069 | lr 3.24e-05 | grad 1.63 | tok/s 45148
step    290 | loss 1.9319 | lr 9.84e-06 | grad 1.05 | tok/s 47607
step    300 | loss 1.9068 | lr 1.07e-06 | grad 0.97 | tok/s 47530
step    310 | loss 1.9106 | lr 6.94e-06 | grad 0.91 | tok/s 47569
step    320 | loss 1.9954 | lr 2.69e-05 | grad 1.66 | tok/s 45654
step    330 | loss 2.0340 | lr 5.89e-05 | grad 0.77 | tok/s 44380
step    340 | loss 2.0383 | lr 9.99e-05 | grad 1.88 | tok/s 45432
step    350 | loss 2.0595 | lr 1.46e-04 | grad 1.05 | tok/s 44349
step    360 | loss 2.0059 | lr 1.92e-04 | grad 2.77 | tok/s 44659
step    370 | loss 1.8386 | lr 2.35e-04 | grad 1.26 | tok/s 45697
step    380 | loss 2.3380 | lr 2.69e-04 | grad 1.54 | tok/s 46832
step    390 | loss 1.9423 | lr 2.91e-04 | grad 1.35 | tok/s 45134
step    400 | loss 2.0254 | lr 3.00e-04 | grad 2.58 | tok/s 46072
step    410 | loss 1.7844 | lr 2.94e-04 | grad 2.08 | tok/s 44866
step    420 | loss 1.9268 | lr 2.74e-04 | grad 1.48 | tok/s 44474
step    430 | loss 2.0574 | lr 2.42e-04 | grad 1.67 | tok/s 44576
step    440 | loss 2.0894 | lr 2.01e-04 | grad 1.56 | tok/s 45908
step    450 | loss 1.8612 | lr 1.55e-04 | grad 0.87 | tok/s 44990
step    460 | loss 1.8316 | lr 1.09e-04 | grad 0.85 | tok/s 45132
step    470 | loss 1.8196 | lr 6.65e-05 | grad 1.22 | tok/s 45807
step    480 | loss 1.7266 | lr 3.24e-05 | grad 0.78 | tok/s 44048
step    490 | loss 1.7011 | lr 9.84e-06 | grad 0.75 | tok/s 44970
step    500 | loss 2.6678 | lr 1.07e-06 | grad 1.23 | tok/s 46271
step    510 | loss 1.7243 | lr 6.94e-06 | grad 0.83 | tok/s 45260
step    520 | loss 1.7916 | lr 2.69e-05 | grad 0.70 | tok/s 46678
step    530 | loss 2.2879 | lr 5.89e-05 | grad 0.72 | tok/s 45842
step    540 | loss 1.7248 | lr 9.99e-05 | grad 1.02 | tok/s 45768
step    550 | loss 1.6344 | lr 1.46e-04 | grad 0.86 | tok/s 46867
step    560 | loss 1.5060 | lr 1.92e-04 | grad 0.82 | tok/s 47685
step    570 | loss 1.7570 | lr 2.35e-04 | grad 2.19 | tok/s 46438
step    580 | loss 2.0726 | lr 2.69e-04 | grad 1.11 | tok/s 45940
step    590 | loss 2.3595 | lr 2.91e-04 | grad 1.95 | tok/s 45071
step    600 | loss 1.8486 | lr 3.00e-04 | grad 1.52 | tok/s 45307
step    610 | loss 1.8450 | lr 2.94e-04 | grad 1.32 | tok/s 47249
step    620 | loss 1.7636 | lr 2.74e-04 | grad 1.02 | tok/s 44954
step    630 | loss 1.6907 | lr 2.42e-04 | grad 1.02 | tok/s 46107
step    640 | loss 1.9652 | lr 2.01e-04 | grad 1.30 | tok/s 46370
step    650 | loss 1.7314 | lr 1.55e-04 | grad 1.31 | tok/s 45299
step    660 | loss 2.0276 | lr 1.09e-04 | grad 7.62 | tok/s 44771
step    670 | loss 1.8446 | lr 6.65e-05 | grad 1.81 | tok/s 46139
step    680 | loss 1.7937 | lr 3.24e-05 | grad 1.27 | tok/s 44923
step    690 | loss 1.8376 | lr 9.84e-06 | grad 1.58 | tok/s 44965
step    700 | loss 1.9042 | lr 1.07e-06 | grad 1.52 | tok/s 45523
step    710 | loss 1.8237 | lr 6.94e-06 | grad 1.30 | tok/s 45524
step    720 | loss 1.9714 | lr 2.68e-05 | grad 1.63 | tok/s 45240
step    730 | loss 1.9036 | lr 5.89e-05 | grad 1.52 | tok/s 45913
step    740 | loss 1.8371 | lr 9.99e-05 | grad 2.11 | tok/s 45527
step    750 | loss 1.6532 | lr 1.46e-04 | grad 1.86 | tok/s 44817
step    760 | loss 1.9813 | lr 1.92e-04 | grad 0.88 | tok/s 45258
step    770 | loss 1.7311 | lr 2.35e-04 | grad 1.45 | tok/s 45172
step    780 | loss 1.7427 | lr 2.69e-04 | grad 1.16 | tok/s 45908
step    790 | loss 1.6864 | lr 2.91e-04 | grad 1.10 | tok/s 45960
step    800 | loss 1.6600 | lr 3.00e-04 | grad 1.23 | tok/s 46301
step    810 | loss 1.7798 | lr 2.94e-04 | grad 2.06 | tok/s 45536
step    820 | loss 2.4764 | lr 2.74e-04 | grad 1.55 | tok/s 47031
step    830 | loss 2.0605 | lr 2.42e-04 | grad 0.79 | tok/s 46777
step    840 | loss 1.7189 | lr 2.01e-04 | grad 0.68 | tok/s 47565
step    850 | loss 2.0516 | lr 1.55e-04 | grad 1.62 | tok/s 45127
step    860 | loss 1.8103 | lr 1.09e-04 | grad 1.20 | tok/s 44542
step    870 | loss 1.7340 | lr 6.65e-05 | grad 0.95 | tok/s 45462
step    880 | loss 1.7891 | lr 3.24e-05 | grad 1.23 | tok/s 45645
step    890 | loss 1.7098 | lr 9.84e-06 | grad 1.03 | tok/s 45532
step    900 | loss 2.1174 | lr 1.07e-06 | grad 1.05 | tok/s 44335
step    910 | loss 1.7526 | lr 6.94e-06 | grad 0.83 | tok/s 45068
step    920 | loss 1.7427 | lr 2.68e-05 | grad 0.88 | tok/s 44960
step    930 | loss 1.8404 | lr 5.89e-05 | grad 1.77 | tok/s 44408
step    940 | loss 1.7761 | lr 9.99e-05 | grad 1.81 | tok/s 44484
step    950 | loss 1.8184 | lr 1.46e-04 | grad 1.34 | tok/s 45285
step    960 | loss 1.6241 | lr 1.92e-04 | grad 0.74 | tok/s 47610
step    970 | loss 1.4321 | lr 2.35e-04 | grad 0.62 | tok/s 47293
step    980 | loss 1.5633 | lr 2.69e-04 | grad 2.00 | tok/s 46252
step    990 | loss 1.8740 | lr 2.91e-04 | grad 0.93 | tok/s 44796
step   1000 | loss 1.7556 | lr 3.00e-04 | grad 0.78 | tok/s 43808
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7556.pt
step   1010 | loss 1.9597 | lr 2.94e-04 | grad 1.91 | tok/s 44686
step   1020 | loss 1.6267 | lr 2.74e-04 | grad 1.15 | tok/s 45058
step   1030 | loss 2.0216 | lr 2.42e-04 | grad 1.08 | tok/s 44116
step   1040 | loss 1.6486 | lr 2.01e-04 | grad 1.63 | tok/s 45125
step   1050 | loss 1.6634 | lr 1.55e-04 | grad 0.96 | tok/s 45048
step   1060 | loss 1.8554 | lr 1.09e-04 | grad 1.38 | tok/s 45684
step   1070 | loss 1.9840 | lr 6.65e-05 | grad 1.03 | tok/s 45207
step   1080 | loss 2.3466 | lr 3.24e-05 | grad 1.16 | tok/s 45093
step   1090 | loss 2.0426 | lr 9.84e-06 | grad 1.05 | tok/s 44999
step   1100 | loss 1.6915 | lr 1.07e-06 | grad 0.88 | tok/s 44956
step   1110 | loss 1.6832 | lr 6.93e-06 | grad 1.01 | tok/s 45864
step   1120 | loss 1.9042 | lr 2.68e-05 | grad 0.83 | tok/s 46411
step   1130 | loss 1.7103 | lr 5.89e-05 | grad 0.93 | tok/s 43973
step   1140 | loss 1.5843 | lr 9.99e-05 | grad 0.81 | tok/s 45249
step   1150 | loss 1.8764 | lr 1.46e-04 | grad 1.30 | tok/s 44671
step   1160 | loss 1.5462 | lr 1.92e-04 | grad 0.75 | tok/s 44428
step   1170 | loss 1.8507 | lr 2.35e-04 | grad 1.02 | tok/s 44420
step   1180 | loss 1.6126 | lr 2.69e-04 | grad 0.93 | tok/s 47504
step   1190 | loss 1.4727 | lr 2.91e-04 | grad 0.73 | tok/s 47614
step   1200 | loss 1.3837 | lr 3.00e-04 | grad 0.77 | tok/s 47593
step   1210 | loss 1.3535 | lr 2.94e-04 | grad 0.80 | tok/s 47490
step   1220 | loss 1.3866 | lr 2.74e-04 | grad 1.11 | tok/s 47090
step   1230 | loss 1.5687 | lr 2.42e-04 | grad 0.97 | tok/s 45332
step   1240 | loss 1.6491 | lr 2.01e-04 | grad 0.86 | tok/s 44712
step   1250 | loss 1.7315 | lr 1.55e-04 | grad 3.91 | tok/s 45781
step   1260 | loss 1.8148 | lr 1.09e-04 | grad 2.84 | tok/s 46079
step   1270 | loss 1.8501 | lr 6.65e-05 | grad 1.48 | tok/s 45274
step   1280 | loss 1.6812 | lr 3.24e-05 | grad 1.01 | tok/s 44720
step   1290 | loss 1.6361 | lr 9.84e-06 | grad 1.16 | tok/s 44583
step   1300 | loss 1.6941 | lr 1.07e-06 | grad 0.94 | tok/s 44487
step   1310 | loss 1.7926 | lr 6.93e-06 | grad 0.91 | tok/s 44308
step   1320 | loss 1.7492 | lr 2.68e-05 | grad 1.37 | tok/s 45376
step   1330 | loss 1.6669 | lr 5.89e-05 | grad 0.75 | tok/s 45277
step   1340 | loss 1.5828 | lr 9.99e-05 | grad 1.26 | tok/s 45437
step   1350 | loss 1.6679 | lr 1.46e-04 | grad 2.09 | tok/s 46062
step   1360 | loss 1.5789 | lr 1.92e-04 | grad 0.93 | tok/s 44171
step   1370 | loss 1.6875 | lr 2.35e-04 | grad 0.88 | tok/s 44495
step   1380 | loss 1.7591 | lr 2.69e-04 | grad 1.09 | tok/s 45263
step   1390 | loss 1.6895 | lr 2.91e-04 | grad 2.00 | tok/s 43955
step   1400 | loss 1.6920 | lr 3.00e-04 | grad 6.16 | tok/s 45759
step   1410 | loss 1.7210 | lr 2.94e-04 | grad 1.17 | tok/s 46164
step   1420 | loss 1.7531 | lr 2.74e-04 | grad 1.13 | tok/s 44190
step   1430 | loss 1.5682 | lr 2.42e-04 | grad 0.97 | tok/s 42883
step   1440 | loss 1.4836 | lr 2.01e-04 | grad 0.86 | tok/s 45577
step   1450 | loss 1.5286 | lr 1.55e-04 | grad 1.92 | tok/s 46134
step   1460 | loss 1.5943 | lr 1.09e-04 | grad 0.71 | tok/s 43275
step   1470 | loss 1.7128 | lr 6.65e-05 | grad 2.50 | tok/s 44876
step   1480 | loss 1.5947 | lr 3.24e-05 | grad 2.05 | tok/s 45372
step   1490 | loss 1.7221 | lr 9.84e-06 | grad 2.22 | tok/s 45114
step   1500 | loss 1.8395 | lr 1.07e-06 | grad 1.87 | tok/s 43866
step   1510 | loss 1.7160 | lr 6.93e-06 | grad 1.22 | tok/s 46163
step   1520 | loss 1.6750 | lr 2.68e-05 | grad 1.34 | tok/s 45562
step   1530 | loss 1.6290 | lr 5.89e-05 | grad 0.71 | tok/s 45547
step   1540 | loss 1.6084 | lr 9.99e-05 | grad 0.67 | tok/s 44749
step   1550 | loss 1.5862 | lr 1.46e-04 | grad 2.59 | tok/s 46164
step   1560 | loss 2.1778 | lr 1.92e-04 | grad 1.50 | tok/s 45317
step   1570 | loss 1.6261 | lr 2.35e-04 | grad 1.34 | tok/s 44435
step   1580 | loss 1.8133 | lr 2.69e-04 | grad 1.38 | tok/s 45950
step   1590 | loss 1.5646 | lr 2.91e-04 | grad 0.97 | tok/s 44727
step   1600 | loss 1.6768 | lr 3.00e-04 | grad 1.06 | tok/s 44031
step   1610 | loss 1.5205 | lr 2.94e-04 | grad 0.93 | tok/s 46588
step   1620 | loss 1.6764 | lr 2.74e-04 | grad 0.84 | tok/s 45909
step   1630 | loss 1.6666 | lr 2.42e-04 | grad 0.96 | tok/s 46322
step   1640 | loss 1.5966 | lr 2.01e-04 | grad 0.78 | tok/s 44526
step   1650 | loss 1.6141 | lr 1.55e-04 | grad 1.40 | tok/s 44060
step   1660 | loss 1.6122 | lr 1.09e-04 | grad 0.85 | tok/s 45333
step   1670 | loss 1.6718 | lr 6.65e-05 | grad 2.50 | tok/s 42889
step   1680 | loss 2.0634 | lr 3.24e-05 | grad 0.71 | tok/s 44429
step   1690 | loss 1.5853 | lr 9.84e-06 | grad 1.09 | tok/s 46692
step   1700 | loss 1.8822 | lr 1.07e-06 | grad 0.74 | tok/s 47309
step   1710 | loss 1.6329 | lr 6.93e-06 | grad 1.22 | tok/s 46196

Training complete! Final step: 1718
