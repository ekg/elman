# Job 35: 35
# GPU: 0
# Command: python train.py --level 35 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/35
# Started: 2026-01-19T22:20:35.606697
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/35/level35_100m_20260119_222040
Auto r_h_mode: none (level 35 has bounded/no W_h)
Model: Level 35, 98,506,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.1134 | lr 2.70e-05 | grad 7.03 | tok/s 21550
step     20 | loss 3.8587 | lr 5.70e-05 | grad 6.16 | tok/s 59779
step     30 | loss 4.5226 | lr 8.70e-05 | grad 3.59 | tok/s 61869
step     40 | loss 3.3824 | lr 1.17e-04 | grad 2.34 | tok/s 62388
step     50 | loss 2.6741 | lr 1.47e-04 | grad 1.96 | tok/s 62934
step     60 | loss 2.9067 | lr 1.77e-04 | grad 2.55 | tok/s 61651
step     70 | loss 2.5611 | lr 2.07e-04 | grad 2.34 | tok/s 59207
step     80 | loss 2.8834 | lr 2.37e-04 | grad 2.34 | tok/s 60248
step     90 | loss 2.9694 | lr 2.67e-04 | grad 3.00 | tok/s 59074
step    100 | loss 2.5566 | lr 2.97e-04 | grad 1.29 | tok/s 60013
step    110 | loss 2.4707 | lr 6.94e-06 | grad 1.80 | tok/s 56431
step    120 | loss 2.7672 | lr 2.69e-05 | grad 1.06 | tok/s 55930
step    130 | loss 2.5514 | lr 5.89e-05 | grad 0.96 | tok/s 57666
step    140 | loss 2.3275 | lr 9.99e-05 | grad 0.80 | tok/s 57209
step    150 | loss 2.2270 | lr 1.46e-04 | grad 1.60 | tok/s 55201
step    160 | loss 2.1203 | lr 1.92e-04 | grad 1.35 | tok/s 55079
step    170 | loss 2.3310 | lr 2.35e-04 | grad 3.39 | tok/s 57614
step    180 | loss 2.3557 | lr 2.69e-04 | grad 1.23 | tok/s 57266
step    190 | loss 2.1117 | lr 2.91e-04 | grad 1.23 | tok/s 58822
step    200 | loss 1.7677 | lr 3.00e-04 | grad 0.91 | tok/s 58764
step    210 | loss 2.3737 | lr 2.94e-04 | grad 1.51 | tok/s 57618
step    220 | loss 2.2638 | lr 2.74e-04 | grad 1.22 | tok/s 59026
step    230 | loss 2.1031 | lr 2.42e-04 | grad 1.22 | tok/s 57346
step    240 | loss 2.1077 | lr 2.01e-04 | grad 1.40 | tok/s 58519
step    250 | loss 2.0785 | lr 1.55e-04 | grad 1.03 | tok/s 57824
step    260 | loss 2.1211 | lr 1.09e-04 | grad 0.66 | tok/s 55412
step    270 | loss 1.9815 | lr 6.65e-05 | grad 0.77 | tok/s 57727
step    280 | loss 1.8696 | lr 3.24e-05 | grad 1.42 | tok/s 57093
step    290 | loss 1.8649 | lr 9.84e-06 | grad 0.62 | tok/s 60117
step    300 | loss 1.8316 | lr 1.07e-06 | grad 0.66 | tok/s 60747
step    310 | loss 1.8326 | lr 6.94e-06 | grad 0.57 | tok/s 60750
step    320 | loss 1.9305 | lr 2.69e-05 | grad 1.27 | tok/s 58405
step    330 | loss 1.9642 | lr 5.89e-05 | grad 0.57 | tok/s 56832
step    340 | loss 1.9765 | lr 9.99e-05 | grad 1.46 | tok/s 58258
step    350 | loss 1.9814 | lr 1.46e-04 | grad 0.94 | tok/s 56462
step    360 | loss 1.9749 | lr 1.92e-04 | grad 1.79 | tok/s 56823
step    370 | loss 1.8376 | lr 2.35e-04 | grad 1.26 | tok/s 57598
step    380 | loss 2.3983 | lr 2.69e-04 | grad 1.41 | tok/s 59824
step    390 | loss 2.0193 | lr 2.91e-04 | grad 1.03 | tok/s 57680
step    400 | loss 2.1159 | lr 3.00e-04 | grad 2.19 | tok/s 59150
step    410 | loss 1.8880 | lr 2.94e-04 | grad 1.77 | tok/s 57240
step    420 | loss 2.0253 | lr 2.74e-04 | grad 1.02 | tok/s 56959
step    430 | loss 2.1570 | lr 2.42e-04 | grad 1.22 | tok/s 56488
step    440 | loss 2.2429 | lr 2.01e-04 | grad 1.11 | tok/s 58177
step    450 | loss 1.9634 | lr 1.55e-04 | grad 0.64 | tok/s 57198
step    460 | loss 1.8999 | lr 1.09e-04 | grad 0.64 | tok/s 57474
step    470 | loss 1.8857 | lr 6.65e-05 | grad 0.94 | tok/s 58137
step    480 | loss 1.7836 | lr 3.24e-05 | grad 0.49 | tok/s 56275
step    490 | loss 1.7429 | lr 9.84e-06 | grad 0.46 | tok/s 57009
step    500 | loss 2.6931 | lr 1.07e-06 | grad 0.77 | tok/s 58915
step    510 | loss 1.7485 | lr 6.94e-06 | grad 0.51 | tok/s 57595
step    520 | loss 1.8211 | lr 2.69e-05 | grad 0.43 | tok/s 59046
step    530 | loss 2.3187 | lr 5.89e-05 | grad 0.57 | tok/s 58072
step    540 | loss 1.7580 | lr 9.99e-05 | grad 0.80 | tok/s 57736
step    550 | loss 1.6761 | lr 1.46e-04 | grad 0.57 | tok/s 59824
step    560 | loss 1.5473 | lr 1.92e-04 | grad 0.69 | tok/s 60519
step    570 | loss 1.8280 | lr 2.35e-04 | grad 1.59 | tok/s 59023
step    580 | loss 2.1976 | lr 2.69e-04 | grad 1.09 | tok/s 58438
step    590 | loss 2.4961 | lr 2.91e-04 | grad 1.91 | tok/s 56605
step    600 | loss 1.9953 | lr 3.00e-04 | grad 1.33 | tok/s 57135
step    610 | loss 1.9743 | lr 2.94e-04 | grad 1.07 | tok/s 59879
step    620 | loss 1.8980 | lr 2.74e-04 | grad 0.83 | tok/s 56000
step    630 | loss 1.8031 | lr 2.42e-04 | grad 0.81 | tok/s 58540
step    640 | loss 2.0837 | lr 2.01e-04 | grad 0.88 | tok/s 58506
step    650 | loss 1.8514 | lr 1.55e-04 | grad 0.85 | tok/s 57442
step    660 | loss 2.1482 | lr 1.09e-04 | grad 3.95 | tok/s 56775
step    670 | loss 1.9507 | lr 6.65e-05 | grad 1.36 | tok/s 58663
step    680 | loss 1.8906 | lr 3.24e-05 | grad 0.82 | tok/s 56978
step    690 | loss 1.8991 | lr 9.84e-06 | grad 0.86 | tok/s 56919
step    700 | loss 1.9841 | lr 1.07e-06 | grad 0.95 | tok/s 57377
step    710 | loss 1.9085 | lr 6.94e-06 | grad 0.79 | tok/s 57781
step    720 | loss 2.0111 | lr 2.68e-05 | grad 1.07 | tok/s 57550
step    730 | loss 1.9740 | lr 5.89e-05 | grad 0.90 | tok/s 58269
step    740 | loss 1.9111 | lr 9.99e-05 | grad 1.55 | tok/s 57420
step    750 | loss 1.7209 | lr 1.46e-04 | grad 1.08 | tok/s 56785
step    760 | loss 2.0513 | lr 1.92e-04 | grad 0.67 | tok/s 57913
step    770 | loss 1.8036 | lr 2.35e-04 | grad 1.05 | tok/s 57479
step    780 | loss 1.8367 | lr 2.69e-04 | grad 0.77 | tok/s 58308
step    790 | loss 1.7940 | lr 2.91e-04 | grad 0.77 | tok/s 57985
step    800 | loss 1.7733 | lr 3.00e-04 | grad 0.84 | tok/s 58674
step    810 | loss 1.8790 | lr 2.94e-04 | grad 1.66 | tok/s 57333
step    820 | loss 2.5137 | lr 2.74e-04 | grad 1.30 | tok/s 59517
step    830 | loss 2.0655 | lr 2.42e-04 | grad 0.85 | tok/s 60620
step    840 | loss 1.7873 | lr 2.01e-04 | grad 0.58 | tok/s 60733
step    850 | loss 2.2137 | lr 1.55e-04 | grad 0.96 | tok/s 57732
step    860 | loss 1.9438 | lr 1.09e-04 | grad 0.93 | tok/s 56065
step    870 | loss 1.8463 | lr 6.65e-05 | grad 0.73 | tok/s 57752
step    880 | loss 1.8982 | lr 3.24e-05 | grad 0.91 | tok/s 57666
step    890 | loss 1.7867 | lr 9.84e-06 | grad 0.58 | tok/s 57571
step    900 | loss 2.2523 | lr 1.07e-06 | grad 0.77 | tok/s 56138
step    910 | loss 1.8487 | lr 6.94e-06 | grad 0.52 | tok/s 57626
step    920 | loss 1.8060 | lr 2.68e-05 | grad 0.51 | tok/s 57217
step    930 | loss 1.9176 | lr 5.89e-05 | grad 1.07 | tok/s 57243
step    940 | loss 1.8188 | lr 9.99e-05 | grad 1.10 | tok/s 56681
step    950 | loss 1.8991 | lr 1.46e-04 | grad 1.02 | tok/s 57995
step    960 | loss 1.6657 | lr 1.92e-04 | grad 0.51 | tok/s 60008
step    970 | loss 1.4780 | lr 2.35e-04 | grad 0.39 | tok/s 60838
step    980 | loss 1.6405 | lr 2.69e-04 | grad 1.62 | tok/s 59164
step    990 | loss 1.9840 | lr 2.91e-04 | grad 0.69 | tok/s 57463
step   1000 | loss 1.8725 | lr 3.00e-04 | grad 0.59 | tok/s 56218
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8725.pt
step   1010 | loss 2.0840 | lr 2.94e-04 | grad 0.95 | tok/s 48767
step   1020 | loss 1.7558 | lr 2.74e-04 | grad 0.70 | tok/s 57364
step   1030 | loss 2.1411 | lr 2.42e-04 | grad 0.72 | tok/s 56398
step   1040 | loss 1.7615 | lr 2.01e-04 | grad 1.05 | tok/s 57824
step   1050 | loss 1.7710 | lr 1.55e-04 | grad 0.65 | tok/s 58146
step   1060 | loss 1.9773 | lr 1.09e-04 | grad 1.09 | tok/s 57867
step   1070 | loss 2.0561 | lr 6.65e-05 | grad 0.62 | tok/s 58445
step   1080 | loss 2.4191 | lr 3.24e-05 | grad 0.84 | tok/s 57769
step   1090 | loss 2.1118 | lr 9.84e-06 | grad 0.69 | tok/s 58087
step   1100 | loss 1.7954 | lr 1.07e-06 | grad 0.65 | tok/s 57589
step   1110 | loss 1.7857 | lr 6.93e-06 | grad 0.58 | tok/s 58637
step   1120 | loss 1.9903 | lr 2.68e-05 | grad 0.68 | tok/s 58862
step   1130 | loss 1.7710 | lr 5.89e-05 | grad 0.56 | tok/s 56528
step   1140 | loss 1.6481 | lr 9.99e-05 | grad 0.60 | tok/s 57998
step   1150 | loss 1.9353 | lr 1.46e-04 | grad 0.84 | tok/s 57309
step   1160 | loss 1.6248 | lr 1.92e-04 | grad 0.54 | tok/s 57089
step   1170 | loss 1.9240 | lr 2.35e-04 | grad 0.75 | tok/s 57830
step   1180 | loss 1.6751 | lr 2.69e-04 | grad 0.71 | tok/s 60788
step   1190 | loss 1.5422 | lr 2.91e-04 | grad 0.63 | tok/s 60598
step   1200 | loss 1.4538 | lr 3.00e-04 | grad 0.55 | tok/s 60797
step   1210 | loss 1.4096 | lr 2.94e-04 | grad 0.65 | tok/s 60941
step   1220 | loss 1.4543 | lr 2.74e-04 | grad 0.82 | tok/s 60355
step   1230 | loss 1.6903 | lr 2.42e-04 | grad 0.85 | tok/s 58112
step   1240 | loss 1.7600 | lr 2.01e-04 | grad 0.70 | tok/s 56822
step   1250 | loss 1.8263 | lr 1.55e-04 | grad 2.39 | tok/s 58905
step   1260 | loss 1.8877 | lr 1.09e-04 | grad 1.85 | tok/s 58419
step   1270 | loss 1.9323 | lr 6.65e-05 | grad 0.98 | tok/s 58174
step   1280 | loss 1.7616 | lr 3.24e-05 | grad 0.68 | tok/s 57019
step   1290 | loss 1.6977 | lr 9.84e-06 | grad 0.69 | tok/s 57274
step   1300 | loss 1.7518 | lr 1.07e-06 | grad 0.58 | tok/s 56926
step   1310 | loss 1.8466 | lr 6.93e-06 | grad 0.58 | tok/s 56869
step   1320 | loss 1.8030 | lr 2.68e-05 | grad 0.89 | tok/s 57886
step   1330 | loss 1.7289 | lr 5.89e-05 | grad 0.56 | tok/s 58044
step   1340 | loss 1.6475 | lr 9.99e-05 | grad 0.83 | tok/s 57759
step   1350 | loss 1.6994 | lr 1.46e-04 | grad 1.32 | tok/s 59595
step   1360 | loss 1.6710 | lr 1.92e-04 | grad 0.58 | tok/s 56566
step   1370 | loss 1.7606 | lr 2.35e-04 | grad 0.71 | tok/s 57394
step   1380 | loss 1.8742 | lr 2.69e-04 | grad 0.84 | tok/s 57911
step   1390 | loss 1.7862 | lr 2.91e-04 | grad 1.31 | tok/s 56479
step   1400 | loss 1.8670 | lr 3.00e-04 | grad 6.44 | tok/s 58654
step   1410 | loss 1.8688 | lr 2.94e-04 | grad 1.12 | tok/s 58779
step   1420 | loss 1.8772 | lr 2.74e-04 | grad 0.80 | tok/s 56259
step   1430 | loss 1.6727 | lr 2.42e-04 | grad 0.80 | tok/s 55182
step   1440 | loss 1.5793 | lr 2.01e-04 | grad 0.61 | tok/s 58167
step   1450 | loss 1.6247 | lr 1.55e-04 | grad 1.51 | tok/s 59522
step   1460 | loss 1.6684 | lr 1.09e-04 | grad 0.50 | tok/s 55449
step   1470 | loss 1.7719 | lr 6.65e-05 | grad 1.38 | tok/s 57305
step   1480 | loss 1.6476 | lr 3.24e-05 | grad 1.30 | tok/s 58160
step   1490 | loss 1.7734 | lr 9.84e-06 | grad 1.74 | tok/s 57742
step   1500 | loss 1.8772 | lr 1.07e-06 | grad 1.76 | tok/s 56329
step   1510 | loss 1.7761 | lr 6.93e-06 | grad 0.82 | tok/s 59159
step   1520 | loss 1.7321 | lr 2.68e-05 | grad 0.88 | tok/s 58518
step   1530 | loss 1.6785 | lr 5.89e-05 | grad 0.46 | tok/s 58245
step   1540 | loss 1.6560 | lr 9.99e-05 | grad 0.45 | tok/s 56975
step   1550 | loss 1.6242 | lr 1.46e-04 | grad 1.76 | tok/s 59055
step   1560 | loss 2.2410 | lr 1.92e-04 | grad 1.09 | tok/s 57586
step   1570 | loss 1.7084 | lr 2.35e-04 | grad 0.85 | tok/s 56992
step   1580 | loss 1.8936 | lr 2.69e-04 | grad 1.00 | tok/s 58904
step   1590 | loss 1.6646 | lr 2.91e-04 | grad 0.77 | tok/s 57384
step   1600 | loss 1.8047 | lr 3.00e-04 | grad 0.85 | tok/s 56251
step   1610 | loss 1.6184 | lr 2.94e-04 | grad 0.65 | tok/s 59652
step   1620 | loss 1.7844 | lr 2.74e-04 | grad 0.61 | tok/s 58646
step   1630 | loss 1.7733 | lr 2.42e-04 | grad 0.74 | tok/s 59199
step   1640 | loss 1.6979 | lr 2.01e-04 | grad 0.61 | tok/s 57064
step   1650 | loss 1.7019 | lr 1.55e-04 | grad 0.96 | tok/s 56365
step   1660 | loss 1.6895 | lr 1.09e-04 | grad 0.59 | tok/s 56686
step   1670 | loss 1.7414 | lr 6.65e-05 | grad 1.50 | tok/s 59216
step   1680 | loss 2.0813 | lr 3.24e-05 | grad 0.50 | tok/s 59148
step   1690 | loss 1.6577 | lr 9.84e-06 | grad 0.71 | tok/s 57796
step   1700 | loss 1.9996 | lr 1.07e-06 | grad 0.47 | tok/s 58630
step   1710 | loss 1.7032 | lr 6.93e-06 | grad 0.71 | tok/s 56902
step   1720 | loss 1.7083 | lr 2.68e-05 | grad 0.68 | tok/s 57118
step   1730 | loss 1.8095 | lr 5.89e-05 | grad 0.60 | tok/s 57053
step   1740 | loss 1.6992 | lr 9.99e-05 | grad 0.52 | tok/s 58383
step   1750 | loss 1.6106 | lr 1.46e-04 | grad 0.47 | tok/s 56307
step   1760 | loss 1.8797 | lr 1.92e-04 | grad 0.60 | tok/s 57126
step   1770 | loss 1.8030 | lr 2.35e-04 | grad 0.64 | tok/s 58392
step   1780 | loss 1.7091 | lr 2.69e-04 | grad 0.87 | tok/s 56304
step   1790 | loss 1.9020 | lr 2.91e-04 | grad 0.70 | tok/s 57033
step   1800 | loss 1.6362 | lr 3.00e-04 | grad 0.57 | tok/s 58266
step   1810 | loss 1.7277 | lr 2.94e-04 | grad 0.71 | tok/s 58040
step   1820 | loss 1.6685 | lr 2.74e-04 | grad 0.62 | tok/s 57369
step   1830 | loss 1.7163 | lr 2.42e-04 | grad 0.60 | tok/s 57206
step   1840 | loss 1.6791 | lr 2.01e-04 | grad 0.87 | tok/s 56699
step   1850 | loss 1.8900 | lr 1.55e-04 | grad 0.78 | tok/s 57299
step   1860 | loss 1.6335 | lr 1.09e-04 | grad 0.44 | tok/s 56884
step   1870 | loss 1.6868 | lr 6.65e-05 | grad 0.98 | tok/s 58490
step   1880 | loss 1.6198 | lr 3.24e-05 | grad 0.48 | tok/s 58152
step   1890 | loss 1.7244 | lr 9.84e-06 | grad 0.43 | tok/s 57537
step   1900 | loss 1.7613 | lr 1.07e-06 | grad 0.92 | tok/s 58105
step   1910 | loss 1.7244 | lr 6.93e-06 | grad 0.80 | tok/s 57444
step   1920 | loss 1.6403 | lr 2.68e-05 | grad 0.74 | tok/s 59193
step   1930 | loss 1.6205 | lr 5.89e-05 | grad 0.68 | tok/s 58320
step   1940 | loss 1.5374 | lr 9.99e-05 | grad 0.60 | tok/s 59513
step   1950 | loss 1.5994 | lr 1.46e-04 | grad 0.68 | tok/s 58170
step   1960 | loss 1.9941 | lr 1.92e-04 | grad 3.02 | tok/s 59453
step   1970 | loss 1.7088 | lr 2.35e-04 | grad 1.05 | tok/s 57087
step   1980 | loss 1.7920 | lr 2.69e-04 | grad 1.75 | tok/s 57294
step   1990 | loss 1.8963 | lr 2.91e-04 | grad 0.87 | tok/s 58479
step   2000 | loss 1.8141 | lr 3.00e-04 | grad 0.96 | tok/s 59238
  >>> saved checkpoint: checkpoint_step_002000_loss_1.8141.pt
step   2010 | loss 1.5331 | lr 2.94e-04 | grad 0.55 | tok/s 49576
step   2020 | loss 1.3602 | lr 2.74e-04 | grad 0.54 | tok/s 60876
step   2030 | loss 1.6697 | lr 2.42e-04 | grad 0.78 | tok/s 60254
step   2040 | loss 1.4981 | lr 2.01e-04 | grad 0.43 | tok/s 60892
step   2050 | loss 1.4386 | lr 1.55e-04 | grad 0.70 | tok/s 59652
step   2060 | loss 1.7467 | lr 1.09e-04 | grad 0.57 | tok/s 57193
step   2070 | loss 1.6734 | lr 6.65e-05 | grad 0.89 | tok/s 60263
step   2080 | loss 1.8106 | lr 3.24e-05 | grad 2.30 | tok/s 56858
step   2090 | loss 1.7300 | lr 9.84e-06 | grad 0.73 | tok/s 58609
step   2100 | loss 1.6886 | lr 1.07e-06 | grad 0.66 | tok/s 57180
step   2110 | loss 1.5630 | lr 6.93e-06 | grad 0.54 | tok/s 59213
step   2120 | loss 1.5735 | lr 2.68e-05 | grad 0.89 | tok/s 58269
step   2130 | loss 1.6670 | lr 5.89e-05 | grad 1.61 | tok/s 57036
step   2140 | loss 1.7160 | lr 9.99e-05 | grad 1.96 | tok/s 56805
step   2150 | loss 1.7329 | lr 1.46e-04 | grad 0.48 | tok/s 57725
step   2160 | loss 1.6925 | lr 1.92e-04 | grad 0.51 | tok/s 56732
step   2170 | loss 1.7995 | lr 2.35e-04 | grad 0.63 | tok/s 57829
step   2180 | loss 1.6905 | lr 2.69e-04 | grad 0.62 | tok/s 58933
step   2190 | loss 2.0098 | lr 2.91e-04 | grad 0.91 | tok/s 58095

Training complete! Final step: 2198
