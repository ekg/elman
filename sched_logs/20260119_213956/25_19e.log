# Job 25: 19e
# GPU: 1
# Command: python train.py --level 19e --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/19e
# Started: 2026-01-19T22:10:22.404233
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/19e/level19e_100m_20260119_221027
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level 19e, 98,532,480 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.2114 | lr 2.70e-05 | grad 6.91 | tok/s 16411
step     20 | loss 4.1552 | lr 5.70e-05 | grad 5.28 | tok/s 31443
step     30 | loss 4.8076 | lr 8.70e-05 | grad 2.77 | tok/s 33351
step     40 | loss 4.0300 | lr 1.17e-04 | grad 2.19 | tok/s 34757
step     50 | loss 3.3251 | lr 1.47e-04 | grad 2.42 | tok/s 32924
step     60 | loss 3.3270 | lr 1.77e-04 | grad 4.41 | tok/s 35330
step     70 | loss 2.9857 | lr 2.07e-04 | grad 2.88 | tok/s 53704
step     80 | loss 3.0939 | lr 2.37e-04 | grad 2.02 | tok/s 56665
step     90 | loss 3.1809 | lr 2.67e-04 | grad 3.06 | tok/s 54733
step    100 | loss 2.9013 | lr 2.97e-04 | grad 1.82 | tok/s 54730
step    110 | loss 2.8107 | lr 6.94e-06 | grad 1.95 | tok/s 51754
step    120 | loss 3.1154 | lr 2.69e-05 | grad 0.95 | tok/s 51516
step    130 | loss 2.9130 | lr 5.89e-05 | grad 0.98 | tok/s 52937
step    140 | loss 2.6876 | lr 9.99e-05 | grad 0.68 | tok/s 52889
step    150 | loss 2.5854 | lr 1.46e-04 | grad 1.80 | tok/s 50404
step    160 | loss 2.5009 | lr 1.92e-04 | grad 1.35 | tok/s 50869
step    170 | loss 2.6856 | lr 2.35e-04 | grad 3.48 | tok/s 52610
step    180 | loss 2.7641 | lr 2.69e-04 | grad 1.94 | tok/s 52228
step    190 | loss 2.5330 | lr 2.91e-04 | grad 2.16 | tok/s 53488
step    200 | loss 2.3460 | lr 3.00e-04 | grad 2.31 | tok/s 54725
step    210 | loss 2.6612 | lr 2.94e-04 | grad 1.82 | tok/s 52514
step    220 | loss 2.5724 | lr 2.74e-04 | grad 0.80 | tok/s 54227
step    230 | loss 2.3945 | lr 2.42e-04 | grad 1.05 | tok/s 52459
step    240 | loss 2.4586 | lr 2.01e-04 | grad 1.30 | tok/s 53473
step    250 | loss 2.3944 | lr 1.55e-04 | grad 1.16 | tok/s 52366
step    260 | loss 2.3808 | lr 1.09e-04 | grad 0.56 | tok/s 50246
step    270 | loss 2.2712 | lr 6.65e-05 | grad 0.67 | tok/s 52489
step    280 | loss 2.1701 | lr 3.24e-05 | grad 1.34 | tok/s 52394
step    290 | loss 2.2266 | lr 9.84e-06 | grad 0.57 | tok/s 55164
step    300 | loss 2.1998 | lr 1.07e-06 | grad 0.63 | tok/s 55153
step    310 | loss 2.2059 | lr 6.94e-06 | grad 0.52 | tok/s 54311
step    320 | loss 2.2352 | lr 2.69e-05 | grad 1.23 | tok/s 53122
step    330 | loss 2.2800 | lr 5.89e-05 | grad 0.67 | tok/s 51719
step    340 | loss 2.2997 | lr 9.99e-05 | grad 1.34 | tok/s 52566
step    350 | loss 2.3195 | lr 1.46e-04 | grad 0.88 | tok/s 51380
step    360 | loss 2.3015 | lr 1.92e-04 | grad 1.50 | tok/s 51927
step    370 | loss 2.2147 | lr 2.35e-04 | grad 1.31 | tok/s 53046
step    380 | loss 2.6761 | lr 2.69e-04 | grad 1.76 | tok/s 53841
step    390 | loss 2.2925 | lr 2.91e-04 | grad 1.66 | tok/s 51737
step    400 | loss 2.3919 | lr 3.00e-04 | grad 2.33 | tok/s 52680
step    410 | loss 2.1751 | lr 2.94e-04 | grad 2.05 | tok/s 51958
step    420 | loss 2.2969 | lr 2.74e-04 | grad 0.97 | tok/s 51868
step    430 | loss 2.3776 | lr 2.42e-04 | grad 1.31 | tok/s 51147
step    440 | loss 2.4817 | lr 2.01e-04 | grad 1.16 | tok/s 53210
step    450 | loss 2.1962 | lr 1.55e-04 | grad 0.86 | tok/s 51366
step    460 | loss 2.1796 | lr 1.09e-04 | grad 0.83 | tok/s 52386
step    470 | loss 2.1755 | lr 6.65e-05 | grad 1.63 | tok/s 53064
step    480 | loss 2.0649 | lr 3.24e-05 | grad 0.51 | tok/s 51325
step    490 | loss 2.0115 | lr 9.84e-06 | grad 0.46 | tok/s 52101
step    500 | loss 2.9192 | lr 1.07e-06 | grad 0.63 | tok/s 53927
step    510 | loss 2.0467 | lr 6.94e-06 | grad 0.43 | tok/s 52596
step    520 | loss 2.0846 | lr 2.69e-05 | grad 0.42 | tok/s 54046
step    530 | loss 2.5134 | lr 5.89e-05 | grad 0.77 | tok/s 53183
step    540 | loss 2.0389 | lr 9.99e-05 | grad 1.34 | tok/s 52950
step    550 | loss 1.9851 | lr 1.46e-04 | grad 0.79 | tok/s 54652
step    560 | loss 1.8763 | lr 1.92e-04 | grad 0.75 | tok/s 54818
step    570 | loss 2.1353 | lr 2.35e-04 | grad 1.83 | tok/s 53903
step    580 | loss 2.4682 | lr 2.69e-04 | grad 1.23 | tok/s 53246
step    590 | loss 2.7334 | lr 2.91e-04 | grad 1.98 | tok/s 52129
step    600 | loss 2.2810 | lr 3.00e-04 | grad 1.65 | tok/s 52487
step    610 | loss 2.2933 | lr 2.94e-04 | grad 0.86 | tok/s 54666
step    620 | loss 2.1180 | lr 2.74e-04 | grad 0.69 | tok/s 52074
step    630 | loss 2.0723 | lr 2.42e-04 | grad 1.24 | tok/s 53790
step    640 | loss 2.4316 | lr 2.01e-04 | grad 1.13 | tok/s 53423
step    650 | loss 2.0976 | lr 1.55e-04 | grad 1.00 | tok/s 52592
step    660 | loss 2.3756 | lr 1.09e-04 | grad 2.70 | tok/s 52011
step    670 | loss 2.1760 | lr 6.65e-05 | grad 1.33 | tok/s 53962
step    680 | loss 2.1115 | lr 3.24e-05 | grad 0.69 | tok/s 52016
step    690 | loss 2.1410 | lr 9.84e-06 | grad 0.68 | tok/s 52205
step    700 | loss 2.2198 | lr 1.07e-06 | grad 0.75 | tok/s 52785
step    710 | loss 2.1646 | lr 6.94e-06 | grad 0.66 | tok/s 52627
step    720 | loss 2.2698 | lr 2.68e-05 | grad 1.00 | tok/s 52455
step    730 | loss 2.2005 | lr 5.89e-05 | grad 0.79 | tok/s 53137
step    740 | loss 2.1185 | lr 9.99e-05 | grad 1.67 | tok/s 51674
step    750 | loss 1.9718 | lr 1.46e-04 | grad 0.93 | tok/s 52063
step    760 | loss 2.3339 | lr 1.92e-04 | grad 1.45 | tok/s 52658
step    770 | loss 2.0371 | lr 2.35e-04 | grad 0.96 | tok/s 52203
step    780 | loss 2.0723 | lr 2.69e-04 | grad 1.15 | tok/s 52920
step    790 | loss 2.0491 | lr 2.91e-04 | grad 1.12 | tok/s 53058
step    800 | loss 2.0612 | lr 3.00e-04 | grad 0.99 | tok/s 53435
step    810 | loss 2.1212 | lr 2.94e-04 | grad 1.87 | tok/s 52805
step    820 | loss 2.6843 | lr 2.74e-04 | grad 1.23 | tok/s 53514
step    830 | loss 2.3065 | lr 2.42e-04 | grad 0.73 | tok/s 55117
step    840 | loss 2.0688 | lr 2.01e-04 | grad 0.69 | tok/s 55082
step    850 | loss 2.4132 | lr 1.55e-04 | grad 0.86 | tok/s 52329
step    860 | loss 2.1841 | lr 1.09e-04 | grad 0.92 | tok/s 51238
step    870 | loss 2.1198 | lr 6.65e-05 | grad 1.00 | tok/s 52892
step    880 | loss 2.1791 | lr 3.24e-05 | grad 0.95 | tok/s 52597
step    890 | loss 2.0462 | lr 9.84e-06 | grad 0.55 | tok/s 52695
step    900 | loss 2.4482 | lr 1.07e-06 | grad 0.65 | tok/s 51336
step    910 | loss 2.1024 | lr 6.94e-06 | grad 0.55 | tok/s 52258
step    920 | loss 2.0672 | lr 2.68e-05 | grad 0.50 | tok/s 52102
step    930 | loss 2.1878 | lr 5.89e-05 | grad 0.84 | tok/s 51753
step    940 | loss 2.0763 | lr 9.99e-05 | grad 1.04 | tok/s 51351
step    950 | loss 2.1286 | lr 1.46e-04 | grad 1.50 | tok/s 52635
step    960 | loss 1.9614 | lr 1.92e-04 | grad 0.97 | tok/s 55144
step    970 | loss 1.7610 | lr 2.35e-04 | grad 0.75 | tok/s 55119
step    980 | loss 1.8921 | lr 2.69e-04 | grad 1.63 | tok/s 52932
step    990 | loss 2.2128 | lr 2.91e-04 | grad 0.93 | tok/s 51971
step   1000 | loss 2.0672 | lr 3.00e-04 | grad 0.87 | tok/s 50907
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0672.pt
step   1010 | loss 2.3267 | lr 2.94e-04 | grad 1.88 | tok/s 45130
step   1020 | loss 2.0107 | lr 2.74e-04 | grad 0.98 | tok/s 52063
step   1030 | loss 2.3050 | lr 2.42e-04 | grad 0.77 | tok/s 51513
step   1040 | loss 1.9377 | lr 2.01e-04 | grad 1.00 | tok/s 52749
step   1050 | loss 1.9366 | lr 1.55e-04 | grad 0.71 | tok/s 52868
step   1060 | loss 2.2312 | lr 1.09e-04 | grad 1.18 | tok/s 52535
step   1070 | loss 2.2721 | lr 6.65e-05 | grad 0.70 | tok/s 52821
step   1080 | loss 2.5324 | lr 3.24e-05 | grad 0.95 | tok/s 52510
step   1090 | loss 2.2705 | lr 9.84e-06 | grad 0.77 | tok/s 52708
step   1100 | loss 1.9524 | lr 1.07e-06 | grad 0.65 | tok/s 52242
step   1110 | loss 1.9946 | lr 6.93e-06 | grad 0.68 | tok/s 53159
step   1120 | loss 2.2596 | lr 2.68e-05 | grad 0.70 | tok/s 53502
step   1130 | loss 1.9496 | lr 5.89e-05 | grad 0.51 | tok/s 50008
step   1140 | loss 1.8502 | lr 9.99e-05 | grad 0.59 | tok/s 52437
step   1150 | loss 2.1597 | lr 1.46e-04 | grad 0.95 | tok/s 52203
step   1160 | loss 1.8109 | lr 1.92e-04 | grad 0.65 | tok/s 51935
step   1170 | loss 2.0979 | lr 2.35e-04 | grad 0.70 | tok/s 52248
step   1180 | loss 1.8491 | lr 2.69e-04 | grad 0.75 | tok/s 55009
step   1190 | loss 1.7535 | lr 2.91e-04 | grad 0.88 | tok/s 55184
step   1200 | loss 1.6787 | lr 3.00e-04 | grad 0.62 | tok/s 54690
step   1210 | loss 1.6246 | lr 2.94e-04 | grad 0.91 | tok/s 54926
step   1220 | loss 1.6644 | lr 2.74e-04 | grad 0.78 | tok/s 54677
step   1230 | loss 1.8421 | lr 2.42e-04 | grad 0.85 | tok/s 52703
step   1240 | loss 1.9260 | lr 2.01e-04 | grad 0.74 | tok/s 51806
step   1250 | loss 2.0088 | lr 1.55e-04 | grad 1.73 | tok/s 53553
step   1260 | loss 2.0579 | lr 1.09e-04 | grad 1.51 | tok/s 52985
step   1270 | loss 2.1048 | lr 6.65e-05 | grad 0.81 | tok/s 52260
step   1280 | loss 1.9413 | lr 3.24e-05 | grad 0.58 | tok/s 52032
step   1290 | loss 1.8708 | lr 9.84e-06 | grad 0.61 | tok/s 51951
step   1300 | loss 1.9166 | lr 1.07e-06 | grad 0.46 | tok/s 51442
step   1310 | loss 2.0510 | lr 6.93e-06 | grad 0.55 | tok/s 51568
step   1320 | loss 1.9886 | lr 2.68e-05 | grad 0.70 | tok/s 52488
step   1330 | loss 1.9231 | lr 5.89e-05 | grad 0.54 | tok/s 52271
step   1340 | loss 1.8555 | lr 9.99e-05 | grad 0.59 | tok/s 52186
step   1350 | loss 1.9487 | lr 1.46e-04 | grad 1.13 | tok/s 54071
step   1360 | loss 1.8699 | lr 1.92e-04 | grad 0.77 | tok/s 51275
step   1370 | loss 1.9746 | lr 2.35e-04 | grad 0.97 | tok/s 52082
step   1380 | loss 2.0729 | lr 2.69e-04 | grad 1.07 | tok/s 51771
step   1390 | loss 1.9503 | lr 2.91e-04 | grad 0.84 | tok/s 51019
step   1400 | loss 2.0385 | lr 3.00e-04 | grad 6.00 | tok/s 52713
step   1410 | loss 2.0920 | lr 2.94e-04 | grad 1.62 | tok/s 53503
step   1420 | loss 2.0663 | lr 2.74e-04 | grad 1.38 | tok/s 51174
step   1430 | loss 1.8353 | lr 2.42e-04 | grad 0.69 | tok/s 50053
step   1440 | loss 1.7374 | lr 2.01e-04 | grad 0.72 | tok/s 52731
step   1450 | loss 1.8142 | lr 1.55e-04 | grad 1.57 | tok/s 53960
step   1460 | loss 1.8140 | lr 1.09e-04 | grad 0.72 | tok/s 50253
step   1470 | loss 1.9332 | lr 6.65e-05 | grad 1.33 | tok/s 51789
step   1480 | loss 1.8177 | lr 3.24e-05 | grad 1.23 | tok/s 52612
step   1490 | loss 1.9514 | lr 9.84e-06 | grad 1.80 | tok/s 52337
step   1500 | loss 2.0381 | lr 1.07e-06 | grad 1.20 | tok/s 51131
step   1510 | loss 1.9591 | lr 6.93e-06 | grad 0.66 | tok/s 53870
step   1520 | loss 1.9270 | lr 2.68e-05 | grad 0.71 | tok/s 53255
step   1530 | loss 1.8577 | lr 5.89e-05 | grad 0.45 | tok/s 52423
step   1540 | loss 1.8453 | lr 9.99e-05 | grad 0.42 | tok/s 51514
step   1550 | loss 1.8328 | lr 1.46e-04 | grad 1.59 | tok/s 53905
step   1560 | loss 2.4515 | lr 1.92e-04 | grad 1.06 | tok/s 52679
step   1570 | loss 1.9293 | lr 2.35e-04 | grad 0.98 | tok/s 51570
step   1580 | loss 2.0993 | lr 2.69e-04 | grad 0.97 | tok/s 53344
step   1590 | loss 1.8439 | lr 2.91e-04 | grad 0.70 | tok/s 52051
step   1600 | loss 1.8994 | lr 3.00e-04 | grad 0.83 | tok/s 50668
step   1610 | loss 1.7748 | lr 2.94e-04 | grad 0.63 | tok/s 54054
step   1620 | loss 1.9262 | lr 2.74e-04 | grad 1.77 | tok/s 52636
step   1630 | loss 1.9822 | lr 2.42e-04 | grad 1.45 | tok/s 53779
step   1640 | loss 1.8547 | lr 2.01e-04 | grad 0.78 | tok/s 51871
step   1650 | loss 1.8493 | lr 1.55e-04 | grad 0.92 | tok/s 51020
step   1660 | loss 1.8335 | lr 1.09e-04 | grad 0.78 | tok/s 51221
step   1670 | loss 1.9292 | lr 6.65e-05 | grad 1.41 | tok/s 53308
step   1680 | loss 2.2915 | lr 3.24e-05 | grad 0.50 | tok/s 53370
step   1690 | loss 1.8434 | lr 9.84e-06 | grad 0.64 | tok/s 51665
step   1700 | loss 2.2910 | lr 1.07e-06 | grad 0.42 | tok/s 52918
step   1710 | loss 1.8420 | lr 6.93e-06 | grad 0.52 | tok/s 51784
step   1720 | loss 1.8804 | lr 2.68e-05 | grad 0.61 | tok/s 52167
step   1730 | loss 1.9762 | lr 5.89e-05 | grad 0.56 | tok/s 52175
step   1740 | loss 1.8800 | lr 9.99e-05 | grad 0.44 | tok/s 52718
step   1750 | loss 1.7682 | lr 1.46e-04 | grad 0.62 | tok/s 50999
step   1760 | loss 2.0908 | lr 1.92e-04 | grad 0.77 | tok/s 51775
step   1770 | loss 1.9785 | lr 2.35e-04 | grad 0.73 | tok/s 52991
step   1780 | loss 1.8652 | lr 2.69e-04 | grad 0.88 | tok/s 51042
step   1790 | loss 2.0906 | lr 2.91e-04 | grad 0.82 | tok/s 51884
step   1800 | loss 1.7811 | lr 3.00e-04 | grad 0.71 | tok/s 52848
step   1810 | loss 1.8509 | lr 2.94e-04 | grad 0.69 | tok/s 51992
step   1820 | loss 1.8219 | lr 2.74e-04 | grad 0.89 | tok/s 52073
step   1830 | loss 1.8503 | lr 2.42e-04 | grad 0.85 | tok/s 51663
step   1840 | loss 1.8180 | lr 2.01e-04 | grad 0.95 | tok/s 50855
step   1850 | loss 2.0208 | lr 1.55e-04 | grad 1.01 | tok/s 51766
step   1860 | loss 1.7901 | lr 1.09e-04 | grad 0.42 | tok/s 51750
step   1870 | loss 1.8544 | lr 6.65e-05 | grad 0.84 | tok/s 52773
step   1880 | loss 1.7702 | lr 3.24e-05 | grad 0.37 | tok/s 52655
step   1890 | loss 1.8719 | lr 9.84e-06 | grad 0.40 | tok/s 52079
step   1900 | loss 1.9761 | lr 1.07e-06 | grad 1.21 | tok/s 52659
step   1910 | loss 1.8637 | lr 6.93e-06 | grad 0.72 | tok/s 52002
step   1920 | loss 1.7879 | lr 2.68e-05 | grad 0.68 | tok/s 53629
step   1930 | loss 1.7740 | lr 5.89e-05 | grad 0.61 | tok/s 53240
step   1940 | loss 1.6672 | lr 9.99e-05 | grad 0.52 | tok/s 53950
step   1950 | loss 1.7532 | lr 1.46e-04 | grad 0.62 | tok/s 52252
step   1960 | loss 2.1873 | lr 1.92e-04 | grad 2.59 | tok/s 53939

Training complete! Final step: 1960
