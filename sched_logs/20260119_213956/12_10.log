# Job 12: 10
# GPU: 3
# Command: python train.py --level 10 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/10
# Started: 2026-01-19T21:50:06.722314
============================================================

/home/erikg/elman/elman/models/multiscale_elman.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  logit = torch.log(torch.tensor(decay / (1 - decay)))
Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/10/level10_100m_20260119_215012
Auto r_h_mode: none (level 10 has bounded/no W_h)
Model: Level 10, 180,529,280 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.1759 | lr 2.70e-05 | grad 6.56 | tok/s 15947
step     20 | loss 3.7734 | lr 5.70e-05 | grad 11.25 | tok/s 29444
step     30 | loss 4.0512 | lr 8.70e-05 | grad 3.08 | tok/s 31344
step     40 | loss 2.9086 | lr 1.17e-04 | grad 2.77 | tok/s 31371
step     50 | loss 2.4519 | lr 1.47e-04 | grad 1.69 | tok/s 31508
step     60 | loss 2.8289 | lr 1.77e-04 | grad 2.88 | tok/s 30910
step     70 | loss 2.5969 | lr 2.07e-04 | grad 1.66 | tok/s 29669
step     80 | loss 2.7742 | lr 2.37e-04 | grad 1.98 | tok/s 30982
step     90 | loss 2.8433 | lr 2.67e-04 | grad 2.88 | tok/s 29868
step    100 | loss 2.4940 | lr 2.97e-04 | grad 1.18 | tok/s 30130
step    110 | loss 2.4800 | lr 6.94e-06 | grad 2.20 | tok/s 29077
step    120 | loss 2.6352 | lr 2.69e-05 | grad 0.89 | tok/s 28885
step    130 | loss 2.5241 | lr 5.89e-05 | grad 0.85 | tok/s 29530
step    140 | loss 2.3134 | lr 9.99e-05 | grad 0.65 | tok/s 29549
step    150 | loss 2.2063 | lr 1.46e-04 | grad 1.28 | tok/s 28234
step    160 | loss 2.1134 | lr 1.92e-04 | grad 1.02 | tok/s 28443
step    170 | loss 2.3232 | lr 2.35e-04 | grad 3.56 | tok/s 29476
step    180 | loss 2.2956 | lr 2.69e-04 | grad 1.12 | tok/s 29510
step    190 | loss 2.0570 | lr 2.91e-04 | grad 1.09 | tok/s 30025
step    200 | loss 1.6901 | lr 3.00e-04 | grad 0.86 | tok/s 30695
step    210 | loss 2.2976 | lr 2.94e-04 | grad 1.19 | tok/s 29454
step    220 | loss 2.1383 | lr 2.74e-04 | grad 0.68 | tok/s 30398
step    230 | loss 2.0006 | lr 2.42e-04 | grad 0.95 | tok/s 29366
step    240 | loss 1.9846 | lr 2.01e-04 | grad 1.27 | tok/s 29963
step    250 | loss 1.9767 | lr 1.55e-04 | grad 0.89 | tok/s 29555
step    260 | loss 2.0307 | lr 1.09e-04 | grad 0.52 | tok/s 28396
step    270 | loss 1.8885 | lr 6.65e-05 | grad 0.80 | tok/s 29439
step    280 | loss 1.7735 | lr 3.24e-05 | grad 1.27 | tok/s 29408
step    290 | loss 1.7770 | lr 9.84e-06 | grad 0.56 | tok/s 31007
step    300 | loss 1.7431 | lr 1.07e-06 | grad 0.59 | tok/s 30991
step    310 | loss 1.7393 | lr 6.94e-06 | grad 0.51 | tok/s 30966
step    320 | loss 1.8350 | lr 2.69e-05 | grad 1.20 | tok/s 29844
step    330 | loss 1.8606 | lr 5.89e-05 | grad 0.56 | tok/s 29162
step    340 | loss 1.8581 | lr 9.99e-05 | grad 1.38 | tok/s 29737
step    350 | loss 1.8587 | lr 1.46e-04 | grad 0.75 | tok/s 28874
step    360 | loss 1.8288 | lr 1.92e-04 | grad 1.55 | tok/s 29267
step    370 | loss 1.6825 | lr 2.35e-04 | grad 0.75 | tok/s 29843
step    380 | loss 2.1131 | lr 2.69e-04 | grad 0.85 | tok/s 30576
step    390 | loss 1.8380 | lr 2.91e-04 | grad 1.36 | tok/s 29454
step    400 | loss 1.9400 | lr 3.00e-04 | grad 1.84 | tok/s 30231
step    410 | loss 1.6722 | lr 2.94e-04 | grad 1.47 | tok/s 29307
step    420 | loss 1.8185 | lr 2.74e-04 | grad 0.79 | tok/s 29093
step    430 | loss 1.9393 | lr 2.42e-04 | grad 0.89 | tok/s 28987
step    440 | loss 1.9482 | lr 2.01e-04 | grad 0.64 | tok/s 30161
step    450 | loss 1.7742 | lr 1.55e-04 | grad 0.54 | tok/s 29349
step    460 | loss 1.7327 | lr 1.09e-04 | grad 0.50 | tok/s 29429
step    470 | loss 1.7170 | lr 6.65e-05 | grad 0.76 | tok/s 29258
step    480 | loss 1.6097 | lr 3.24e-05 | grad 0.44 | tok/s 27337
step    490 | loss 1.6015 | lr 9.84e-06 | grad 0.43 | tok/s 27830
step    500 | loss 2.4802 | lr 1.07e-06 | grad 0.69 | tok/s 28714
step    510 | loss 1.6170 | lr 6.94e-06 | grad 0.47 | tok/s 28058
step    520 | loss 1.6845 | lr 2.69e-05 | grad 0.41 | tok/s 28974
step    530 | loss 2.1319 | lr 5.89e-05 | grad 0.51 | tok/s 28521
step    540 | loss 1.5839 | lr 9.99e-05 | grad 0.78 | tok/s 28329
step    550 | loss 1.5155 | lr 1.46e-04 | grad 0.52 | tok/s 29106
step    560 | loss 1.3900 | lr 1.92e-04 | grad 0.55 | tok/s 29504
step    570 | loss 1.6239 | lr 2.35e-04 | grad 1.29 | tok/s 28895
step    580 | loss 1.9196 | lr 2.69e-04 | grad 0.68 | tok/s 29044
step    590 | loss 2.1849 | lr 2.91e-04 | grad 1.38 | tok/s 28309
step    600 | loss 1.8121 | lr 3.00e-04 | grad 1.04 | tok/s 29141
step    610 | loss 1.7047 | lr 2.94e-04 | grad 0.82 | tok/s 30543
step    620 | loss 1.6948 | lr 2.74e-04 | grad 0.60 | tok/s 28992
step    630 | loss 1.5951 | lr 2.42e-04 | grad 0.55 | tok/s 29889
step    640 | loss 1.8342 | lr 2.01e-04 | grad 0.66 | tok/s 29933
step    650 | loss 1.6382 | lr 1.55e-04 | grad 0.73 | tok/s 29152
step    660 | loss 1.9164 | lr 1.09e-04 | grad 3.17 | tok/s 29051
step    670 | loss 1.7756 | lr 6.65e-05 | grad 1.28 | tok/s 30003
step    680 | loss 1.7118 | lr 3.24e-05 | grad 0.76 | tok/s 28940
step    690 | loss 1.7232 | lr 9.84e-06 | grad 0.90 | tok/s 29189
step    700 | loss 1.8257 | lr 1.07e-06 | grad 1.01 | tok/s 29321
step    710 | loss 1.7352 | lr 6.94e-06 | grad 0.77 | tok/s 29241
step    720 | loss 1.8077 | lr 2.68e-05 | grad 1.02 | tok/s 29394
step    730 | loss 1.7936 | lr 5.89e-05 | grad 0.82 | tok/s 29591
step    740 | loss 1.7369 | lr 9.99e-05 | grad 1.35 | tok/s 29415
step    750 | loss 1.5512 | lr 1.46e-04 | grad 0.91 | tok/s 29067
step    760 | loss 1.9340 | lr 1.92e-04 | grad 0.53 | tok/s 29350
step    770 | loss 1.6082 | lr 2.35e-04 | grad 0.79 | tok/s 29117
step    780 | loss 1.6488 | lr 2.69e-04 | grad 0.57 | tok/s 29790
step    790 | loss 1.5714 | lr 2.91e-04 | grad 0.48 | tok/s 30063
step    800 | loss 1.5714 | lr 3.00e-04 | grad 0.68 | tok/s 30061
step    810 | loss 1.6415 | lr 2.94e-04 | grad 1.21 | tok/s 29789
step    820 | loss 2.3089 | lr 2.74e-04 | grad 1.42 | tok/s 30545
step    830 | loss 1.8086 | lr 2.42e-04 | grad 0.55 | tok/s 30971
step    840 | loss 1.5231 | lr 2.01e-04 | grad 0.46 | tok/s 31049
step    850 | loss 1.8924 | lr 1.55e-04 | grad 0.93 | tok/s 29516
step    860 | loss 1.6612 | lr 1.09e-04 | grad 0.68 | tok/s 28946
step    870 | loss 1.6007 | lr 6.65e-05 | grad 0.60 | tok/s 29791
step    880 | loss 1.6586 | lr 3.24e-05 | grad 0.79 | tok/s 29665
step    890 | loss 1.5927 | lr 9.84e-06 | grad 0.60 | tok/s 29610
step    900 | loss 1.9924 | lr 1.07e-06 | grad 0.61 | tok/s 28865
step    910 | loss 1.6317 | lr 6.94e-06 | grad 0.47 | tok/s 29397
step    920 | loss 1.6287 | lr 2.68e-05 | grad 0.51 | tok/s 29253
step    930 | loss 1.7188 | lr 5.89e-05 | grad 1.00 | tok/s 29195
step    940 | loss 1.6162 | lr 9.99e-05 | grad 0.96 | tok/s 28928
step    950 | loss 1.6809 | lr 1.46e-04 | grad 0.84 | tok/s 29620
step    960 | loss 1.4543 | lr 1.92e-04 | grad 0.43 | tok/s 31031
step    970 | loss 1.3075 | lr 2.35e-04 | grad 0.39 | tok/s 30943
step    980 | loss 1.4495 | lr 2.69e-04 | grad 1.48 | tok/s 30088
step    990 | loss 1.7420 | lr 2.91e-04 | grad 0.52 | tok/s 29081
step   1000 | loss 1.6463 | lr 3.00e-04 | grad 0.48 | tok/s 28590
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6463.pt
step   1010 | loss 1.8032 | lr 2.94e-04 | grad 0.81 | tok/s 24745
step   1020 | loss 1.5090 | lr 2.74e-04 | grad 0.59 | tok/s 29399
step   1030 | loss 1.9056 | lr 2.42e-04 | grad 0.56 | tok/s 28625
step   1040 | loss 1.5789 | lr 2.01e-04 | grad 0.93 | tok/s 29394
step   1050 | loss 1.5829 | lr 1.55e-04 | grad 0.51 | tok/s 29468
step   1060 | loss 1.7186 | lr 1.09e-04 | grad 1.02 | tok/s 29668
step   1070 | loss 1.8354 | lr 6.65e-05 | grad 0.57 | tok/s 29774
step   1080 | loss 2.1981 | lr 3.24e-05 | grad 0.86 | tok/s 29407
step   1090 | loss 1.9533 | lr 9.84e-06 | grad 0.61 | tok/s 29643
step   1100 | loss 1.6407 | lr 1.07e-06 | grad 0.48 | tok/s 29099
step   1110 | loss 1.6878 | lr 6.93e-06 | grad 0.58 | tok/s 29024

Training complete! Final step: 1116
