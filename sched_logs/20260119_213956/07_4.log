# Job 7: 4
# GPU: 7
# Command: python train.py --level 4 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/4
# Started: 2026-01-19T21:39:56.178392
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/4/level4_100m_20260119_214002
Auto r_h_mode: none (level 4 has bounded/no W_h)
Model: Level 4, 90,314,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4267 | lr 2.70e-05 | grad 10.69 | tok/s 12071
step     20 | loss 4.3901 | lr 5.70e-05 | grad 8.81 | tok/s 18726
step     30 | loss 4.5606 | lr 8.70e-05 | grad 4.41 | tok/s 19819
step     40 | loss 3.3523 | lr 1.17e-04 | grad 3.11 | tok/s 19802
step     50 | loss 2.6806 | lr 1.47e-04 | grad 2.36 | tok/s 19665
step     60 | loss 2.8488 | lr 1.77e-04 | grad 3.81 | tok/s 19376
step     70 | loss 2.5264 | lr 2.07e-04 | grad 2.38 | tok/s 18615
step     80 | loss 2.8322 | lr 2.37e-04 | grad 2.88 | tok/s 19262
step     90 | loss 2.7846 | lr 2.67e-04 | grad 3.72 | tok/s 18570
step    100 | loss 2.4130 | lr 2.97e-04 | grad 1.04 | tok/s 18854
step    110 | loss 2.4373 | lr 6.94e-06 | grad 2.30 | tok/s 18171
step    120 | loss 2.6584 | lr 2.69e-05 | grad 1.47 | tok/s 17956
step    130 | loss 2.5189 | lr 5.89e-05 | grad 1.00 | tok/s 18498
step    140 | loss 2.2795 | lr 9.99e-05 | grad 0.86 | tok/s 18649
step    150 | loss 2.1733 | lr 1.46e-04 | grad 1.74 | tok/s 17701
step    160 | loss 2.0752 | lr 1.92e-04 | grad 1.31 | tok/s 17732
step    170 | loss 2.2786 | lr 2.35e-04 | grad 3.03 | tok/s 18568
step    180 | loss 2.2766 | lr 2.69e-04 | grad 1.37 | tok/s 18768
step    190 | loss 2.0208 | lr 2.91e-04 | grad 1.28 | tok/s 19884
step    200 | loss 1.6904 | lr 3.00e-04 | grad 1.11 | tok/s 20163
step    210 | loss 2.3264 | lr 2.94e-04 | grad 1.48 | tok/s 19488
step    220 | loss 2.1958 | lr 2.74e-04 | grad 0.95 | tok/s 20045
step    230 | loss 2.0388 | lr 2.42e-04 | grad 1.17 | tok/s 19345
step    240 | loss 2.0522 | lr 2.01e-04 | grad 1.30 | tok/s 19753
step    250 | loss 2.0244 | lr 1.55e-04 | grad 1.05 | tok/s 19511
step    260 | loss 2.0848 | lr 1.09e-04 | grad 0.62 | tok/s 18656
step    270 | loss 1.9403 | lr 6.65e-05 | grad 0.91 | tok/s 19366
step    280 | loss 1.8357 | lr 3.24e-05 | grad 1.40 | tok/s 19410
step    290 | loss 1.8343 | lr 9.84e-06 | grad 0.75 | tok/s 20463
step    300 | loss 1.8007 | lr 1.07e-06 | grad 0.72 | tok/s 20519
step    310 | loss 1.8009 | lr 6.94e-06 | grad 0.66 | tok/s 20578
step    320 | loss 1.8942 | lr 2.69e-05 | grad 1.52 | tok/s 19819
step    330 | loss 1.9289 | lr 5.89e-05 | grad 0.64 | tok/s 19324
step    340 | loss 1.9408 | lr 9.99e-05 | grad 1.79 | tok/s 19642
step    350 | loss 1.9429 | lr 1.46e-04 | grad 0.80 | tok/s 18999
step    360 | loss 1.9159 | lr 1.92e-04 | grad 2.03 | tok/s 19166
step    370 | loss 1.7707 | lr 2.35e-04 | grad 0.94 | tok/s 19582
step    380 | loss 2.2894 | lr 2.69e-04 | grad 1.09 | tok/s 20076
step    390 | loss 1.9446 | lr 2.91e-04 | grad 1.08 | tok/s 19835
step    400 | loss 2.0611 | lr 3.00e-04 | grad 2.38 | tok/s 20905
step    410 | loss 1.8308 | lr 2.94e-04 | grad 1.70 | tok/s 20324
step    420 | loss 1.9367 | lr 2.74e-04 | grad 1.02 | tok/s 20094
step    430 | loss 2.0838 | lr 2.42e-04 | grad 0.98 | tok/s 20142
step    440 | loss 2.1460 | lr 2.01e-04 | grad 0.95 | tok/s 21025
step    450 | loss 1.9047 | lr 1.55e-04 | grad 0.67 | tok/s 20407
step    460 | loss 1.8567 | lr 1.09e-04 | grad 0.60 | tok/s 20573
step    470 | loss 1.8436 | lr 6.65e-05 | grad 0.85 | tok/s 20766
step    480 | loss 1.7342 | lr 3.24e-05 | grad 0.56 | tok/s 19040
step    490 | loss 1.7121 | lr 9.84e-06 | grad 0.51 | tok/s 18535
step    500 | loss 2.6898 | lr 1.07e-06 | grad 0.85 | tok/s 19164
step    510 | loss 1.7033 | lr 6.94e-06 | grad 0.55 | tok/s 18652
step    520 | loss 1.7926 | lr 2.69e-05 | grad 0.48 | tok/s 19135
step    530 | loss 2.3058 | lr 5.89e-05 | grad 0.50 | tok/s 18689
step    540 | loss 1.7216 | lr 9.99e-05 | grad 0.81 | tok/s 18800
step    550 | loss 1.6323 | lr 1.46e-04 | grad 0.61 | tok/s 19627
step    560 | loss 1.4938 | lr 1.92e-04 | grad 0.54 | tok/s 19406
step    570 | loss 1.7914 | lr 2.35e-04 | grad 1.73 | tok/s 18975
step    580 | loss 2.0876 | lr 2.69e-04 | grad 0.90 | tok/s 18724
step    590 | loss 2.3812 | lr 2.91e-04 | grad 1.20 | tok/s 18361
step    600 | loss 1.8841 | lr 3.00e-04 | grad 1.34 | tok/s 18467
step    610 | loss 1.9083 | lr 2.94e-04 | grad 1.07 | tok/s 19333
step    620 | loss 1.8315 | lr 2.74e-04 | grad 0.75 | tok/s 18284
step    630 | loss 1.7341 | lr 2.42e-04 | grad 0.71 | tok/s 18872
step    640 | loss 2.0897 | lr 2.01e-04 | grad 0.85 | tok/s 18879
step    650 | loss 1.7675 | lr 1.55e-04 | grad 0.88 | tok/s 18523
step    660 | loss 2.1120 | lr 1.09e-04 | grad 4.41 | tok/s 18314
step    670 | loss 1.9019 | lr 6.65e-05 | grad 1.38 | tok/s 18944
step    680 | loss 1.8638 | lr 3.24e-05 | grad 0.83 | tok/s 18247
step    690 | loss 1.8498 | lr 9.84e-06 | grad 1.06 | tok/s 18403
step    700 | loss 1.9536 | lr 1.07e-06 | grad 1.13 | tok/s 18480
step    710 | loss 1.8825 | lr 6.94e-06 | grad 0.86 | tok/s 18595
step    720 | loss 1.9519 | lr 2.68e-05 | grad 1.11 | tok/s 18648

Training complete! Final step: 727
