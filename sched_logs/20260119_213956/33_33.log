# Job 33: 33
# GPU: 5
# Command: python train.py --level 33 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/33
# Started: 2026-01-19T22:10:41.310679
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/33/level33_100m_20260119_221048
Auto r_h_mode: spectral_norm (level 33 has full W_h)
Model: Level 33, 98,506,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.8169 | lr 2.70e-05 | grad 4.94 | tok/s 15926
step     20 | loss 3.6758 | lr 5.70e-05 | grad 8.62 | tok/s 39004
step     30 | loss 4.4044 | lr 8.70e-05 | grad 4.41 | tok/s 54343
step     40 | loss 3.2377 | lr 1.17e-04 | grad 3.03 | tok/s 54158
step     50 | loss 2.5790 | lr 1.47e-04 | grad 3.06 | tok/s 53300
step     60 | loss 2.7969 | lr 1.77e-04 | grad 3.86 | tok/s 52895
step     70 | loss 2.5309 | lr 2.07e-04 | grad 2.36 | tok/s 50187
step     80 | loss 2.6402 | lr 2.37e-04 | grad 3.12 | tok/s 52787
step     90 | loss 2.8048 | lr 2.67e-04 | grad 3.61 | tok/s 50881
step    100 | loss 2.3935 | lr 2.97e-04 | grad 1.51 | tok/s 51521
step    110 | loss 2.4088 | lr 6.94e-06 | grad 2.16 | tok/s 48683
step    120 | loss 2.6449 | lr 2.69e-05 | grad 1.23 | tok/s 48549
step    130 | loss 2.4416 | lr 5.89e-05 | grad 1.29 | tok/s 48862
step    140 | loss 2.2144 | lr 9.99e-05 | grad 0.96 | tok/s 49454
step    150 | loss 2.1085 | lr 1.46e-04 | grad 2.20 | tok/s 47264
step    160 | loss 2.0022 | lr 1.92e-04 | grad 1.62 | tok/s 48145
step    170 | loss 2.1985 | lr 2.35e-04 | grad 3.84 | tok/s 49581
step    180 | loss 2.1801 | lr 2.69e-04 | grad 1.53 | tok/s 49818
step    190 | loss 1.9557 | lr 2.91e-04 | grad 1.66 | tok/s 50191
step    200 | loss 1.5274 | lr 3.00e-04 | grad 1.23 | tok/s 51149
step    210 | loss 2.2547 | lr 2.94e-04 | grad 1.86 | tok/s 49464
step    220 | loss 2.0715 | lr 2.74e-04 | grad 1.35 | tok/s 51173
step    230 | loss 1.9535 | lr 2.42e-04 | grad 1.87 | tok/s 49201
step    240 | loss 1.9296 | lr 2.01e-04 | grad 1.77 | tok/s 50283
step    250 | loss 1.9256 | lr 1.55e-04 | grad 1.27 | tok/s 48975
step    260 | loss 1.9899 | lr 1.09e-04 | grad 0.93 | tok/s 46831
step    270 | loss 1.8513 | lr 6.65e-05 | grad 1.03 | tok/s 49098
step    280 | loss 1.7265 | lr 3.24e-05 | grad 1.52 | tok/s 48504
step    290 | loss 1.7123 | lr 9.84e-06 | grad 0.85 | tok/s 51443
step    300 | loss 1.6783 | lr 1.07e-06 | grad 0.86 | tok/s 51759
step    310 | loss 1.6706 | lr 6.94e-06 | grad 0.72 | tok/s 51841
step    320 | loss 1.7804 | lr 2.69e-05 | grad 1.68 | tok/s 48981
step    330 | loss 1.8130 | lr 5.89e-05 | grad 0.85 | tok/s 47832
step    340 | loss 1.8199 | lr 9.99e-05 | grad 2.27 | tok/s 48083
step    350 | loss 1.8352 | lr 1.46e-04 | grad 1.05 | tok/s 48127
step    360 | loss 1.8197 | lr 1.92e-04 | grad 2.62 | tok/s 48851
step    370 | loss 1.6557 | lr 2.35e-04 | grad 1.23 | tok/s 48726
step    380 | loss 2.1600 | lr 2.69e-04 | grad 1.32 | tok/s 49909
step    390 | loss 1.8372 | lr 2.91e-04 | grad 1.55 | tok/s 48894
step    400 | loss 1.9385 | lr 3.00e-04 | grad 2.86 | tok/s 50738
step    410 | loss 1.7088 | lr 2.94e-04 | grad 2.12 | tok/s 49244
step    420 | loss 1.8493 | lr 2.74e-04 | grad 1.29 | tok/s 48930
step    430 | loss 1.9681 | lr 2.42e-04 | grad 1.48 | tok/s 48967
step    440 | loss 1.9920 | lr 2.01e-04 | grad 1.34 | tok/s 50913
step    450 | loss 1.8156 | lr 1.55e-04 | grad 0.80 | tok/s 48817
step    460 | loss 1.7613 | lr 1.09e-04 | grad 0.87 | tok/s 49598
step    470 | loss 1.7465 | lr 6.65e-05 | grad 1.20 | tok/s 49899
step    480 | loss 1.6448 | lr 3.24e-05 | grad 0.65 | tok/s 47861
step    490 | loss 1.6186 | lr 9.84e-06 | grad 0.61 | tok/s 49084
step    500 | loss 2.5610 | lr 1.07e-06 | grad 1.05 | tok/s 50645
step    510 | loss 1.6358 | lr 6.94e-06 | grad 0.71 | tok/s 49233
step    520 | loss 1.7073 | lr 2.69e-05 | grad 0.56 | tok/s 51020
step    530 | loss 2.2031 | lr 5.89e-05 | grad 0.77 | tok/s 49917
step    540 | loss 1.6255 | lr 9.99e-05 | grad 0.99 | tok/s 50173
step    550 | loss 1.5304 | lr 1.46e-04 | grad 0.71 | tok/s 51339
step    560 | loss 1.3944 | lr 1.92e-04 | grad 0.74 | tok/s 51387
step    570 | loss 1.6801 | lr 2.35e-04 | grad 2.09 | tok/s 50747
step    580 | loss 2.0691 | lr 2.69e-04 | grad 1.27 | tok/s 48856
step    590 | loss 2.3423 | lr 2.91e-04 | grad 2.25 | tok/s 48411
step    600 | loss 1.8490 | lr 3.00e-04 | grad 1.46 | tok/s 48613
step    610 | loss 1.7344 | lr 2.94e-04 | grad 1.20 | tok/s 50389
step    620 | loss 1.7528 | lr 2.74e-04 | grad 0.97 | tok/s 47604
step    630 | loss 1.6458 | lr 2.42e-04 | grad 0.85 | tok/s 49704
step    640 | loss 1.9401 | lr 2.01e-04 | grad 1.05 | tok/s 48961
step    650 | loss 1.7074 | lr 1.55e-04 | grad 1.13 | tok/s 48567
step    660 | loss 1.9896 | lr 1.09e-04 | grad 4.75 | tok/s 48065
step    670 | loss 1.8309 | lr 6.65e-05 | grad 1.75 | tok/s 50372
step    680 | loss 1.7733 | lr 3.24e-05 | grad 1.00 | tok/s 49257
step    690 | loss 1.7810 | lr 9.84e-06 | grad 1.30 | tok/s 49450
step    700 | loss 1.8724 | lr 1.07e-06 | grad 1.47 | tok/s 49175
step    710 | loss 1.7961 | lr 6.94e-06 | grad 1.05 | tok/s 50179
step    720 | loss 1.8796 | lr 2.68e-05 | grad 1.44 | tok/s 49709
step    730 | loss 1.8633 | lr 5.89e-05 | grad 1.24 | tok/s 49452
step    740 | loss 1.7991 | lr 9.99e-05 | grad 1.89 | tok/s 49853
step    750 | loss 1.6023 | lr 1.46e-04 | grad 1.31 | tok/s 49342
step    760 | loss 1.9489 | lr 1.92e-04 | grad 0.74 | tok/s 49710
step    770 | loss 1.6851 | lr 2.35e-04 | grad 1.23 | tok/s 49386
step    780 | loss 1.7312 | lr 2.69e-04 | grad 0.91 | tok/s 49990
step    790 | loss 1.6426 | lr 2.91e-04 | grad 0.70 | tok/s 50513
step    800 | loss 1.6099 | lr 3.00e-04 | grad 1.02 | tok/s 50774
step    810 | loss 1.7339 | lr 2.94e-04 | grad 1.73 | tok/s 50247
step    820 | loss 2.3920 | lr 2.74e-04 | grad 1.41 | tok/s 51509
step    830 | loss 1.8316 | lr 2.42e-04 | grad 0.82 | tok/s 52068
step    840 | loss 1.5356 | lr 2.01e-04 | grad 0.68 | tok/s 52419
step    850 | loss 1.9429 | lr 1.55e-04 | grad 1.19 | tok/s 49772
step    860 | loss 1.7095 | lr 1.09e-04 | grad 0.95 | tok/s 49017
step    870 | loss 1.6495 | lr 6.65e-05 | grad 0.91 | tok/s 50169
step    880 | loss 1.7122 | lr 3.24e-05 | grad 1.09 | tok/s 49328
step    890 | loss 1.6310 | lr 9.84e-06 | grad 0.87 | tok/s 49691
step    900 | loss 2.0782 | lr 1.07e-06 | grad 0.88 | tok/s 48499
step    910 | loss 1.6873 | lr 6.94e-06 | grad 0.66 | tok/s 49354
step    920 | loss 1.6703 | lr 2.68e-05 | grad 0.72 | tok/s 48598
step    930 | loss 1.7657 | lr 5.89e-05 | grad 1.55 | tok/s 49291
step    940 | loss 1.6716 | lr 9.99e-05 | grad 1.53 | tok/s 48844
step    950 | loss 1.7633 | lr 1.46e-04 | grad 1.28 | tok/s 50044
step    960 | loss 1.5097 | lr 1.92e-04 | grad 0.62 | tok/s 51655
step    970 | loss 1.3371 | lr 2.35e-04 | grad 0.49 | tok/s 52168
step    980 | loss 1.5159 | lr 2.69e-04 | grad 1.91 | tok/s 51078
step    990 | loss 1.8762 | lr 2.91e-04 | grad 1.04 | tok/s 49261
step   1000 | loss 1.7497 | lr 3.00e-04 | grad 0.71 | tok/s 47980
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7497.pt
step   1010 | loss 1.9209 | lr 2.94e-04 | grad 1.34 | tok/s 40805
step   1020 | loss 1.6035 | lr 2.74e-04 | grad 0.90 | tok/s 47577
step   1030 | loss 2.0010 | lr 2.42e-04 | grad 0.80 | tok/s 48398
step   1040 | loss 1.6465 | lr 2.01e-04 | grad 1.40 | tok/s 49770
step   1050 | loss 1.6576 | lr 1.55e-04 | grad 0.85 | tok/s 49757
step   1060 | loss 1.8448 | lr 1.09e-04 | grad 1.36 | tok/s 49810
step   1070 | loss 1.9492 | lr 6.65e-05 | grad 0.91 | tok/s 50044
step   1080 | loss 2.3662 | lr 3.24e-05 | grad 1.05 | tok/s 49589
step   1090 | loss 2.0359 | lr 9.84e-06 | grad 0.85 | tok/s 49493
step   1100 | loss 1.7226 | lr 1.07e-06 | grad 0.73 | tok/s 49406
step   1110 | loss 1.7072 | lr 6.93e-06 | grad 0.80 | tok/s 50304
step   1120 | loss 1.8915 | lr 2.68e-05 | grad 0.79 | tok/s 51030
step   1130 | loss 1.6983 | lr 5.89e-05 | grad 0.74 | tok/s 48603
step   1140 | loss 1.5620 | lr 9.99e-05 | grad 0.66 | tok/s 49848
step   1150 | loss 1.8639 | lr 1.46e-04 | grad 1.05 | tok/s 48828
step   1160 | loss 1.5394 | lr 1.92e-04 | grad 0.71 | tok/s 48801
step   1170 | loss 1.8603 | lr 2.35e-04 | grad 0.86 | tok/s 50074
step   1180 | loss 1.5615 | lr 2.69e-04 | grad 0.75 | tok/s 52287
step   1190 | loss 1.4128 | lr 2.91e-04 | grad 0.65 | tok/s 52461
step   1200 | loss 1.3311 | lr 3.00e-04 | grad 0.53 | tok/s 52303
step   1210 | loss 1.3067 | lr 2.94e-04 | grad 0.67 | tok/s 51885
step   1220 | loss 1.3597 | lr 2.74e-04 | grad 1.00 | tok/s 51414
step   1230 | loss 1.5905 | lr 2.42e-04 | grad 0.85 | tok/s 50098
step   1240 | loss 1.6501 | lr 2.01e-04 | grad 0.70 | tok/s 49234
step   1250 | loss 1.7401 | lr 1.55e-04 | grad 3.02 | tok/s 50822
step   1260 | loss 1.7994 | lr 1.09e-04 | grad 2.38 | tok/s 49823
step   1270 | loss 1.8398 | lr 6.65e-05 | grad 1.20 | tok/s 49731
step   1280 | loss 1.6751 | lr 3.24e-05 | grad 0.74 | tok/s 48684
step   1290 | loss 1.6236 | lr 9.84e-06 | grad 0.76 | tok/s 48953
step   1300 | loss 1.6720 | lr 1.07e-06 | grad 0.70 | tok/s 48909
step   1310 | loss 1.7578 | lr 6.93e-06 | grad 0.67 | tok/s 48967
step   1320 | loss 1.7264 | lr 2.68e-05 | grad 1.05 | tok/s 49898
step   1330 | loss 1.6460 | lr 5.89e-05 | grad 0.60 | tok/s 49776
step   1340 | loss 1.5767 | lr 9.99e-05 | grad 0.96 | tok/s 49569
step   1350 | loss 1.5985 | lr 1.46e-04 | grad 1.51 | tok/s 50676
step   1360 | loss 1.5723 | lr 1.92e-04 | grad 0.75 | tok/s 48377
step   1370 | loss 1.6720 | lr 2.35e-04 | grad 0.83 | tok/s 49376
step   1380 | loss 1.7643 | lr 2.69e-04 | grad 0.95 | tok/s 49668
step   1390 | loss 1.6857 | lr 2.91e-04 | grad 1.86 | tok/s 48618
step   1400 | loss 1.7100 | lr 3.00e-04 | grad 6.22 | tok/s 50354
step   1410 | loss 1.6991 | lr 2.94e-04 | grad 1.25 | tok/s 49774
step   1420 | loss 1.7740 | lr 2.74e-04 | grad 1.05 | tok/s 48212
step   1430 | loss 1.5743 | lr 2.42e-04 | grad 0.84 | tok/s 47239
step   1440 | loss 1.4902 | lr 2.01e-04 | grad 0.63 | tok/s 49965
step   1450 | loss 1.5175 | lr 1.55e-04 | grad 1.89 | tok/s 50819
step   1460 | loss 1.5915 | lr 1.09e-04 | grad 0.56 | tok/s 47518
step   1470 | loss 1.7042 | lr 6.65e-05 | grad 1.81 | tok/s 48843
step   1480 | loss 1.5757 | lr 3.24e-05 | grad 1.75 | tok/s 49865
step   1490 | loss 1.7050 | lr 9.84e-06 | grad 2.06 | tok/s 49133
step   1500 | loss 1.8158 | lr 1.07e-06 | grad 1.92 | tok/s 48571
step   1510 | loss 1.7088 | lr 6.93e-06 | grad 0.93 | tok/s 50522
step   1520 | loss 1.6535 | lr 2.68e-05 | grad 1.15 | tok/s 49375
step   1530 | loss 1.6081 | lr 5.89e-05 | grad 0.54 | tok/s 47199
step   1540 | loss 1.5809 | lr 9.99e-05 | grad 0.53 | tok/s 47753
step   1550 | loss 1.5510 | lr 1.46e-04 | grad 1.96 | tok/s 49476
step   1560 | loss 2.1594 | lr 1.92e-04 | grad 1.53 | tok/s 47661
step   1570 | loss 1.6294 | lr 2.35e-04 | grad 0.97 | tok/s 47457
step   1580 | loss 1.7860 | lr 2.69e-04 | grad 1.12 | tok/s 49473
step   1590 | loss 1.6424 | lr 2.91e-04 | grad 0.87 | tok/s 48250
step   1600 | loss 1.7036 | lr 3.00e-04 | grad 1.09 | tok/s 46844
step   1610 | loss 1.5495 | lr 2.94e-04 | grad 0.65 | tok/s 50093
step   1620 | loss 1.6875 | lr 2.74e-04 | grad 0.68 | tok/s 49284
step   1630 | loss 1.6652 | lr 2.42e-04 | grad 0.81 | tok/s 49738
step   1640 | loss 1.6054 | lr 2.01e-04 | grad 0.64 | tok/s 48058
step   1650 | loss 1.6179 | lr 1.55e-04 | grad 1.27 | tok/s 47163
step   1660 | loss 1.6169 | lr 1.09e-04 | grad 0.68 | tok/s 47192
step   1670 | loss 1.6732 | lr 6.65e-05 | grad 1.95 | tok/s 50164
step   1680 | loss 2.0096 | lr 3.24e-05 | grad 0.68 | tok/s 50599
step   1690 | loss 1.5876 | lr 9.84e-06 | grad 0.90 | tok/s 48775
step   1700 | loss 1.9249 | lr 1.07e-06 | grad 0.57 | tok/s 50410
step   1710 | loss 1.6398 | lr 6.93e-06 | grad 0.86 | tok/s 49078
step   1720 | loss 1.6311 | lr 2.68e-05 | grad 0.79 | tok/s 49014
step   1730 | loss 1.7398 | lr 5.89e-05 | grad 0.70 | tok/s 49130
step   1740 | loss 1.6327 | lr 9.99e-05 | grad 0.55 | tok/s 49938
step   1750 | loss 1.5497 | lr 1.46e-04 | grad 0.55 | tok/s 48495
step   1760 | loss 1.8089 | lr 1.92e-04 | grad 0.86 | tok/s 49232
step   1770 | loss 1.7187 | lr 2.35e-04 | grad 0.63 | tok/s 50307
step   1780 | loss 1.6255 | lr 2.69e-04 | grad 1.05 | tok/s 48525
step   1790 | loss 1.8167 | lr 2.91e-04 | grad 0.86 | tok/s 48932
step   1800 | loss 1.5459 | lr 3.00e-04 | grad 0.59 | tok/s 49380
step   1810 | loss 1.6383 | lr 2.94e-04 | grad 0.82 | tok/s 50080
step   1820 | loss 1.5809 | lr 2.74e-04 | grad 0.84 | tok/s 50130
step   1830 | loss 1.6121 | lr 2.42e-04 | grad 0.71 | tok/s 49593
step   1840 | loss 1.6187 | lr 2.01e-04 | grad 1.41 | tok/s 48551
step   1850 | loss 1.8457 | lr 1.55e-04 | grad 0.92 | tok/s 48404
step   1860 | loss 1.5752 | lr 1.09e-04 | grad 0.49 | tok/s 49035
step   1870 | loss 1.6157 | lr 6.65e-05 | grad 1.07 | tok/s 49178

Training complete! Final step: 1872
