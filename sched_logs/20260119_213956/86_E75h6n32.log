# Job 86: E75h6n32
# GPU: 4
# Command: python train.py --level E75h6n32 --dim 1152 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/E75h6n32
# Started: 2026-01-19T23:11:50.617457
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/E75h6n32/levelE75h6n32_100m_20260119_231155
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75h6n32, 93,220,224 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.6462 | lr 2.70e-05 | grad 114.00 | tok/s 22648
step     20 | loss 5.4604 | lr 5.70e-05 | grad 49.75 | tok/s 61481
step     30 | loss 5.3330 | lr 8.70e-05 | grad 27.25 | tok/s 64884
step     40 | loss 4.7688 | lr 1.17e-04 | grad 11.69 | tok/s 64908
step     50 | loss 4.1390 | lr 1.47e-04 | grad 6.56 | tok/s 65099
step     60 | loss 3.8137 | lr 1.77e-04 | grad 9.38 | tok/s 63539
step     70 | loss 3.1918 | lr 2.07e-04 | grad 3.03 | tok/s 61414
step     80 | loss 3.3220 | lr 2.37e-04 | grad 3.73 | tok/s 63760
step     90 | loss 3.0802 | lr 2.67e-04 | grad 4.94 | tok/s 61438
step    100 | loss 2.7270 | lr 2.97e-04 | grad 1.73 | tok/s 61907
step    110 | loss 2.7131 | lr 6.94e-06 | grad 2.67 | tok/s 57212
step    120 | loss 3.0353 | lr 2.69e-05 | grad 2.17 | tok/s 57343
step    130 | loss 2.7964 | lr 5.89e-05 | grad 1.51 | tok/s 58644
step    140 | loss 2.5494 | lr 9.99e-05 | grad 1.12 | tok/s 58719
step    150 | loss 2.4682 | lr 1.46e-04 | grad 2.59 | tok/s 56067
step    160 | loss 2.3169 | lr 1.92e-04 | grad 1.62 | tok/s 56488
step    170 | loss 2.5104 | lr 2.35e-04 | grad 4.88 | tok/s 58288
step    180 | loss 2.5625 | lr 2.69e-04 | grad 1.47 | tok/s 58532
step    190 | loss 2.3070 | lr 2.91e-04 | grad 1.52 | tok/s 59350
step    200 | loss 1.9312 | lr 3.00e-04 | grad 1.24 | tok/s 60665
step    210 | loss 2.4419 | lr 2.94e-04 | grad 2.09 | tok/s 58238
step    220 | loss 2.4134 | lr 2.74e-04 | grad 1.01 | tok/s 60093
step    230 | loss 2.2086 | lr 2.42e-04 | grad 2.42 | tok/s 57947
step    240 | loss 2.2161 | lr 2.01e-04 | grad 1.64 | tok/s 59224
step    250 | loss 2.1442 | lr 1.55e-04 | grad 1.20 | tok/s 58391
step    260 | loss 2.1668 | lr 1.09e-04 | grad 0.81 | tok/s 56049
step    270 | loss 2.0345 | lr 6.65e-05 | grad 1.02 | tok/s 58150
step    280 | loss 1.9413 | lr 3.24e-05 | grad 1.70 | tok/s 58115
step    290 | loss 1.9328 | lr 9.84e-06 | grad 0.86 | tok/s 61204
step    300 | loss 1.8988 | lr 1.07e-06 | grad 0.85 | tok/s 61178
step    310 | loss 1.9050 | lr 6.94e-06 | grad 0.73 | tok/s 61099
step    320 | loss 2.0053 | lr 2.69e-05 | grad 1.84 | tok/s 58865
step    330 | loss 2.0420 | lr 5.89e-05 | grad 0.69 | tok/s 57501
step    340 | loss 2.0540 | lr 9.99e-05 | grad 1.80 | tok/s 58744
step    350 | loss 2.0768 | lr 1.46e-04 | grad 0.95 | tok/s 57092
step    360 | loss 2.0336 | lr 1.92e-04 | grad 2.31 | tok/s 57834
step    370 | loss 1.8783 | lr 2.35e-04 | grad 1.09 | tok/s 58905
step    380 | loss 2.4186 | lr 2.69e-04 | grad 1.28 | tok/s 60296
step    390 | loss 2.0043 | lr 2.91e-04 | grad 1.38 | tok/s 58158
step    400 | loss 2.1227 | lr 3.00e-04 | grad 3.05 | tok/s 59617
step    410 | loss 1.8810 | lr 2.94e-04 | grad 2.25 | tok/s 57947
step    420 | loss 1.9932 | lr 2.74e-04 | grad 1.10 | tok/s 57454
step    430 | loss 2.1337 | lr 2.42e-04 | grad 1.36 | tok/s 57258
step    440 | loss 2.2072 | lr 2.01e-04 | grad 0.99 | tok/s 59562
step    450 | loss 1.9195 | lr 1.55e-04 | grad 0.77 | tok/s 57804
step    460 | loss 1.8988 | lr 1.09e-04 | grad 0.68 | tok/s 58070
step    470 | loss 1.8777 | lr 6.65e-05 | grad 1.03 | tok/s 58834
step    480 | loss 1.7799 | lr 3.24e-05 | grad 0.60 | tok/s 56759
step    490 | loss 1.7380 | lr 9.84e-06 | grad 0.52 | tok/s 57716
step    500 | loss 2.6774 | lr 1.07e-06 | grad 0.95 | tok/s 59543
step    510 | loss 1.7779 | lr 6.94e-06 | grad 0.62 | tok/s 58298
step    520 | loss 1.8466 | lr 2.69e-05 | grad 0.55 | tok/s 60247
step    530 | loss 2.3020 | lr 5.89e-05 | grad 0.60 | tok/s 58886
step    540 | loss 1.7760 | lr 9.99e-05 | grad 1.05 | tok/s 58840
step    550 | loss 1.6614 | lr 1.46e-04 | grad 0.71 | tok/s 60342
step    560 | loss 1.5232 | lr 1.92e-04 | grad 0.70 | tok/s 61296
step    570 | loss 1.8148 | lr 2.35e-04 | grad 2.38 | tok/s 59789
step    580 | loss 2.1608 | lr 2.69e-04 | grad 1.00 | tok/s 59075
step    590 | loss 2.4447 | lr 2.91e-04 | grad 1.81 | tok/s 57840
step    600 | loss 1.9499 | lr 3.00e-04 | grad 1.39 | tok/s 57945
step    610 | loss 1.9476 | lr 2.94e-04 | grad 1.22 | tok/s 60833
step    620 | loss 1.8502 | lr 2.74e-04 | grad 0.84 | tok/s 57557
step    630 | loss 1.7644 | lr 2.42e-04 | grad 0.87 | tok/s 59523
step    640 | loss 2.1195 | lr 2.01e-04 | grad 0.84 | tok/s 59424
step    650 | loss 1.7783 | lr 1.55e-04 | grad 1.01 | tok/s 58402
step    660 | loss 2.1124 | lr 1.09e-04 | grad 4.31 | tok/s 57583
step    670 | loss 1.9168 | lr 6.65e-05 | grad 1.88 | tok/s 59735
step    680 | loss 1.8745 | lr 3.24e-05 | grad 0.95 | tok/s 57522
step    690 | loss 1.8816 | lr 9.84e-06 | grad 1.09 | tok/s 58101
step    700 | loss 1.9643 | lr 1.07e-06 | grad 1.09 | tok/s 58362
step    710 | loss 1.9026 | lr 6.94e-06 | grad 1.00 | tok/s 58635
step    720 | loss 1.9973 | lr 2.68e-05 | grad 1.44 | tok/s 58483
step    730 | loss 1.9530 | lr 5.89e-05 | grad 1.03 | tok/s 58968
step    740 | loss 1.8970 | lr 9.99e-05 | grad 1.77 | tok/s 58461
step    750 | loss 1.6960 | lr 1.46e-04 | grad 1.23 | tok/s 57850
step    760 | loss 2.0735 | lr 1.92e-04 | grad 0.66 | tok/s 58530
step    770 | loss 1.7459 | lr 2.35e-04 | grad 1.03 | tok/s 58130
step    780 | loss 1.8053 | lr 2.69e-04 | grad 0.96 | tok/s 58752
step    790 | loss 1.7324 | lr 2.91e-04 | grad 0.72 | tok/s 59292
step    800 | loss 1.7114 | lr 3.00e-04 | grad 0.89 | tok/s 59268
step    810 | loss 1.8301 | lr 2.94e-04 | grad 1.95 | tok/s 58722
step    820 | loss 2.5245 | lr 2.74e-04 | grad 1.51 | tok/s 60078
step    830 | loss 2.0135 | lr 2.42e-04 | grad 0.70 | tok/s 61031
step    840 | loss 1.7059 | lr 2.01e-04 | grad 0.51 | tok/s 61153
step    850 | loss 2.1990 | lr 1.55e-04 | grad 1.32 | tok/s 58299
step    860 | loss 1.9052 | lr 1.09e-04 | grad 1.02 | tok/s 57018
step    870 | loss 1.8357 | lr 6.65e-05 | grad 0.73 | tok/s 58793
step    880 | loss 1.8776 | lr 3.24e-05 | grad 1.09 | tok/s 58523
step    890 | loss 1.7733 | lr 9.84e-06 | grad 0.71 | tok/s 58454
step    900 | loss 2.2842 | lr 1.07e-06 | grad 0.91 | tok/s 57032
step    910 | loss 1.8579 | lr 6.94e-06 | grad 0.61 | tok/s 57905
step    920 | loss 1.8069 | lr 2.68e-05 | grad 0.64 | tok/s 57870
step    930 | loss 1.9318 | lr 5.89e-05 | grad 1.26 | tok/s 57819
step    940 | loss 1.8101 | lr 9.99e-05 | grad 1.31 | tok/s 57060
step    950 | loss 1.8933 | lr 1.46e-04 | grad 1.23 | tok/s 58379
step    960 | loss 1.6275 | lr 1.92e-04 | grad 0.62 | tok/s 61210
step    970 | loss 1.4313 | lr 2.35e-04 | grad 0.46 | tok/s 61204
step    980 | loss 1.6049 | lr 2.69e-04 | grad 1.62 | tok/s 59557
step    990 | loss 1.9296 | lr 2.91e-04 | grad 0.76 | tok/s 57990
step   1000 | loss 1.8042 | lr 3.00e-04 | grad 0.58 | tok/s 56563
  >>> saved checkpoint: checkpoint_step_001000_loss_1.8042.pt
step   1010 | loss 2.0562 | lr 2.94e-04 | grad 1.19 | tok/s 49684
step   1020 | loss 1.6817 | lr 2.74e-04 | grad 0.76 | tok/s 58145
step   1030 | loss 2.0803 | lr 2.42e-04 | grad 0.83 | tok/s 57213
step   1040 | loss 1.7006 | lr 2.01e-04 | grad 1.12 | tok/s 58398
step   1050 | loss 1.7209 | lr 1.55e-04 | grad 0.71 | tok/s 58679
step   1060 | loss 1.9626 | lr 1.09e-04 | grad 1.45 | tok/s 58533
step   1070 | loss 2.0254 | lr 6.65e-05 | grad 0.76 | tok/s 58878
step   1080 | loss 2.3816 | lr 3.24e-05 | grad 0.90 | tok/s 58064
step   1090 | loss 2.0665 | lr 9.84e-06 | grad 0.76 | tok/s 58516
step   1100 | loss 1.7448 | lr 1.07e-06 | grad 0.71 | tok/s 58207
step   1110 | loss 1.7739 | lr 6.93e-06 | grad 0.66 | tok/s 59322
step   1120 | loss 2.0110 | lr 2.68e-05 | grad 0.80 | tok/s 59821
step   1130 | loss 1.7463 | lr 5.89e-05 | grad 0.62 | tok/s 56884
step   1140 | loss 1.6176 | lr 9.99e-05 | grad 0.58 | tok/s 58414
step   1150 | loss 1.9169 | lr 1.46e-04 | grad 1.06 | tok/s 58227
step   1160 | loss 1.5796 | lr 1.92e-04 | grad 0.53 | tok/s 57628
step   1170 | loss 1.9088 | lr 2.35e-04 | grad 0.69 | tok/s 58372
step   1180 | loss 1.6160 | lr 2.69e-04 | grad 0.71 | tok/s 61292
step   1190 | loss 1.4705 | lr 2.91e-04 | grad 0.55 | tok/s 61287
step   1200 | loss 1.3780 | lr 3.00e-04 | grad 0.57 | tok/s 61321
step   1210 | loss 1.3515 | lr 2.94e-04 | grad 0.73 | tok/s 61261
step   1220 | loss 1.3984 | lr 2.74e-04 | grad 0.71 | tok/s 60724
step   1230 | loss 1.6231 | lr 2.42e-04 | grad 0.86 | tok/s 58709
step   1240 | loss 1.6879 | lr 2.01e-04 | grad 0.79 | tok/s 57605
step   1250 | loss 1.7918 | lr 1.55e-04 | grad 2.52 | tok/s 59359
step   1260 | loss 1.8278 | lr 1.09e-04 | grad 2.14 | tok/s 59450
step   1270 | loss 1.8991 | lr 6.65e-05 | grad 1.16 | tok/s 58708
step   1280 | loss 1.7245 | lr 3.24e-05 | grad 0.71 | tok/s 57936
step   1290 | loss 1.6565 | lr 9.84e-06 | grad 0.75 | tok/s 57766
step   1300 | loss 1.7119 | lr 1.07e-06 | grad 0.66 | tok/s 57259
step   1310 | loss 1.7918 | lr 6.93e-06 | grad 0.68 | tok/s 57438
step   1320 | loss 1.7713 | lr 2.68e-05 | grad 0.98 | tok/s 58299
step   1330 | loss 1.7053 | lr 5.89e-05 | grad 0.53 | tok/s 58436
step   1340 | loss 1.6097 | lr 9.99e-05 | grad 0.84 | tok/s 58603
step   1350 | loss 1.6668 | lr 1.46e-04 | grad 1.47 | tok/s 60037
step   1360 | loss 1.5953 | lr 1.92e-04 | grad 0.66 | tok/s 57225
step   1370 | loss 1.7148 | lr 2.35e-04 | grad 0.76 | tok/s 57867
step   1380 | loss 1.8044 | lr 2.69e-04 | grad 0.80 | tok/s 58369
step   1390 | loss 1.7049 | lr 2.91e-04 | grad 1.34 | tok/s 57089
step   1400 | loss 1.8312 | lr 3.00e-04 | grad 11.50 | tok/s 59507
step   1410 | loss 1.8133 | lr 2.94e-04 | grad 1.02 | tok/s 59992
step   1420 | loss 1.8111 | lr 2.74e-04 | grad 0.80 | tok/s 57070
step   1430 | loss 1.5981 | lr 2.42e-04 | grad 0.96 | tok/s 55795
step   1440 | loss 1.5124 | lr 2.01e-04 | grad 0.63 | tok/s 58778
step   1450 | loss 1.5557 | lr 1.55e-04 | grad 1.79 | tok/s 59973
step   1460 | loss 1.6214 | lr 1.09e-04 | grad 0.52 | tok/s 55945
step   1470 | loss 1.7289 | lr 6.65e-05 | grad 1.86 | tok/s 58014
step   1480 | loss 1.5996 | lr 3.24e-05 | grad 1.50 | tok/s 58687
step   1490 | loss 1.7593 | lr 9.84e-06 | grad 1.97 | tok/s 58532
step   1500 | loss 1.8585 | lr 1.07e-06 | grad 2.00 | tok/s 57032
step   1510 | loss 1.7415 | lr 6.93e-06 | grad 0.98 | tok/s 60053
step   1520 | loss 1.6960 | lr 2.68e-05 | grad 1.01 | tok/s 59432
step   1530 | loss 1.6434 | lr 5.89e-05 | grad 0.51 | tok/s 58912
step   1540 | loss 1.6251 | lr 9.99e-05 | grad 0.50 | tok/s 57973
step   1550 | loss 1.6046 | lr 1.46e-04 | grad 2.12 | tok/s 60088
step   1560 | loss 2.1904 | lr 1.92e-04 | grad 0.98 | tok/s 58576
step   1570 | loss 1.6421 | lr 2.35e-04 | grad 1.05 | tok/s 57474
step   1580 | loss 1.8081 | lr 2.69e-04 | grad 0.94 | tok/s 59338
step   1590 | loss 1.5810 | lr 2.91e-04 | grad 0.66 | tok/s 57989
step   1600 | loss 1.6947 | lr 3.00e-04 | grad 0.79 | tok/s 56950
step   1610 | loss 1.5292 | lr 2.94e-04 | grad 0.69 | tok/s 60398
step   1620 | loss 1.6925 | lr 2.74e-04 | grad 0.58 | tok/s 59331
step   1630 | loss 1.7059 | lr 2.42e-04 | grad 0.73 | tok/s 59772
step   1640 | loss 1.6250 | lr 2.01e-04 | grad 0.56 | tok/s 57759
step   1650 | loss 1.6315 | lr 1.55e-04 | grad 0.97 | tok/s 56942
step   1660 | loss 1.6230 | lr 1.09e-04 | grad 0.61 | tok/s 57327
step   1670 | loss 1.6878 | lr 6.65e-05 | grad 1.76 | tok/s 59787
step   1680 | loss 2.0348 | lr 3.24e-05 | grad 0.48 | tok/s 59755
step   1690 | loss 1.6059 | lr 9.84e-06 | grad 0.84 | tok/s 58291
step   1700 | loss 2.0618 | lr 1.07e-06 | grad 0.47 | tok/s 59383
step   1710 | loss 1.6487 | lr 6.93e-06 | grad 0.75 | tok/s 57846
step   1720 | loss 1.6565 | lr 2.68e-05 | grad 0.73 | tok/s 58045
step   1730 | loss 1.7636 | lr 5.89e-05 | grad 0.69 | tok/s 58136
step   1740 | loss 1.6652 | lr 9.99e-05 | grad 0.57 | tok/s 59120
step   1750 | loss 1.5622 | lr 1.46e-04 | grad 0.46 | tok/s 56938
step   1760 | loss 1.8618 | lr 1.92e-04 | grad 0.58 | tok/s 57676
step   1770 | loss 1.7450 | lr 2.35e-04 | grad 0.59 | tok/s 59129
step   1780 | loss 1.6246 | lr 2.69e-04 | grad 0.98 | tok/s 56840
step   1790 | loss 1.8365 | lr 2.91e-04 | grad 0.68 | tok/s 57898
step   1800 | loss 1.5534 | lr 3.00e-04 | grad 0.64 | tok/s 59026
step   1810 | loss 1.6514 | lr 2.94e-04 | grad 0.75 | tok/s 58637
step   1820 | loss 1.6050 | lr 2.74e-04 | grad 0.76 | tok/s 58006
step   1830 | loss 1.6325 | lr 2.42e-04 | grad 0.65 | tok/s 57914
step   1840 | loss 1.6123 | lr 2.01e-04 | grad 0.78 | tok/s 57325
step   1850 | loss 1.8295 | lr 1.55e-04 | grad 0.94 | tok/s 57861
step   1860 | loss 1.5802 | lr 1.09e-04 | grad 0.46 | tok/s 57828
step   1870 | loss 1.6399 | lr 6.65e-05 | grad 1.14 | tok/s 59055
step   1880 | loss 1.5769 | lr 3.24e-05 | grad 0.52 | tok/s 59109
step   1890 | loss 1.6668 | lr 9.84e-06 | grad 0.51 | tok/s 58226
step   1900 | loss 1.7506 | lr 1.07e-06 | grad 1.36 | tok/s 58711
step   1910 | loss 1.6925 | lr 6.93e-06 | grad 1.06 | tok/s 57930
step   1920 | loss 1.5933 | lr 2.68e-05 | grad 0.86 | tok/s 59715
step   1930 | loss 1.5826 | lr 5.89e-05 | grad 0.83 | tok/s 59294
step   1940 | loss 1.5017 | lr 9.99e-05 | grad 0.62 | tok/s 60461
step   1950 | loss 1.5599 | lr 1.46e-04 | grad 0.79 | tok/s 58920
step   1960 | loss 1.9616 | lr 1.92e-04 | grad 3.95 | tok/s 60043
step   1970 | loss 1.6188 | lr 2.35e-04 | grad 1.16 | tok/s 57995
step   1980 | loss 1.6807 | lr 2.69e-04 | grad 1.81 | tok/s 57977
step   1990 | loss 1.7883 | lr 2.91e-04 | grad 0.99 | tok/s 59282
step   2000 | loss 1.6962 | lr 3.00e-04 | grad 1.09 | tok/s 59812
  >>> saved checkpoint: checkpoint_step_002000_loss_1.6962.pt
step   2010 | loss 1.4281 | lr 2.94e-04 | grad 0.57 | tok/s 49156
step   2020 | loss 1.2712 | lr 2.74e-04 | grad 0.53 | tok/s 61353
step   2030 | loss 1.5923 | lr 2.42e-04 | grad 0.81 | tok/s 60833
step   2040 | loss 1.4242 | lr 2.01e-04 | grad 0.47 | tok/s 61459
step   2050 | loss 1.3724 | lr 1.55e-04 | grad 0.77 | tok/s 60562
step   2060 | loss 1.6897 | lr 1.09e-04 | grad 0.64 | tok/s 57822
step   2070 | loss 1.6322 | lr 6.65e-05 | grad 1.06 | tok/s 60824
step   2080 | loss 1.7825 | lr 3.24e-05 | grad 3.50 | tok/s 57506
step   2090 | loss 1.7084 | lr 9.84e-06 | grad 0.82 | tok/s 59672
step   2100 | loss 1.6452 | lr 1.07e-06 | grad 0.79 | tok/s 57831
step   2110 | loss 1.5238 | lr 6.93e-06 | grad 0.65 | tok/s 59840
step   2120 | loss 1.5162 | lr 2.68e-05 | grad 0.98 | tok/s 59099
step   2130 | loss 1.6213 | lr 5.89e-05 | grad 2.31 | tok/s 57525
step   2140 | loss 1.6896 | lr 9.99e-05 | grad 2.38 | tok/s 57448
step   2150 | loss 1.6917 | lr 1.46e-04 | grad 0.50 | tok/s 58380
step   2160 | loss 1.6326 | lr 1.92e-04 | grad 0.56 | tok/s 57665
step   2170 | loss 1.7410 | lr 2.35e-04 | grad 0.64 | tok/s 58306
step   2180 | loss 1.6003 | lr 2.69e-04 | grad 0.71 | tok/s 59569
step   2190 | loss 1.9252 | lr 2.91e-04 | grad 0.69 | tok/s 59425
step   2200 | loss 1.3637 | lr 3.00e-04 | grad 0.52 | tok/s 61400
step   2210 | loss 1.3279 | lr 2.94e-04 | grad 0.42 | tok/s 61492
step   2220 | loss 1.2824 | lr 2.74e-04 | grad 0.44 | tok/s 61607

Training complete! Final step: 2229
