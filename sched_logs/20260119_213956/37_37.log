# Job 37: 37
# GPU: 5
# Command: python train.py --level 37 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/37
# Started: 2026-01-19T22:20:52.219376
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/37/level37_100m_20260119_222057
Auto r_h_mode: none (level 37 has bounded/no W_h)
Model: Level 37, 65,738,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 8.9958 | lr 2.70e-05 | grad 9.50 | tok/s 20440
step     20 | loss 4.2807 | lr 5.70e-05 | grad 6.44 | tok/s 43285
step     30 | loss 4.6590 | lr 8.70e-05 | grad 3.12 | tok/s 48663
step     40 | loss 3.6915 | lr 1.17e-04 | grad 2.72 | tok/s 48827
step     50 | loss 3.0521 | lr 1.47e-04 | grad 2.59 | tok/s 47348
step     60 | loss 3.1398 | lr 1.77e-04 | grad 2.53 | tok/s 44512
step     70 | loss 2.7841 | lr 2.07e-04 | grad 1.47 | tok/s 43360
step     80 | loss 3.1851 | lr 2.37e-04 | grad 3.25 | tok/s 44413
step     90 | loss 3.0585 | lr 2.67e-04 | grad 3.47 | tok/s 42920
step    100 | loss 2.6670 | lr 2.97e-04 | grad 1.04 | tok/s 43357
step    110 | loss 2.6102 | lr 6.94e-06 | grad 1.79 | tok/s 41223
step    120 | loss 2.9110 | lr 2.69e-05 | grad 0.97 | tok/s 41094
step    130 | loss 2.6520 | lr 5.89e-05 | grad 1.02 | tok/s 42083
step    140 | loss 2.4396 | lr 9.99e-05 | grad 1.01 | tok/s 42249
step    150 | loss 2.3550 | lr 1.46e-04 | grad 1.82 | tok/s 40364
step    160 | loss 2.2618 | lr 1.92e-04 | grad 1.25 | tok/s 40653
step    170 | loss 2.4584 | lr 2.35e-04 | grad 3.55 | tok/s 42196
step    180 | loss 2.4864 | lr 2.69e-04 | grad 1.13 | tok/s 45234
step    190 | loss 2.2254 | lr 2.91e-04 | grad 1.05 | tok/s 46091
step    200 | loss 1.9356 | lr 3.00e-04 | grad 0.90 | tok/s 46843
step    210 | loss 2.4232 | lr 2.94e-04 | grad 1.32 | tok/s 45066
step    220 | loss 2.3048 | lr 2.74e-04 | grad 0.93 | tok/s 46770
step    230 | loss 2.1430 | lr 2.42e-04 | grad 0.99 | tok/s 44350
step    240 | loss 2.1620 | lr 2.01e-04 | grad 1.24 | tok/s 43537
step    250 | loss 2.1259 | lr 1.55e-04 | grad 0.93 | tok/s 42888
step    260 | loss 2.1622 | lr 1.09e-04 | grad 0.63 | tok/s 41181
step    270 | loss 2.0296 | lr 6.65e-05 | grad 0.80 | tok/s 42741
step    280 | loss 1.9155 | lr 3.24e-05 | grad 1.23 | tok/s 42539
step    290 | loss 1.9236 | lr 9.84e-06 | grad 0.60 | tok/s 44908
step    300 | loss 1.8848 | lr 1.07e-06 | grad 0.59 | tok/s 44886
step    310 | loss 1.8866 | lr 6.94e-06 | grad 0.52 | tok/s 44954
step    320 | loss 1.9657 | lr 2.69e-05 | grad 1.27 | tok/s 43191
step    330 | loss 2.0073 | lr 5.89e-05 | grad 0.59 | tok/s 42061
step    340 | loss 2.0203 | lr 9.99e-05 | grad 1.41 | tok/s 43063
step    350 | loss 2.0113 | lr 1.46e-04 | grad 0.78 | tok/s 41871
step    360 | loss 2.0000 | lr 1.92e-04 | grad 1.95 | tok/s 42230
step    370 | loss 1.8608 | lr 2.35e-04 | grad 0.93 | tok/s 43251
step    380 | loss 2.3915 | lr 2.69e-04 | grad 1.16 | tok/s 44239
step    390 | loss 1.9861 | lr 2.91e-04 | grad 1.00 | tok/s 42590
step    400 | loss 2.0848 | lr 3.00e-04 | grad 2.38 | tok/s 43732
step    410 | loss 1.8382 | lr 2.94e-04 | grad 1.74 | tok/s 42272
step    420 | loss 1.9593 | lr 2.74e-04 | grad 0.92 | tok/s 42141
step    430 | loss 2.0975 | lr 2.42e-04 | grad 1.02 | tok/s 42008
step    440 | loss 2.1457 | lr 2.01e-04 | grad 1.02 | tok/s 43602
step    450 | loss 1.9118 | lr 1.55e-04 | grad 0.58 | tok/s 42440
step    460 | loss 1.8696 | lr 1.09e-04 | grad 0.61 | tok/s 42565
step    470 | loss 1.8546 | lr 6.65e-05 | grad 0.95 | tok/s 43361
step    480 | loss 1.7518 | lr 3.24e-05 | grad 0.50 | tok/s 41628
step    490 | loss 1.7166 | lr 9.84e-06 | grad 0.45 | tok/s 42302
step    500 | loss 2.6404 | lr 1.07e-06 | grad 0.79 | tok/s 43673
step    510 | loss 1.7149 | lr 6.94e-06 | grad 0.51 | tok/s 42756
step    520 | loss 1.7894 | lr 2.69e-05 | grad 0.43 | tok/s 43988
step    530 | loss 2.2658 | lr 5.89e-05 | grad 0.63 | tok/s 43085
step    540 | loss 1.7230 | lr 9.99e-05 | grad 0.86 | tok/s 42898
step    550 | loss 1.6351 | lr 1.46e-04 | grad 0.66 | tok/s 44168
step    560 | loss 1.4892 | lr 1.92e-04 | grad 0.62 | tok/s 44823
step    570 | loss 1.7806 | lr 2.35e-04 | grad 1.57 | tok/s 43762
step    580 | loss 2.1235 | lr 2.69e-04 | grad 0.91 | tok/s 43301
step    590 | loss 2.4302 | lr 2.91e-04 | grad 2.12 | tok/s 42393
step    600 | loss 1.8942 | lr 3.00e-04 | grad 1.22 | tok/s 42466
step    610 | loss 1.8665 | lr 2.94e-04 | grad 1.00 | tok/s 44606
step    620 | loss 1.8090 | lr 2.74e-04 | grad 0.70 | tok/s 42195
step    630 | loss 1.7002 | lr 2.42e-04 | grad 0.74 | tok/s 43567
step    640 | loss 2.0174 | lr 2.01e-04 | grad 0.87 | tok/s 43556
step    650 | loss 1.7692 | lr 1.55e-04 | grad 0.89 | tok/s 42877
step    660 | loss 2.0536 | lr 1.09e-04 | grad 3.70 | tok/s 42305
step    670 | loss 1.8788 | lr 6.65e-05 | grad 1.30 | tok/s 43740
step    680 | loss 1.8216 | lr 3.24e-05 | grad 0.83 | tok/s 42085
step    690 | loss 1.8334 | lr 9.84e-06 | grad 1.04 | tok/s 42573
step    700 | loss 1.9247 | lr 1.07e-06 | grad 1.16 | tok/s 42669
step    710 | loss 1.8472 | lr 6.94e-06 | grad 0.88 | tok/s 43102
step    720 | loss 1.9246 | lr 2.68e-05 | grad 1.16 | tok/s 42844
step    730 | loss 1.9027 | lr 5.89e-05 | grad 0.95 | tok/s 43193
step    740 | loss 1.8482 | lr 9.99e-05 | grad 1.56 | tok/s 42935
step    750 | loss 1.6522 | lr 1.46e-04 | grad 1.13 | tok/s 42423
step    760 | loss 2.0760 | lr 1.92e-04 | grad 0.59 | tok/s 43428
step    770 | loss 1.7205 | lr 2.35e-04 | grad 0.96 | tok/s 42728
step    780 | loss 1.7747 | lr 2.69e-04 | grad 0.86 | tok/s 43118
step    790 | loss 1.6970 | lr 2.91e-04 | grad 0.70 | tok/s 43468
step    800 | loss 1.6532 | lr 3.00e-04 | grad 0.77 | tok/s 43392
step    810 | loss 1.7764 | lr 2.94e-04 | grad 1.43 | tok/s 43009
step    820 | loss 2.4189 | lr 2.74e-04 | grad 1.08 | tok/s 44222
step    830 | loss 1.9228 | lr 2.42e-04 | grad 0.71 | tok/s 44897
step    840 | loss 1.6225 | lr 2.01e-04 | grad 0.51 | tok/s 44785
step    850 | loss 2.0034 | lr 1.55e-04 | grad 1.08 | tok/s 42849
step    860 | loss 1.7598 | lr 1.09e-04 | grad 0.85 | tok/s 41787
step    870 | loss 1.6906 | lr 6.65e-05 | grad 0.67 | tok/s 43063
step    880 | loss 1.7504 | lr 3.24e-05 | grad 0.88 | tok/s 42926
step    890 | loss 1.6687 | lr 9.84e-06 | grad 0.65 | tok/s 42704
step    900 | loss 2.1170 | lr 1.07e-06 | grad 0.74 | tok/s 41676
step    910 | loss 1.7235 | lr 6.94e-06 | grad 0.55 | tok/s 42595
step    920 | loss 1.7019 | lr 2.68e-05 | grad 0.58 | tok/s 42377
step    930 | loss 1.7961 | lr 5.89e-05 | grad 1.16 | tok/s 42344
step    940 | loss 1.6996 | lr 9.99e-05 | grad 1.22 | tok/s 41799
step    950 | loss 1.7918 | lr 1.46e-04 | grad 1.05 | tok/s 42782
step    960 | loss 1.5484 | lr 1.92e-04 | grad 0.52 | tok/s 45062
step    970 | loss 1.3794 | lr 2.35e-04 | grad 0.42 | tok/s 44938
step    980 | loss 1.5370 | lr 2.69e-04 | grad 1.49 | tok/s 43690
step    990 | loss 1.8899 | lr 2.91e-04 | grad 0.73 | tok/s 42869
step   1000 | loss 1.7555 | lr 3.00e-04 | grad 0.60 | tok/s 41423
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7555.pt
step   1010 | loss 1.9647 | lr 2.94e-04 | grad 1.09 | tok/s 39641
step   1020 | loss 1.6271 | lr 2.74e-04 | grad 0.68 | tok/s 42528
step   1030 | loss 2.0391 | lr 2.42e-04 | grad 0.73 | tok/s 41820
step   1040 | loss 1.6616 | lr 2.01e-04 | grad 1.16 | tok/s 42848
step   1050 | loss 1.6904 | lr 1.55e-04 | grad 0.87 | tok/s 42763
step   1060 | loss 1.8478 | lr 1.09e-04 | grad 1.16 | tok/s 42983
step   1070 | loss 1.9705 | lr 6.65e-05 | grad 0.75 | tok/s 43096
step   1080 | loss 2.3415 | lr 3.24e-05 | grad 0.86 | tok/s 42511
step   1090 | loss 2.0396 | lr 9.84e-06 | grad 0.70 | tok/s 42854
step   1100 | loss 1.7336 | lr 1.07e-06 | grad 0.66 | tok/s 42517
step   1110 | loss 1.7110 | lr 6.93e-06 | grad 0.66 | tok/s 43251
step   1120 | loss 1.9127 | lr 2.68e-05 | grad 0.72 | tok/s 43914
step   1130 | loss 1.7048 | lr 5.89e-05 | grad 0.61 | tok/s 41677
step   1140 | loss 1.5734 | lr 9.99e-05 | grad 0.64 | tok/s 42805
step   1150 | loss 1.8667 | lr 1.46e-04 | grad 0.96 | tok/s 42641
step   1160 | loss 1.5496 | lr 1.92e-04 | grad 0.54 | tok/s 42214
step   1170 | loss 1.8496 | lr 2.35e-04 | grad 0.67 | tok/s 42761
step   1180 | loss 1.5894 | lr 2.69e-04 | grad 0.58 | tok/s 44880
step   1190 | loss 1.4363 | lr 2.91e-04 | grad 0.56 | tok/s 45001
step   1200 | loss 1.3519 | lr 3.00e-04 | grad 0.50 | tok/s 44836
step   1210 | loss 1.3182 | lr 2.94e-04 | grad 0.60 | tok/s 45371
step   1220 | loss 1.3665 | lr 2.74e-04 | grad 0.92 | tok/s 44609
step   1230 | loss 1.6005 | lr 2.42e-04 | grad 0.76 | tok/s 42876
step   1240 | loss 1.6667 | lr 2.01e-04 | grad 0.70 | tok/s 42164
step   1250 | loss 1.7421 | lr 1.55e-04 | grad 2.59 | tok/s 43428
step   1260 | loss 1.8090 | lr 1.09e-04 | grad 2.05 | tok/s 43483
step   1270 | loss 1.8492 | lr 6.65e-05 | grad 1.03 | tok/s 42929
step   1280 | loss 1.6784 | lr 3.24e-05 | grad 0.66 | tok/s 42416
step   1290 | loss 1.6282 | lr 9.84e-06 | grad 0.71 | tok/s 42177
step   1300 | loss 1.6832 | lr 1.07e-06 | grad 0.63 | tok/s 42050
step   1310 | loss 1.7734 | lr 6.93e-06 | grad 0.61 | tok/s 41813
step   1320 | loss 1.7337 | lr 2.68e-05 | grad 0.93 | tok/s 42690
step   1330 | loss 1.6498 | lr 5.89e-05 | grad 0.54 | tok/s 42752
step   1340 | loss 1.5945 | lr 9.99e-05 | grad 0.84 | tok/s 42677
step   1350 | loss 1.6049 | lr 1.46e-04 | grad 1.41 | tok/s 43946
step   1360 | loss 1.5733 | lr 1.92e-04 | grad 0.65 | tok/s 41775
step   1370 | loss 1.6785 | lr 2.35e-04 | grad 0.71 | tok/s 42368
step   1380 | loss 1.7589 | lr 2.69e-04 | grad 0.77 | tok/s 42698
step   1390 | loss 1.6782 | lr 2.91e-04 | grad 1.49 | tok/s 41499
step   1400 | loss 1.7098 | lr 3.00e-04 | grad 6.53 | tok/s 43336
step   1410 | loss 1.7049 | lr 2.94e-04 | grad 1.20 | tok/s 43806
step   1420 | loss 1.7613 | lr 2.74e-04 | grad 0.80 | tok/s 41616
step   1430 | loss 1.5702 | lr 2.42e-04 | grad 0.78 | tok/s 40680
step   1440 | loss 1.4798 | lr 2.01e-04 | grad 0.61 | tok/s 42917
step   1450 | loss 1.5132 | lr 1.55e-04 | grad 1.84 | tok/s 43844
step   1460 | loss 1.5938 | lr 1.09e-04 | grad 0.54 | tok/s 40966
step   1470 | loss 1.6901 | lr 6.65e-05 | grad 1.62 | tok/s 42485
step   1480 | loss 1.5587 | lr 3.24e-05 | grad 1.48 | tok/s 42950
step   1490 | loss 1.7000 | lr 9.84e-06 | grad 1.95 | tok/s 42861
step   1500 | loss 1.8077 | lr 1.07e-06 | grad 1.75 | tok/s 41743
step   1510 | loss 1.6888 | lr 6.93e-06 | grad 0.85 | tok/s 44174
step   1520 | loss 1.6389 | lr 2.68e-05 | grad 0.92 | tok/s 43411
step   1530 | loss 1.5982 | lr 5.89e-05 | grad 0.50 | tok/s 43553
step   1540 | loss 1.5720 | lr 9.99e-05 | grad 0.47 | tok/s 42321
step   1550 | loss 1.5585 | lr 1.46e-04 | grad 2.50 | tok/s 44018
step   1560 | loss 2.1631 | lr 1.92e-04 | grad 1.51 | tok/s 42897
step   1570 | loss 1.6285 | lr 2.35e-04 | grad 0.93 | tok/s 42098
step   1580 | loss 1.7776 | lr 2.69e-04 | grad 1.14 | tok/s 44471
step   1590 | loss 1.6183 | lr 2.91e-04 | grad 0.75 | tok/s 43674
step   1600 | loss 1.6860 | lr 3.00e-04 | grad 0.86 | tok/s 43197
step   1610 | loss 1.5132 | lr 2.94e-04 | grad 0.65 | tok/s 45050
step   1620 | loss 1.7011 | lr 2.74e-04 | grad 0.60 | tok/s 43683
step   1630 | loss 1.6614 | lr 2.42e-04 | grad 0.72 | tok/s 45024

Training complete! Final step: 1637
