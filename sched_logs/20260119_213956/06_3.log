# Job 6: 3
# GPU: 6
# Command: python train.py --level 3 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/3
# Started: 2026-01-19T21:39:56.178338
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/3/level3_100m_20260119_214001
Auto r_h_mode: none (level 3 has bounded/no W_h)
Model: Level 3, 82,327,840 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.5479 | lr 2.70e-05 | grad 7.53 | tok/s 19534
step     20 | loss 4.7486 | lr 5.70e-05 | grad 5.25 | tok/s 60585
step     30 | loss 4.8654 | lr 8.70e-05 | grad 3.08 | tok/s 64166
step     40 | loss 3.6141 | lr 1.17e-04 | grad 1.98 | tok/s 64129
step     50 | loss 2.7823 | lr 1.47e-04 | grad 1.55 | tok/s 64017
step     60 | loss 2.9574 | lr 1.77e-04 | grad 1.63 | tok/s 62768
step     70 | loss 2.6516 | lr 2.07e-04 | grad 1.98 | tok/s 60627
step     80 | loss 2.9577 | lr 2.37e-04 | grad 2.70 | tok/s 62956
step     90 | loss 2.8948 | lr 2.67e-04 | grad 2.92 | tok/s 60726
step    100 | loss 2.5298 | lr 2.97e-04 | grad 1.12 | tok/s 61278
step    110 | loss 2.6869 | lr 6.94e-06 | grad 2.06 | tok/s 56452
step    120 | loss 2.8164 | lr 2.69e-05 | grad 1.52 | tok/s 56408
step    130 | loss 2.6907 | lr 5.89e-05 | grad 1.22 | tok/s 57727
step    140 | loss 2.4709 | lr 9.99e-05 | grad 0.91 | tok/s 57851
step    150 | loss 2.3447 | lr 1.46e-04 | grad 2.14 | tok/s 55100
step    160 | loss 2.2226 | lr 1.92e-04 | grad 1.66 | tok/s 55512
step    170 | loss 2.4029 | lr 2.35e-04 | grad 3.92 | tok/s 57477
step    180 | loss 2.4180 | lr 2.69e-04 | grad 1.58 | tok/s 57662
step    190 | loss 2.2120 | lr 2.91e-04 | grad 2.03 | tok/s 58424
step    200 | loss 1.8764 | lr 3.00e-04 | grad 1.83 | tok/s 59778
step    210 | loss 2.4345 | lr 2.94e-04 | grad 2.92 | tok/s 57244
step    220 | loss 2.3658 | lr 2.74e-04 | grad 1.49 | tok/s 59056
step    230 | loss 2.2648 | lr 2.42e-04 | grad 1.89 | tok/s 56933
step    240 | loss 2.2158 | lr 2.01e-04 | grad 2.20 | tok/s 58184
step    250 | loss 2.1610 | lr 1.55e-04 | grad 1.55 | tok/s 57307
step    260 | loss 2.1858 | lr 1.09e-04 | grad 0.91 | tok/s 55004
step    270 | loss 2.0556 | lr 6.65e-05 | grad 1.30 | tok/s 56750
step    280 | loss 1.9320 | lr 3.24e-05 | grad 1.66 | tok/s 56729
step    290 | loss 1.9596 | lr 9.84e-06 | grad 1.09 | tok/s 59756
step    300 | loss 1.9215 | lr 1.07e-06 | grad 1.00 | tok/s 59746
step    310 | loss 1.9179 | lr 6.94e-06 | grad 0.92 | tok/s 59722
step    320 | loss 1.9915 | lr 2.69e-05 | grad 1.59 | tok/s 57394
step    330 | loss 2.0421 | lr 5.89e-05 | grad 1.06 | tok/s 55908
step    340 | loss 2.0608 | lr 9.99e-05 | grad 2.22 | tok/s 57034
step    350 | loss 2.0570 | lr 1.46e-04 | grad 1.54 | tok/s 55371
step    360 | loss 2.0528 | lr 1.92e-04 | grad 2.66 | tok/s 56079
step    370 | loss 1.9152 | lr 2.35e-04 | grad 1.63 | tok/s 57145
step    380 | loss 2.4205 | lr 2.69e-04 | grad 1.91 | tok/s 58501
step    390 | loss 2.1492 | lr 2.91e-04 | grad 1.95 | tok/s 56411
step    400 | loss 2.2333 | lr 3.00e-04 | grad 3.08 | tok/s 57817
step    410 | loss 2.1070 | lr 2.94e-04 | grad 2.98 | tok/s 56102
step    420 | loss 2.1483 | lr 2.74e-04 | grad 1.62 | tok/s 55810
step    430 | loss 2.2237 | lr 2.42e-04 | grad 2.38 | tok/s 55692
step    440 | loss 2.3187 | lr 2.01e-04 | grad 1.53 | tok/s 57877
step    450 | loss 2.0928 | lr 1.55e-04 | grad 1.16 | tok/s 56305
step    460 | loss 2.0148 | lr 1.09e-04 | grad 1.02 | tok/s 56289
step    470 | loss 1.9870 | lr 6.65e-05 | grad 1.20 | tok/s 57156
step    480 | loss 1.8743 | lr 3.24e-05 | grad 0.86 | tok/s 55154
step    490 | loss 1.8374 | lr 9.84e-06 | grad 0.83 | tok/s 55989
step    500 | loss 2.8001 | lr 1.07e-06 | grad 1.20 | tok/s 57641
step    510 | loss 1.8787 | lr 6.94e-06 | grad 0.94 | tok/s 56407
step    520 | loss 1.9665 | lr 2.69e-05 | grad 0.67 | tok/s 58143
step    530 | loss 2.3939 | lr 5.89e-05 | grad 0.86 | tok/s 56946
step    540 | loss 1.8712 | lr 9.99e-05 | grad 1.26 | tok/s 56942
step    550 | loss 1.7725 | lr 1.46e-04 | grad 1.31 | tok/s 58492
step    560 | loss 1.6291 | lr 1.92e-04 | grad 1.47 | tok/s 59302
step    570 | loss 1.9299 | lr 2.35e-04 | grad 2.44 | tok/s 57842
step    580 | loss 2.3359 | lr 2.69e-04 | grad 1.53 | tok/s 57203
step    590 | loss 2.5829 | lr 2.91e-04 | grad 3.02 | tok/s 56168
step    600 | loss 2.1625 | lr 3.00e-04 | grad 2.03 | tok/s 56400
step    610 | loss 2.2141 | lr 2.94e-04 | grad 3.36 | tok/s 58906
step    620 | loss 2.1025 | lr 2.74e-04 | grad 1.93 | tok/s 55881
step    630 | loss 1.9561 | lr 2.42e-04 | grad 1.58 | tok/s 57757
step    640 | loss 2.2625 | lr 2.01e-04 | grad 1.62 | tok/s 57646
step    650 | loss 1.9869 | lr 1.55e-04 | grad 2.45 | tok/s 56658
step    660 | loss 2.2605 | lr 1.09e-04 | grad 5.81 | tok/s 56098
step    670 | loss 2.0757 | lr 6.65e-05 | grad 1.85 | tok/s 57802
step    680 | loss 2.0440 | lr 3.24e-05 | grad 1.96 | tok/s 55890
step    690 | loss 2.0268 | lr 9.84e-06 | grad 1.75 | tok/s 56176
step    700 | loss 2.0956 | lr 1.07e-06 | grad 1.60 | tok/s 56553
step    710 | loss 2.0194 | lr 6.94e-06 | grad 1.20 | tok/s 56833
step    720 | loss 2.1339 | lr 2.68e-05 | grad 2.06 | tok/s 56631
step    730 | loss 2.0860 | lr 5.89e-05 | grad 1.59 | tok/s 57138
step    740 | loss 2.0160 | lr 9.99e-05 | grad 2.16 | tok/s 56663
step    750 | loss 1.8370 | lr 1.46e-04 | grad 2.12 | tok/s 56063
step    760 | loss 2.1424 | lr 1.92e-04 | grad 1.30 | tok/s 56717
step    770 | loss 1.8993 | lr 2.35e-04 | grad 2.05 | tok/s 56354
step    780 | loss 2.0185 | lr 2.69e-04 | grad 2.06 | tok/s 57000
step    790 | loss 1.9134 | lr 2.91e-04 | grad 1.48 | tok/s 57457
step    800 | loss 1.8864 | lr 3.00e-04 | grad 1.91 | tok/s 57437
step    810 | loss 2.0068 | lr 2.94e-04 | grad 2.97 | tok/s 57003
step    820 | loss 2.5538 | lr 2.74e-04 | grad 1.52 | tok/s 58346
step    830 | loss 1.9610 | lr 2.42e-04 | grad 1.18 | tok/s 59360
step    840 | loss 1.6917 | lr 2.01e-04 | grad 1.09 | tok/s 59320
step    850 | loss 2.3332 | lr 1.55e-04 | grad 2.20 | tok/s 56509
step    860 | loss 2.0533 | lr 1.09e-04 | grad 1.45 | tok/s 55306
step    870 | loss 1.9613 | lr 6.65e-05 | grad 1.11 | tok/s 56999
step    880 | loss 2.0012 | lr 3.24e-05 | grad 1.51 | tok/s 56705
step    890 | loss 1.8815 | lr 9.84e-06 | grad 1.07 | tok/s 56616
step    900 | loss 2.3692 | lr 1.07e-06 | grad 1.59 | tok/s 55225
step    910 | loss 1.9783 | lr 6.94e-06 | grad 1.04 | tok/s 56146
step    920 | loss 1.9171 | lr 2.68e-05 | grad 1.00 | tok/s 55988
step    930 | loss 2.0265 | lr 5.89e-05 | grad 2.02 | tok/s 55940
step    940 | loss 1.9319 | lr 9.99e-05 | grad 2.09 | tok/s 55266
step    950 | loss 2.0216 | lr 1.46e-04 | grad 2.61 | tok/s 56571
step    960 | loss 1.7531 | lr 1.92e-04 | grad 1.47 | tok/s 59441
step    970 | loss 1.5332 | lr 2.35e-04 | grad 1.15 | tok/s 59314
step    980 | loss 1.7484 | lr 2.69e-04 | grad 2.73 | tok/s 57719
step    990 | loss 2.2057 | lr 2.91e-04 | grad 1.56 | tok/s 56201
step   1000 | loss 2.0637 | lr 3.00e-04 | grad 2.31 | tok/s 54782
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0637.pt
step   1010 | loss 2.2338 | lr 2.94e-04 | grad 1.26 | tok/s 46411
step   1020 | loss 1.8721 | lr 2.74e-04 | grad 1.52 | tok/s 56477
step   1030 | loss 2.2212 | lr 2.42e-04 | grad 1.41 | tok/s 55507
step   1040 | loss 1.9000 | lr 2.01e-04 | grad 1.87 | tok/s 56769
step   1050 | loss 1.9499 | lr 1.55e-04 | grad 1.99 | tok/s 57000
step   1060 | loss 2.1482 | lr 1.09e-04 | grad 1.62 | tok/s 56877
step   1070 | loss 2.1993 | lr 6.65e-05 | grad 1.17 | tok/s 57117
step   1080 | loss 2.4658 | lr 3.24e-05 | grad 1.36 | tok/s 56470
step   1090 | loss 2.2050 | lr 9.84e-06 | grad 1.16 | tok/s 56904
step   1100 | loss 1.9017 | lr 1.07e-06 | grad 1.05 | tok/s 56484
step   1110 | loss 1.9282 | lr 6.93e-06 | grad 1.01 | tok/s 57243
step   1120 | loss 2.0993 | lr 2.68e-05 | grad 0.91 | tok/s 57918
step   1130 | loss 1.8798 | lr 5.89e-05 | grad 0.93 | tok/s 55056
step   1140 | loss 1.7605 | lr 9.99e-05 | grad 0.98 | tok/s 56460
step   1150 | loss 2.0640 | lr 1.46e-04 | grad 1.52 | tok/s 56382
step   1160 | loss 1.7939 | lr 1.92e-04 | grad 1.97 | tok/s 55915
step   1170 | loss 2.0481 | lr 2.35e-04 | grad 1.55 | tok/s 56532
step   1180 | loss 1.7670 | lr 2.69e-04 | grad 1.52 | tok/s 59505
step   1190 | loss 1.6146 | lr 2.91e-04 | grad 1.14 | tok/s 59544
step   1200 | loss 1.5334 | lr 3.00e-04 | grad 1.15 | tok/s 59611
step   1210 | loss 1.4968 | lr 2.94e-04 | grad 0.98 | tok/s 59718
step   1220 | loss 1.5563 | lr 2.74e-04 | grad 1.36 | tok/s 59100
step   1230 | loss 2.1245 | lr 2.42e-04 | grad 2.44 | tok/s 57074
step   1240 | loss 1.9325 | lr 2.01e-04 | grad 1.93 | tok/s 56067
step   1250 | loss 2.0178 | lr 1.55e-04 | grad 3.20 | tok/s 57625
step   1260 | loss 2.0360 | lr 1.09e-04 | grad 3.12 | tok/s 57789
step   1270 | loss 2.0534 | lr 6.65e-05 | grad 1.65 | tok/s 56989
step   1280 | loss 1.8729 | lr 3.24e-05 | grad 1.09 | tok/s 56381
step   1290 | loss 1.8136 | lr 9.84e-06 | grad 1.21 | tok/s 56107
step   1300 | loss 1.8574 | lr 1.07e-06 | grad 1.05 | tok/s 55796
step   1310 | loss 1.9627 | lr 6.93e-06 | grad 1.08 | tok/s 55727
step   1320 | loss 1.9178 | lr 2.68e-05 | grad 1.44 | tok/s 56782
step   1330 | loss 1.8478 | lr 5.89e-05 | grad 0.99 | tok/s 56808
step   1340 | loss 1.7677 | lr 9.99e-05 | grad 1.40 | tok/s 56952
step   1350 | loss 1.8473 | lr 1.46e-04 | grad 2.33 | tok/s 58396
step   1360 | loss 1.7808 | lr 1.92e-04 | grad 1.22 | tok/s 55495
step   1370 | loss 1.8945 | lr 2.35e-04 | grad 1.72 | tok/s 56286
step   1380 | loss 2.0315 | lr 2.69e-04 | grad 2.27 | tok/s 56751
step   1390 | loss 1.9409 | lr 2.91e-04 | grad 2.53 | tok/s 55279
step   1400 | loss 1.9461 | lr 3.00e-04 | grad 4.22 | tok/s 57728
step   1410 | loss 1.9911 | lr 2.94e-04 | grad 1.69 | tok/s 58014
step   1420 | loss 2.0174 | lr 2.74e-04 | grad 1.50 | tok/s 54963
step   1430 | loss 1.8293 | lr 2.42e-04 | grad 1.65 | tok/s 53704
step   1440 | loss 1.7236 | lr 2.01e-04 | grad 1.27 | tok/s 56601
step   1450 | loss 1.7486 | lr 1.55e-04 | grad 2.36 | tok/s 57666
step   1460 | loss 1.8007 | lr 1.09e-04 | grad 1.05 | tok/s 53754
step   1470 | loss 1.9466 | lr 6.65e-05 | grad 2.97 | tok/s 55786
step   1480 | loss 1.7503 | lr 3.24e-05 | grad 2.06 | tok/s 56320
step   1490 | loss 1.9019 | lr 9.84e-06 | grad 2.50 | tok/s 56237
step   1500 | loss 1.9926 | lr 1.07e-06 | grad 2.64 | tok/s 54917
step   1510 | loss 1.9303 | lr 6.93e-06 | grad 1.37 | tok/s 57722
step   1520 | loss 1.8771 | lr 2.68e-05 | grad 1.45 | tok/s 57272
step   1530 | loss 1.8121 | lr 5.89e-05 | grad 0.80 | tok/s 56770
step   1540 | loss 1.7831 | lr 9.99e-05 | grad 0.84 | tok/s 55761
step   1550 | loss 1.7893 | lr 1.46e-04 | grad 2.64 | tok/s 57957
step   1560 | loss 2.4033 | lr 1.92e-04 | grad 2.91 | tok/s 56446
step   1570 | loss 1.8791 | lr 2.35e-04 | grad 2.45 | tok/s 55484
step   1580 | loss 2.0539 | lr 2.69e-04 | grad 2.70 | tok/s 57258
step   1590 | loss 1.8754 | lr 2.91e-04 | grad 2.47 | tok/s 55824
step   1600 | loss 2.0146 | lr 3.00e-04 | grad 2.61 | tok/s 54858
step   1610 | loss 1.9463 | lr 2.94e-04 | grad 7.75 | tok/s 57947
step   1620 | loss 1.9608 | lr 2.74e-04 | grad 1.32 | tok/s 57392
step   1630 | loss 1.9110 | lr 2.42e-04 | grad 1.24 | tok/s 57715
step   1640 | loss 1.8814 | lr 2.01e-04 | grad 1.14 | tok/s 55525
step   1650 | loss 1.8362 | lr 1.55e-04 | grad 1.87 | tok/s 54628
step   1660 | loss 1.8855 | lr 1.09e-04 | grad 1.02 | tok/s 55003
step   1670 | loss 1.9187 | lr 6.65e-05 | grad 2.44 | tok/s 57462
step   1680 | loss 2.2513 | lr 3.24e-05 | grad 1.04 | tok/s 57437
step   1690 | loss 1.8027 | lr 9.84e-06 | grad 1.11 | tok/s 56237
step   1700 | loss 2.1803 | lr 1.07e-06 | grad 0.83 | tok/s 57355
step   1710 | loss 1.8164 | lr 6.93e-06 | grad 1.23 | tok/s 55708
step   1720 | loss 1.8501 | lr 2.68e-05 | grad 1.01 | tok/s 56016
step   1730 | loss 1.9387 | lr 5.89e-05 | grad 1.00 | tok/s 56008
step   1740 | loss 1.8333 | lr 9.99e-05 | grad 1.04 | tok/s 56796
step   1750 | loss 1.7364 | lr 1.46e-04 | grad 1.07 | tok/s 54839
step   1760 | loss 2.0545 | lr 1.92e-04 | grad 1.21 | tok/s 55525
step   1770 | loss 1.9297 | lr 2.35e-04 | grad 1.59 | tok/s 56848
step   1780 | loss 1.8259 | lr 2.69e-04 | grad 1.79 | tok/s 54703
step   1790 | loss 2.0584 | lr 2.91e-04 | grad 1.74 | tok/s 55745
step   1800 | loss 1.7903 | lr 3.00e-04 | grad 1.67 | tok/s 56766
step   1810 | loss 1.8388 | lr 2.94e-04 | grad 1.52 | tok/s 56420
step   1820 | loss 1.8391 | lr 2.74e-04 | grad 2.69 | tok/s 55821
step   1830 | loss 1.8565 | lr 2.42e-04 | grad 1.47 | tok/s 55657
step   1840 | loss 1.8205 | lr 2.01e-04 | grad 1.81 | tok/s 55177
step   1850 | loss 2.0531 | lr 1.55e-04 | grad 1.24 | tok/s 55701
step   1860 | loss 1.7872 | lr 1.09e-04 | grad 0.96 | tok/s 55552
step   1870 | loss 1.8347 | lr 6.65e-05 | grad 1.47 | tok/s 56868
step   1880 | loss 1.7502 | lr 3.24e-05 | grad 0.85 | tok/s 57060
step   1890 | loss 1.8490 | lr 9.84e-06 | grad 0.77 | tok/s 56008
step   1900 | loss 1.9205 | lr 1.07e-06 | grad 1.52 | tok/s 56529
step   1910 | loss 1.8541 | lr 6.93e-06 | grad 1.28 | tok/s 55804
step   1920 | loss 1.7697 | lr 2.68e-05 | grad 1.20 | tok/s 57573
step   1930 | loss 1.7513 | lr 5.89e-05 | grad 1.02 | tok/s 57019
step   1940 | loss 1.6537 | lr 9.99e-05 | grad 0.98 | tok/s 58129
step   1950 | loss 1.7258 | lr 1.46e-04 | grad 1.16 | tok/s 56652
step   1960 | loss 2.1860 | lr 1.92e-04 | grad 5.00 | tok/s 57778
step   1970 | loss 1.8958 | lr 2.35e-04 | grad 1.50 | tok/s 55832
step   1980 | loss 1.9118 | lr 2.69e-04 | grad 2.81 | tok/s 55913
step   1990 | loss 2.0590 | lr 2.91e-04 | grad 2.11 | tok/s 57105
step   2000 | loss 2.0098 | lr 3.00e-04 | grad 1.70 | tok/s 57501
  >>> saved checkpoint: checkpoint_step_002000_loss_2.0098.pt
step   2010 | loss 1.6737 | lr 2.94e-04 | grad 2.81 | tok/s 45896
step   2020 | loss 1.4917 | lr 2.74e-04 | grad 3.41 | tok/s 59410
step   2030 | loss 1.8206 | lr 2.42e-04 | grad 1.87 | tok/s 58542
step   2040 | loss 1.5642 | lr 2.01e-04 | grad 0.88 | tok/s 59252
step   2050 | loss 1.5072 | lr 1.55e-04 | grad 1.11 | tok/s 58343
step   2060 | loss 1.8788 | lr 1.09e-04 | grad 1.02 | tok/s 55780
step   2070 | loss 1.7891 | lr 6.65e-05 | grad 1.42 | tok/s 58529
step   2080 | loss 1.9743 | lr 3.24e-05 | grad 3.84 | tok/s 55408
step   2090 | loss 1.8703 | lr 9.84e-06 | grad 1.23 | tok/s 57356
step   2100 | loss 1.8236 | lr 1.07e-06 | grad 1.12 | tok/s 55722
step   2110 | loss 1.7221 | lr 6.93e-06 | grad 0.96 | tok/s 57646
step   2120 | loss 1.7102 | lr 2.68e-05 | grad 1.58 | tok/s 57087
step   2130 | loss 1.8025 | lr 5.89e-05 | grad 1.85 | tok/s 55450
step   2140 | loss 1.8492 | lr 9.99e-05 | grad 2.77 | tok/s 55407
step   2150 | loss 1.8704 | lr 1.46e-04 | grad 0.96 | tok/s 56199

Training complete! Final step: 2156
