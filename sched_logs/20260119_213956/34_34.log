# Job 34: 34
# GPU: 1
# Command: python train.py --level 34 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/34
# Started: 2026-01-19T22:20:30.843290
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/34/level34_100m_20260119_222035
Auto r_h_mode: none (level 34 has bounded/no W_h)
Model: Level 34, 65,764,480 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4794 | lr 2.70e-05 | grad 11.56 | tok/s 25955
step     20 | loss 4.5975 | lr 5.70e-05 | grad 6.16 | tok/s 85742
step     30 | loss 4.9296 | lr 8.70e-05 | grad 3.12 | tok/s 80220
step     40 | loss 4.2781 | lr 1.17e-04 | grad 2.94 | tok/s 82424
step     50 | loss 3.6910 | lr 1.47e-04 | grad 1.80 | tok/s 82461
step     60 | loss 3.6183 | lr 1.77e-04 | grad 5.41 | tok/s 79037
step     70 | loss 3.1777 | lr 2.07e-04 | grad 2.62 | tok/s 79003
step     80 | loss 3.2279 | lr 2.37e-04 | grad 1.84 | tok/s 98055
step     90 | loss 3.3370 | lr 2.67e-04 | grad 3.09 | tok/s 95366
step    100 | loss 3.0768 | lr 2.97e-04 | grad 4.22 | tok/s 100269
step    110 | loss 2.9372 | lr 6.94e-06 | grad 1.66 | tok/s 86692
step    120 | loss 3.2112 | lr 2.69e-05 | grad 1.41 | tok/s 86844
step    130 | loss 3.0589 | lr 5.89e-05 | grad 1.20 | tok/s 89170
step    140 | loss 2.8260 | lr 9.99e-05 | grad 0.88 | tok/s 86504
step    150 | loss 2.6731 | lr 1.46e-04 | grad 3.83 | tok/s 80799
step    160 | loss 2.5548 | lr 1.92e-04 | grad 5.00 | tok/s 83383
step    170 | loss 2.7584 | lr 2.35e-04 | grad 4.53 | tok/s 90521
step    180 | loss 2.8245 | lr 2.69e-04 | grad 1.79 | tok/s 90557
step    190 | loss 2.5611 | lr 2.91e-04 | grad 2.56 | tok/s 88820
step    200 | loss 2.4191 | lr 3.00e-04 | grad 2.27 | tok/s 89278
step    210 | loss 2.6387 | lr 2.94e-04 | grad 3.12 | tok/s 84589
step    220 | loss 2.6223 | lr 2.74e-04 | grad 2.58 | tok/s 85041
step    230 | loss 2.4276 | lr 2.42e-04 | grad 3.38 | tok/s 81711
step    240 | loss 2.4716 | lr 2.01e-04 | grad 1.80 | tok/s 83028
step    250 | loss 2.4109 | lr 1.55e-04 | grad 1.81 | tok/s 82984
step    260 | loss 2.3991 | lr 1.09e-04 | grad 1.20 | tok/s 77248
step    270 | loss 2.2904 | lr 6.65e-05 | grad 1.77 | tok/s 81566
step    280 | loss 2.1807 | lr 3.24e-05 | grad 2.80 | tok/s 81258
step    290 | loss 2.2347 | lr 9.84e-06 | grad 0.97 | tok/s 85852
step    300 | loss 2.2115 | lr 1.07e-06 | grad 1.16 | tok/s 85734
step    310 | loss 2.2160 | lr 6.94e-06 | grad 0.86 | tok/s 85466
step    320 | loss 2.2396 | lr 2.69e-05 | grad 2.02 | tok/s 81386
step    330 | loss 2.2807 | lr 5.89e-05 | grad 1.03 | tok/s 80684
step    340 | loss 2.3024 | lr 9.99e-05 | grad 1.91 | tok/s 82037
step    350 | loss 2.3209 | lr 1.46e-04 | grad 2.67 | tok/s 79824
step    360 | loss 2.3089 | lr 1.92e-04 | grad 3.33 | tok/s 80723
step    370 | loss 2.2364 | lr 2.35e-04 | grad 3.64 | tok/s 82499
step    380 | loss 2.7031 | lr 2.69e-04 | grad 2.44 | tok/s 84468
step    390 | loss 2.3162 | lr 2.91e-04 | grad 2.17 | tok/s 81442
step    400 | loss 2.4341 | lr 3.00e-04 | grad 3.86 | tok/s 83767
step    410 | loss 2.1765 | lr 2.94e-04 | grad 2.67 | tok/s 81012
step    420 | loss 2.3124 | lr 2.74e-04 | grad 2.55 | tok/s 80132
step    430 | loss 2.4256 | lr 2.42e-04 | grad 3.30 | tok/s 80353
step    440 | loss 2.5084 | lr 2.01e-04 | grad 2.31 | tok/s 83239
step    450 | loss 2.2114 | lr 1.55e-04 | grad 1.49 | tok/s 82525
step    460 | loss 2.1709 | lr 1.09e-04 | grad 1.61 | tok/s 83930
step    470 | loss 2.1778 | lr 6.65e-05 | grad 2.42 | tok/s 84376
step    480 | loss 2.0748 | lr 3.24e-05 | grad 1.08 | tok/s 82271
step    490 | loss 2.0262 | lr 9.84e-06 | grad 0.79 | tok/s 83159
step    500 | loss 2.8856 | lr 1.07e-06 | grad 1.20 | tok/s 85422
step    510 | loss 2.0281 | lr 6.94e-06 | grad 1.05 | tok/s 83671
step    520 | loss 2.1103 | lr 2.69e-05 | grad 0.74 | tok/s 86824
step    530 | loss 2.5128 | lr 5.89e-05 | grad 1.85 | tok/s 84661
step    540 | loss 2.0551 | lr 9.99e-05 | grad 3.92 | tok/s 85450
step    550 | loss 2.0160 | lr 1.46e-04 | grad 1.71 | tok/s 86883
step    560 | loss 1.8871 | lr 1.92e-04 | grad 1.68 | tok/s 85954
step    570 | loss 2.1075 | lr 2.35e-04 | grad 2.92 | tok/s 83936
step    580 | loss 2.5247 | lr 2.69e-04 | grad 3.16 | tok/s 82679
step    590 | loss 2.7129 | lr 2.91e-04 | grad 2.31 | tok/s 81072
step    600 | loss 2.2739 | lr 3.00e-04 | grad 2.39 | tok/s 81295
step    610 | loss 2.2811 | lr 2.94e-04 | grad 4.12 | tok/s 84873
step    620 | loss 2.2134 | lr 2.74e-04 | grad 2.53 | tok/s 80777
step    630 | loss 2.1163 | lr 2.42e-04 | grad 1.64 | tok/s 83176
step    640 | loss 2.3660 | lr 2.01e-04 | grad 2.36 | tok/s 83328
step    650 | loss 2.1401 | lr 1.55e-04 | grad 3.95 | tok/s 81729
step    660 | loss 2.3914 | lr 1.09e-04 | grad 5.38 | tok/s 80551
step    670 | loss 2.2125 | lr 6.65e-05 | grad 2.91 | tok/s 83397
step    680 | loss 2.1325 | lr 3.24e-05 | grad 1.19 | tok/s 80513
step    690 | loss 2.1668 | lr 9.84e-06 | grad 1.41 | tok/s 81269
step    700 | loss 2.2539 | lr 1.07e-06 | grad 1.45 | tok/s 81647
step    710 | loss 2.1750 | lr 6.94e-06 | grad 1.29 | tok/s 82131
step    720 | loss 2.2691 | lr 2.68e-05 | grad 1.97 | tok/s 81782
step    730 | loss 2.2278 | lr 5.89e-05 | grad 1.49 | tok/s 82576
step    740 | loss 2.1356 | lr 9.99e-05 | grad 2.17 | tok/s 81718
step    750 | loss 1.9873 | lr 1.46e-04 | grad 2.39 | tok/s 80848
step    760 | loss 2.3036 | lr 1.92e-04 | grad 2.55 | tok/s 81640
step    770 | loss 2.0811 | lr 2.35e-04 | grad 2.53 | tok/s 81516
step    780 | loss 2.1219 | lr 2.69e-04 | grad 2.31 | tok/s 82206
step    790 | loss 2.1318 | lr 2.91e-04 | grad 4.53 | tok/s 83168
step    800 | loss 2.0826 | lr 3.00e-04 | grad 2.44 | tok/s 82926
step    810 | loss 2.1072 | lr 2.94e-04 | grad 2.56 | tok/s 82188
step    820 | loss 2.6881 | lr 2.74e-04 | grad 2.62 | tok/s 84501
step    830 | loss 2.2383 | lr 2.42e-04 | grad 1.14 | tok/s 85595
step    840 | loss 1.9601 | lr 2.01e-04 | grad 1.50 | tok/s 85630
step    850 | loss 2.3282 | lr 1.55e-04 | grad 1.49 | tok/s 81739
step    860 | loss 2.1648 | lr 1.09e-04 | grad 1.73 | tok/s 79989
step    870 | loss 2.0912 | lr 6.65e-05 | grad 1.26 | tok/s 82208
step    880 | loss 2.1324 | lr 3.24e-05 | grad 1.80 | tok/s 81816
step    890 | loss 2.0208 | lr 9.84e-06 | grad 1.06 | tok/s 81708
step    900 | loss 2.4369 | lr 1.07e-06 | grad 1.18 | tok/s 79721
step    910 | loss 2.0871 | lr 6.94e-06 | grad 0.98 | tok/s 81418
step    920 | loss 2.0503 | lr 2.68e-05 | grad 1.06 | tok/s 80797
step    930 | loss 2.1728 | lr 5.89e-05 | grad 1.79 | tok/s 80789
step    940 | loss 2.0728 | lr 9.99e-05 | grad 2.08 | tok/s 79960
step    950 | loss 2.1157 | lr 1.46e-04 | grad 2.69 | tok/s 81670
step    960 | loss 1.9136 | lr 1.92e-04 | grad 1.87 | tok/s 85788
step    970 | loss 1.7372 | lr 2.35e-04 | grad 3.00 | tok/s 85452
step    980 | loss 1.8822 | lr 2.69e-04 | grad 3.19 | tok/s 83338
step    990 | loss 2.2469 | lr 2.91e-04 | grad 2.48 | tok/s 81111
step   1000 | loss 2.0948 | lr 3.00e-04 | grad 2.09 | tok/s 79050
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0948.pt
step   1010 | loss 2.3022 | lr 2.94e-04 | grad 2.67 | tok/s 65025
step   1020 | loss 1.9914 | lr 2.74e-04 | grad 1.68 | tok/s 81322
step   1030 | loss 2.3048 | lr 2.42e-04 | grad 3.44 | tok/s 79779
step   1040 | loss 2.0130 | lr 2.01e-04 | grad 2.72 | tok/s 81147
step   1050 | loss 1.9596 | lr 1.55e-04 | grad 2.12 | tok/s 81741
step   1060 | loss 2.2226 | lr 1.09e-04 | grad 2.06 | tok/s 82025
step   1070 | loss 2.2736 | lr 6.65e-05 | grad 1.69 | tok/s 82424
step   1080 | loss 2.5166 | lr 3.24e-05 | grad 1.80 | tok/s 81297
step   1090 | loss 2.2567 | lr 9.84e-06 | grad 1.34 | tok/s 82074
step   1100 | loss 1.9610 | lr 1.07e-06 | grad 1.31 | tok/s 81177
step   1110 | loss 1.9859 | lr 6.93e-06 | grad 1.23 | tok/s 82785
step   1120 | loss 2.2531 | lr 2.68e-05 | grad 1.16 | tok/s 83575
step   1130 | loss 1.9624 | lr 5.89e-05 | grad 1.43 | tok/s 79715
step   1140 | loss 1.8607 | lr 9.99e-05 | grad 1.55 | tok/s 81405
step   1150 | loss 2.1578 | lr 1.46e-04 | grad 1.70 | tok/s 81371
step   1160 | loss 1.8182 | lr 1.92e-04 | grad 1.68 | tok/s 80498
step   1170 | loss 2.1141 | lr 2.35e-04 | grad 2.47 | tok/s 81354
step   1180 | loss 1.8489 | lr 2.69e-04 | grad 1.86 | tok/s 85754
step   1190 | loss 1.7334 | lr 2.91e-04 | grad 1.91 | tok/s 85243
step   1200 | loss 1.6716 | lr 3.00e-04 | grad 3.95 | tok/s 85637
step   1210 | loss 1.6272 | lr 2.94e-04 | grad 2.64 | tok/s 85875
step   1220 | loss 1.6391 | lr 2.74e-04 | grad 2.22 | tok/s 84833
step   1230 | loss 1.8750 | lr 2.42e-04 | grad 1.74 | tok/s 82283
step   1240 | loss 1.9392 | lr 2.01e-04 | grad 1.74 | tok/s 80349
step   1250 | loss 2.0108 | lr 1.55e-04 | grad 4.84 | tok/s 82912
step   1260 | loss 2.0661 | lr 1.09e-04 | grad 2.84 | tok/s 82809
step   1270 | loss 2.0853 | lr 6.65e-05 | grad 1.77 | tok/s 81892
step   1280 | loss 1.9122 | lr 3.24e-05 | grad 1.40 | tok/s 80942
step   1290 | loss 1.8532 | lr 9.84e-06 | grad 1.74 | tok/s 80719
step   1300 | loss 1.9015 | lr 1.07e-06 | grad 1.06 | tok/s 79933
step   1310 | loss 2.0131 | lr 6.93e-06 | grad 1.27 | tok/s 79958
step   1320 | loss 1.9663 | lr 2.68e-05 | grad 2.12 | tok/s 81553
step   1330 | loss 1.8968 | lr 5.89e-05 | grad 1.60 | tok/s 81501
step   1340 | loss 1.8357 | lr 9.99e-05 | grad 1.41 | tok/s 81671
step   1350 | loss 1.9355 | lr 1.46e-04 | grad 2.70 | tok/s 83793
step   1360 | loss 1.8654 | lr 1.92e-04 | grad 1.98 | tok/s 79602
step   1370 | loss 1.9462 | lr 2.35e-04 | grad 2.73 | tok/s 80820
step   1380 | loss 2.0641 | lr 2.69e-04 | grad 3.11 | tok/s 81369
step   1390 | loss 1.9665 | lr 2.91e-04 | grad 3.41 | tok/s 79546
step   1400 | loss 2.0406 | lr 3.00e-04 | grad 12.56 | tok/s 82558
step   1410 | loss 2.0529 | lr 2.94e-04 | grad 2.19 | tok/s 83506
step   1420 | loss 2.0571 | lr 2.74e-04 | grad 1.93 | tok/s 79662
step   1430 | loss 1.8703 | lr 2.42e-04 | grad 1.63 | tok/s 77551
step   1440 | loss 1.7319 | lr 2.01e-04 | grad 2.31 | tok/s 82152
step   1450 | loss 1.7790 | lr 1.55e-04 | grad 3.16 | tok/s 83312
step   1460 | loss 1.8129 | lr 1.09e-04 | grad 2.50 | tok/s 78423
step   1470 | loss 1.9165 | lr 6.65e-05 | grad 2.41 | tok/s 80967
step   1480 | loss 1.8001 | lr 3.24e-05 | grad 3.05 | tok/s 81831
step   1490 | loss 1.9222 | lr 9.84e-06 | grad 4.31 | tok/s 81747
step   1500 | loss 2.0061 | lr 1.07e-06 | grad 3.06 | tok/s 79490
step   1510 | loss 1.9385 | lr 6.93e-06 | grad 1.66 | tok/s 83648
step   1520 | loss 1.9047 | lr 2.68e-05 | grad 1.73 | tok/s 82861
step   1530 | loss 1.8298 | lr 5.89e-05 | grad 1.06 | tok/s 82339
step   1540 | loss 1.8259 | lr 9.99e-05 | grad 1.59 | tok/s 80938
step   1550 | loss 1.8111 | lr 1.46e-04 | grad 3.22 | tok/s 83939
step   1560 | loss 2.4101 | lr 1.92e-04 | grad 2.92 | tok/s 82344
step   1570 | loss 1.9457 | lr 2.35e-04 | grad 3.39 | tok/s 80805
step   1580 | loss 2.1496 | lr 2.69e-04 | grad 3.00 | tok/s 83096
step   1590 | loss 1.8787 | lr 2.91e-04 | grad 2.88 | tok/s 80871
step   1600 | loss 1.9583 | lr 3.00e-04 | grad 4.69 | tok/s 79346
step   1610 | loss 1.7993 | lr 2.94e-04 | grad 2.14 | tok/s 83722
step   1620 | loss 1.9144 | lr 2.74e-04 | grad 1.91 | tok/s 82851
step   1630 | loss 1.9423 | lr 2.42e-04 | grad 2.31 | tok/s 83621
step   1640 | loss 1.8457 | lr 2.01e-04 | grad 1.72 | tok/s 80463
step   1650 | loss 1.8432 | lr 1.55e-04 | grad 2.66 | tok/s 79489
step   1660 | loss 1.8277 | lr 1.09e-04 | grad 1.30 | tok/s 79678
step   1670 | loss 1.9025 | lr 6.65e-05 | grad 3.06 | tok/s 83673
step   1680 | loss 2.2897 | lr 3.24e-05 | grad 1.54 | tok/s 83203
step   1690 | loss 1.8016 | lr 9.84e-06 | grad 1.43 | tok/s 81352
step   1700 | loss 2.1946 | lr 1.07e-06 | grad 1.12 | tok/s 83000
step   1710 | loss 1.8214 | lr 6.93e-06 | grad 1.31 | tok/s 80546
step   1720 | loss 1.8452 | lr 2.68e-05 | grad 1.64 | tok/s 81029
step   1730 | loss 1.9624 | lr 5.89e-05 | grad 1.52 | tok/s 81085
step   1740 | loss 1.8454 | lr 9.99e-05 | grad 1.41 | tok/s 82324
step   1750 | loss 1.7548 | lr 1.46e-04 | grad 1.63 | tok/s 79446
step   1760 | loss 2.0834 | lr 1.92e-04 | grad 2.52 | tok/s 80538
step   1770 | loss 1.9797 | lr 2.35e-04 | grad 2.08 | tok/s 82894
step   1780 | loss 1.8647 | lr 2.69e-04 | grad 1.92 | tok/s 79665
step   1790 | loss 2.0865 | lr 2.91e-04 | grad 2.36 | tok/s 81220
step   1800 | loss 1.7754 | lr 3.00e-04 | grad 2.16 | tok/s 82761
step   1810 | loss 1.8300 | lr 2.94e-04 | grad 2.44 | tok/s 80642
step   1820 | loss 1.8306 | lr 2.74e-04 | grad 3.27 | tok/s 81107
step   1830 | loss 1.8404 | lr 2.42e-04 | grad 1.90 | tok/s 80618
step   1840 | loss 1.8428 | lr 2.01e-04 | grad 3.56 | tok/s 79992
step   1850 | loss 2.0458 | lr 1.55e-04 | grad 2.17 | tok/s 80772
step   1860 | loss 1.7693 | lr 1.09e-04 | grad 1.86 | tok/s 80711
step   1870 | loss 1.8321 | lr 6.65e-05 | grad 2.02 | tok/s 82350
step   1880 | loss 1.7554 | lr 3.24e-05 | grad 1.37 | tok/s 82608
step   1890 | loss 1.8671 | lr 9.84e-06 | grad 0.79 | tok/s 81104
step   1900 | loss 1.9189 | lr 1.07e-06 | grad 2.02 | tok/s 81975
step   1910 | loss 1.8428 | lr 6.93e-06 | grad 1.96 | tok/s 80845
step   1920 | loss 1.7601 | lr 2.68e-05 | grad 1.31 | tok/s 83339
step   1930 | loss 1.7454 | lr 5.89e-05 | grad 1.20 | tok/s 82458
step   1940 | loss 1.6519 | lr 9.99e-05 | grad 1.92 | tok/s 84289
step   1950 | loss 1.7373 | lr 1.46e-04 | grad 1.48 | tok/s 82053
step   1960 | loss 2.1658 | lr 1.92e-04 | grad 5.12 | tok/s 83100
step   1970 | loss 1.9158 | lr 2.35e-04 | grad 2.88 | tok/s 80369
step   1980 | loss 1.9285 | lr 2.69e-04 | grad 3.16 | tok/s 80581
step   1990 | loss 2.0890 | lr 2.91e-04 | grad 4.16 | tok/s 83208
step   2000 | loss 2.0149 | lr 3.00e-04 | grad 3.20 | tok/s 84053
  >>> saved checkpoint: checkpoint_step_002000_loss_2.0149.pt
step   2010 | loss 1.6968 | lr 2.94e-04 | grad 2.16 | tok/s 65918
step   2020 | loss 1.4969 | lr 2.74e-04 | grad 1.56 | tok/s 86490
step   2030 | loss 1.8116 | lr 2.42e-04 | grad 2.50 | tok/s 83690
step   2040 | loss 1.6006 | lr 2.01e-04 | grad 0.93 | tok/s 85720
step   2050 | loss 1.5266 | lr 1.55e-04 | grad 1.65 | tok/s 85169
step   2060 | loss 1.8363 | lr 1.09e-04 | grad 1.22 | tok/s 81392
step   2070 | loss 1.7718 | lr 6.65e-05 | grad 2.11 | tok/s 85182
step   2080 | loss 1.9440 | lr 3.24e-05 | grad 3.25 | tok/s 80794
step   2090 | loss 1.8523 | lr 9.84e-06 | grad 1.52 | tok/s 83752
step   2100 | loss 1.8076 | lr 1.07e-06 | grad 1.36 | tok/s 81282
step   2110 | loss 1.6945 | lr 6.93e-06 | grad 1.15 | tok/s 83891
step   2120 | loss 1.6917 | lr 2.68e-05 | grad 1.66 | tok/s 82877
step   2130 | loss 1.7991 | lr 5.89e-05 | grad 3.31 | tok/s 80837
step   2140 | loss 1.8362 | lr 9.99e-05 | grad 3.62 | tok/s 80828
step   2150 | loss 1.8743 | lr 1.46e-04 | grad 1.55 | tok/s 81784
step   2160 | loss 1.8190 | lr 1.92e-04 | grad 1.56 | tok/s 81129
step   2170 | loss 1.9313 | lr 2.35e-04 | grad 2.86 | tok/s 81912
step   2180 | loss 1.8627 | lr 2.69e-04 | grad 1.76 | tok/s 82459
step   2190 | loss 2.1712 | lr 2.91e-04 | grad 2.64 | tok/s 83638
step   2200 | loss 1.6247 | lr 3.00e-04 | grad 2.56 | tok/s 86032
step   2210 | loss 1.5484 | lr 2.94e-04 | grad 3.03 | tok/s 86226
step   2220 | loss 1.4906 | lr 2.74e-04 | grad 1.88 | tok/s 86285
step   2230 | loss 1.7253 | lr 2.42e-04 | grad 1.98 | tok/s 83106
step   2240 | loss 2.0204 | lr 2.01e-04 | grad 2.59 | tok/s 85522
step   2250 | loss 1.9061 | lr 1.55e-04 | grad 1.80 | tok/s 84651
step   2260 | loss 1.9971 | lr 1.09e-04 | grad 1.67 | tok/s 83267
step   2270 | loss 1.7557 | lr 6.65e-05 | grad 1.41 | tok/s 81577
step   2280 | loss 1.8879 | lr 3.24e-05 | grad 1.10 | tok/s 81511
step   2290 | loss 1.7025 | lr 9.84e-06 | grad 1.37 | tok/s 82601
step   2300 | loss 1.7183 | lr 1.07e-06 | grad 1.05 | tok/s 79878
step   2310 | loss 1.7824 | lr 6.93e-06 | grad 1.82 | tok/s 85050
step   2320 | loss 1.8461 | lr 2.68e-05 | grad 1.30 | tok/s 86311
step   2330 | loss 1.7661 | lr 5.89e-05 | grad 1.07 | tok/s 86227
step   2340 | loss 1.7608 | lr 9.98e-05 | grad 1.66 | tok/s 84364
step   2350 | loss 1.8347 | lr 1.46e-04 | grad 1.42 | tok/s 81910
step   2360 | loss 2.3455 | lr 1.92e-04 | grad 4.94 | tok/s 83356
step   2370 | loss 1.9357 | lr 2.35e-04 | grad 2.06 | tok/s 82982
step   2380 | loss 1.7287 | lr 2.69e-04 | grad 2.17 | tok/s 81994
step   2390 | loss 1.8224 | lr 2.91e-04 | grad 3.39 | tok/s 81828
step   2400 | loss 1.9297 | lr 3.00e-04 | grad 4.59 | tok/s 80836
step   2410 | loss 1.8319 | lr 2.94e-04 | grad 2.02 | tok/s 81588
step   2420 | loss 2.1084 | lr 2.74e-04 | grad 1.83 | tok/s 83022
step   2430 | loss 1.9448 | lr 2.42e-04 | grad 2.48 | tok/s 81431
step   2440 | loss 2.0097 | lr 2.01e-04 | grad 2.33 | tok/s 83394
step   2450 | loss 1.6346 | lr 1.55e-04 | grad 1.80 | tok/s 82408
step   2460 | loss 1.9224 | lr 1.09e-04 | grad 2.44 | tok/s 84194
step   2470 | loss 2.0612 | lr 6.65e-05 | grad 1.51 | tok/s 85383
step   2480 | loss 1.7135 | lr 3.24e-05 | grad 1.48 | tok/s 83269
step   2490 | loss 1.6451 | lr 9.84e-06 | grad 1.28 | tok/s 81186
step   2500 | loss 1.7788 | lr 1.07e-06 | grad 1.66 | tok/s 83352
step   2510 | loss 1.8334 | lr 6.93e-06 | grad 1.50 | tok/s 83435
step   2520 | loss 1.9616 | lr 2.68e-05 | grad 1.40 | tok/s 81268
step   2530 | loss 1.7858 | lr 5.89e-05 | grad 1.48 | tok/s 80553
step   2540 | loss 1.7771 | lr 9.98e-05 | grad 1.40 | tok/s 81229
step   2550 | loss 2.2892 | lr 1.46e-04 | grad 3.09 | tok/s 82262
step   2560 | loss 2.0520 | lr 1.92e-04 | grad 2.11 | tok/s 83826
step   2570 | loss 1.8162 | lr 2.35e-04 | grad 2.84 | tok/s 80499
step   2580 | loss 1.9787 | lr 2.69e-04 | grad 2.05 | tok/s 83466
step   2590 | loss 1.6680 | lr 2.91e-04 | grad 2.36 | tok/s 81937
step   2600 | loss 1.8920 | lr 3.00e-04 | grad 2.98 | tok/s 83609
step   2610 | loss 1.8496 | lr 2.94e-04 | grad 2.50 | tok/s 84369
step   2620 | loss 1.6533 | lr 2.74e-04 | grad 1.52 | tok/s 86230
step   2630 | loss 1.6569 | lr 2.42e-04 | grad 1.10 | tok/s 84008
step   2640 | loss 1.8048 | lr 2.01e-04 | grad 2.17 | tok/s 81709
step   2650 | loss 1.7242 | lr 1.55e-04 | grad 1.45 | tok/s 78866
step   2660 | loss 2.1260 | lr 1.09e-04 | grad 3.36 | tok/s 84620
step   2670 | loss 1.6882 | lr 6.65e-05 | grad 1.17 | tok/s 86456
step   2680 | loss 1.6063 | lr 3.24e-05 | grad 0.84 | tok/s 86352
step   2690 | loss 1.5465 | lr 9.84e-06 | grad 0.90 | tok/s 86295
step   2700 | loss 1.7390 | lr 1.07e-06 | grad 0.98 | tok/s 83348
step   2710 | loss 1.8556 | lr 6.93e-06 | grad 4.06 | tok/s 80948
step   2720 | loss 2.3719 | lr 2.68e-05 | grad 1.03 | tok/s 85199
step   2730 | loss 1.7851 | lr 5.89e-05 | grad 1.20 | tok/s 82349
step   2740 | loss 1.6077 | lr 9.98e-05 | grad 1.62 | tok/s 82468
step   2750 | loss 1.8430 | lr 1.46e-04 | grad 1.45 | tok/s 83132
step   2760 | loss 1.9170 | lr 1.92e-04 | grad 2.45 | tok/s 79740
step   2770 | loss 1.7576 | lr 2.35e-04 | grad 2.30 | tok/s 83051
step   2780 | loss 1.7441 | lr 2.69e-04 | grad 1.66 | tok/s 80241
step   2790 | loss 1.6308 | lr 2.91e-04 | grad 1.98 | tok/s 82062
step   2800 | loss 2.1775 | lr 3.00e-04 | grad 3.23 | tok/s 82450
step   2810 | loss 2.0188 | lr 2.94e-04 | grad 3.25 | tok/s 83726
step   2820 | loss 2.1467 | lr 2.74e-04 | grad 2.94 | tok/s 84481
step   2830 | loss 1.9505 | lr 2.42e-04 | grad 1.61 | tok/s 83908
step   2840 | loss 1.9646 | lr 2.01e-04 | grad 2.31 | tok/s 82581
step   2850 | loss 1.7523 | lr 1.55e-04 | grad 1.51 | tok/s 84556
step   2860 | loss 1.7019 | lr 1.09e-04 | grad 1.67 | tok/s 82290
step   2870 | loss 1.8372 | lr 6.65e-05 | grad 2.00 | tok/s 82127
step   2880 | loss 2.2412 | lr 3.24e-05 | grad 1.07 | tok/s 82548
step   2890 | loss 1.9320 | lr 9.84e-06 | grad 1.08 | tok/s 83994
step   2900 | loss 1.6687 | lr 1.07e-06 | grad 1.31 | tok/s 80396
step   2910 | loss 1.6068 | lr 6.93e-06 | grad 1.07 | tok/s 80701
step   2920 | loss 2.1846 | lr 2.68e-05 | grad 1.19 | tok/s 84535
step   2930 | loss 1.6603 | lr 5.89e-05 | grad 0.84 | tok/s 83057
step   2940 | loss 1.7209 | lr 9.98e-05 | grad 1.53 | tok/s 80844
step   2950 | loss 1.7327 | lr 1.46e-04 | grad 1.45 | tok/s 81204
step   2960 | loss 1.9987 | lr 1.92e-04 | grad 2.28 | tok/s 82218
step   2970 | loss 1.7624 | lr 2.35e-04 | grad 1.88 | tok/s 81123
step   2980 | loss 1.7940 | lr 2.69e-04 | grad 3.14 | tok/s 82496
step   2990 | loss 1.7681 | lr 2.91e-04 | grad 1.81 | tok/s 83162
step   3000 | loss 1.6525 | lr 3.00e-04 | grad 2.02 | tok/s 83667
  >>> saved checkpoint: checkpoint_step_003000_loss_1.6525.pt
step   3010 | loss 2.0078 | lr 2.94e-04 | grad 3.89 | tok/s 63885
step   3020 | loss 1.7526 | lr 2.74e-04 | grad 1.67 | tok/s 82134
step   3030 | loss 1.8794 | lr 2.42e-04 | grad 2.95 | tok/s 81811
step   3040 | loss 1.7116 | lr 2.01e-04 | grad 2.31 | tok/s 82169
step   3050 | loss 1.8813 | lr 1.55e-04 | grad 1.77 | tok/s 82423
step   3060 | loss 1.6519 | lr 1.09e-04 | grad 0.96 | tok/s 81163
step   3070 | loss 1.6455 | lr 6.65e-05 | grad 1.25 | tok/s 81958
step   3080 | loss 1.6758 | lr 3.24e-05 | grad 0.99 | tok/s 83283
step   3090 | loss 1.6148 | lr 9.84e-06 | grad 0.79 | tok/s 83415
step   3100 | loss 1.6465 | lr 1.07e-06 | grad 0.88 | tok/s 81500
step   3110 | loss 1.6287 | lr 6.93e-06 | grad 0.93 | tok/s 83592
step   3120 | loss 1.6381 | lr 2.68e-05 | grad 1.02 | tok/s 81039

Training complete! Final step: 3127
