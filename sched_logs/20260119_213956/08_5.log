# Job 8: 5
# GPU: 6
# Command: python train.py --level 5 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/5
# Started: 2026-01-19T21:50:04.394308
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/5/level5_100m_20260119_215010
Auto r_h_mode: none (level 5 has bounded/no W_h)
Model: Level 5, 6,334,080 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 8.7640 | lr 2.70e-05 | grad 9.81 | tok/s 8105
step     20 | loss 7.3742 | lr 5.70e-05 | grad 7.81 | tok/s 10750
step     30 | loss 7.6258 | lr 8.70e-05 | grad 5.38 | tok/s 11397
step     40 | loss 5.5181 | lr 1.17e-04 | grad 2.77 | tok/s 11295
step     50 | loss 4.1206 | lr 1.47e-04 | grad 2.17 | tok/s 11317
step     60 | loss 3.6511 | lr 1.77e-04 | grad 1.52 | tok/s 11098
step     70 | loss 2.8970 | lr 2.07e-04 | grad 1.32 | tok/s 10733
step     80 | loss 3.4781 | lr 2.37e-04 | grad 3.98 | tok/s 11092
step     90 | loss 3.0693 | lr 2.67e-04 | grad 3.38 | tok/s 10665
step    100 | loss 2.6964 | lr 2.97e-04 | grad 1.16 | tok/s 10802
step    110 | loss 2.6435 | lr 6.94e-06 | grad 1.97 | tok/s 10521
step    120 | loss 2.7770 | lr 2.69e-05 | grad 1.69 | tok/s 10387
step    130 | loss 2.7277 | lr 5.89e-05 | grad 1.04 | tok/s 10612
step    140 | loss 2.5329 | lr 9.99e-05 | grad 0.76 | tok/s 10661
step    150 | loss 2.4287 | lr 1.46e-04 | grad 1.50 | tok/s 10167
step    160 | loss 2.3041 | lr 1.92e-04 | grad 1.28 | tok/s 10266
step    170 | loss 2.4750 | lr 2.35e-04 | grad 2.64 | tok/s 10670
step    180 | loss 2.5327 | lr 2.69e-04 | grad 1.41 | tok/s 10538
step    190 | loss 2.2578 | lr 2.91e-04 | grad 1.11 | tok/s 10516
step    200 | loss 2.0084 | lr 3.00e-04 | grad 0.88 | tok/s 10734
step    210 | loss 2.4255 | lr 2.94e-04 | grad 1.57 | tok/s 10411
step    220 | loss 2.3434 | lr 2.74e-04 | grad 1.11 | tok/s 10879
step    230 | loss 2.1657 | lr 2.42e-04 | grad 1.13 | tok/s 10569
step    240 | loss 2.1910 | lr 2.01e-04 | grad 1.42 | tok/s 10754
step    250 | loss 2.1523 | lr 1.55e-04 | grad 1.11 | tok/s 10613
step    260 | loss 2.1909 | lr 1.09e-04 | grad 0.57 | tok/s 10096
step    270 | loss 2.0588 | lr 6.65e-05 | grad 0.96 | tok/s 10623
step    280 | loss 1.9608 | lr 3.24e-05 | grad 1.39 | tok/s 10532
step    290 | loss 1.9901 | lr 9.84e-06 | grad 0.87 | tok/s 11125
step    300 | loss 1.9555 | lr 1.07e-06 | grad 0.80 | tok/s 11117
step    310 | loss 1.9665 | lr 6.94e-06 | grad 0.74 | tok/s 11108
step    320 | loss 2.0127 | lr 2.69e-05 | grad 1.86 | tok/s 10711
step    330 | loss 2.0674 | lr 5.89e-05 | grad 0.66 | tok/s 10423
step    340 | loss 2.0950 | lr 9.99e-05 | grad 1.54 | tok/s 10653
step    350 | loss 2.0939 | lr 1.46e-04 | grad 0.79 | tok/s 10330
step    360 | loss 2.0564 | lr 1.92e-04 | grad 2.23 | tok/s 10272
step    370 | loss 1.9325 | lr 2.35e-04 | grad 0.90 | tok/s 10585
step    380 | loss 2.4194 | lr 2.69e-04 | grad 1.21 | tok/s 10967
step    390 | loss 2.0184 | lr 2.91e-04 | grad 0.92 | tok/s 10540
step    400 | loss 2.1339 | lr 3.00e-04 | grad 2.27 | tok/s 10848

Training complete! Final step: 405
