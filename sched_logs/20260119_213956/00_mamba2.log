# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 896  --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/mamba2
# Started: 2026-01-19T21:39:56.176015
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/mamba2/levelmamba2_100m_20260119_214002
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4576 | lr 2.70e-05 | grad 5.00 | tok/s 9550
step     20 | loss 4.3086 | lr 5.70e-05 | grad 6.47 | tok/s 66707
step     30 | loss 4.5681 | lr 8.70e-05 | grad 3.27 | tok/s 70275
step     40 | loss 3.1654 | lr 1.17e-04 | grad 1.87 | tok/s 69735
step     50 | loss 2.3871 | lr 1.47e-04 | grad 1.48 | tok/s 69638
step     60 | loss 2.7602 | lr 1.77e-04 | grad 3.27 | tok/s 67936
step     70 | loss 2.4721 | lr 2.07e-04 | grad 1.88 | tok/s 65370
step     80 | loss 2.8040 | lr 2.37e-04 | grad 3.19 | tok/s 67483
step     90 | loss 2.7423 | lr 2.67e-04 | grad 4.06 | tok/s 64992
step    100 | loss 2.2442 | lr 2.97e-04 | grad 1.20 | tok/s 65282
step    110 | loss 2.2791 | lr 6.94e-06 | grad 1.96 | tok/s 61892
step    120 | loss 2.4864 | lr 2.69e-05 | grad 1.52 | tok/s 61399
step    130 | loss 2.3556 | lr 5.89e-05 | grad 1.16 | tok/s 62600
step    140 | loss 2.0978 | lr 9.99e-05 | grad 1.00 | tok/s 62701
step    150 | loss 1.9593 | lr 1.46e-04 | grad 1.89 | tok/s 59612
step    160 | loss 1.8689 | lr 1.92e-04 | grad 1.62 | tok/s 59978
step    170 | loss 2.0567 | lr 2.35e-04 | grad 3.31 | tok/s 61841
step    180 | loss 2.0396 | lr 2.69e-04 | grad 1.27 | tok/s 61798
step    190 | loss 1.7566 | lr 2.91e-04 | grad 1.27 | tok/s 62448
step    200 | loss 1.3555 | lr 3.00e-04 | grad 1.05 | tok/s 63583
step    210 | loss 2.1286 | lr 2.94e-04 | grad 1.72 | tok/s 60856
step    220 | loss 1.9220 | lr 2.74e-04 | grad 1.30 | tok/s 62693
step    230 | loss 1.8167 | lr 2.42e-04 | grad 1.48 | tok/s 60016
step    240 | loss 1.7883 | lr 2.01e-04 | grad 1.52 | tok/s 61494
step    250 | loss 1.7930 | lr 1.55e-04 | grad 1.31 | tok/s 60483
step    260 | loss 1.8786 | lr 1.09e-04 | grad 0.85 | tok/s 58036
step    270 | loss 1.7248 | lr 6.65e-05 | grad 0.95 | tok/s 59894
step    280 | loss 1.6076 | lr 3.24e-05 | grad 1.73 | tok/s 59770
step    290 | loss 1.6132 | lr 9.84e-06 | grad 0.76 | tok/s 63109
step    300 | loss 1.5957 | lr 1.07e-06 | grad 0.86 | tok/s 62988
step    310 | loss 1.5796 | lr 6.94e-06 | grad 0.68 | tok/s 62566
step    320 | loss 1.6797 | lr 2.69e-05 | grad 1.66 | tok/s 60190
step    330 | loss 1.7104 | lr 5.89e-05 | grad 0.79 | tok/s 58633
step    340 | loss 1.7104 | lr 9.99e-05 | grad 2.16 | tok/s 59737
step    350 | loss 1.7262 | lr 1.46e-04 | grad 0.92 | tok/s 58039
step    360 | loss 1.6728 | lr 1.92e-04 | grad 2.34 | tok/s 58606
step    370 | loss 1.5127 | lr 2.35e-04 | grad 1.00 | tok/s 59686
step    380 | loss 1.9872 | lr 2.69e-04 | grad 1.06 | tok/s 61122
step    390 | loss 1.7107 | lr 2.91e-04 | grad 1.46 | tok/s 58680
step    400 | loss 1.8063 | lr 3.00e-04 | grad 2.44 | tok/s 60276
step    410 | loss 1.5780 | lr 2.94e-04 | grad 2.33 | tok/s 58234
step    420 | loss 1.7149 | lr 2.74e-04 | grad 1.22 | tok/s 57865
step    430 | loss 1.8061 | lr 2.42e-04 | grad 1.23 | tok/s 57649
step    440 | loss 1.8231 | lr 2.01e-04 | grad 0.89 | tok/s 59824
step    450 | loss 1.6817 | lr 1.55e-04 | grad 0.77 | tok/s 58125
step    460 | loss 1.6684 | lr 1.09e-04 | grad 0.67 | tok/s 58407
step    470 | loss 1.6154 | lr 6.65e-05 | grad 1.04 | tok/s 58824
step    480 | loss 1.5190 | lr 3.24e-05 | grad 0.61 | tok/s 56857
step    490 | loss 1.5152 | lr 9.84e-06 | grad 0.64 | tok/s 57813
step    500 | loss 2.4405 | lr 1.07e-06 | grad 0.98 | tok/s 59665
step    510 | loss 1.5621 | lr 6.94e-06 | grad 0.70 | tok/s 58411
step    520 | loss 1.6061 | lr 2.69e-05 | grad 0.63 | tok/s 60161
step    530 | loss 2.0977 | lr 5.89e-05 | grad 0.67 | tok/s 58945
step    540 | loss 1.4940 | lr 9.99e-05 | grad 1.06 | tok/s 58683
step    550 | loss 1.4274 | lr 1.46e-04 | grad 0.59 | tok/s 60192
step    560 | loss 1.3013 | lr 1.92e-04 | grad 0.65 | tok/s 61255
step    570 | loss 1.5541 | lr 2.35e-04 | grad 1.98 | tok/s 59595
step    580 | loss 1.8476 | lr 2.69e-04 | grad 0.88 | tok/s 59027
step    590 | loss 2.1058 | lr 2.91e-04 | grad 1.00 | tok/s 57641
step    600 | loss 1.6728 | lr 3.00e-04 | grad 1.40 | tok/s 57834
step    610 | loss 1.6282 | lr 2.94e-04 | grad 1.29 | tok/s 60614
step    620 | loss 1.6194 | lr 2.74e-04 | grad 0.90 | tok/s 57539
step    630 | loss 1.5244 | lr 2.42e-04 | grad 0.84 | tok/s 59338
step    640 | loss 1.7566 | lr 2.01e-04 | grad 0.96 | tok/s 59376
step    650 | loss 1.5460 | lr 1.55e-04 | grad 0.90 | tok/s 58264
step    660 | loss 1.8173 | lr 1.09e-04 | grad 5.56 | tok/s 57468
step    670 | loss 1.6826 | lr 6.65e-05 | grad 1.80 | tok/s 59470
step    680 | loss 1.6205 | lr 3.24e-05 | grad 1.17 | tok/s 57312
step    690 | loss 1.6463 | lr 9.84e-06 | grad 1.52 | tok/s 57696
step    700 | loss 1.7591 | lr 1.07e-06 | grad 1.55 | tok/s 58113
step    710 | loss 1.6746 | lr 6.94e-06 | grad 1.13 | tok/s 58409
step    720 | loss 1.7480 | lr 2.68e-05 | grad 1.60 | tok/s 58077
step    730 | loss 1.7236 | lr 5.89e-05 | grad 1.19 | tok/s 58597
step    740 | loss 1.6580 | lr 9.99e-05 | grad 1.67 | tok/s 58107
step    750 | loss 1.4766 | lr 1.46e-04 | grad 1.19 | tok/s 57564
step    760 | loss 1.7471 | lr 1.92e-04 | grad 0.68 | tok/s 58085
step    770 | loss 1.5323 | lr 2.35e-04 | grad 0.96 | tok/s 57835
step    780 | loss 1.5788 | lr 2.69e-04 | grad 0.80 | tok/s 58646
step    790 | loss 1.4844 | lr 2.91e-04 | grad 0.57 | tok/s 58964
step    800 | loss 1.4782 | lr 3.00e-04 | grad 0.93 | tok/s 59026
step    810 | loss 1.5830 | lr 2.94e-04 | grad 1.76 | tok/s 58566
step    820 | loss 2.2995 | lr 2.74e-04 | grad 1.38 | tok/s 59885
step    830 | loss 1.7626 | lr 2.42e-04 | grad 0.71 | tok/s 60788
step    840 | loss 1.4267 | lr 2.01e-04 | grad 0.66 | tok/s 60853
step    850 | loss 1.8704 | lr 1.55e-04 | grad 1.11 | tok/s 57858
step    860 | loss 1.6271 | lr 1.09e-04 | grad 0.89 | tok/s 56689
step    870 | loss 1.5426 | lr 6.65e-05 | grad 0.90 | tok/s 58520
step    880 | loss 1.6029 | lr 3.24e-05 | grad 1.11 | tok/s 58166
step    890 | loss 1.5474 | lr 9.84e-06 | grad 0.90 | tok/s 58109
step    900 | loss 1.9395 | lr 1.07e-06 | grad 0.86 | tok/s 56627
step    910 | loss 1.5628 | lr 6.94e-06 | grad 0.75 | tok/s 57721
step    920 | loss 1.5782 | lr 2.68e-05 | grad 0.79 | tok/s 57442
step    930 | loss 1.6638 | lr 5.89e-05 | grad 1.49 | tok/s 57288
step    940 | loss 1.5797 | lr 9.99e-05 | grad 1.71 | tok/s 56668
step    950 | loss 1.6237 | lr 1.46e-04 | grad 1.04 | tok/s 58161
step    960 | loss 1.4067 | lr 1.92e-04 | grad 0.62 | tok/s 60803
step    970 | loss 1.2514 | lr 2.35e-04 | grad 0.45 | tok/s 60746
step    980 | loss 1.4003 | lr 2.69e-04 | grad 2.73 | tok/s 59220
step    990 | loss 1.6782 | lr 2.91e-04 | grad 0.71 | tok/s 57459
step   1000 | loss 1.5895 | lr 3.00e-04 | grad 0.63 | tok/s 56192
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5895.pt
step   1010 | loss 1.7123 | lr 2.94e-04 | grad 1.08 | tok/s 45960
step   1020 | loss 1.4147 | lr 2.74e-04 | grad 0.83 | tok/s 57892
step   1030 | loss 1.8958 | lr 2.42e-04 | grad 0.83 | tok/s 56842
step   1040 | loss 1.5028 | lr 2.01e-04 | grad 1.09 | tok/s 57898
step   1050 | loss 1.5189 | lr 1.55e-04 | grad 0.67 | tok/s 58132
step   1060 | loss 1.6204 | lr 1.09e-04 | grad 1.32 | tok/s 58211
step   1070 | loss 1.7591 | lr 6.65e-05 | grad 0.84 | tok/s 58332
step   1080 | loss 2.1753 | lr 3.24e-05 | grad 1.01 | tok/s 57680
step   1090 | loss 1.8827 | lr 9.84e-06 | grad 0.83 | tok/s 58089
step   1100 | loss 1.5545 | lr 1.07e-06 | grad 0.65 | tok/s 57721
step   1110 | loss 1.5154 | lr 6.93e-06 | grad 0.83 | tok/s 58831
step   1120 | loss 1.7142 | lr 2.68e-05 | grad 0.84 | tok/s 59456
step   1130 | loss 1.5793 | lr 5.89e-05 | grad 0.67 | tok/s 56458
step   1140 | loss 1.4449 | lr 9.99e-05 | grad 0.58 | tok/s 57935
step   1150 | loss 1.6907 | lr 1.46e-04 | grad 1.07 | tok/s 57776
step   1160 | loss 1.3994 | lr 1.92e-04 | grad 0.55 | tok/s 57153
step   1170 | loss 1.7233 | lr 2.35e-04 | grad 0.75 | tok/s 57849
step   1180 | loss 1.4575 | lr 2.69e-04 | grad 0.68 | tok/s 60749
step   1190 | loss 1.3067 | lr 2.91e-04 | grad 0.55 | tok/s 60935
step   1200 | loss 1.2304 | lr 3.00e-04 | grad 0.55 | tok/s 60849
step   1210 | loss 1.2081 | lr 2.94e-04 | grad 0.64 | tok/s 60803
step   1220 | loss 1.2535 | lr 2.74e-04 | grad 0.86 | tok/s 60279
step   1230 | loss 1.4903 | lr 2.42e-04 | grad 0.73 | tok/s 58255
step   1240 | loss 1.5114 | lr 2.01e-04 | grad 0.69 | tok/s 57271
step   1250 | loss 1.5908 | lr 1.55e-04 | grad 3.30 | tok/s 58922
step   1260 | loss 1.6367 | lr 1.09e-04 | grad 2.34 | tok/s 58981
step   1270 | loss 1.6972 | lr 6.65e-05 | grad 1.20 | tok/s 58144
step   1280 | loss 1.5409 | lr 3.24e-05 | grad 0.73 | tok/s 57630
step   1290 | loss 1.4952 | lr 9.84e-06 | grad 0.72 | tok/s 57294
step   1300 | loss 1.5398 | lr 1.07e-06 | grad 0.68 | tok/s 57010
step   1310 | loss 1.6090 | lr 6.93e-06 | grad 0.69 | tok/s 56976
step   1320 | loss 1.6023 | lr 2.68e-05 | grad 1.05 | tok/s 57814
step   1330 | loss 1.5103 | lr 5.89e-05 | grad 0.59 | tok/s 58076
step   1340 | loss 1.4221 | lr 9.99e-05 | grad 0.86 | tok/s 57861
step   1350 | loss 1.4387 | lr 1.46e-04 | grad 1.30 | tok/s 59524
step   1360 | loss 1.4247 | lr 1.92e-04 | grad 0.64 | tok/s 56689
step   1370 | loss 1.5178 | lr 2.35e-04 | grad 0.69 | tok/s 57482
step   1380 | loss 1.5852 | lr 2.69e-04 | grad 0.75 | tok/s 57848
step   1390 | loss 1.5148 | lr 2.91e-04 | grad 1.31 | tok/s 56532
step   1400 | loss 1.5493 | lr 3.00e-04 | grad 9.00 | tok/s 58905
step   1410 | loss 1.5332 | lr 2.94e-04 | grad 0.84 | tok/s 59383
step   1420 | loss 1.5958 | lr 2.74e-04 | grad 0.72 | tok/s 56406
step   1430 | loss 1.4209 | lr 2.42e-04 | grad 0.71 | tok/s 55253
step   1440 | loss 1.3549 | lr 2.01e-04 | grad 0.57 | tok/s 58276
step   1450 | loss 1.3517 | lr 1.55e-04 | grad 1.40 | tok/s 59458
step   1460 | loss 1.4707 | lr 1.09e-04 | grad 0.52 | tok/s 55515
step   1470 | loss 1.5557 | lr 6.65e-05 | grad 2.02 | tok/s 57613
step   1480 | loss 1.4296 | lr 3.24e-05 | grad 1.49 | tok/s 58085
step   1490 | loss 1.5752 | lr 9.84e-06 | grad 1.99 | tok/s 57998
step   1500 | loss 1.6735 | lr 1.07e-06 | grad 1.84 | tok/s 56401
step   1510 | loss 1.5627 | lr 6.93e-06 | grad 0.98 | tok/s 59450
step   1520 | loss 1.5075 | lr 2.68e-05 | grad 1.12 | tok/s 58917
step   1530 | loss 1.4836 | lr 5.89e-05 | grad 0.52 | tok/s 58236
step   1540 | loss 1.4526 | lr 9.99e-05 | grad 0.48 | tok/s 57242
step   1550 | loss 1.4302 | lr 1.46e-04 | grad 2.19 | tok/s 59445
step   1560 | loss 1.8741 | lr 1.92e-04 | grad 0.98 | tok/s 57962
step   1570 | loss 1.4623 | lr 2.35e-04 | grad 0.93 | tok/s 56838
step   1580 | loss 1.5948 | lr 2.69e-04 | grad 0.99 | tok/s 58822
step   1590 | loss 1.4061 | lr 2.91e-04 | grad 0.57 | tok/s 57360
step   1600 | loss 1.5547 | lr 3.00e-04 | grad 0.71 | tok/s 56278
step   1610 | loss 1.3528 | lr 2.94e-04 | grad 0.73 | tok/s 59657
step   1620 | loss 1.5299 | lr 2.74e-04 | grad 0.55 | tok/s 58733
step   1630 | loss 1.4846 | lr 2.42e-04 | grad 0.70 | tok/s 59158
step   1640 | loss 1.4452 | lr 2.01e-04 | grad 0.61 | tok/s 57162
step   1650 | loss 1.4582 | lr 1.55e-04 | grad 0.91 | tok/s 56186
step   1660 | loss 1.4584 | lr 1.09e-04 | grad 0.55 | tok/s 56418
step   1670 | loss 1.4859 | lr 6.65e-05 | grad 1.73 | tok/s 58731
step   1680 | loss 1.8195 | lr 3.24e-05 | grad 0.53 | tok/s 58779
step   1690 | loss 1.4420 | lr 9.84e-06 | grad 0.81 | tok/s 57543
step   1700 | loss 1.6798 | lr 1.07e-06 | grad 0.50 | tok/s 58708
step   1710 | loss 1.4886 | lr 6.93e-06 | grad 0.91 | tok/s 56898
step   1720 | loss 1.4494 | lr 2.68e-05 | grad 0.61 | tok/s 57264
step   1730 | loss 1.5698 | lr 5.89e-05 | grad 0.76 | tok/s 57208
step   1740 | loss 1.4868 | lr 9.99e-05 | grad 0.58 | tok/s 58160
step   1750 | loss 1.4143 | lr 1.46e-04 | grad 0.50 | tok/s 56090
step   1760 | loss 1.5895 | lr 1.92e-04 | grad 0.62 | tok/s 57027
step   1770 | loss 1.5528 | lr 2.35e-04 | grad 0.52 | tok/s 58292
step   1780 | loss 1.4493 | lr 2.69e-04 | grad 0.92 | tok/s 55955
step   1790 | loss 1.5985 | lr 2.91e-04 | grad 0.68 | tok/s 57046
step   1800 | loss 1.3991 | lr 3.00e-04 | grad 0.60 | tok/s 58039
step   1810 | loss 1.4929 | lr 2.94e-04 | grad 0.82 | tok/s 57572
step   1820 | loss 1.4057 | lr 2.74e-04 | grad 0.60 | tok/s 57164
step   1830 | loss 1.4487 | lr 2.42e-04 | grad 0.62 | tok/s 56962
step   1840 | loss 1.4535 | lr 2.01e-04 | grad 0.73 | tok/s 56553
step   1850 | loss 1.6525 | lr 1.55e-04 | grad 0.92 | tok/s 56697
step   1860 | loss 1.4240 | lr 1.09e-04 | grad 0.50 | tok/s 57086
step   1870 | loss 1.4545 | lr 6.65e-05 | grad 1.05 | tok/s 58351
step   1880 | loss 1.4322 | lr 3.24e-05 | grad 0.54 | tok/s 58503
step   1890 | loss 1.4998 | lr 9.84e-06 | grad 0.51 | tok/s 57427
step   1900 | loss 1.4794 | lr 1.07e-06 | grad 0.67 | tok/s 57938
step   1910 | loss 1.5346 | lr 6.93e-06 | grad 1.32 | tok/s 57442
step   1920 | loss 1.4251 | lr 2.68e-05 | grad 0.79 | tok/s 59033
step   1930 | loss 1.4096 | lr 5.89e-05 | grad 0.86 | tok/s 58657
step   1940 | loss 1.3900 | lr 9.99e-05 | grad 0.60 | tok/s 59712
step   1950 | loss 1.4311 | lr 1.46e-04 | grad 0.71 | tok/s 58093
step   1960 | loss 1.7131 | lr 1.92e-04 | grad 3.33 | tok/s 59290
step   1970 | loss 1.3826 | lr 2.35e-04 | grad 1.02 | tok/s 57411
step   1980 | loss 1.4948 | lr 2.69e-04 | grad 1.92 | tok/s 57313
step   1990 | loss 1.5356 | lr 2.91e-04 | grad 0.92 | tok/s 58765
step   2000 | loss 1.4464 | lr 3.00e-04 | grad 0.89 | tok/s 59251
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4464.pt
step   2010 | loss 1.2814 | lr 2.94e-04 | grad 0.57 | tok/s 47339
step   2020 | loss 1.1537 | lr 2.74e-04 | grad 0.48 | tok/s 61077
step   2030 | loss 1.4081 | lr 2.42e-04 | grad 0.72 | tok/s 60261
step   2040 | loss 1.2928 | lr 2.01e-04 | grad 0.46 | tok/s 61122
step   2050 | loss 1.2370 | lr 1.55e-04 | grad 0.73 | tok/s 60107
step   2060 | loss 1.5598 | lr 1.09e-04 | grad 0.59 | tok/s 57540
step   2070 | loss 1.5184 | lr 6.65e-05 | grad 1.03 | tok/s 60312
step   2080 | loss 1.5369 | lr 3.24e-05 | grad 2.25 | tok/s 56952
step   2090 | loss 1.5213 | lr 9.84e-06 | grad 0.76 | tok/s 58994
step   2100 | loss 1.4730 | lr 1.07e-06 | grad 0.77 | tok/s 57175
step   2110 | loss 1.3567 | lr 6.93e-06 | grad 0.57 | tok/s 59429
step   2120 | loss 1.3367 | lr 2.68e-05 | grad 1.01 | tok/s 58811
step   2130 | loss 1.4644 | lr 5.89e-05 | grad 1.46 | tok/s 56931
step   2140 | loss 1.5100 | lr 9.99e-05 | grad 2.34 | tok/s 56824
step   2150 | loss 1.5042 | lr 1.46e-04 | grad 0.47 | tok/s 57695
step   2160 | loss 1.4831 | lr 1.92e-04 | grad 0.53 | tok/s 57090
step   2170 | loss 1.5438 | lr 2.35e-04 | grad 0.66 | tok/s 57730
step   2180 | loss 1.4284 | lr 2.69e-04 | grad 0.66 | tok/s 58817
step   2190 | loss 1.6817 | lr 2.91e-04 | grad 0.67 | tok/s 58763

Training complete! Final step: 2195
