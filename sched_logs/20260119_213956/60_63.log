# Job 60: 63
# GPU: 6
# Command: python train.py --level 63 --dim 512 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/all_models_20260119_213946/63
# Started: 2026-01-19T22:51:08.548840
============================================================

Using device: cuda
Output directory: benchmark_results/all_models_20260119_213946/63/level63_100m_20260119_225113
Auto r_h_mode: none (level 63 has bounded/no W_h)
Model: Level 63, 84,068,864 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4566 | lr 2.70e-05 | grad 9.81 | tok/s 23515
step     20 | loss 4.4547 | lr 5.70e-05 | grad 7.25 | tok/s 58455
step     30 | loss 4.7046 | lr 8.70e-05 | grad 3.20 | tok/s 61299
step     40 | loss 3.9198 | lr 1.17e-04 | grad 1.57 | tok/s 60348
step     50 | loss 3.5852 | lr 1.47e-04 | grad 2.48 | tok/s 59198
step     60 | loss 3.6536 | lr 1.77e-04 | grad 1.66 | tok/s 58590
step     70 | loss 3.2801 | lr 2.07e-04 | grad 1.10 | tok/s 56019
step     80 | loss 3.5285 | lr 2.37e-04 | grad 1.82 | tok/s 59663
step     90 | loss 3.4612 | lr 2.67e-04 | grad 2.83 | tok/s 57447
step    100 | loss 3.3243 | lr 2.97e-04 | grad 0.93 | tok/s 58222
step    110 | loss 3.2438 | lr 6.94e-06 | grad 1.61 | tok/s 53004
step    120 | loss 3.6146 | lr 2.69e-05 | grad 1.14 | tok/s 53134
step    130 | loss 3.4149 | lr 5.89e-05 | grad 0.98 | tok/s 54921
step    140 | loss 3.2066 | lr 9.99e-05 | grad 0.69 | tok/s 54680
step    150 | loss 3.1032 | lr 1.46e-04 | grad 1.79 | tok/s 51746
step    160 | loss 3.0677 | lr 1.92e-04 | grad 0.93 | tok/s 52425
step    170 | loss 3.2417 | lr 2.35e-04 | grad 1.98 | tok/s 53996
step    180 | loss 3.3324 | lr 2.69e-04 | grad 0.96 | tok/s 54149
step    190 | loss 3.1697 | lr 2.91e-04 | grad 1.19 | tok/s 54915
step    200 | loss 3.1420 | lr 3.00e-04 | grad 3.86 | tok/s 56254
step    210 | loss 3.1562 | lr 2.94e-04 | grad 2.64 | tok/s 54035
step    220 | loss 3.1279 | lr 2.74e-04 | grad 1.23 | tok/s 55725
step    230 | loss 2.8329 | lr 2.42e-04 | grad 1.35 | tok/s 53885
step    240 | loss 2.8152 | lr 2.01e-04 | grad 1.54 | tok/s 54995
step    250 | loss 2.6674 | lr 1.55e-04 | grad 1.10 | tok/s 54340
step    260 | loss 2.6170 | lr 1.09e-04 | grad 1.00 | tok/s 51893
step    270 | loss 2.5322 | lr 6.65e-05 | grad 0.91 | tok/s 54070
step    280 | loss 2.4154 | lr 3.24e-05 | grad 1.76 | tok/s 53600
step    290 | loss 2.4907 | lr 9.84e-06 | grad 0.71 | tok/s 57034
step    300 | loss 2.4819 | lr 1.07e-06 | grad 0.70 | tok/s 57050
step    310 | loss 2.4769 | lr 6.94e-06 | grad 0.60 | tok/s 56749
step    320 | loss 2.4553 | lr 2.69e-05 | grad 1.42 | tok/s 54749
step    330 | loss 2.5270 | lr 5.89e-05 | grad 0.69 | tok/s 53346
step    340 | loss 2.5443 | lr 9.99e-05 | grad 1.23 | tok/s 54485
step    350 | loss 2.5335 | lr 1.46e-04 | grad 1.77 | tok/s 52820
step    360 | loss 2.5120 | lr 1.92e-04 | grad 2.02 | tok/s 53681
step    370 | loss 2.4246 | lr 2.35e-04 | grad 1.67 | tok/s 54637
step    380 | loss 2.7991 | lr 2.69e-04 | grad 1.82 | tok/s 55978
step    390 | loss 2.4073 | lr 2.91e-04 | grad 1.42 | tok/s 54000
step    400 | loss 2.5052 | lr 3.00e-04 | grad 1.73 | tok/s 55310
step    410 | loss 2.2115 | lr 2.94e-04 | grad 2.66 | tok/s 53671
step    420 | loss 2.3395 | lr 2.74e-04 | grad 1.40 | tok/s 53347
step    430 | loss 2.4189 | lr 2.42e-04 | grad 1.41 | tok/s 53059
step    440 | loss 2.5410 | lr 2.01e-04 | grad 1.16 | tok/s 55364
step    450 | loss 2.2247 | lr 1.55e-04 | grad 0.82 | tok/s 53618
step    460 | loss 2.1901 | lr 1.09e-04 | grad 0.76 | tok/s 53979
step    470 | loss 2.2162 | lr 6.65e-05 | grad 1.57 | tok/s 54824
step    480 | loss 2.1157 | lr 3.24e-05 | grad 0.64 | tok/s 52614
step    490 | loss 2.0674 | lr 9.84e-06 | grad 0.55 | tok/s 53596
step    500 | loss 2.9364 | lr 1.07e-06 | grad 0.82 | tok/s 55376
step    510 | loss 2.0578 | lr 6.94e-06 | grad 0.63 | tok/s 54094
step    520 | loss 2.1415 | lr 2.69e-05 | grad 0.53 | tok/s 55734
step    530 | loss 2.5474 | lr 5.89e-05 | grad 0.98 | tok/s 54280
step    540 | loss 2.0695 | lr 9.99e-05 | grad 0.97 | tok/s 54588
step    550 | loss 2.0448 | lr 1.46e-04 | grad 0.99 | tok/s 55968
step    560 | loss 1.9294 | lr 1.92e-04 | grad 1.09 | tok/s 56448
step    570 | loss 2.1488 | lr 2.35e-04 | grad 2.53 | tok/s 55755
step    580 | loss 2.4082 | lr 2.69e-04 | grad 1.11 | tok/s 54811
step    590 | loss 2.6186 | lr 2.91e-04 | grad 1.91 | tok/s 53659
step    600 | loss 2.2442 | lr 3.00e-04 | grad 1.48 | tok/s 53794
step    610 | loss 2.2235 | lr 2.94e-04 | grad 1.45 | tok/s 56390
step    620 | loss 2.1112 | lr 2.74e-04 | grad 0.87 | tok/s 53339
step    630 | loss 2.0644 | lr 2.42e-04 | grad 0.96 | tok/s 54821
step    640 | loss 2.3518 | lr 2.01e-04 | grad 1.34 | tok/s 54977
step    650 | loss 2.0699 | lr 1.55e-04 | grad 1.18 | tok/s 54313
step    660 | loss 2.3368 | lr 1.09e-04 | grad 3.86 | tok/s 53617
step    670 | loss 2.1654 | lr 6.65e-05 | grad 2.16 | tok/s 55707
step    680 | loss 2.0929 | lr 3.24e-05 | grad 0.91 | tok/s 53738
step    690 | loss 2.1219 | lr 9.84e-06 | grad 1.06 | tok/s 54113
step    700 | loss 2.2114 | lr 1.07e-06 | grad 1.00 | tok/s 53926
step    710 | loss 2.1275 | lr 6.94e-06 | grad 0.93 | tok/s 54579
step    720 | loss 2.2397 | lr 2.68e-05 | grad 1.25 | tok/s 54286
step    730 | loss 2.1785 | lr 5.89e-05 | grad 1.20 | tok/s 54673
step    740 | loss 2.0999 | lr 9.99e-05 | grad 1.75 | tok/s 54116
step    750 | loss 1.9438 | lr 1.46e-04 | grad 1.48 | tok/s 53812
step    760 | loss 2.2902 | lr 1.92e-04 | grad 1.39 | tok/s 54477
step    770 | loss 1.9812 | lr 2.35e-04 | grad 1.30 | tok/s 54159
step    780 | loss 2.0273 | lr 2.69e-04 | grad 1.39 | tok/s 54783
step    790 | loss 2.0263 | lr 2.91e-04 | grad 1.16 | tok/s 55199
step    800 | loss 1.9901 | lr 3.00e-04 | grad 1.23 | tok/s 55266
step    810 | loss 1.9925 | lr 2.94e-04 | grad 1.72 | tok/s 54697
step    820 | loss 2.5676 | lr 2.74e-04 | grad 2.05 | tok/s 56149
step    830 | loss 2.2024 | lr 2.42e-04 | grad 0.85 | tok/s 57092
step    840 | loss 1.9282 | lr 2.01e-04 | grad 0.96 | tok/s 57071
step    850 | loss 2.2258 | lr 1.55e-04 | grad 1.37 | tok/s 54446
step    860 | loss 2.0314 | lr 1.09e-04 | grad 1.15 | tok/s 53201
step    870 | loss 1.9761 | lr 6.65e-05 | grad 0.91 | tok/s 54785
step    880 | loss 2.0364 | lr 3.24e-05 | grad 1.27 | tok/s 54496
step    890 | loss 1.9210 | lr 9.84e-06 | grad 0.85 | tok/s 54465
step    900 | loss 2.3393 | lr 1.07e-06 | grad 0.78 | tok/s 53003
step    910 | loss 1.9813 | lr 6.94e-06 | grad 0.75 | tok/s 53987
step    920 | loss 1.9528 | lr 2.68e-05 | grad 0.74 | tok/s 53816
step    930 | loss 2.0738 | lr 5.89e-05 | grad 1.38 | tok/s 53691
step    940 | loss 1.9719 | lr 9.99e-05 | grad 1.25 | tok/s 53114
step    950 | loss 2.0165 | lr 1.46e-04 | grad 1.52 | tok/s 54213
step    960 | loss 1.8174 | lr 1.92e-04 | grad 0.82 | tok/s 56997
step    970 | loss 1.6416 | lr 2.35e-04 | grad 1.12 | tok/s 57008
step    980 | loss 1.7734 | lr 2.69e-04 | grad 2.47 | tok/s 55425
step    990 | loss 2.1771 | lr 2.91e-04 | grad 1.47 | tok/s 53894
step   1000 | loss 1.9791 | lr 3.00e-04 | grad 0.99 | tok/s 52625
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9791.pt
step   1010 | loss 2.2052 | lr 2.94e-04 | grad 1.44 | tok/s 46940
step   1020 | loss 1.8504 | lr 2.74e-04 | grad 1.28 | tok/s 54011
step   1030 | loss 2.1578 | lr 2.42e-04 | grad 1.13 | tok/s 53128
step   1040 | loss 1.8565 | lr 2.01e-04 | grad 1.59 | tok/s 54304
step   1050 | loss 1.8589 | lr 1.55e-04 | grad 0.95 | tok/s 54363
step   1060 | loss 2.1189 | lr 1.09e-04 | grad 1.35 | tok/s 54576
step   1070 | loss 2.1926 | lr 6.65e-05 | grad 0.87 | tok/s 54697
step   1080 | loss 2.4319 | lr 3.24e-05 | grad 1.02 | tok/s 54055
step   1090 | loss 2.1646 | lr 9.84e-06 | grad 0.99 | tok/s 54438
step   1100 | loss 1.8482 | lr 1.07e-06 | grad 0.87 | tok/s 54149
step   1110 | loss 1.8947 | lr 6.93e-06 | grad 0.83 | tok/s 54830
step   1120 | loss 2.1625 | lr 2.68e-05 | grad 0.80 | tok/s 55684
step   1130 | loss 1.8603 | lr 5.89e-05 | grad 0.79 | tok/s 52981
step   1140 | loss 1.7654 | lr 9.99e-05 | grad 0.87 | tok/s 54307
step   1150 | loss 2.0429 | lr 1.46e-04 | grad 1.12 | tok/s 54228
step   1160 | loss 1.7087 | lr 1.92e-04 | grad 0.82 | tok/s 53611
step   1170 | loss 1.9843 | lr 2.35e-04 | grad 0.89 | tok/s 54262
step   1180 | loss 1.7377 | lr 2.69e-04 | grad 0.93 | tok/s 57052
step   1190 | loss 1.6157 | lr 2.91e-04 | grad 0.89 | tok/s 57063
step   1200 | loss 1.5309 | lr 3.00e-04 | grad 0.84 | tok/s 57036
step   1210 | loss 1.4828 | lr 2.94e-04 | grad 0.92 | tok/s 57039
step   1220 | loss 1.5247 | lr 2.74e-04 | grad 1.20 | tok/s 56557
step   1230 | loss 1.7234 | lr 2.42e-04 | grad 1.04 | tok/s 54536
step   1240 | loss 1.8158 | lr 2.01e-04 | grad 1.05 | tok/s 53626
step   1250 | loss 1.9166 | lr 1.55e-04 | grad 3.19 | tok/s 55280
step   1260 | loss 1.9647 | lr 1.09e-04 | grad 2.12 | tok/s 55212
step   1270 | loss 1.9853 | lr 6.65e-05 | grad 1.25 | tok/s 54534
step   1280 | loss 1.8297 | lr 3.24e-05 | grad 1.01 | tok/s 53887
step   1290 | loss 1.7706 | lr 9.84e-06 | grad 1.09 | tok/s 53684
step   1300 | loss 1.8239 | lr 1.07e-06 | grad 0.88 | tok/s 53352
step   1310 | loss 1.9259 | lr 6.93e-06 | grad 0.85 | tok/s 53475
step   1320 | loss 1.8870 | lr 2.68e-05 | grad 1.31 | tok/s 54469
step   1330 | loss 1.8076 | lr 5.89e-05 | grad 0.85 | tok/s 55442
step   1340 | loss 1.7422 | lr 9.99e-05 | grad 1.06 | tok/s 55778
step   1350 | loss 1.8138 | lr 1.46e-04 | grad 1.86 | tok/s 57631
step   1360 | loss 1.7391 | lr 1.92e-04 | grad 1.05 | tok/s 54710
step   1370 | loss 1.8360 | lr 2.35e-04 | grad 1.30 | tok/s 55724
step   1380 | loss 1.9522 | lr 2.69e-04 | grad 1.76 | tok/s 55882
step   1390 | loss 1.8336 | lr 2.91e-04 | grad 1.53 | tok/s 54305
step   1400 | loss 1.8400 | lr 3.00e-04 | grad 3.53 | tok/s 56448
step   1410 | loss 1.9259 | lr 2.94e-04 | grad 1.38 | tok/s 57496
step   1420 | loss 1.9395 | lr 2.74e-04 | grad 1.09 | tok/s 54859
step   1430 | loss 1.7251 | lr 2.42e-04 | grad 1.04 | tok/s 53335
step   1440 | loss 1.6188 | lr 2.01e-04 | grad 0.85 | tok/s 55483
step   1450 | loss 1.6690 | lr 1.55e-04 | grad 2.12 | tok/s 55650
step   1460 | loss 1.7108 | lr 1.09e-04 | grad 0.95 | tok/s 51983
step   1470 | loss 1.8236 | lr 6.65e-05 | grad 1.82 | tok/s 53665
step   1480 | loss 1.6955 | lr 3.24e-05 | grad 1.56 | tok/s 54435
step   1490 | loss 1.8325 | lr 9.84e-06 | grad 2.36 | tok/s 54367
step   1500 | loss 1.9221 | lr 1.07e-06 | grad 1.94 | tok/s 52894
step   1510 | loss 1.8303 | lr 6.93e-06 | grad 1.11 | tok/s 55648
step   1520 | loss 1.7958 | lr 2.68e-05 | grad 1.22 | tok/s 55158
step   1530 | loss 1.7397 | lr 5.89e-05 | grad 0.63 | tok/s 54720
step   1540 | loss 1.7250 | lr 9.99e-05 | grad 0.78 | tok/s 53582
step   1550 | loss 1.6961 | lr 1.46e-04 | grad 2.09 | tok/s 55850
step   1560 | loss 2.2146 | lr 1.92e-04 | grad 1.33 | tok/s 54727
step   1570 | loss 1.7367 | lr 2.35e-04 | grad 1.27 | tok/s 53552
step   1580 | loss 1.9394 | lr 2.69e-04 | grad 1.53 | tok/s 55167
step   1590 | loss 1.6920 | lr 2.91e-04 | grad 1.14 | tok/s 53826
step   1600 | loss 1.7642 | lr 3.00e-04 | grad 1.09 | tok/s 52778
step   1610 | loss 1.6436 | lr 2.94e-04 | grad 0.96 | tok/s 55826
step   1620 | loss 1.7764 | lr 2.74e-04 | grad 1.05 | tok/s 55040
step   1630 | loss 1.8010 | lr 2.42e-04 | grad 1.40 | tok/s 55499
step   1640 | loss 1.6929 | lr 2.01e-04 | grad 0.72 | tok/s 53597
step   1650 | loss 1.7094 | lr 1.55e-04 | grad 1.51 | tok/s 52821
step   1660 | loss 1.7057 | lr 1.09e-04 | grad 0.85 | tok/s 53105
step   1670 | loss 1.7637 | lr 6.65e-05 | grad 1.99 | tok/s 55199
step   1680 | loss 2.3531 | lr 3.24e-05 | grad 0.72 | tok/s 55297
step   1690 | loss 1.6850 | lr 9.84e-06 | grad 1.04 | tok/s 54028
step   1700 | loss 2.0403 | lr 1.07e-06 | grad 0.72 | tok/s 55194
step   1710 | loss 1.7167 | lr 6.93e-06 | grad 0.98 | tok/s 53549
step   1720 | loss 1.7256 | lr 2.68e-05 | grad 0.91 | tok/s 53803
step   1730 | loss 1.8383 | lr 5.89e-05 | grad 0.85 | tok/s 53756
step   1740 | loss 1.7303 | lr 9.99e-05 | grad 0.64 | tok/s 54606
step   1750 | loss 1.6395 | lr 1.46e-04 | grad 0.76 | tok/s 52655
step   1760 | loss 1.9137 | lr 1.92e-04 | grad 1.01 | tok/s 53430
step   1770 | loss 1.8454 | lr 2.35e-04 | grad 1.12 | tok/s 54616
step   1780 | loss 1.7161 | lr 2.69e-04 | grad 1.20 | tok/s 52562
step   1790 | loss 1.9301 | lr 2.91e-04 | grad 1.01 | tok/s 53579
step   1800 | loss 1.6402 | lr 3.00e-04 | grad 0.86 | tok/s 54506
step   1810 | loss 1.7215 | lr 2.94e-04 | grad 1.25 | tok/s 54250
step   1820 | loss 1.6781 | lr 2.74e-04 | grad 1.14 | tok/s 53640
step   1830 | loss 1.7065 | lr 2.42e-04 | grad 0.98 | tok/s 53542
step   1840 | loss 1.6770 | lr 2.01e-04 | grad 1.16 | tok/s 53074
step   1850 | loss 1.8926 | lr 1.55e-04 | grad 1.07 | tok/s 53555
step   1860 | loss 1.6509 | lr 1.09e-04 | grad 0.61 | tok/s 53439
step   1870 | loss 1.6997 | lr 6.65e-05 | grad 1.29 | tok/s 54651
step   1880 | loss 1.6404 | lr 3.24e-05 | grad 0.68 | tok/s 54773
step   1890 | loss 1.7453 | lr 9.84e-06 | grad 0.64 | tok/s 53851
step   1900 | loss 1.7832 | lr 1.07e-06 | grad 1.33 | tok/s 54365
step   1910 | loss 1.7425 | lr 6.93e-06 | grad 1.18 | tok/s 53613
step   1920 | loss 1.6481 | lr 2.68e-05 | grad 1.02 | tok/s 55281
step   1930 | loss 1.6283 | lr 5.89e-05 | grad 0.88 | tok/s 54852
step   1940 | loss 1.5586 | lr 9.99e-05 | grad 0.86 | tok/s 55894
step   1950 | loss 1.6271 | lr 1.46e-04 | grad 0.92 | tok/s 54482
step   1960 | loss 2.0168 | lr 1.92e-04 | grad 3.38 | tok/s 55934
step   1970 | loss 1.7094 | lr 2.35e-04 | grad 1.35 | tok/s 53654
step   1980 | loss 1.7688 | lr 2.69e-04 | grad 2.16 | tok/s 53596
step   1990 | loss 1.8634 | lr 2.91e-04 | grad 1.06 | tok/s 54844
step   2000 | loss 1.7666 | lr 3.00e-04 | grad 1.38 | tok/s 55348
  >>> saved checkpoint: checkpoint_step_002000_loss_1.7666.pt
step   2010 | loss 1.5194 | lr 2.94e-04 | grad 0.75 | tok/s 47207
step   2020 | loss 1.3575 | lr 2.74e-04 | grad 0.84 | tok/s 57077
step   2030 | loss 1.6615 | lr 2.42e-04 | grad 1.16 | tok/s 56322
step   2040 | loss 1.4991 | lr 2.01e-04 | grad 0.68 | tok/s 57267
step   2050 | loss 1.4466 | lr 1.55e-04 | grad 1.04 | tok/s 57008
step   2060 | loss 1.7380 | lr 1.09e-04 | grad 0.85 | tok/s 55775
step   2070 | loss 1.6752 | lr 6.65e-05 | grad 1.27 | tok/s 57605

Training complete! Final step: 2075
