# Job 4: mamba2
# GPU: 4
# Command: python train.py --level mamba2 --dim 2944 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_bigstate/mamba2
# Started: 2026-01-20T02:46:29.278465
============================================================

Using device: cuda
Output directory: benchmark_results/e75_bigstate/mamba2/levelmamba2_100m_20260120_024635
Model: Level mamba2, 1,062,161,552 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9855 | lr 2.70e-05 | grad 13.50 | tok/s 3674
step     20 | loss 2.8033 | lr 5.70e-05 | grad 7.72 | tok/s 9948
step     30 | loss 2.7976 | lr 8.70e-05 | grad 11.12 | tok/s 9805
step     40 | loss 2.9042 | lr 1.17e-04 | grad 15.94 | tok/s 10062
step     50 | loss 4.4227 | lr 1.47e-04 | grad 9.31 | tok/s 10345
step     60 | loss 3.8587 | lr 1.77e-04 | grad 10.94 | tok/s 10261
step     70 | loss 3.1983 | lr 2.07e-04 | grad 13.06 | tok/s 10142
step     80 | loss 3.0270 | lr 2.37e-04 | grad 11.00 | tok/s 10068
step     90 | loss 2.8117 | lr 2.67e-04 | grad 6.72 | tok/s 10007
step    100 | loss 2.7048 | lr 2.97e-04 | grad 8.75 | tok/s 9973
step    110 | loss 2.4982 | lr 6.94e-06 | grad 8.81 | tok/s 9907
step    120 | loss 3.6396 | lr 2.69e-05 | grad 4.97 | tok/s 9586
step    130 | loss 2.4345 | lr 5.89e-05 | grad 3.02 | tok/s 9375
step    140 | loss 2.1080 | lr 9.99e-05 | grad 4.53 | tok/s 9397
step    150 | loss 2.1720 | lr 1.46e-04 | grad 3.69 | tok/s 9705
step    160 | loss 2.1378 | lr 1.92e-04 | grad 5.03 | tok/s 9767
step    170 | loss 2.4077 | lr 2.35e-04 | grad 6.16 | tok/s 9224
step    180 | loss 2.1170 | lr 2.69e-04 | grad 4.31 | tok/s 9551
step    190 | loss 2.0860 | lr 2.91e-04 | grad 3.42 | tok/s 9148
step    200 | loss 1.7900 | lr 3.00e-04 | grad 3.72 | tok/s 9775
step    210 | loss 1.6885 | lr 2.94e-04 | grad 2.81 | tok/s 9489
step    220 | loss 2.2676 | lr 2.74e-04 | grad 4.91 | tok/s 9172
step    230 | loss 2.6820 | lr 2.42e-04 | grad 2.22 | tok/s 9175
step    240 | loss 2.0709 | lr 2.01e-04 | grad 3.02 | tok/s 9222
step    250 | loss 2.2290 | lr 1.55e-04 | grad 4.66 | tok/s 9262
step    260 | loss 1.8524 | lr 1.09e-04 | grad 1.73 | tok/s 9577
step    270 | loss 2.0114 | lr 6.65e-05 | grad 2.30 | tok/s 9568
step    280 | loss 1.7223 | lr 3.24e-05 | grad 1.80 | tok/s 9287
step    290 | loss 1.6786 | lr 9.84e-06 | grad 2.31 | tok/s 8948
step    300 | loss 1.7837 | lr 1.07e-06 | grad 2.28 | tok/s 9099
step    310 | loss 1.8196 | lr 6.94e-06 | grad 1.24 | tok/s 9297
step    320 | loss 1.6491 | lr 2.69e-05 | grad 2.33 | tok/s 8896
step    330 | loss 1.8553 | lr 5.89e-05 | grad 1.70 | tok/s 9330
step    340 | loss 1.8977 | lr 9.99e-05 | grad 6.41 | tok/s 9499
step    350 | loss 1.9368 | lr 1.46e-04 | grad 3.58 | tok/s 9323
step    360 | loss 1.7979 | lr 1.92e-04 | grad 2.11 | tok/s 9555
step    370 | loss 1.5808 | lr 2.35e-04 | grad 1.30 | tok/s 9382
step    380 | loss 1.5987 | lr 2.69e-04 | grad 1.97 | tok/s 9805
step    390 | loss 1.2029 | lr 2.91e-04 | grad 2.05 | tok/s 9894
step    400 | loss 1.1417 | lr 3.00e-04 | grad 2.89 | tok/s 9735
step    410 | loss 2.2128 | lr 2.94e-04 | grad 2.81 | tok/s 9404
step    420 | loss 1.9912 | lr 2.74e-04 | grad 3.44 | tok/s 9399
step    430 | loss 1.8119 | lr 2.42e-04 | grad 2.91 | tok/s 9865
step    440 | loss 1.7666 | lr 2.01e-04 | grad 2.48 | tok/s 9563
step    450 | loss 1.8979 | lr 1.55e-04 | grad 1.98 | tok/s 9428
step    460 | loss 1.6254 | lr 1.09e-04 | grad 3.02 | tok/s 9347
step    470 | loss 1.7128 | lr 6.65e-05 | grad 2.17 | tok/s 9352
step    480 | loss 1.7285 | lr 3.24e-05 | grad 1.98 | tok/s 9809
step    490 | loss 1.8168 | lr 9.84e-06 | grad 1.38 | tok/s 9481
step    500 | loss 1.6773 | lr 1.07e-06 | grad 1.59 | tok/s 9422
step    510 | loss 1.9630 | lr 6.94e-06 | grad 6.28 | tok/s 9269
step    520 | loss 1.7385 | lr 2.69e-05 | grad 1.33 | tok/s 8866
step    530 | loss 1.5838 | lr 5.89e-05 | grad 1.33 | tok/s 9406
step    540 | loss 1.7518 | lr 9.99e-05 | grad 1.71 | tok/s 9388
step    550 | loss 1.6680 | lr 1.46e-04 | grad 2.16 | tok/s 9183
step    560 | loss 1.4351 | lr 1.92e-04 | grad 2.47 | tok/s 9616
step    570 | loss 1.4994 | lr 2.35e-04 | grad 1.40 | tok/s 9890
step    580 | loss 1.3331 | lr 2.69e-04 | grad 1.34 | tok/s 9896
step    590 | loss 1.2832 | lr 2.91e-04 | grad 1.61 | tok/s 9896
step    600 | loss 1.3796 | lr 3.00e-04 | grad 1.73 | tok/s 9881
step    610 | loss 1.2981 | lr 2.94e-04 | grad 1.60 | tok/s 9884
step    620 | loss 1.3353 | lr 2.74e-04 | grad 1.39 | tok/s 9889
step    630 | loss 1.4747 | lr 2.42e-04 | grad 6.88 | tok/s 9744
step    640 | loss 1.9310 | lr 2.01e-04 | grad 6.44 | tok/s 9318
step    650 | loss 1.8418 | lr 1.55e-04 | grad 2.22 | tok/s 9280
step    660 | loss 1.6740 | lr 1.09e-04 | grad 2.30 | tok/s 9355
step    670 | loss 1.6733 | lr 6.65e-05 | grad 2.25 | tok/s 9671
step    680 | loss 1.6795 | lr 3.24e-05 | grad 1.71 | tok/s 9333
step    690 | loss 1.7168 | lr 9.84e-06 | grad 2.00 | tok/s 9256
step    700 | loss 1.6549 | lr 1.07e-06 | grad 1.45 | tok/s 9183

Training complete! Final step: 709
