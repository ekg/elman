# Job 3: E75pch8n24
# GPU: 3
# Command: python train.py --level E75pch8n24 --dim 4480 --expansion 2.0 --n_state 24 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75pc_100m/E75pch8n24
# Started: 2026-01-20T12:47:06.124294
============================================================

Using device: cuda
Output directory: benchmark_results/e75pc_100m/E75pch8n24/levelE75pch8n24_100m_20260120_124712
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75pch8n24, 87,318,400 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 6.2408 | lr 2.70e-05 | grad 34.50 | tok/s 20534
step     20 | loss 5.1934 | lr 5.70e-05 | grad 26.38 | tok/s 59617
step     30 | loss 5.5352 | lr 8.70e-05 | grad 13.12 | tok/s 63056
step     40 | loss 4.3336 | lr 1.17e-04 | grad 6.94 | tok/s 62787
step     50 | loss 3.6952 | lr 1.47e-04 | grad 5.06 | tok/s 62751
step     60 | loss 3.7537 | lr 1.77e-04 | grad 30.12 | tok/s 61485
step     70 | loss 3.3596 | lr 2.07e-04 | grad 8.75 | tok/s 59390
step     80 | loss 3.5313 | lr 2.37e-04 | grad 8.62 | tok/s 61624
step     90 | loss 3.4763 | lr 2.67e-04 | grad 8.62 | tok/s 59480
step    100 | loss 3.2209 | lr 2.97e-04 | grad 6.00 | tok/s 60011
step    110 | loss 3.2900 | lr 6.94e-06 | grad 7.50 | tok/s 54139
step    120 | loss 3.6506 | lr 2.69e-05 | grad 7.03 | tok/s 53973
step    130 | loss 3.3719 | lr 5.89e-05 | grad 4.22 | tok/s 55236
step    140 | loss 3.0619 | lr 9.99e-05 | grad 2.84 | tok/s 55292
step    150 | loss 2.8683 | lr 1.46e-04 | grad 5.62 | tok/s 52653
step    160 | loss 2.7278 | lr 1.92e-04 | grad 3.77 | tok/s 53065
step    170 | loss 2.8567 | lr 2.35e-04 | grad 11.00 | tok/s 54867
step    180 | loss 2.8397 | lr 2.69e-04 | grad 4.44 | tok/s 54897
step    190 | loss 2.5692 | lr 2.91e-04 | grad 4.75 | tok/s 55678
step    200 | loss 2.2834 | lr 3.00e-04 | grad 3.02 | tok/s 56909
step    210 | loss 2.7578 | lr 2.94e-04 | grad 5.19 | tok/s 54502
step    220 | loss 2.6167 | lr 2.74e-04 | grad 4.19 | tok/s 56360
step    230 | loss 2.4412 | lr 2.42e-04 | grad 4.12 | tok/s 54295
step    240 | loss 2.4277 | lr 2.01e-04 | grad 4.50 | tok/s 55359
step    250 | loss 2.3663 | lr 1.55e-04 | grad 3.70 | tok/s 54548
step    260 | loss 2.3738 | lr 1.09e-04 | grad 2.52 | tok/s 52417
step    270 | loss 2.2369 | lr 6.65e-05 | grad 2.42 | tok/s 54316
step    280 | loss 2.1285 | lr 3.24e-05 | grad 4.56 | tok/s 54109
step    290 | loss 2.1600 | lr 9.84e-06 | grad 2.09 | tok/s 56965
step    300 | loss 2.1330 | lr 1.07e-06 | grad 1.69 | tok/s 57034
step    310 | loss 2.1342 | lr 6.94e-06 | grad 1.69 | tok/s 56959
step    320 | loss 2.2137 | lr 2.69e-05 | grad 4.28 | tok/s 54935
step    330 | loss 2.2409 | lr 5.89e-05 | grad 1.64 | tok/s 53500
step    340 | loss 2.2647 | lr 9.99e-05 | grad 4.69 | tok/s 54698
step    350 | loss 2.2838 | lr 1.46e-04 | grad 3.27 | tok/s 53165
step    360 | loss 2.2364 | lr 1.92e-04 | grad 5.62 | tok/s 53777
step    370 | loss 2.0954 | lr 2.35e-04 | grad 4.06 | tok/s 54981
step    380 | loss 2.6341 | lr 2.69e-04 | grad 3.39 | tok/s 56204
step    390 | loss 2.2269 | lr 2.91e-04 | grad 3.22 | tok/s 54133
step    400 | loss 2.3543 | lr 3.00e-04 | grad 9.69 | tok/s 55530
step    410 | loss 2.0919 | lr 2.94e-04 | grad 6.56 | tok/s 53925
step    420 | loss 2.2465 | lr 2.74e-04 | grad 5.28 | tok/s 53498
step    430 | loss 2.3468 | lr 2.42e-04 | grad 5.91 | tok/s 53280
step    440 | loss 2.4929 | lr 2.01e-04 | grad 4.66 | tok/s 55477
step    450 | loss 2.1371 | lr 1.55e-04 | grad 2.58 | tok/s 53857
step    460 | loss 2.0940 | lr 1.09e-04 | grad 2.20 | tok/s 54044
step    470 | loss 2.0790 | lr 6.65e-05 | grad 3.42 | tok/s 54802
step    480 | loss 1.9594 | lr 3.24e-05 | grad 1.64 | tok/s 52930
step    490 | loss 1.9121 | lr 9.84e-06 | grad 1.18 | tok/s 53885
step    500 | loss 2.9066 | lr 1.07e-06 | grad 1.93 | tok/s 55607
step    510 | loss 1.9567 | lr 6.94e-06 | grad 1.55 | tok/s 54302
step    520 | loss 1.9962 | lr 2.69e-05 | grad 1.24 | tok/s 56071
step    530 | loss 2.4686 | lr 5.89e-05 | grad 2.27 | tok/s 54731
step    540 | loss 1.9361 | lr 9.99e-05 | grad 3.59 | tok/s 54780
step    550 | loss 1.8666 | lr 1.46e-04 | grad 1.95 | tok/s 56303
step    560 | loss 1.7140 | lr 1.92e-04 | grad 2.81 | tok/s 57043
step    570 | loss 2.0405 | lr 2.35e-04 | grad 6.03 | tok/s 55698
step    580 | loss 2.4247 | lr 2.69e-04 | grad 2.95 | tok/s 55060
step    590 | loss 2.6849 | lr 2.91e-04 | grad 6.41 | tok/s 53966
step    600 | loss 2.1927 | lr 3.00e-04 | grad 4.12 | tok/s 54049
step    610 | loss 2.2562 | lr 2.94e-04 | grad 3.80 | tok/s 56674
step    620 | loss 2.0530 | lr 2.74e-04 | grad 3.69 | tok/s 53807
step    630 | loss 1.9783 | lr 2.42e-04 | grad 3.08 | tok/s 55502
step    640 | loss 2.3551 | lr 2.01e-04 | grad 3.17 | tok/s 55514
step    650 | loss 2.0167 | lr 1.55e-04 | grad 3.48 | tok/s 54458
step    660 | loss 2.3184 | lr 1.09e-04 | grad 8.94 | tok/s 53835
step    670 | loss 2.1155 | lr 6.65e-05 | grad 5.12 | tok/s 55609
step    680 | loss 2.0286 | lr 3.24e-05 | grad 2.11 | tok/s 53623
step    690 | loss 2.0944 | lr 9.84e-06 | grad 2.38 | tok/s 54193
step    700 | loss 2.1709 | lr 1.07e-06 | grad 2.66 | tok/s 54454
step    710 | loss 2.0847 | lr 6.94e-06 | grad 2.44 | tok/s 54710
step    720 | loss 2.1983 | lr 2.68e-05 | grad 4.00 | tok/s 54478
step    730 | loss 2.1319 | lr 5.89e-05 | grad 2.42 | tok/s 54995
step    740 | loss 2.0428 | lr 9.99e-05 | grad 4.12 | tok/s 54423
step    750 | loss 1.8704 | lr 1.46e-04 | grad 2.81 | tok/s 53914
step    760 | loss 2.3264 | lr 1.92e-04 | grad 2.69 | tok/s 54576
step    770 | loss 1.9548 | lr 2.35e-04 | grad 2.84 | tok/s 54275
step    780 | loss 2.0174 | lr 2.69e-04 | grad 3.72 | tok/s 54822
step    790 | loss 1.9386 | lr 2.91e-04 | grad 2.77 | tok/s 55229
step    800 | loss 1.9563 | lr 3.00e-04 | grad 3.11 | tok/s 55348
step    810 | loss 2.0387 | lr 2.94e-04 | grad 5.22 | tok/s 54793
step    820 | loss 2.7980 | lr 2.74e-04 | grad 4.50 | tok/s 56166
step    830 | loss 2.3071 | lr 2.42e-04 | grad 3.41 | tok/s 57139
step    840 | loss 1.9676 | lr 2.01e-04 | grad 1.78 | tok/s 57141
step    850 | loss 2.3274 | lr 1.55e-04 | grad 3.42 | tok/s 54311
step    860 | loss 2.1272 | lr 1.09e-04 | grad 2.61 | tok/s 53312
step    870 | loss 2.0121 | lr 6.65e-05 | grad 2.52 | tok/s 54804
step    880 | loss 2.0633 | lr 3.24e-05 | grad 2.81 | tok/s 54617
step    890 | loss 1.9583 | lr 9.84e-06 | grad 2.00 | tok/s 54511
step    900 | loss 2.4740 | lr 1.07e-06 | grad 2.17 | tok/s 53116
step    910 | loss 2.0281 | lr 6.94e-06 | grad 1.72 | tok/s 54124
step    920 | loss 1.9908 | lr 2.68e-05 | grad 1.45 | tok/s 53889
step    930 | loss 2.1317 | lr 5.89e-05 | grad 3.28 | tok/s 53807
step    940 | loss 2.0233 | lr 9.99e-05 | grad 3.19 | tok/s 53225
step    950 | loss 2.0463 | lr 1.46e-04 | grad 3.30 | tok/s 54486
step    960 | loss 1.8632 | lr 1.92e-04 | grad 1.84 | tok/s 57115
step    970 | loss 1.6444 | lr 2.35e-04 | grad 1.95 | tok/s 57073
step    980 | loss 1.8167 | lr 2.69e-04 | grad 5.72 | tok/s 55388
step    990 | loss 2.1792 | lr 2.91e-04 | grad 3.12 | tok/s 54008
step   1000 | loss 2.0226 | lr 3.00e-04 | grad 2.52 | tok/s 52798
  >>> saved checkpoint: checkpoint_step_001000_loss_2.0226.pt
step   1010 | loss 2.2732 | lr 2.94e-04 | grad 4.78 | tok/s 47187
step   1020 | loss 1.9001 | lr 2.74e-04 | grad 2.34 | tok/s 54013
step   1030 | loss 2.3074 | lr 2.42e-04 | grad 3.14 | tok/s 53179
step   1040 | loss 1.8858 | lr 2.01e-04 | grad 4.19 | tok/s 54310
step   1050 | loss 1.9014 | lr 1.55e-04 | grad 3.42 | tok/s 54479
step   1060 | loss 2.1694 | lr 1.09e-04 | grad 5.38 | tok/s 54619
step   1070 | loss 2.2723 | lr 6.65e-05 | grad 2.50 | tok/s 54750
step   1080 | loss 2.5052 | lr 3.24e-05 | grad 3.42 | tok/s 54040
step   1090 | loss 2.2332 | lr 9.84e-06 | grad 2.31 | tok/s 54450
step   1100 | loss 1.9058 | lr 1.07e-06 | grad 2.28 | tok/s 54108
step   1110 | loss 1.9811 | lr 6.93e-06 | grad 1.86 | tok/s 55113
step   1120 | loss 2.1901 | lr 2.68e-05 | grad 2.56 | tok/s 55782
step   1130 | loss 1.9202 | lr 5.89e-05 | grad 1.71 | tok/s 52978
step   1140 | loss 1.7963 | lr 9.99e-05 | grad 2.25 | tok/s 54249
step   1150 | loss 2.1030 | lr 1.46e-04 | grad 3.47 | tok/s 54244
step   1160 | loss 1.7304 | lr 1.92e-04 | grad 2.34 | tok/s 53631
step   1170 | loss 2.1844 | lr 2.35e-04 | grad 2.05 | tok/s 54211
step   1180 | loss 1.7862 | lr 2.69e-04 | grad 1.91 | tok/s 57076
step   1190 | loss 1.6621 | lr 2.91e-04 | grad 2.62 | tok/s 56948
step   1200 | loss 1.5777 | lr 3.00e-04 | grad 1.57 | tok/s 56953
step   1210 | loss 1.5317 | lr 2.94e-04 | grad 2.06 | tok/s 57001
step   1220 | loss 1.5928 | lr 2.74e-04 | grad 2.16 | tok/s 56395
step   1230 | loss 1.8460 | lr 2.42e-04 | grad 2.64 | tok/s 54497
step   1240 | loss 1.8946 | lr 2.01e-04 | grad 2.67 | tok/s 53569
step   1250 | loss 1.9845 | lr 1.55e-04 | grad 6.41 | tok/s 55224
step   1260 | loss 2.0470 | lr 1.09e-04 | grad 4.75 | tok/s 55154
step   1270 | loss 2.0692 | lr 6.65e-05 | grad 2.78 | tok/s 54503
step   1280 | loss 1.8860 | lr 3.24e-05 | grad 1.85 | tok/s 53903
step   1290 | loss 1.8275 | lr 9.84e-06 | grad 1.88 | tok/s 53667
step   1300 | loss 1.8883 | lr 1.07e-06 | grad 1.52 | tok/s 53264
step   1310 | loss 2.0209 | lr 6.93e-06 | grad 1.55 | tok/s 53384
step   1320 | loss 1.9479 | lr 2.68e-05 | grad 2.47 | tok/s 54250
step   1330 | loss 1.8721 | lr 5.89e-05 | grad 1.95 | tok/s 54386
step   1340 | loss 1.8085 | lr 9.99e-05 | grad 2.28 | tok/s 54336
step   1350 | loss 1.9045 | lr 1.46e-04 | grad 3.70 | tok/s 55743
step   1360 | loss 1.8112 | lr 1.92e-04 | grad 2.22 | tok/s 52936
step   1370 | loss 1.9212 | lr 2.35e-04 | grad 2.62 | tok/s 53783
step   1380 | loss 2.0103 | lr 2.69e-04 | grad 2.98 | tok/s 54271
step   1390 | loss 1.9104 | lr 2.91e-04 | grad 3.08 | tok/s 52999
step   1400 | loss 2.0389 | lr 3.00e-04 | grad 21.50 | tok/s 55089
step   1410 | loss 2.1105 | lr 2.94e-04 | grad 5.31 | tok/s 55497
step   1420 | loss 2.0654 | lr 2.74e-04 | grad 3.44 | tok/s 53034
step   1430 | loss 1.8090 | lr 2.42e-04 | grad 2.03 | tok/s 51686
step   1440 | loss 1.6967 | lr 2.01e-04 | grad 2.06 | tok/s 54501
step   1450 | loss 1.7531 | lr 1.55e-04 | grad 5.00 | tok/s 55682
step   1460 | loss 1.8212 | lr 1.09e-04 | grad 1.73 | tok/s 51884
step   1470 | loss 1.9201 | lr 6.65e-05 | grad 4.09 | tok/s 53822
step   1480 | loss 1.8090 | lr 3.24e-05 | grad 4.31 | tok/s 54277
step   1490 | loss 1.9128 | lr 9.84e-06 | grad 5.72 | tok/s 54355
step   1500 | loss 1.9996 | lr 1.07e-06 | grad 3.75 | tok/s 52972
step   1510 | loss 1.9393 | lr 6.93e-06 | grad 2.16 | tok/s 55555
step   1520 | loss 1.8811 | lr 2.68e-05 | grad 2.42 | tok/s 55165
step   1530 | loss 1.8240 | lr 5.89e-05 | grad 1.70 | tok/s 54812
step   1540 | loss 1.8029 | lr 9.99e-05 | grad 1.47 | tok/s 53640
step   1550 | loss 1.8392 | lr 1.46e-04 | grad 6.41 | tok/s 55596
step   1560 | loss 2.4139 | lr 1.92e-04 | grad 5.75 | tok/s 54460
step   1570 | loss 1.8371 | lr 2.35e-04 | grad 2.86 | tok/s 53416
step   1580 | loss 2.0777 | lr 2.69e-04 | grad 3.88 | tok/s 54838
step   1590 | loss 1.7902 | lr 2.91e-04 | grad 2.94 | tok/s 53989
step   1600 | loss 1.8647 | lr 3.00e-04 | grad 2.67 | tok/s 52885
step   1610 | loss 1.7038 | lr 2.94e-04 | grad 2.14 | tok/s 56094
step   1620 | loss 1.9114 | lr 2.74e-04 | grad 2.72 | tok/s 55140
step   1630 | loss 1.9234 | lr 2.42e-04 | grad 3.59 | tok/s 55658
step   1640 | loss 1.7948 | lr 2.01e-04 | grad 2.05 | tok/s 53673
step   1650 | loss 1.8050 | lr 1.55e-04 | grad 2.81 | tok/s 52885
step   1660 | loss 1.8226 | lr 1.09e-04 | grad 2.94 | tok/s 53277
step   1670 | loss 1.8853 | lr 6.65e-05 | grad 4.41 | tok/s 55543
step   1680 | loss 2.4354 | lr 3.24e-05 | grad 1.74 | tok/s 55502
step   1690 | loss 1.7638 | lr 9.84e-06 | grad 2.05 | tok/s 54323
step   1700 | loss 2.1385 | lr 1.07e-06 | grad 1.43 | tok/s 55398
step   1710 | loss 1.7939 | lr 6.93e-06 | grad 1.76 | tok/s 53791
step   1720 | loss 1.7989 | lr 2.68e-05 | grad 1.91 | tok/s 54146
step   1730 | loss 1.9603 | lr 5.89e-05 | grad 1.78 | tok/s 54099
step   1740 | loss 1.7940 | lr 9.99e-05 | grad 1.40 | tok/s 54885
step   1750 | loss 1.7336 | lr 1.46e-04 | grad 2.52 | tok/s 52968
step   1760 | loss 2.0242 | lr 1.92e-04 | grad 2.17 | tok/s 53793
step   1770 | loss 1.9304 | lr 2.35e-04 | grad 2.70 | tok/s 54926
step   1780 | loss 1.8180 | lr 2.69e-04 | grad 2.77 | tok/s 52772
step   1790 | loss 2.0899 | lr 2.91e-04 | grad 2.62 | tok/s 53811
step   1800 | loss 1.7364 | lr 3.00e-04 | grad 2.92 | tok/s 54843
step   1810 | loss 1.8286 | lr 2.94e-04 | grad 2.44 | tok/s 54541
step   1820 | loss 1.7526 | lr 2.74e-04 | grad 2.08 | tok/s 53935
step   1830 | loss 1.8164 | lr 2.42e-04 | grad 3.16 | tok/s 53803
step   1840 | loss 1.8022 | lr 2.01e-04 | grad 2.23 | tok/s 53392
step   1850 | loss 2.0290 | lr 1.55e-04 | grad 3.50 | tok/s 53780
step   1860 | loss 1.7417 | lr 1.09e-04 | grad 1.69 | tok/s 53734
step   1870 | loss 1.7831 | lr 6.65e-05 | grad 2.97 | tok/s 54928
step   1880 | loss 1.7270 | lr 3.24e-05 | grad 1.30 | tok/s 55096
step   1890 | loss 1.8416 | lr 9.84e-06 | grad 1.09 | tok/s 54055
step   1900 | loss 1.9078 | lr 1.07e-06 | grad 3.69 | tok/s 54601
step   1910 | loss 1.8334 | lr 6.93e-06 | grad 2.58 | tok/s 54001
step   1920 | loss 1.7290 | lr 2.68e-05 | grad 2.16 | tok/s 55637
step   1930 | loss 1.7217 | lr 5.89e-05 | grad 2.25 | tok/s 51496
step   1940 | loss 1.6360 | lr 9.99e-05 | grad 2.05 | tok/s 56240
step   1950 | loss 1.7646 | lr 1.46e-04 | grad 2.39 | tok/s 54686
step   1960 | loss 2.2241 | lr 1.92e-04 | grad 9.44 | tok/s 55886
step   1970 | loss 1.8902 | lr 2.35e-04 | grad 3.91 | tok/s 54043
step   1980 | loss 1.8782 | lr 2.69e-04 | grad 5.75 | tok/s 53851
step   1990 | loss 2.0166 | lr 2.91e-04 | grad 3.19 | tok/s 55136
step   2000 | loss 1.9311 | lr 3.00e-04 | grad 5.88 | tok/s 55501
  >>> saved checkpoint: checkpoint_step_002000_loss_1.9311.pt
step   2010 | loss 1.6307 | lr 2.94e-04 | grad 2.25 | tok/s 47544
step   2020 | loss 1.4463 | lr 2.74e-04 | grad 1.91 | tok/s 57209
step   2030 | loss 1.7825 | lr 2.42e-04 | grad 3.27 | tok/s 56647
step   2040 | loss 1.6264 | lr 2.01e-04 | grad 1.70 | tok/s 57233
step   2050 | loss 1.5405 | lr 1.55e-04 | grad 2.09 | tok/s 56413
step   2060 | loss 1.8425 | lr 1.09e-04 | grad 1.79 | tok/s 53924
step   2070 | loss 1.7870 | lr 6.65e-05 | grad 2.94 | tok/s 56410

Training complete! Final step: 2077
