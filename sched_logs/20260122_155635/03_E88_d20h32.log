# Job 3: E88_d20h32
# GPU: 3
# Command: python train.py --level E88d_h20 --dim 1920 --depth 20 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10.0 --output benchmark_results/mega_100m_20260122_155533/E88_d20h32
# Started: 2026-01-22T15:56:35.597641
============================================================

Using device: cuda
Output directory: benchmark_results/mega_100m_20260122_155533/E88_d20h32/levelE88d_h20_100m_20260122_155642
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88d_h20, 99,605,280 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.1570 | lr 3.00e-04 | grad 7.38 | tok/s 6201
step     20 | loss 3.2403 | lr 3.00e-04 | grad 11.56 | tok/s 9064
step     30 | loss 4.1848 | lr 3.00e-04 | grad 5.00 | tok/s 9915
Traceback (most recent call last):
  File "/home/erikg/elman/train.py", line 584, in <module>
    train(args)
  File "/home/erikg/elman/train.py", line 510, in train
    scaled_loss.backward()
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/erikg/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

