# Job 1: fla-gdn
# GPU: 1
# Command: python train.py --level fla-gdn --dim 2432 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_010008/fla-gdn
# Started: 2026-01-20T01:00:31.257736
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_010008/fla-gdn/levelfla-gdn_100m_20260120_010037
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 949,645,944 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8648 | lr 2.70e-05 | grad 136.00 | tok/s 437
step     20 | loss 5.1845 | lr 5.70e-05 | grad 86.50 | tok/s 12726
step     30 | loss 4.3919 | lr 8.70e-05 | grad 53.75 | tok/s 12576
step     40 | loss 3.9166 | lr 1.17e-04 | grad 61.25 | tok/s 12976
step     50 | loss 5.2816 | lr 1.47e-04 | grad 44.75 | tok/s 13388
step     60 | loss 3.9452 | lr 1.77e-04 | grad 31.25 | tok/s 13312
step     70 | loss 3.4541 | lr 2.07e-04 | grad 15.62 | tok/s 13237
step     80 | loss 3.7329 | lr 2.37e-04 | grad 13.12 | tok/s 13193
step     90 | loss 3.2248 | lr 2.67e-04 | grad 11.56 | tok/s 13145
step    100 | loss 2.9691 | lr 2.97e-04 | grad 6.44 | tok/s 13097
step    110 | loss 2.6077 | lr 6.94e-06 | grad 6.78 | tok/s 13147
step    120 | loss 3.6849 | lr 2.69e-05 | grad 5.25 | tok/s 12731
step    130 | loss 2.6849 | lr 5.89e-05 | grad 4.66 | tok/s 12422
step    140 | loss 2.3834 | lr 9.99e-05 | grad 5.03 | tok/s 12468
step    150 | loss 2.3015 | lr 1.46e-04 | grad 4.28 | tok/s 12882
step    160 | loss 2.1926 | lr 1.92e-04 | grad 5.97 | tok/s 12916
step    170 | loss 2.5484 | lr 2.35e-04 | grad 10.81 | tok/s 12193
step    180 | loss 2.2367 | lr 2.69e-04 | grad 4.78 | tok/s 12614
step    190 | loss 2.0639 | lr 2.91e-04 | grad 6.16 | tok/s 12098
step    200 | loss 1.8270 | lr 3.00e-04 | grad 2.53 | tok/s 12894
step    210 | loss 1.7183 | lr 2.94e-04 | grad 4.69 | tok/s 12515
step    220 | loss 2.3307 | lr 2.74e-04 | grad 6.78 | tok/s 12085
step    230 | loss 2.4753 | lr 2.42e-04 | grad 4.38 | tok/s 12093
step    240 | loss 2.0884 | lr 2.01e-04 | grad 4.31 | tok/s 12145
step    250 | loss 2.2156 | lr 1.55e-04 | grad 3.55 | tok/s 12197
step    260 | loss 1.8255 | lr 1.09e-04 | grad 2.56 | tok/s 12585
step    270 | loss 2.0095 | lr 6.65e-05 | grad 2.89 | tok/s 12609
step    280 | loss 1.7274 | lr 3.24e-05 | grad 3.06 | tok/s 12225
step    290 | loss 1.7063 | lr 9.84e-06 | grad 2.94 | tok/s 11773
step    300 | loss 1.7981 | lr 1.07e-06 | grad 3.55 | tok/s 11956
step    310 | loss 1.8365 | lr 6.94e-06 | grad 1.66 | tok/s 12227
step    320 | loss 1.6575 | lr 2.69e-05 | grad 2.97 | tok/s 11694
step    330 | loss 1.8703 | lr 5.89e-05 | grad 2.31 | tok/s 12229
step    340 | loss 1.9276 | lr 9.99e-05 | grad 16.12 | tok/s 12471
step    350 | loss 1.9432 | lr 1.46e-04 | grad 5.50 | tok/s 12233
step    360 | loss 1.7718 | lr 1.92e-04 | grad 3.67 | tok/s 12537
step    370 | loss 1.5657 | lr 2.35e-04 | grad 2.03 | tok/s 12310
step    380 | loss 1.5869 | lr 2.69e-04 | grad 1.88 | tok/s 12865
step    390 | loss 1.1641 | lr 2.91e-04 | grad 2.16 | tok/s 12976
step    400 | loss 1.1167 | lr 3.00e-04 | grad 3.12 | tok/s 12780
step    410 | loss 2.1469 | lr 2.94e-04 | grad 3.97 | tok/s 12349
step    420 | loss 1.9794 | lr 2.74e-04 | grad 3.72 | tok/s 12340
step    430 | loss 1.7707 | lr 2.42e-04 | grad 3.31 | tok/s 12928
step    440 | loss 1.7251 | lr 2.01e-04 | grad 3.22 | tok/s 12534
step    450 | loss 1.8679 | lr 1.55e-04 | grad 1.82 | tok/s 12375
step    460 | loss 1.6191 | lr 1.09e-04 | grad 4.62 | tok/s 12261
step    470 | loss 1.7248 | lr 6.65e-05 | grad 3.12 | tok/s 12270
step    480 | loss 1.7405 | lr 3.24e-05 | grad 3.14 | tok/s 12843
step    490 | loss 1.8557 | lr 9.84e-06 | grad 1.68 | tok/s 12415
step    500 | loss 1.6659 | lr 1.07e-06 | grad 2.03 | tok/s 12333
step    510 | loss 1.9687 | lr 6.94e-06 | grad 7.97 | tok/s 12165
step    520 | loss 1.7340 | lr 2.69e-05 | grad 1.80 | tok/s 11659
step    530 | loss 1.5712 | lr 5.89e-05 | grad 1.70 | tok/s 12369
step    540 | loss 1.7502 | lr 9.99e-05 | grad 1.95 | tok/s 12331
step    550 | loss 1.6477 | lr 1.46e-04 | grad 2.41 | tok/s 12059
step    560 | loss 1.4453 | lr 1.92e-04 | grad 3.47 | tok/s 12653
step    570 | loss 1.4787 | lr 2.35e-04 | grad 2.14 | tok/s 12962
step    580 | loss 1.3175 | lr 2.69e-04 | grad 1.72 | tok/s 12966
step    590 | loss 1.2721 | lr 2.91e-04 | grad 1.48 | tok/s 12957
step    600 | loss 1.3699 | lr 3.00e-04 | grad 2.00 | tok/s 12958
step    610 | loss 1.2739 | lr 2.94e-04 | grad 2.03 | tok/s 12962
step    620 | loss 1.3188 | lr 2.74e-04 | grad 1.84 | tok/s 12959
step    630 | loss 1.4755 | lr 2.42e-04 | grad 5.91 | tok/s 12772
step    640 | loss 1.8470 | lr 2.01e-04 | grad 3.86 | tok/s 12235
step    650 | loss 1.8386 | lr 1.55e-04 | grad 3.64 | tok/s 12134
step    660 | loss 1.6579 | lr 1.09e-04 | grad 2.39 | tok/s 12250
step    670 | loss 1.6558 | lr 6.65e-05 | grad 2.73 | tok/s 12649

Training complete! Final step: 677
