# Job 11: 32
# GPU: 6
# Command: python train.py --level 32 --dim 1920 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_010008/32
# Started: 2026-01-20T01:10:43.183927
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_010008/32/level32_100m_20260120_011047
Auto r_h_mode: none (level 32 has bounded/no W_h)
Model: Level 32, 1,032,800,640 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.2659 | lr 2.70e-05 | grad 17.00 | tok/s 5795
step     20 | loss 2.6494 | lr 5.70e-05 | grad 8.88 | tok/s 8293
step     30 | loss 2.9373 | lr 8.70e-05 | grad 6.84 | tok/s 8238
step     40 | loss 2.9704 | lr 1.17e-04 | grad 28.00 | tok/s 8525
step     50 | loss 4.4225 | lr 1.47e-04 | grad 7.72 | tok/s 8847
step     60 | loss 3.8704 | lr 1.77e-04 | grad 7.50 | tok/s 8836
step     70 | loss 3.5600 | lr 2.07e-04 | grad 6.25 | tok/s 8823
step     80 | loss 3.4055 | lr 2.37e-04 | grad 5.81 | tok/s 8825
step     90 | loss 3.0477 | lr 2.67e-04 | grad 2.88 | tok/s 8831
step    100 | loss 2.7730 | lr 2.97e-04 | grad 3.00 | tok/s 8824
step    110 | loss 2.4445 | lr 6.94e-06 | grad 81.00 | tok/s 8913
step    120 | loss 3.8574 | lr 2.69e-05 | grad 2.55 | tok/s 8675
step    130 | loss 2.6485 | lr 5.89e-05 | grad 1.64 | tok/s 8492
step    140 | loss 2.3811 | lr 9.99e-05 | grad 2.09 | tok/s 8519
step    150 | loss 2.6065 | lr 1.46e-04 | grad 4.12 | tok/s 8826
step    160 | loss 2.4982 | lr 1.92e-04 | grad 2.12 | tok/s 8873
step    170 | loss 2.7660 | lr 2.35e-04 | grad 4.50 | tok/s 8379
step    180 | loss 2.5811 | lr 2.69e-04 | grad 4.44 | tok/s 8687
step    190 | loss 2.4901 | lr 2.91e-04 | grad 3.70 | tok/s 8320
step    200 | loss 2.1430 | lr 3.00e-04 | grad 1.88 | tok/s 8921
step    210 | loss 2.1501 | lr 2.94e-04 | grad 3.06 | tok/s 8661
step    220 | loss 2.4749 | lr 2.74e-04 | grad 3.22 | tok/s 8372
step    230 | loss 3.1105 | lr 2.42e-04 | grad 2.77 | tok/s 8383
step    240 | loss 2.3100 | lr 2.01e-04 | grad 2.69 | tok/s 8430
step    250 | loss 2.5004 | lr 1.55e-04 | grad 2.14 | tok/s 8460
step    260 | loss 2.0968 | lr 1.09e-04 | grad 1.14 | tok/s 8739
step    270 | loss 2.2123 | lr 6.65e-05 | grad 1.66 | tok/s 8743
step    280 | loss 1.9241 | lr 3.24e-05 | grad 1.27 | tok/s 8494
step    290 | loss 1.9200 | lr 9.84e-06 | grad 1.55 | tok/s 8162
step    300 | loss 2.0052 | lr 1.07e-06 | grad 1.56 | tok/s 8292
step    310 | loss 2.0213 | lr 6.94e-06 | grad 0.89 | tok/s 8487
step    320 | loss 1.8446 | lr 2.69e-05 | grad 1.38 | tok/s 8127
step    330 | loss 2.0684 | lr 5.89e-05 | grad 1.15 | tok/s 8501
step    340 | loss 2.1462 | lr 9.99e-05 | grad 4.94 | tok/s 8673
step    350 | loss 2.1468 | lr 1.46e-04 | grad 2.61 | tok/s 8513
step    360 | loss 2.0867 | lr 1.92e-04 | grad 2.06 | tok/s 8723
step    370 | loss 1.8115 | lr 2.35e-04 | grad 1.29 | tok/s 8554
step    380 | loss 1.8879 | lr 2.69e-04 | grad 1.30 | tok/s 8947
step    390 | loss 1.4484 | lr 2.91e-04 | grad 1.16 | tok/s 9025
step    400 | loss 1.3469 | lr 3.00e-04 | grad 1.52 | tok/s 8878
step    410 | loss 2.4263 | lr 2.94e-04 | grad 2.55 | tok/s 8573
step    420 | loss 2.1842 | lr 2.74e-04 | grad 2.39 | tok/s 8560
step    430 | loss 2.1043 | lr 2.42e-04 | grad 2.47 | tok/s 9016
step    440 | loss 1.9726 | lr 2.01e-04 | grad 1.95 | tok/s 8727
step    450 | loss 2.0750 | lr 1.55e-04 | grad 1.55 | tok/s 8600
step    460 | loss 1.7799 | lr 1.09e-04 | grad 2.77 | tok/s 8536
step    470 | loss 1.8849 | lr 6.65e-05 | grad 1.41 | tok/s 8545
step    480 | loss 1.8951 | lr 3.24e-05 | grad 1.56 | tok/s 8964
step    490 | loss 1.9767 | lr 9.84e-06 | grad 0.88 | tok/s 8672
step    500 | loss 1.8242 | lr 1.07e-06 | grad 1.07 | tok/s 8616
step    510 | loss 2.1097 | lr 6.94e-06 | grad 3.28 | tok/s 8477
step    520 | loss 1.8687 | lr 2.69e-05 | grad 0.99 | tok/s 8117
step    530 | loss 1.7295 | lr 5.89e-05 | grad 0.88 | tok/s 8629
step    540 | loss 1.9055 | lr 9.99e-05 | grad 1.22 | tok/s 8601
step    550 | loss 1.8198 | lr 1.46e-04 | grad 1.45 | tok/s 8404
step    560 | loss 1.5946 | lr 1.92e-04 | grad 2.09 | tok/s 8813
step    570 | loss 1.6206 | lr 2.35e-04 | grad 1.38 | tok/s 9076
step    580 | loss 1.4478 | lr 2.69e-04 | grad 0.98 | tok/s 9080
step    590 | loss 1.3932 | lr 2.91e-04 | grad 1.15 | tok/s 9083
step    600 | loss 1.5025 | lr 3.00e-04 | grad 1.34 | tok/s 9085
step    610 | loss 1.3971 | lr 2.94e-04 | grad 1.22 | tok/s 9085
step    620 | loss 1.4318 | lr 2.74e-04 | grad 0.84 | tok/s 9090
step    630 | loss 1.5907 | lr 2.42e-04 | grad 4.28 | tok/s 8957
step    640 | loss 1.9703 | lr 2.01e-04 | grad 1.99 | tok/s 8554
step    650 | loss 1.9565 | lr 1.55e-04 | grad 1.96 | tok/s 8502

Training complete! Final step: 652
