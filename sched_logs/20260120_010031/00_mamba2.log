# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 2816  --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_010008/mamba2
# Started: 2026-01-20T01:00:31.257403
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_010008/mamba2/levelmamba2_100m_20260120_010037
Model: Level mamba2, 972,502,688 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9787 | lr 2.70e-05 | grad 12.44 | tok/s 3699
step     20 | loss 2.7861 | lr 5.70e-05 | grad 7.44 | tok/s 12415
step     30 | loss 2.7923 | lr 8.70e-05 | grad 10.31 | tok/s 12232
step     40 | loss 2.8964 | lr 1.17e-04 | grad 19.12 | tok/s 12544
step     50 | loss 4.4675 | lr 1.47e-04 | grad 10.94 | tok/s 12867
step     60 | loss 3.9257 | lr 1.77e-04 | grad 15.25 | tok/s 12730
step     70 | loss 3.1450 | lr 2.07e-04 | grad 12.56 | tok/s 12597
step     80 | loss 3.1276 | lr 2.37e-04 | grad 8.81 | tok/s 12492
step     90 | loss 2.5217 | lr 2.67e-04 | grad 7.78 | tok/s 12400
step    100 | loss 2.6600 | lr 2.97e-04 | grad 8.75 | tok/s 12295
step    110 | loss 2.3814 | lr 6.94e-06 | grad 8.50 | tok/s 12207
step    120 | loss 3.7389 | lr 2.69e-05 | grad 19.88 | tok/s 11773
step    130 | loss 2.5070 | lr 5.89e-05 | grad 4.88 | tok/s 11507
step    140 | loss 2.1530 | lr 9.99e-05 | grad 3.92 | tok/s 11530
step    150 | loss 2.1206 | lr 1.46e-04 | grad 14.44 | tok/s 11909
step    160 | loss 2.0595 | lr 1.92e-04 | grad 6.44 | tok/s 11912
step    170 | loss 2.4119 | lr 2.35e-04 | grad 5.56 | tok/s 11257
step    180 | loss 2.1103 | lr 2.69e-04 | grad 3.59 | tok/s 11619
step    190 | loss 2.0520 | lr 2.91e-04 | grad 3.66 | tok/s 11160
step    200 | loss 1.7867 | lr 3.00e-04 | grad 2.39 | tok/s 11901
step    210 | loss 1.6837 | lr 2.94e-04 | grad 2.72 | tok/s 11525
step    220 | loss 2.2808 | lr 2.74e-04 | grad 4.56 | tok/s 11131
step    230 | loss 2.7137 | lr 2.42e-04 | grad 1.83 | tok/s 11131
step    240 | loss 2.0584 | lr 2.01e-04 | grad 2.70 | tok/s 11183
step    250 | loss 2.2636 | lr 1.55e-04 | grad 3.30 | tok/s 11219
step    260 | loss 1.8451 | lr 1.09e-04 | grad 1.83 | tok/s 11587
step    270 | loss 1.9934 | lr 6.65e-05 | grad 2.33 | tok/s 11586
step    280 | loss 1.7263 | lr 3.24e-05 | grad 1.93 | tok/s 11239
step    290 | loss 1.6893 | lr 9.84e-06 | grad 2.48 | tok/s 10814
step    300 | loss 1.7922 | lr 1.07e-06 | grad 2.33 | tok/s 10987
step    310 | loss 1.8221 | lr 6.94e-06 | grad 1.27 | tok/s 11212
step    320 | loss 1.6514 | lr 2.69e-05 | grad 2.16 | tok/s 10737
step    330 | loss 1.8399 | lr 5.89e-05 | grad 1.62 | tok/s 11220
step    340 | loss 1.8993 | lr 9.99e-05 | grad 6.38 | tok/s 11456
step    350 | loss 1.9258 | lr 1.46e-04 | grad 3.61 | tok/s 11221
step    360 | loss 1.8003 | lr 1.92e-04 | grad 2.28 | tok/s 11470
step    370 | loss 1.5764 | lr 2.35e-04 | grad 1.41 | tok/s 11273
step    380 | loss 1.5991 | lr 2.69e-04 | grad 1.75 | tok/s 11771
step    390 | loss 1.2002 | lr 2.91e-04 | grad 1.68 | tok/s 11864
step    400 | loss 1.1443 | lr 3.00e-04 | grad 2.64 | tok/s 11688
step    410 | loss 2.2190 | lr 2.94e-04 | grad 2.84 | tok/s 11311
step    420 | loss 2.0342 | lr 2.74e-04 | grad 3.52 | tok/s 11320
step    430 | loss 1.8172 | lr 2.42e-04 | grad 2.70 | tok/s 11850
step    440 | loss 1.7760 | lr 2.01e-04 | grad 3.06 | tok/s 11486
step    450 | loss 1.9115 | lr 1.55e-04 | grad 1.87 | tok/s 11337
step    460 | loss 1.6291 | lr 1.09e-04 | grad 2.78 | tok/s 11245
step    470 | loss 1.7144 | lr 6.65e-05 | grad 2.06 | tok/s 11249
step    480 | loss 1.7310 | lr 3.24e-05 | grad 2.23 | tok/s 11771
step    490 | loss 1.8323 | lr 9.84e-06 | grad 1.20 | tok/s 11390
step    500 | loss 1.6773 | lr 1.07e-06 | grad 1.46 | tok/s 11304
step    510 | loss 1.9652 | lr 6.94e-06 | grad 5.62 | tok/s 11131
step    520 | loss 1.7480 | lr 2.69e-05 | grad 1.31 | tok/s 10666
step    530 | loss 1.5946 | lr 5.89e-05 | grad 1.38 | tok/s 11306
step    540 | loss 1.7605 | lr 9.99e-05 | grad 1.57 | tok/s 11292
step    550 | loss 1.6671 | lr 1.46e-04 | grad 2.08 | tok/s 11039
step    560 | loss 1.4480 | lr 1.92e-04 | grad 2.50 | tok/s 11607
step    570 | loss 1.4976 | lr 2.35e-04 | grad 1.52 | tok/s 11877
step    580 | loss 1.3296 | lr 2.69e-04 | grad 1.31 | tok/s 11885
step    590 | loss 1.2827 | lr 2.91e-04 | grad 1.59 | tok/s 11880
step    600 | loss 1.3756 | lr 3.00e-04 | grad 1.59 | tok/s 11895
step    610 | loss 1.2941 | lr 2.94e-04 | grad 1.49 | tok/s 11896
step    620 | loss 1.3357 | lr 2.74e-04 | grad 1.30 | tok/s 11908
step    630 | loss 1.4659 | lr 2.42e-04 | grad 4.09 | tok/s 11745
step    640 | loss 1.8616 | lr 2.01e-04 | grad 3.80 | tok/s 11267
step    650 | loss 1.8249 | lr 1.55e-04 | grad 2.19 | tok/s 11174
step    660 | loss 1.6544 | lr 1.09e-04 | grad 2.09 | tok/s 11280
step    670 | loss 1.6777 | lr 6.65e-05 | grad 2.20 | tok/s 11651
step    680 | loss 1.6820 | lr 3.24e-05 | grad 2.14 | tok/s 11257
step    690 | loss 1.7088 | lr 9.84e-06 | grad 1.84 | tok/s 11180
step    700 | loss 1.6452 | lr 1.07e-06 | grad 1.38 | tok/s 11079
step    710 | loss 1.5636 | lr 6.94e-06 | grad 1.89 | tok/s 11389
step    720 | loss 1.7819 | lr 2.68e-05 | grad 3.03 | tok/s 11140
step    730 | loss 1.4406 | lr 5.89e-05 | grad 1.30 | tok/s 11638
step    740 | loss 1.5236 | lr 9.99e-05 | grad 1.59 | tok/s 11311
step    750 | loss 1.9963 | lr 1.46e-04 | grad 3.56 | tok/s 11752
step    760 | loss 1.7272 | lr 1.92e-04 | grad 1.70 | tok/s 11761
step    770 | loss 1.6406 | lr 2.35e-04 | grad 2.48 | tok/s 11508
step    780 | loss 1.6970 | lr 2.69e-04 | grad 1.57 | tok/s 11167
step    790 | loss 1.6723 | lr 2.91e-04 | grad 3.19 | tok/s 11456
step    800 | loss 1.8633 | lr 3.00e-04 | grad 4.66 | tok/s 11775
step    810 | loss 1.6189 | lr 2.94e-04 | grad 3.22 | tok/s 11438
step    820 | loss 1.4297 | lr 2.74e-04 | grad 3.59 | tok/s 11178
step    830 | loss 1.5950 | lr 2.42e-04 | grad 1.91 | tok/s 11318
step    840 | loss 1.6772 | lr 2.01e-04 | grad 1.69 | tok/s 11079
step    850 | loss 1.7748 | lr 1.55e-04 | grad 1.33 | tok/s 11094

Training complete! Final step: 856
