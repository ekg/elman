# Job 7: E75pch8n32
# GPU: 7
# Command: python train.py --level E75pch8n32 --dim 4480 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e88_100m/E75pch8n32
# Started: 2026-01-20T14:12:46.618225
============================================================

Using device: cuda
Output directory: benchmark_results/e88_100m/E75pch8n32/levelE75pch8n32_100m_20260120_141252
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75pch8n32, 116,010,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 6.1285 | lr 2.70e-05 | grad 356.00 | tok/s 19086
step     20 | loss 5.4531 | lr 5.70e-05 | grad 24.00 | tok/s 48076
step     30 | loss 5.4479 | lr 8.70e-05 | grad 12.12 | tok/s 50793
step     40 | loss 4.2257 | lr 1.17e-04 | grad 7.84 | tok/s 50665
step     50 | loss 3.5858 | lr 1.47e-04 | grad 10.88 | tok/s 50591
step     60 | loss 3.6663 | lr 1.77e-04 | grad 12.12 | tok/s 49480
step     70 | loss 3.2517 | lr 2.07e-04 | grad 6.72 | tok/s 47724
step     80 | loss 3.5671 | lr 2.37e-04 | grad 14.06 | tok/s 49536
step     90 | loss 3.5278 | lr 2.67e-04 | grad 9.12 | tok/s 47788
step    100 | loss 3.3518 | lr 2.97e-04 | grad 10.12 | tok/s 48194
step    110 | loss 3.3831 | lr 6.94e-06 | grad 15.62 | tok/s 44113
step    120 | loss 3.8198 | lr 2.69e-05 | grad 7.94 | tok/s 43902
step    130 | loss 3.4143 | lr 5.89e-05 | grad 4.75 | tok/s 44898
step    140 | loss 3.1084 | lr 9.99e-05 | grad 3.39 | tok/s 44842
step    150 | loss 2.8909 | lr 1.46e-04 | grad 5.66 | tok/s 42721
step    160 | loss 2.6942 | lr 1.92e-04 | grad 3.61 | tok/s 43079
step    170 | loss 2.8280 | lr 2.35e-04 | grad 26.88 | tok/s 44450
step    180 | loss 2.7665 | lr 2.69e-04 | grad 6.91 | tok/s 44507
step    190 | loss 2.5013 | lr 2.91e-04 | grad 5.47 | tok/s 45144
step    200 | loss 2.1213 | lr 3.00e-04 | grad 3.34 | tok/s 46023
step    210 | loss 2.7115 | lr 2.94e-04 | grad 5.56 | tok/s 44093
step    220 | loss 2.5462 | lr 2.74e-04 | grad 4.41 | tok/s 45583
step    230 | loss 2.3525 | lr 2.42e-04 | grad 4.94 | tok/s 43996
step    240 | loss 2.3661 | lr 2.01e-04 | grad 4.50 | tok/s 44706
step    250 | loss 2.2999 | lr 1.55e-04 | grad 2.86 | tok/s 44182
step    260 | loss 2.3167 | lr 1.09e-04 | grad 2.80 | tok/s 42378
step    270 | loss 2.1746 | lr 6.65e-05 | grad 3.05 | tok/s 43904
step    280 | loss 2.0652 | lr 3.24e-05 | grad 4.62 | tok/s 43801
step    290 | loss 2.0702 | lr 9.84e-06 | grad 2.02 | tok/s 46049
step    300 | loss 2.0371 | lr 1.07e-06 | grad 1.61 | tok/s 46096
step    310 | loss 2.0432 | lr 6.94e-06 | grad 1.59 | tok/s 46001
step    320 | loss 2.1431 | lr 2.69e-05 | grad 4.16 | tok/s 44428
step    330 | loss 2.1631 | lr 5.89e-05 | grad 1.60 | tok/s 43366
step    340 | loss 2.1669 | lr 9.99e-05 | grad 4.22 | tok/s 44271
step    350 | loss 2.2035 | lr 1.46e-04 | grad 2.95 | tok/s 43005
step    360 | loss 2.1448 | lr 1.92e-04 | grad 5.62 | tok/s 43546
step    370 | loss 2.0049 | lr 2.35e-04 | grad 4.09 | tok/s 44325
step    380 | loss 2.5475 | lr 2.69e-04 | grad 4.41 | tok/s 45446
step    390 | loss 2.1644 | lr 2.91e-04 | grad 6.28 | tok/s 43698
step    400 | loss 2.2511 | lr 3.00e-04 | grad 8.50 | tok/s 44761
step    410 | loss 2.0236 | lr 2.94e-04 | grad 6.91 | tok/s 43562
step    420 | loss 2.1808 | lr 2.74e-04 | grad 4.59 | tok/s 43196
step    430 | loss 2.2864 | lr 2.42e-04 | grad 4.78 | tok/s 43062
step    440 | loss 2.3218 | lr 2.01e-04 | grad 3.48 | tok/s 44790
step    450 | loss 2.0516 | lr 1.55e-04 | grad 1.70 | tok/s 43527
step    460 | loss 2.0095 | lr 1.09e-04 | grad 1.92 | tok/s 43595
step    470 | loss 1.9515 | lr 6.65e-05 | grad 3.12 | tok/s 44181
step    480 | loss 1.8572 | lr 3.24e-05 | grad 1.38 | tok/s 42618
step    490 | loss 1.8159 | lr 9.84e-06 | grad 1.14 | tok/s 43544
step    500 | loss 2.8574 | lr 1.07e-06 | grad 1.86 | tok/s 44950
step    510 | loss 1.9280 | lr 6.94e-06 | grad 1.39 | tok/s 43854
step    520 | loss 1.9028 | lr 2.69e-05 | grad 1.30 | tok/s 45313
step    530 | loss 2.3954 | lr 5.89e-05 | grad 1.91 | tok/s 44286
step    540 | loss 1.8485 | lr 9.99e-05 | grad 3.80 | tok/s 44316
step    550 | loss 1.7451 | lr 1.46e-04 | grad 1.70 | tok/s 45389
step    560 | loss 1.5904 | lr 1.92e-04 | grad 2.77 | tok/s 46008
step    570 | loss 1.9233 | lr 2.35e-04 | grad 6.28 | tok/s 45040
step    580 | loss 2.3123 | lr 2.69e-04 | grad 3.41 | tok/s 44492
step    590 | loss 2.6518 | lr 2.91e-04 | grad 7.94 | tok/s 43542
step    600 | loss 2.1433 | lr 3.00e-04 | grad 3.69 | tok/s 43686
step    610 | loss 2.1713 | lr 2.94e-04 | grad 3.66 | tok/s 45757
step    620 | loss 1.9758 | lr 2.74e-04 | grad 3.27 | tok/s 43464
step    630 | loss 1.8589 | lr 2.42e-04 | grad 2.66 | tok/s 44806
step    640 | loss 2.1993 | lr 2.01e-04 | grad 2.67 | tok/s 44766
step    650 | loss 1.9189 | lr 1.55e-04 | grad 2.83 | tok/s 43970
step    660 | loss 2.2064 | lr 1.09e-04 | grad 9.00 | tok/s 43448
step    670 | loss 2.0155 | lr 6.65e-05 | grad 5.22 | tok/s 44896
step    680 | loss 1.9351 | lr 3.24e-05 | grad 1.98 | tok/s 43357
step    690 | loss 1.9805 | lr 9.84e-06 | grad 2.23 | tok/s 43726
step    700 | loss 2.0630 | lr 1.07e-06 | grad 2.42 | tok/s 43928
step    710 | loss 2.0079 | lr 6.94e-06 | grad 2.39 | tok/s 44233
step    720 | loss 2.0985 | lr 2.68e-05 | grad 3.98 | tok/s 43928
step    730 | loss 2.0345 | lr 5.89e-05 | grad 2.36 | tok/s 44398
step    740 | loss 1.9519 | lr 9.99e-05 | grad 4.56 | tok/s 43991
step    750 | loss 1.7660 | lr 1.46e-04 | grad 2.81 | tok/s 43514
step    760 | loss 2.1575 | lr 1.92e-04 | grad 2.30 | tok/s 44057
step    770 | loss 1.8434 | lr 2.35e-04 | grad 2.98 | tok/s 43697
step    780 | loss 1.9144 | lr 2.69e-04 | grad 3.78 | tok/s 44146
step    790 | loss 1.8210 | lr 2.91e-04 | grad 2.72 | tok/s 44597
step    800 | loss 1.8267 | lr 3.00e-04 | grad 2.95 | tok/s 44559
step    810 | loss 1.9505 | lr 2.94e-04 | grad 5.16 | tok/s 44197
step    820 | loss 2.7774 | lr 2.74e-04 | grad 6.78 | tok/s 45506
step    830 | loss 2.2770 | lr 2.42e-04 | grad 2.58 | tok/s 46010
step    840 | loss 1.8885 | lr 2.01e-04 | grad 1.55 | tok/s 45952
step    850 | loss 2.2141 | lr 1.55e-04 | grad 3.02 | tok/s 43800
step    860 | loss 1.9901 | lr 1.09e-04 | grad 2.45 | tok/s 42916
step    870 | loss 1.8798 | lr 6.65e-05 | grad 2.23 | tok/s 44250
step    880 | loss 1.9506 | lr 3.24e-05 | grad 2.73 | tok/s 43987
step    890 | loss 1.8441 | lr 9.84e-06 | grad 2.05 | tok/s 43958
step    900 | loss 2.3566 | lr 1.07e-06 | grad 2.09 | tok/s 42878
step    910 | loss 1.9046 | lr 6.94e-06 | grad 1.50 | tok/s 43655
step    920 | loss 1.8686 | lr 2.68e-05 | grad 1.50 | tok/s 43454
step    930 | loss 1.9989 | lr 5.89e-05 | grad 3.02 | tok/s 43313
step    940 | loss 1.8906 | lr 9.99e-05 | grad 3.23 | tok/s 43018
step    950 | loss 1.9408 | lr 1.46e-04 | grad 3.08 | tok/s 43994
step    960 | loss 1.7389 | lr 1.92e-04 | grad 1.73 | tok/s 46006
step    970 | loss 1.5264 | lr 2.35e-04 | grad 1.88 | tok/s 46037
step    980 | loss 1.6915 | lr 2.69e-04 | grad 4.88 | tok/s 44661
step    990 | loss 2.0539 | lr 2.91e-04 | grad 2.61 | tok/s 43566
step   1000 | loss 1.9146 | lr 3.00e-04 | grad 2.31 | tok/s 42587
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9146.pt
step   1010 | loss 2.1510 | lr 2.94e-04 | grad 3.08 | tok/s 37936
step   1020 | loss 1.7716 | lr 2.74e-04 | grad 2.75 | tok/s 43673
step   1030 | loss 2.2028 | lr 2.42e-04 | grad 2.84 | tok/s 42961
step   1040 | loss 1.7810 | lr 2.01e-04 | grad 4.22 | tok/s 43948
step   1050 | loss 1.8148 | lr 1.55e-04 | grad 4.00 | tok/s 44079
step   1060 | loss 2.0292 | lr 1.09e-04 | grad 5.00 | tok/s 44128
step   1070 | loss 2.1309 | lr 6.65e-05 | grad 2.56 | tok/s 44209
step   1080 | loss 2.4245 | lr 3.24e-05 | grad 3.50 | tok/s 43700
step   1090 | loss 2.1571 | lr 9.84e-06 | grad 2.62 | tok/s 44042
step   1100 | loss 1.8019 | lr 1.07e-06 | grad 2.42 | tok/s 43793
step   1110 | loss 1.7979 | lr 6.93e-06 | grad 2.03 | tok/s 44572
step   1120 | loss 2.0598 | lr 2.68e-05 | grad 2.38 | tok/s 45097
step   1130 | loss 1.8137 | lr 5.89e-05 | grad 1.80 | tok/s 42898
step   1140 | loss 1.6815 | lr 9.99e-05 | grad 2.42 | tok/s 43903
step   1150 | loss 2.0082 | lr 1.46e-04 | grad 3.50 | tok/s 43880
step   1160 | loss 1.6354 | lr 1.92e-04 | grad 2.31 | tok/s 43363
step   1170 | loss 2.1032 | lr 2.35e-04 | grad 2.52 | tok/s 43794
step   1180 | loss 1.6893 | lr 2.69e-04 | grad 2.42 | tok/s 46047
step   1190 | loss 1.5566 | lr 2.91e-04 | grad 2.73 | tok/s 45952
step   1200 | loss 1.4585 | lr 3.00e-04 | grad 1.80 | tok/s 46010
step   1210 | loss 1.4182 | lr 2.94e-04 | grad 2.45 | tok/s 45996
step   1220 | loss 1.4843 | lr 2.74e-04 | grad 2.20 | tok/s 45563
step   1230 | loss 1.7381 | lr 2.42e-04 | grad 2.80 | tok/s 44067
step   1240 | loss 1.7903 | lr 2.01e-04 | grad 2.81 | tok/s 43300
step   1250 | loss 1.8753 | lr 1.55e-04 | grad 6.31 | tok/s 44561
step   1260 | loss 1.9517 | lr 1.09e-04 | grad 4.53 | tok/s 44659
step   1270 | loss 1.9628 | lr 6.65e-05 | grad 2.88 | tok/s 44067
step   1280 | loss 1.7901 | lr 3.24e-05 | grad 1.70 | tok/s 43537
step   1290 | loss 1.7324 | lr 9.84e-06 | grad 1.80 | tok/s 43439
step   1300 | loss 1.7892 | lr 1.07e-06 | grad 1.43 | tok/s 43142
step   1310 | loss 1.9090 | lr 6.93e-06 | grad 1.54 | tok/s 43151
step   1320 | loss 1.8486 | lr 2.68e-05 | grad 2.28 | tok/s 43879
step   1330 | loss 1.7721 | lr 5.89e-05 | grad 1.65 | tok/s 43912
step   1340 | loss 1.6870 | lr 9.99e-05 | grad 2.11 | tok/s 43938
step   1350 | loss 1.7680 | lr 1.46e-04 | grad 3.72 | tok/s 45083
step   1360 | loss 1.6978 | lr 1.92e-04 | grad 2.25 | tok/s 42852
step   1370 | loss 1.8127 | lr 2.35e-04 | grad 2.11 | tok/s 43537
step   1380 | loss 1.8980 | lr 2.69e-04 | grad 2.75 | tok/s 43883
step   1390 | loss 1.8065 | lr 2.91e-04 | grad 3.25 | tok/s 42812
step   1400 | loss 1.9643 | lr 3.00e-04 | grad 22.12 | tok/s 44502
step   1410 | loss 2.0137 | lr 2.94e-04 | grad 4.62 | tok/s 44922
step   1420 | loss 1.9484 | lr 2.74e-04 | grad 3.44 | tok/s 42759
step   1430 | loss 1.6993 | lr 2.42e-04 | grad 2.16 | tok/s 41829
step   1440 | loss 1.5853 | lr 2.01e-04 | grad 2.08 | tok/s 44026
step   1450 | loss 1.6295 | lr 1.55e-04 | grad 3.97 | tok/s 45023
step   1460 | loss 1.7031 | lr 1.09e-04 | grad 1.52 | tok/s 42059
step   1470 | loss 1.8398 | lr 6.65e-05 | grad 3.83 | tok/s 43585
step   1480 | loss 1.7110 | lr 3.24e-05 | grad 4.16 | tok/s 44025
step   1490 | loss 1.8169 | lr 9.84e-06 | grad 6.09 | tok/s 43916
step   1500 | loss 1.9322 | lr 1.07e-06 | grad 3.98 | tok/s 42814
step   1510 | loss 1.8165 | lr 6.93e-06 | grad 2.33 | tok/s 45002
step   1520 | loss 1.7753 | lr 2.68e-05 | grad 2.48 | tok/s 44598
step   1530 | loss 1.7251 | lr 5.89e-05 | grad 1.55 | tok/s 44178
step   1540 | loss 1.6957 | lr 9.99e-05 | grad 1.54 | tok/s 43445
step   1550 | loss 1.7330 | lr 1.46e-04 | grad 6.16 | tok/s 45192
step   1560 | loss 2.3470 | lr 1.92e-04 | grad 5.81 | tok/s 44044
step   1570 | loss 1.7327 | lr 2.35e-04 | grad 2.52 | tok/s 43181
step   1580 | loss 1.9732 | lr 2.69e-04 | grad 3.92 | tok/s 44605
step   1590 | loss 1.6688 | lr 2.91e-04 | grad 3.03 | tok/s 43561
step   1600 | loss 1.7815 | lr 3.00e-04 | grad 2.80 | tok/s 42665
step   1610 | loss 1.6345 | lr 2.94e-04 | grad 2.53 | tok/s 45181
step   1620 | loss 1.8085 | lr 2.74e-04 | grad 2.92 | tok/s 44514
step   1630 | loss 1.8171 | lr 2.42e-04 | grad 2.92 | tok/s 44745
step   1640 | loss 1.6975 | lr 2.01e-04 | grad 2.14 | tok/s 43348
step   1650 | loss 1.7137 | lr 1.55e-04 | grad 2.28 | tok/s 42692
step   1660 | loss 1.7156 | lr 1.09e-04 | grad 2.86 | tok/s 42987
step   1670 | loss 1.7511 | lr 6.65e-05 | grad 4.19 | tok/s 44793
step   1680 | loss 2.3014 | lr 3.24e-05 | grad 1.76 | tok/s 44724

Training complete! Final step: 1681
