# Job 1: E75h64n32
# GPU: 1
# Command: python train.py --level E75h64n32 --dim 2048 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_highhead/E75h64n32
# Started: 2026-01-20T02:57:05.847466
============================================================

Using device: cuda
Output directory: benchmark_results/e75_highhead/E75h64n32/levelE75h64n32_100m_20260120_025711
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75h64n32, 923,355,136 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.7683 | lr 2.70e-05 | grad 140.00 | tok/s 6316
step     20 | loss 4.3254 | lr 5.70e-05 | grad 57.25 | tok/s 9239
step     30 | loss 3.8580 | lr 8.70e-05 | grad 20.00 | tok/s 9204
step     40 | loss 3.5851 | lr 1.17e-04 | grad 14.19 | tok/s 9443
step     50 | loss 4.6131 | lr 1.47e-04 | grad 9.94 | tok/s 9723
step     60 | loss 3.8884 | lr 1.77e-04 | grad 9.56 | tok/s 9677
step     70 | loss 3.6170 | lr 2.07e-04 | grad 10.38 | tok/s 9628
step     80 | loss 3.3679 | lr 2.37e-04 | grad 6.22 | tok/s 9598
step     90 | loss 2.9875 | lr 2.67e-04 | grad 4.03 | tok/s 9600
step    100 | loss 2.8119 | lr 2.97e-04 | grad 4.34 | tok/s 9576
step    110 | loss 2.5550 | lr 6.94e-06 | grad 7.47 | tok/s 9657
step    120 | loss 3.9146 | lr 2.69e-05 | grad 7.28 | tok/s 9403
step    130 | loss 2.9021 | lr 5.89e-05 | grad 1.86 | tok/s 9183
step    140 | loss 2.6964 | lr 9.99e-05 | grad 2.81 | tok/s 9213
step    150 | loss 3.0650 | lr 1.46e-04 | grad 3.48 | tok/s 9517
step    160 | loss 2.5964 | lr 1.92e-04 | grad 3.75 | tok/s 9574
step    170 | loss 2.7487 | lr 2.35e-04 | grad 5.53 | tok/s 9041
step    180 | loss 2.7243 | lr 2.69e-04 | grad 2.91 | tok/s 9366
step    190 | loss 2.5874 | lr 2.91e-04 | grad 3.25 | tok/s 8959
step    200 | loss 2.1888 | lr 3.00e-04 | grad 1.38 | tok/s 9573
step    210 | loss 2.1115 | lr 2.94e-04 | grad 4.50 | tok/s 9283
step    220 | loss 2.5591 | lr 2.74e-04 | grad 4.22 | tok/s 8950
step    230 | loss 2.8716 | lr 2.42e-04 | grad 1.45 | tok/s 8974
step    240 | loss 2.3269 | lr 2.01e-04 | grad 3.20 | tok/s 9002
step    250 | loss 2.5156 | lr 1.55e-04 | grad 1.60 | tok/s 9038
step    260 | loss 2.1125 | lr 1.09e-04 | grad 1.34 | tok/s 9323
step    270 | loss 2.2577 | lr 6.65e-05 | grad 1.41 | tok/s 9342
step    280 | loss 1.9527 | lr 3.24e-05 | grad 1.55 | tok/s 9071
step    290 | loss 1.9766 | lr 9.84e-06 | grad 1.98 | tok/s 8724
step    300 | loss 2.0573 | lr 1.07e-06 | grad 1.87 | tok/s 8855
step    310 | loss 2.0715 | lr 6.94e-06 | grad 1.00 | tok/s 9060
step    320 | loss 1.8903 | lr 2.69e-05 | grad 1.78 | tok/s 8670
step    330 | loss 2.1462 | lr 5.89e-05 | grad 1.08 | tok/s 9082
step    340 | loss 2.2382 | lr 9.99e-05 | grad 5.69 | tok/s 9271
step    350 | loss 2.1912 | lr 1.46e-04 | grad 2.45 | tok/s 9089
step    360 | loss 2.1633 | lr 1.92e-04 | grad 1.86 | tok/s 9336
step    370 | loss 1.8794 | lr 2.35e-04 | grad 1.16 | tok/s 9140
step    380 | loss 1.9282 | lr 2.69e-04 | grad 3.17 | tok/s 9557
step    390 | loss 1.4885 | lr 2.91e-04 | grad 1.09 | tok/s 9633
step    400 | loss 1.3997 | lr 3.00e-04 | grad 1.52 | tok/s 9481
step    410 | loss 2.4660 | lr 2.94e-04 | grad 2.11 | tok/s 9158
step    420 | loss 2.1870 | lr 2.74e-04 | grad 2.22 | tok/s 9150
step    430 | loss 2.1815 | lr 2.42e-04 | grad 3.41 | tok/s 9606
step    440 | loss 2.0081 | lr 2.01e-04 | grad 1.74 | tok/s 9314
step    450 | loss 2.1502 | lr 1.55e-04 | grad 1.06 | tok/s 9178
step    460 | loss 1.8093 | lr 1.09e-04 | grad 2.30 | tok/s 9098
step    470 | loss 1.9356 | lr 6.65e-05 | grad 1.84 | tok/s 9098
step    480 | loss 1.9655 | lr 3.24e-05 | grad 2.05 | tok/s 9542
step    490 | loss 2.0212 | lr 9.84e-06 | grad 0.98 | tok/s 9222
step    500 | loss 1.8774 | lr 1.07e-06 | grad 1.38 | tok/s 9167
step    510 | loss 2.1744 | lr 6.94e-06 | grad 4.53 | tok/s 9023
step    520 | loss 1.9186 | lr 2.69e-05 | grad 1.19 | tok/s 8627
step    530 | loss 1.7838 | lr 5.89e-05 | grad 1.17 | tok/s 9159
step    540 | loss 1.9777 | lr 9.99e-05 | grad 1.32 | tok/s 9137
step    550 | loss 1.8890 | lr 1.46e-04 | grad 1.30 | tok/s 8937
step    560 | loss 1.6397 | lr 1.92e-04 | grad 1.95 | tok/s 9372
step    570 | loss 1.6669 | lr 2.35e-04 | grad 1.16 | tok/s 9630
step    580 | loss 1.4957 | lr 2.69e-04 | grad 1.06 | tok/s 9628
step    590 | loss 1.4427 | lr 2.91e-04 | grad 1.08 | tok/s 9630
step    600 | loss 1.5362 | lr 3.00e-04 | grad 1.23 | tok/s 9626
step    610 | loss 1.4363 | lr 2.94e-04 | grad 1.21 | tok/s 9636
step    620 | loss 1.4618 | lr 2.74e-04 | grad 0.95 | tok/s 9650
step    630 | loss 1.6270 | lr 2.42e-04 | grad 6.62 | tok/s 9511
step    640 | loss 2.0448 | lr 2.01e-04 | grad 2.48 | tok/s 9085
step    650 | loss 1.9750 | lr 1.55e-04 | grad 2.14 | tok/s 9025
step    660 | loss 1.8245 | lr 1.09e-04 | grad 1.57 | tok/s 9116
step    670 | loss 1.8371 | lr 6.65e-05 | grad 1.63 | tok/s 9415
step    680 | loss 1.8534 | lr 3.24e-05 | grad 1.65 | tok/s 9091
step    690 | loss 1.8878 | lr 9.84e-06 | grad 1.73 | tok/s 9012
step    700 | loss 1.8720 | lr 1.07e-06 | grad 1.25 | tok/s 8936

Training complete! Final step: 701
