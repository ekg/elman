# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 1600 --depth 32 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/500m_state_v3/mamba2
# Started: 2026-01-22T17:09:23.365880
============================================================

Using device: cuda
Output directory: benchmark_results/500m_state_v3/mamba2/levelmamba2_100m_20260122_170929
Model: Level mamba2, 508,362,560 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4687 | lr 3.00e-04 | grad 30.12 | tok/s 6960
step     20 | loss 3.2925 | lr 3.00e-04 | grad 8.56 | tok/s 17413
step     30 | loss 4.5731 | lr 3.00e-04 | grad 5.16 | tok/s 18086
step     40 | loss 2.7256 | lr 3.00e-04 | grad 1.96 | tok/s 17761
step     50 | loss 2.2004 | lr 3.00e-04 | grad 3.25 | tok/s 17471
step     60 | loss 2.5992 | lr 3.00e-04 | grad 2.27 | tok/s 16930
step     70 | loss 2.2699 | lr 3.00e-04 | grad 4.25 | tok/s 16151
step     80 | loss 2.4023 | lr 3.00e-04 | grad 4.12 | tok/s 16616
step     90 | loss 2.3300 | lr 3.00e-04 | grad 4.91 | tok/s 15938
step    100 | loss 1.9534 | lr 3.00e-04 | grad 2.45 | tok/s 16027
step    110 | loss 1.9569 | lr 3.00e-04 | grad 3.39 | tok/s 15735
step    120 | loss 1.9680 | lr 3.00e-04 | grad 2.02 | tok/s 15442
step    130 | loss 1.9605 | lr 3.00e-04 | grad 2.00 | tok/s 15775
step    140 | loss 1.8345 | lr 3.00e-04 | grad 2.73 | tok/s 15775
step    150 | loss 1.7119 | lr 3.00e-04 | grad 2.73 | tok/s 15040
step    160 | loss 1.7031 | lr 3.00e-04 | grad 1.95 | tok/s 15134
step    170 | loss 1.8346 | lr 3.00e-04 | grad 5.12 | tok/s 15660
step    180 | loss 1.7924 | lr 3.00e-04 | grad 1.40 | tok/s 15692
step    190 | loss 1.5563 | lr 3.00e-04 | grad 1.68 | tok/s 15945
step    200 | loss 1.2436 | lr 3.00e-04 | grad 2.12 | tok/s 16283
step    210 | loss 1.8038 | lr 3.00e-04 | grad 2.23 | tok/s 15601
step    220 | loss 1.6410 | lr 3.00e-04 | grad 1.64 | tok/s 16097
step    230 | loss 1.6287 | lr 3.00e-04 | grad 3.36 | tok/s 15550
step    240 | loss 1.6303 | lr 3.00e-04 | grad 2.42 | tok/s 15871
step    250 | loss 1.6141 | lr 3.00e-04 | grad 1.97 | tok/s 15646
step    260 | loss 1.6980 | lr 3.00e-04 | grad 1.75 | tok/s 15014
step    270 | loss 1.6062 | lr 3.00e-04 | grad 1.48 | tok/s 15576
step    280 | loss 1.4879 | lr 3.00e-04 | grad 3.16 | tok/s 15573
step    290 | loss 1.4064 | lr 3.00e-04 | grad 1.55 | tok/s 16430
step    300 | loss 1.3594 | lr 3.00e-04 | grad 1.55 | tok/s 16426
step    310 | loss 1.3101 | lr 3.00e-04 | grad 1.86 | tok/s 16444
step    320 | loss 1.4862 | lr 3.00e-04 | grad 2.22 | tok/s 15806
step    330 | loss 1.5872 | lr 3.00e-04 | grad 1.45 | tok/s 15453
step    340 | loss 1.5976 | lr 3.00e-04 | grad 3.22 | tok/s 15772
step    350 | loss 1.5976 | lr 3.00e-04 | grad 1.54 | tok/s 15349
step    360 | loss 1.5538 | lr 3.00e-04 | grad 4.12 | tok/s 15542
step    370 | loss 1.3789 | lr 3.00e-04 | grad 1.28 | tok/s 15823
step    380 | loss 1.7257 | lr 3.00e-04 | grad 1.67 | tok/s 16235
step    390 | loss 1.5053 | lr 3.00e-04 | grad 1.20 | tok/s 15631
step    400 | loss 1.5709 | lr 3.00e-04 | grad 3.03 | tok/s 16045
step    410 | loss 1.3492 | lr 3.00e-04 | grad 3.19 | tok/s 15578
step    420 | loss 1.4801 | lr 3.00e-04 | grad 1.20 | tok/s 15431
step    430 | loss 1.5701 | lr 3.00e-04 | grad 1.95 | tok/s 15388
step    440 | loss 1.5476 | lr 3.00e-04 | grad 1.20 | tok/s 15982
step    450 | loss 1.5196 | lr 3.00e-04 | grad 1.40 | tok/s 15544
step    460 | loss 1.4959 | lr 3.00e-04 | grad 1.04 | tok/s 15616
step    470 | loss 1.4741 | lr 3.00e-04 | grad 1.91 | tok/s 15801
step    480 | loss 1.4105 | lr 3.00e-04 | grad 1.40 | tok/s 15248
step    490 | loss 1.4176 | lr 3.00e-04 | grad 1.25 | tok/s 15540
step    500 | loss 1.8355 | lr 3.00e-04 | grad 1.76 | tok/s 15997
step    510 | loss 1.3784 | lr 3.00e-04 | grad 1.58 | tok/s 15639
step    520 | loss 1.3449 | lr 3.00e-04 | grad 1.47 | tok/s 16135
step    530 | loss 1.8722 | lr 3.00e-04 | grad 1.34 | tok/s 15797
step    540 | loss 1.3051 | lr 3.00e-04 | grad 2.12 | tok/s 15828
step    550 | loss 1.3178 | lr 3.00e-04 | grad 1.34 | tok/s 16227
step    560 | loss 1.2299 | lr 3.00e-04 | grad 1.22 | tok/s 16462
step    570 | loss 1.4070 | lr 3.00e-04 | grad 2.47 | tok/s 16076
step    580 | loss 1.6222 | lr 3.00e-04 | grad 1.49 | tok/s 15861
step    590 | loss 1.7870 | lr 3.00e-04 | grad 1.18 | tok/s 15554

Training complete! Final step: 593
