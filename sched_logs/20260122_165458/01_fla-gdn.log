# Job 1: fla-gdn
# GPU: 1
# Command: python train.py --level fla-gdn --dim 2304 --depth 20 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/500m_state_v2/fla-gdn
# Started: 2026-01-22T16:54:58.401278
============================================================

Using device: cuda
Output directory: benchmark_results/500m_state_v2/fla-gdn/levelfla-gdn_100m_20260122_165505
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 533,694,928 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.1588 | lr 3.00e-04 | grad 15.69 | tok/s 13400
step     20 | loss 3.2668 | lr 3.00e-04 | grad 29.00 | tok/s 21578
step     30 | loss 4.7296 | lr 3.00e-04 | grad 14.75 | tok/s 22573
step     40 | loss 3.3907 | lr 3.00e-04 | grad 7.06 | tok/s 22301
step     50 | loss 2.7180 | lr 3.00e-04 | grad 6.41 | tok/s 22079
step     60 | loss 2.8423 | lr 3.00e-04 | grad 3.92 | tok/s 21504
step     70 | loss 2.3629 | lr 3.00e-04 | grad 4.53 | tok/s 20702
step     80 | loss 2.3554 | lr 3.00e-04 | grad 3.81 | tok/s 21415
step     90 | loss 2.4348 | lr 3.00e-04 | grad 8.62 | tok/s 20636
step    100 | loss 2.0208 | lr 3.00e-04 | grad 2.73 | tok/s 20814
step    110 | loss 2.0092 | lr 3.00e-04 | grad 3.33 | tok/s 20509
step    120 | loss 2.0931 | lr 3.00e-04 | grad 2.12 | tok/s 20220
step    130 | loss 2.0282 | lr 3.00e-04 | grad 2.12 | tok/s 20682
step    140 | loss 1.8658 | lr 3.00e-04 | grad 2.30 | tok/s 20731
step    150 | loss 1.7400 | lr 3.00e-04 | grad 3.14 | tok/s 19820
step    160 | loss 1.7120 | lr 3.00e-04 | grad 1.93 | tok/s 19997
step    170 | loss 1.8647 | lr 3.00e-04 | grad 7.97 | tok/s 20672
step    180 | loss 1.8150 | lr 3.00e-04 | grad 1.32 | tok/s 20757
step    190 | loss 1.5250 | lr 3.00e-04 | grad 1.74 | tok/s 21089
step    200 | loss 1.1855 | lr 3.00e-04 | grad 1.87 | tok/s 21561
step    210 | loss 1.8094 | lr 3.00e-04 | grad 2.50 | tok/s 20675
step    220 | loss 1.6150 | lr 3.00e-04 | grad 1.85 | tok/s 21358
step    230 | loss 1.6339 | lr 3.00e-04 | grad 3.19 | tok/s 20655
step    240 | loss 1.6221 | lr 3.00e-04 | grad 2.47 | tok/s 21077
step    250 | loss 1.6021 | lr 3.00e-04 | grad 1.88 | tok/s 20792
step    260 | loss 1.6945 | lr 3.00e-04 | grad 1.75 | tok/s 19981
step    270 | loss 1.6058 | lr 3.00e-04 | grad 1.55 | tok/s 20723
step    280 | loss 1.4825 | lr 3.00e-04 | grad 2.83 | tok/s 20719
step    290 | loss 1.3847 | lr 3.00e-04 | grad 1.71 | tok/s 21772
step    300 | loss 1.3315 | lr 3.00e-04 | grad 1.55 | tok/s 21781
step    310 | loss 1.2867 | lr 3.00e-04 | grad 1.73 | tok/s 21810
step    320 | loss 1.4737 | lr 3.00e-04 | grad 4.00 | tok/s 21050
step    330 | loss 1.5879 | lr 3.00e-04 | grad 1.54 | tok/s 20534
step    340 | loss 1.5925 | lr 3.00e-04 | grad 3.44 | tok/s 20960
step    350 | loss 1.5923 | lr 3.00e-04 | grad 1.58 | tok/s 20389
step    360 | loss 1.5493 | lr 3.00e-04 | grad 4.00 | tok/s 20633
step    370 | loss 1.3618 | lr 3.00e-04 | grad 1.38 | tok/s 21039
step    380 | loss 1.7211 | lr 3.00e-04 | grad 1.59 | tok/s 21570
step    390 | loss 1.5021 | lr 3.00e-04 | grad 1.23 | tok/s 20776
step    400 | loss 1.5611 | lr 3.00e-04 | grad 3.88 | tok/s 21308
step    410 | loss 1.3353 | lr 3.00e-04 | grad 3.25 | tok/s 20759
step    420 | loss 1.4529 | lr 3.00e-04 | grad 1.23 | tok/s 20578
step    430 | loss 1.5546 | lr 3.00e-04 | grad 2.03 | tok/s 20504
step    440 | loss 1.5204 | lr 3.00e-04 | grad 1.13 | tok/s 21320
step    450 | loss 1.5167 | lr 3.00e-04 | grad 1.62 | tok/s 20756
step    460 | loss 1.4941 | lr 3.00e-04 | grad 1.33 | tok/s 20783
step    470 | loss 1.4540 | lr 3.00e-04 | grad 1.91 | tok/s 21033
step    480 | loss 1.3942 | lr 3.00e-04 | grad 1.28 | tok/s 20306
step    490 | loss 1.4088 | lr 3.00e-04 | grad 1.11 | tok/s 20687
step    500 | loss 1.8099 | lr 3.00e-04 | grad 1.97 | tok/s 21292
step    510 | loss 1.3739 | lr 3.00e-04 | grad 1.12 | tok/s 20864
step    520 | loss 1.3073 | lr 3.00e-04 | grad 1.53 | tok/s 21556
step    530 | loss 1.8599 | lr 3.00e-04 | grad 1.38 | tok/s 21045
step    540 | loss 1.2679 | lr 3.00e-04 | grad 2.19 | tok/s 21089
step    550 | loss 1.2970 | lr 3.00e-04 | grad 1.39 | tok/s 21599
step    560 | loss 1.2041 | lr 3.00e-04 | grad 1.23 | tok/s 21880
step    570 | loss 1.3839 | lr 3.00e-04 | grad 2.59 | tok/s 21371
step    580 | loss 1.6078 | lr 3.00e-04 | grad 1.49 | tok/s 21099
step    590 | loss 1.7501 | lr 3.00e-04 | grad 1.38 | tok/s 20677
step    600 | loss 1.4005 | lr 3.00e-04 | grad 2.17 | tok/s 20722
step    610 | loss 1.3238 | lr 3.00e-04 | grad 1.34 | tok/s 21737
step    620 | loss 1.3714 | lr 3.00e-04 | grad 1.23 | tok/s 20610
step    630 | loss 1.3111 | lr 3.00e-04 | grad 1.34 | tok/s 21270
step    640 | loss 1.4550 | lr 3.00e-04 | grad 1.48 | tok/s 21278
step    650 | loss 1.3517 | lr 3.00e-04 | grad 1.48 | tok/s 20887
step    660 | loss 1.5498 | lr 3.00e-04 | grad 4.94 | tok/s 20638
step    670 | loss 1.4986 | lr 3.00e-04 | grad 2.03 | tok/s 21318
step    680 | loss 1.4234 | lr 3.00e-04 | grad 1.65 | tok/s 20581
step    690 | loss 1.4394 | lr 3.00e-04 | grad 1.78 | tok/s 20762
step    700 | loss 1.5464 | lr 3.00e-04 | grad 2.36 | tok/s 20870
step    710 | loss 1.3949 | lr 3.00e-04 | grad 1.62 | tok/s 20981
step    720 | loss 1.3148 | lr 3.00e-04 | grad 3.52 | tok/s 20862
step    730 | loss 1.5149 | lr 3.00e-04 | grad 1.76 | tok/s 21083
step    740 | loss 1.4732 | lr 3.00e-04 | grad 3.31 | tok/s 20911
step    750 | loss 1.3117 | lr 3.00e-04 | grad 1.56 | tok/s 20686
step    760 | loss 1.6622 | lr 3.00e-04 | grad 1.19 | tok/s 20914
step    770 | loss 1.3593 | lr 3.00e-04 | grad 1.68 | tok/s 20812
step    780 | loss 1.4197 | lr 3.00e-04 | grad 1.74 | tok/s 21025
step    790 | loss 1.2807 | lr 3.00e-04 | grad 1.29 | tok/s 21208

Training complete! Final step: 792
