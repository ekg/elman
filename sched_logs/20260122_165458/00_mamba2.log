# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 1600 --depth 32 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/500m_state_v2/mamba2
# Started: 2026-01-22T16:54:58.400856
============================================================

Using device: cuda
Output directory: benchmark_results/500m_state_v2/mamba2/levelmamba2_100m_20260122_165504
Model: Level mamba2, 508,362,560 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4581 | lr 3.00e-04 | grad 30.25 | tok/s 6830
step     20 | loss 3.3010 | lr 3.00e-04 | grad 8.81 | tok/s 16718
step     30 | loss 4.1710 | lr 3.00e-04 | grad 3.55 | tok/s 17359
step     40 | loss 2.5252 | lr 3.00e-04 | grad 2.20 | tok/s 17106
step     50 | loss 2.1248 | lr 3.00e-04 | grad 2.14 | tok/s 16909
step     60 | loss 2.5859 | lr 3.00e-04 | grad 2.39 | tok/s 16402
step     70 | loss 2.2572 | lr 3.00e-04 | grad 3.75 | tok/s 15780
step     80 | loss 2.5500 | lr 3.00e-04 | grad 3.27 | tok/s 16354
step     90 | loss 2.3501 | lr 3.00e-04 | grad 4.78 | tok/s 15764
step    100 | loss 1.9564 | lr 3.00e-04 | grad 2.44 | tok/s 15855
step    110 | loss 1.9500 | lr 3.00e-04 | grad 3.25 | tok/s 15596
step    120 | loss 1.9697 | lr 3.00e-04 | grad 2.03 | tok/s 15368
step    130 | loss 1.9595 | lr 3.00e-04 | grad 1.87 | tok/s 15705
step    140 | loss 1.8306 | lr 3.00e-04 | grad 2.67 | tok/s 15726
step    150 | loss 1.7095 | lr 3.00e-04 | grad 2.70 | tok/s 15004
step    160 | loss 1.7012 | lr 3.00e-04 | grad 1.90 | tok/s 15150
step    170 | loss 1.8298 | lr 3.00e-04 | grad 4.62 | tok/s 15663
step    180 | loss 1.7957 | lr 3.00e-04 | grad 1.34 | tok/s 15692
step    190 | loss 1.5490 | lr 3.00e-04 | grad 1.68 | tok/s 15998
step    200 | loss 1.2440 | lr 3.00e-04 | grad 2.05 | tok/s 16340
step    210 | loss 1.7982 | lr 3.00e-04 | grad 2.25 | tok/s 15640
step    220 | loss 1.6380 | lr 3.00e-04 | grad 1.48 | tok/s 16191
step    230 | loss 1.6252 | lr 3.00e-04 | grad 2.61 | tok/s 15641
step    240 | loss 1.6248 | lr 3.00e-04 | grad 2.50 | tok/s 15963
step    250 | loss 1.6086 | lr 3.00e-04 | grad 1.95 | tok/s 15757
step    260 | loss 1.6934 | lr 3.00e-04 | grad 1.72 | tok/s 15160
step    270 | loss 1.5984 | lr 3.00e-04 | grad 1.47 | tok/s 15722
step    280 | loss 1.4872 | lr 3.00e-04 | grad 2.98 | tok/s 15732
step    290 | loss 1.4085 | lr 3.00e-04 | grad 1.53 | tok/s 16536
step    300 | loss 1.3576 | lr 3.00e-04 | grad 1.56 | tok/s 16540
step    310 | loss 1.3102 | lr 3.00e-04 | grad 1.88 | tok/s 16537
step    320 | loss 1.4820 | lr 3.00e-04 | grad 2.36 | tok/s 15959
step    330 | loss 1.5856 | lr 3.00e-04 | grad 1.44 | tok/s 15595
step    340 | loss 1.5981 | lr 3.00e-04 | grad 3.23 | tok/s 15901
step    350 | loss 1.5939 | lr 3.00e-04 | grad 1.49 | tok/s 15455
step    360 | loss 1.5576 | lr 3.00e-04 | grad 3.72 | tok/s 15624
step    370 | loss 1.3756 | lr 3.00e-04 | grad 1.33 | tok/s 15914
step    380 | loss 1.7349 | lr 3.00e-04 | grad 1.62 | tok/s 16328
step    390 | loss 1.5034 | lr 3.00e-04 | grad 1.15 | tok/s 15729
step    400 | loss 1.5707 | lr 3.00e-04 | grad 2.89 | tok/s 16141
step    410 | loss 1.3474 | lr 3.00e-04 | grad 2.92 | tok/s 15674
step    420 | loss 1.4785 | lr 3.00e-04 | grad 1.18 | tok/s 15556
step    430 | loss 1.5677 | lr 3.00e-04 | grad 1.92 | tok/s 15498
step    440 | loss 1.5534 | lr 3.00e-04 | grad 1.17 | tok/s 16100
step    450 | loss 1.5230 | lr 3.00e-04 | grad 1.39 | tok/s 15663
step    460 | loss 1.4924 | lr 3.00e-04 | grad 1.05 | tok/s 15729
step    470 | loss 1.4744 | lr 3.00e-04 | grad 1.88 | tok/s 15895
step    480 | loss 1.4107 | lr 3.00e-04 | grad 1.39 | tok/s 15346
step    490 | loss 1.4158 | lr 3.00e-04 | grad 1.26 | tok/s 15633
step    500 | loss 1.8384 | lr 3.00e-04 | grad 1.79 | tok/s 16104
step    510 | loss 1.3719 | lr 3.00e-04 | grad 1.58 | tok/s 15764
step    520 | loss 1.3440 | lr 3.00e-04 | grad 1.49 | tok/s 16285
step    530 | loss 1.8785 | lr 3.00e-04 | grad 1.29 | tok/s 15939
step    540 | loss 1.3047 | lr 3.00e-04 | grad 2.03 | tok/s 15936
step    550 | loss 1.3164 | lr 3.00e-04 | grad 1.43 | tok/s 16354
step    560 | loss 1.2271 | lr 3.00e-04 | grad 1.41 | tok/s 16551
step    570 | loss 1.4053 | lr 3.00e-04 | grad 2.47 | tok/s 16183
step    580 | loss 1.6246 | lr 3.00e-04 | grad 1.41 | tok/s 15993
step    590 | loss 1.7909 | lr 3.00e-04 | grad 1.16 | tok/s 15660

Training complete! Final step: 593
