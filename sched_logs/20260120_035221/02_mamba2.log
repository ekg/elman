# Job 2: mamba2
# GPU: 2
# Command: python train.py --level mamba2 --dim 2944 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e75_128head/mamba2
# Started: 2026-01-20T03:52:21.126319
============================================================

Using device: cuda
Output directory: benchmark_results/e75_128head/mamba2/levelmamba2_100m_20260120_035227
Model: Level mamba2, 1,062,161,552 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9856 | lr 2.70e-05 | grad 13.50 | tok/s 3688
step     20 | loss 2.8033 | lr 5.70e-05 | grad 7.69 | tok/s 10071
step     30 | loss 2.7975 | lr 8.70e-05 | grad 11.12 | tok/s 9903
step     40 | loss 2.9046 | lr 1.17e-04 | grad 15.94 | tok/s 10145
step     50 | loss 4.4214 | lr 1.47e-04 | grad 9.31 | tok/s 10403
step     60 | loss 3.8631 | lr 1.77e-04 | grad 10.88 | tok/s 10287
step     70 | loss 3.1897 | lr 2.07e-04 | grad 12.88 | tok/s 10181
step     80 | loss 3.0411 | lr 2.37e-04 | grad 11.06 | tok/s 10089
step     90 | loss 2.6939 | lr 2.67e-04 | grad 6.84 | tok/s 10010
step    100 | loss 2.5578 | lr 2.97e-04 | grad 7.03 | tok/s 9943
step    110 | loss 2.4019 | lr 6.94e-06 | grad 10.31 | tok/s 9864
step    120 | loss 3.6864 | lr 2.69e-05 | grad 3.55 | tok/s 9539
step    130 | loss 2.4396 | lr 5.89e-05 | grad 2.73 | tok/s 9296
step    140 | loss 2.1088 | lr 9.99e-05 | grad 4.38 | tok/s 9317
step    150 | loss 2.2074 | lr 1.46e-04 | grad 3.83 | tok/s 9603
step    160 | loss 2.0468 | lr 1.92e-04 | grad 6.44 | tok/s 9646
step    170 | loss 2.4176 | lr 2.35e-04 | grad 5.97 | tok/s 9113
step    180 | loss 2.1182 | lr 2.69e-04 | grad 4.09 | tok/s 9420
step    190 | loss 2.1042 | lr 2.91e-04 | grad 4.28 | tok/s 9026
step    200 | loss 1.8035 | lr 3.00e-04 | grad 3.47 | tok/s 9635
step    210 | loss 1.7297 | lr 2.94e-04 | grad 3.27 | tok/s 9346
step    220 | loss 2.3536 | lr 2.74e-04 | grad 4.91 | tok/s 9008
step    230 | loss 2.6850 | lr 2.42e-04 | grad 1.91 | tok/s 9014
step    240 | loss 2.0845 | lr 2.01e-04 | grad 4.53 | tok/s 9056
step    250 | loss 2.2847 | lr 1.55e-04 | grad 4.66 | tok/s 9080
step    260 | loss 1.8749 | lr 1.09e-04 | grad 1.74 | tok/s 9376
step    270 | loss 2.0135 | lr 6.65e-05 | grad 2.56 | tok/s 9374
step    280 | loss 1.7306 | lr 3.24e-05 | grad 1.89 | tok/s 9105
step    290 | loss 1.7085 | lr 9.84e-06 | grad 2.30 | tok/s 8750
step    300 | loss 1.8026 | lr 1.07e-06 | grad 2.27 | tok/s 8879
step    310 | loss 1.8270 | lr 6.94e-06 | grad 1.41 | tok/s 9071
step    320 | loss 1.6539 | lr 2.69e-05 | grad 2.34 | tok/s 8701
step    330 | loss 1.8562 | lr 5.89e-05 | grad 1.69 | tok/s 9105
step    340 | loss 1.8972 | lr 9.99e-05 | grad 6.00 | tok/s 9278
step    350 | loss 1.9363 | lr 1.46e-04 | grad 3.48 | tok/s 9105
step    360 | loss 1.7936 | lr 1.92e-04 | grad 1.91 | tok/s 9320
step    370 | loss 1.6039 | lr 2.35e-04 | grad 1.74 | tok/s 9134
step    380 | loss 1.6633 | lr 2.69e-04 | grad 2.02 | tok/s 9558
step    390 | loss 1.2029 | lr 2.91e-04 | grad 1.97 | tok/s 9629
step    400 | loss 1.1523 | lr 3.00e-04 | grad 2.72 | tok/s 9482
step    410 | loss 2.2291 | lr 2.94e-04 | grad 2.84 | tok/s 9158
step    420 | loss 1.9997 | lr 2.74e-04 | grad 3.27 | tok/s 9158
step    430 | loss 1.8220 | lr 2.42e-04 | grad 3.09 | tok/s 9614
step    440 | loss 1.7744 | lr 2.01e-04 | grad 2.67 | tok/s 9316
step    450 | loss 1.9069 | lr 1.55e-04 | grad 2.09 | tok/s 9179
step    460 | loss 1.6341 | lr 1.09e-04 | grad 3.28 | tok/s 9102
step    470 | loss 1.7143 | lr 6.65e-05 | grad 2.25 | tok/s 9107
step    480 | loss 1.7273 | lr 3.24e-05 | grad 2.25 | tok/s 9543
step    490 | loss 1.8268 | lr 9.84e-06 | grad 1.26 | tok/s 9234
step    500 | loss 1.6833 | lr 1.07e-06 | grad 1.66 | tok/s 9169
step    510 | loss 1.9801 | lr 6.94e-06 | grad 5.44 | tok/s 9006
step    520 | loss 1.7497 | lr 2.69e-05 | grad 1.27 | tok/s 8633
step    530 | loss 1.5955 | lr 5.89e-05 | grad 1.36 | tok/s 9156
step    540 | loss 1.7614 | lr 9.99e-05 | grad 1.71 | tok/s 9149
step    550 | loss 1.6703 | lr 1.46e-04 | grad 2.30 | tok/s 8937
step    560 | loss 1.4456 | lr 1.92e-04 | grad 2.81 | tok/s 9368
step    570 | loss 1.5089 | lr 2.35e-04 | grad 1.64 | tok/s 9636
step    580 | loss 1.3338 | lr 2.69e-04 | grad 1.36 | tok/s 9638
step    590 | loss 1.2861 | lr 2.91e-04 | grad 1.51 | tok/s 9631
step    600 | loss 1.3850 | lr 3.00e-04 | grad 1.84 | tok/s 9636
step    610 | loss 1.2953 | lr 2.94e-04 | grad 1.67 | tok/s 9623
step    620 | loss 1.3396 | lr 2.74e-04 | grad 1.42 | tok/s 9613
step    630 | loss 1.4750 | lr 2.42e-04 | grad 4.69 | tok/s 9467
step    640 | loss 1.8443 | lr 2.01e-04 | grad 3.09 | tok/s 9050
step    650 | loss 1.8343 | lr 1.55e-04 | grad 2.44 | tok/s 9001
step    660 | loss 1.6572 | lr 1.09e-04 | grad 2.08 | tok/s 9094
step    670 | loss 1.6753 | lr 6.65e-05 | grad 2.11 | tok/s 9390
step    680 | loss 1.6943 | lr 3.24e-05 | grad 1.92 | tok/s 9064
step    690 | loss 1.7138 | lr 9.84e-06 | grad 2.03 | tok/s 9007

Training complete! Final step: 696
