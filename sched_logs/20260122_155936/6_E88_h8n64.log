Using device: cuda
Output directory: benchmark_results/mega_100m_20260122_155533/E88_h8n64/levelE88c_h8n64_100m_20260122_155942
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88c_h8n64, 125,582,784 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.7609 | lr 3.00e-04 | grad 13.06 | tok/s 17544
step     20 | loss 3.4211 | lr 3.00e-04 | grad 10.44 | tok/s 35391
step     30 | loss 3.8573 | lr 3.00e-04 | grad 3.83 | tok/s 37125
step     40 | loss 2.7094 | lr 3.00e-04 | grad 2.14 | tok/s 36896
step     50 | loss 2.3124 | lr 3.00e-04 | grad 1.84 | tok/s 37312
step     60 | loss 2.7087 | lr 3.00e-04 | grad 2.67 | tok/s 36375
step     70 | loss 2.4114 | lr 3.00e-04 | grad 2.05 | tok/s 35208
step     80 | loss 2.6010 | lr 3.00e-04 | grad 3.64 | tok/s 36498
step     90 | loss 2.5268 | lr 3.00e-04 | grad 5.75 | tok/s 35121
step    100 | loss 2.1677 | lr 3.00e-04 | grad 2.05 | tok/s 35456
step    110 | loss 2.1903 | lr 3.00e-04 | grad 2.47 | tok/s 34948
step    120 | loss 2.1936 | lr 3.00e-04 | grad 2.11 | tok/s 34396
step    130 | loss 2.2153 | lr 3.00e-04 | grad 1.70 | tok/s 35185
step    140 | loss 2.0566 | lr 3.00e-04 | grad 1.69 | tok/s 35247
step    150 | loss 1.9458 | lr 3.00e-04 | grad 2.75 | tok/s 33650
step    160 | loss 1.8979 | lr 3.00e-04 | grad 1.97 | tok/s 33922
step    170 | loss 2.0494 | lr 3.00e-04 | grad 4.47 | tok/s 35165
step    180 | loss 2.0097 | lr 3.00e-04 | grad 1.35 | tok/s 35202
step    190 | loss 1.7539 | lr 3.00e-04 | grad 1.85 | tok/s 35724
step    200 | loss 1.4623 | lr 3.00e-04 | grad 1.83 | tok/s 36523
step    210 | loss 1.9918 | lr 3.00e-04 | grad 2.69 | tok/s 35043
step    220 | loss 1.8584 | lr 3.00e-04 | grad 1.76 | tok/s 36274
step    230 | loss 1.8050 | lr 3.00e-04 | grad 2.81 | tok/s 34960
step    240 | loss 1.7914 | lr 3.00e-04 | grad 2.61 | tok/s 35583
step    250 | loss 1.7683 | lr 3.00e-04 | grad 2.03 | tok/s 35167
step    260 | loss 1.8579 | lr 3.00e-04 | grad 1.53 | tok/s 33786
step    270 | loss 1.7587 | lr 3.00e-04 | grad 1.66 | tok/s 35008
step    280 | loss 1.6513 | lr 3.00e-04 | grad 2.80 | tok/s 34932
step    290 | loss 1.5515 | lr 3.00e-04 | grad 1.81 | tok/s 36857
step    300 | loss 1.4896 | lr 3.00e-04 | grad 1.61 | tok/s 36866
step    310 | loss 1.4467 | lr 3.00e-04 | grad 1.52 | tok/s 36846
step    320 | loss 1.6309 | lr 3.00e-04 | grad 3.58 | tok/s 35457
step    330 | loss 1.7356 | lr 3.00e-04 | grad 1.73 | tok/s 34428
step    340 | loss 1.7436 | lr 3.00e-04 | grad 3.92 | tok/s 35027
step    350 | loss 1.7521 | lr 3.00e-04 | grad 1.71 | tok/s 33972
step    360 | loss 1.7139 | lr 3.00e-04 | grad 4.53 | tok/s 34401
step    370 | loss 1.5259 | lr 3.00e-04 | grad 1.52 | tok/s 35086
step    380 | loss 1.9137 | lr 3.00e-04 | grad 1.76 | tok/s 35981
step    390 | loss 1.6490 | lr 3.00e-04 | grad 1.44 | tok/s 34750
step    400 | loss 1.7213 | lr 3.00e-04 | grad 3.77 | tok/s 35587
step    410 | loss 1.4812 | lr 3.00e-04 | grad 3.50 | tok/s 34474
step    420 | loss 1.6239 | lr 3.00e-04 | grad 1.79 | tok/s 34277
step    430 | loss 1.7271 | lr 3.00e-04 | grad 2.30 | tok/s 34226
step    440 | loss 1.7413 | lr 3.00e-04 | grad 1.41 | tok/s 35497
step    450 | loss 1.6486 | lr 3.00e-04 | grad 1.65 | tok/s 34630
step    460 | loss 1.6343 | lr 3.00e-04 | grad 1.51 | tok/s 34746
step    470 | loss 1.6101 | lr 3.00e-04 | grad 2.06 | tok/s 35014
step    480 | loss 1.5315 | lr 3.00e-04 | grad 1.44 | tok/s 33950
step    490 | loss 1.5401 | lr 3.00e-04 | grad 1.34 | tok/s 34401
step    500 | loss 2.0250 | lr 3.00e-04 | grad 2.36 | tok/s 35557
step    510 | loss 1.4849 | lr 3.00e-04 | grad 1.37 | tok/s 34772
step    520 | loss 1.4826 | lr 3.00e-04 | grad 1.52 | tok/s 35754
step    530 | loss 2.0273 | lr 3.00e-04 | grad 1.56 | tok/s 35109
step    540 | loss 1.4516 | lr 3.00e-04 | grad 2.09 | tok/s 35035
step    550 | loss 1.4258 | lr 3.00e-04 | grad 1.48 | tok/s 36059
step    560 | loss 1.3312 | lr 3.00e-04 | grad 1.36 | tok/s 36428
step    570 | loss 1.5421 | lr 3.00e-04 | grad 3.17 | tok/s 35689
step    580 | loss 1.7770 | lr 3.00e-04 | grad 1.57 | tok/s 35318
step    590 | loss 1.9485 | lr 3.00e-04 | grad 1.91 | tok/s 34537
step    600 | loss 1.5432 | lr 3.00e-04 | grad 2.53 | tok/s 34593
step    610 | loss 1.5163 | lr 3.00e-04 | grad 1.60 | tok/s 36276
step    620 | loss 1.5140 | lr 3.00e-04 | grad 1.61 | tok/s 34416
step    630 | loss 1.4504 | lr 3.00e-04 | grad 1.56 | tok/s 35487
step    640 | loss 1.6516 | lr 3.00e-04 | grad 1.59 | tok/s 35507
step    650 | loss 1.5142 | lr 3.00e-04 | grad 2.09 | tok/s 34889
step    660 | loss 1.7135 | lr 3.00e-04 | grad 6.09 | tok/s 34316
step    670 | loss 1.6519 | lr 3.00e-04 | grad 2.39 | tok/s 35610
step    680 | loss 1.5916 | lr 3.00e-04 | grad 2.20 | tok/s 34417
step    690 | loss 1.6023 | lr 3.00e-04 | grad 2.39 | tok/s 34706
step    700 | loss 1.6948 | lr 3.00e-04 | grad 2.72 | tok/s 34897
step    710 | loss 1.5686 | lr 3.00e-04 | grad 1.86 | tok/s 34996
step    720 | loss 1.5465 | lr 3.00e-04 | grad 5.09 | tok/s 34955
step    730 | loss 1.7094 | lr 3.00e-04 | grad 2.39 | tok/s 35283
step    740 | loss 1.6501 | lr 3.00e-04 | grad 3.39 | tok/s 35103
step    750 | loss 1.4607 | lr 3.00e-04 | grad 2.30 | tok/s 34789
step    760 | loss 1.7456 | lr 3.00e-04 | grad 1.52 | tok/s 34988
step    770 | loss 1.4771 | lr 3.00e-04 | grad 2.08 | tok/s 34664
step    780 | loss 1.5620 | lr 3.00e-04 | grad 1.34 | tok/s 35057
step    790 | loss 1.4468 | lr 3.00e-04 | grad 1.31 | tok/s 35487
step    800 | loss 1.4297 | lr 3.00e-04 | grad 1.79 | tok/s 35394
step    810 | loss 1.4724 | lr 3.00e-04 | grad 8.25 | tok/s 35062
step    820 | loss 2.2065 | lr 3.00e-04 | grad 2.89 | tok/s 35938
step    830 | loss 1.8824 | lr 3.00e-04 | grad 2.34 | tok/s 28219
step    840 | loss 1.6484 | lr 3.00e-04 | grad 2.08 | tok/s 36488
step    850 | loss 1.6353 | lr 3.00e-04 | grad 1.79 | tok/s 34865
step    860 | loss 1.4964 | lr 3.00e-04 | grad 1.50 | tok/s 34181
step    870 | loss 1.4712 | lr 3.00e-04 | grad 1.79 | tok/s 35098
step    880 | loss 1.5313 | lr 3.00e-04 | grad 1.95 | tok/s 35061
step    890 | loss 1.4447 | lr 3.00e-04 | grad 1.55 | tok/s 34933
step    900 | loss 1.7907 | lr 3.00e-04 | grad 1.67 | tok/s 34082
step    910 | loss 1.4342 | lr 3.00e-04 | grad 1.62 | tok/s 34652
step    920 | loss 1.5275 | lr 3.00e-04 | grad 1.60 | tok/s 34604
step    930 | loss 1.5808 | lr 3.00e-04 | grad 2.58 | tok/s 34528
step    940 | loss 1.5056 | lr 3.00e-04 | grad 3.30 | tok/s 34078
step    950 | loss 1.5660 | lr 3.00e-04 | grad 2.14 | tok/s 34956
step    960 | loss 1.3818 | lr 3.00e-04 | grad 1.73 | tok/s 36578
step    970 | loss 1.3094 | lr 3.00e-04 | grad 1.44 | tok/s 36440
step    980 | loss 1.4104 | lr 3.00e-04 | grad 2.97 | tok/s 35547
step    990 | loss 1.5985 | lr 3.00e-04 | grad 1.43 | tok/s 34481
step   1000 | loss 1.5566 | lr 3.00e-04 | grad 1.30 | tok/s 33690
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5566.pt
step   1010 | loss 1.6473 | lr 3.00e-04 | grad 1.55 | tok/s 30289
step   1020 | loss 1.3751 | lr 3.00e-04 | grad 1.55 | tok/s 34708
step   1030 | loss 1.6612 | lr 3.00e-04 | grad 1.56 | tok/s 34094
step   1040 | loss 1.4746 | lr 3.00e-04 | grad 2.12 | tok/s 34798
step   1050 | loss 1.4922 | lr 3.00e-04 | grad 1.59 | tok/s 34930
step   1060 | loss 1.5527 | lr 3.00e-04 | grad 2.25 | tok/s 35021
step   1070 | loss 1.6543 | lr 3.00e-04 | grad 1.86 | tok/s 35128
step   1080 | loss 2.0464 | lr 3.00e-04 | grad 2.14 | tok/s 34637
step   1090 | loss 1.7905 | lr 3.00e-04 | grad 2.12 | tok/s 34988
step   1100 | loss 1.5328 | lr 3.00e-04 | grad 1.84 | tok/s 34604
step   1110 | loss 1.4428 | lr 3.00e-04 | grad 1.91 | tok/s 35330
step   1120 | loss 1.5216 | lr 3.00e-04 | grad 1.53 | tok/s 35680
step   1130 | loss 1.5899 | lr 3.00e-04 | grad 1.68 | tok/s 33922
step   1140 | loss 1.4451 | lr 3.00e-04 | grad 1.65 | tok/s 34761
step   1150 | loss 1.7243 | lr 3.00e-04 | grad 2.47 | tok/s 34840
step   1160 | loss 1.4232 | lr 3.00e-04 | grad 1.40 | tok/s 34355
step   1170 | loss 1.5956 | lr 3.00e-04 | grad 1.73 | tok/s 34814
step   1180 | loss 1.4871 | lr 3.00e-04 | grad 1.79 | tok/s 36445
step   1190 | loss 1.3938 | lr 3.00e-04 | grad 1.41 | tok/s 36560
step   1200 | loss 1.3323 | lr 3.00e-04 | grad 1.51 | tok/s 36192
step   1210 | loss 1.3199 | lr 3.00e-04 | grad 1.66 | tok/s 36437
step   1220 | loss 1.3331 | lr 3.00e-04 | grad 1.80 | tok/s 36215
step   1230 | loss 1.3734 | lr 3.00e-04 | grad 1.50 | tok/s 35002
step   1240 | loss 1.4780 | lr 3.00e-04 | grad 1.55 | tok/s 34367
step   1250 | loss 1.5680 | lr 3.00e-04 | grad 7.44 | tok/s 35394
step   1260 | loss 1.5814 | lr 3.00e-04 | grad 4.16 | tok/s 35403
step   1270 | loss 1.6494 | lr 3.00e-04 | grad 2.38 | tok/s 34944
step   1280 | loss 1.5169 | lr 3.00e-04 | grad 1.95 | tok/s 34625
step   1290 | loss 1.4834 | lr 3.00e-04 | grad 1.47 | tok/s 34690
step   1300 | loss 1.5464 | lr 3.00e-04 | grad 1.68 | tok/s 34290
step   1310 | loss 1.5861 | lr 3.00e-04 | grad 1.51 | tok/s 34150
step   1320 | loss 1.5767 | lr 3.00e-04 | grad 2.17 | tok/s 34801

Training complete! Final step: 1325
