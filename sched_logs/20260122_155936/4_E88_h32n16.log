Using device: cuda
Output directory: benchmark_results/mega_100m_20260122_155533/E88_h32n16/levelE88f_h32n16_100m_20260122_155943
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88f_h32n16, 101,846,464 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 3.9923 | lr 3.00e-04 | grad 9.81 | tok/s 22355
step     20 | loss 3.2803 | lr 3.00e-04 | grad 16.75 | tok/s 61763
step     30 | loss 4.4354 | lr 3.00e-04 | grad 7.72 | tok/s 64860
step     40 | loss 3.1379 | lr 3.00e-04 | grad 5.81 | tok/s 64324
step     50 | loss 2.5499 | lr 3.00e-04 | grad 2.97 | tok/s 63757
step     60 | loss 2.8750 | lr 3.00e-04 | grad 3.62 | tok/s 61856
step     70 | loss 2.5309 | lr 3.00e-04 | grad 3.45 | tok/s 61118
step     80 | loss 2.4883 | lr 3.00e-04 | grad 4.03 | tok/s 63147
step     90 | loss 2.5785 | lr 3.00e-04 | grad 10.25 | tok/s 61169
step    100 | loss 2.2409 | lr 3.00e-04 | grad 2.53 | tok/s 61651
step    110 | loss 2.2422 | lr 3.00e-04 | grad 2.77 | tok/s 60371
step    120 | loss 2.2521 | lr 3.00e-04 | grad 2.36 | tok/s 60003
step    130 | loss 2.2707 | lr 3.00e-04 | grad 2.27 | tok/s 61315
step    140 | loss 2.0874 | lr 3.00e-04 | grad 2.11 | tok/s 61052
step    150 | loss 1.9740 | lr 3.00e-04 | grad 3.23 | tok/s 58130
step    160 | loss 1.9171 | lr 3.00e-04 | grad 2.45 | tok/s 58110
step    170 | loss 2.0884 | lr 3.00e-04 | grad 10.88 | tok/s 59927
step    180 | loss 2.0517 | lr 3.00e-04 | grad 1.63 | tok/s 60549
step    190 | loss 1.7618 | lr 3.00e-04 | grad 2.19 | tok/s 61766
step    200 | loss 1.4548 | lr 3.00e-04 | grad 2.16 | tok/s 62859
step    210 | loss 2.0144 | lr 3.00e-04 | grad 3.08 | tok/s 60366
step    220 | loss 1.8632 | lr 3.00e-04 | grad 1.98 | tok/s 62568
step    230 | loss 1.8297 | lr 3.00e-04 | grad 3.92 | tok/s 59933
step    240 | loss 1.8189 | lr 3.00e-04 | grad 3.27 | tok/s 61298
step    250 | loss 1.8075 | lr 3.00e-04 | grad 2.61 | tok/s 60923
step    260 | loss 1.8853 | lr 3.00e-04 | grad 1.99 | tok/s 58222
step    270 | loss 1.7808 | lr 3.00e-04 | grad 2.06 | tok/s 60422
step    280 | loss 1.6779 | lr 3.00e-04 | grad 3.17 | tok/s 60308
step    290 | loss 1.5639 | lr 3.00e-04 | grad 2.05 | tok/s 63408
step    300 | loss 1.5009 | lr 3.00e-04 | grad 1.95 | tok/s 63890
step    310 | loss 1.4560 | lr 3.00e-04 | grad 1.93 | tok/s 63799
step    320 | loss 1.6678 | lr 3.00e-04 | grad 4.50 | tok/s 61344
step    330 | loss 1.7541 | lr 3.00e-04 | grad 2.09 | tok/s 59533
step    340 | loss 1.7646 | lr 3.00e-04 | grad 4.88 | tok/s 60904
step    350 | loss 1.7879 | lr 3.00e-04 | grad 2.16 | tok/s 58877
step    360 | loss 1.7347 | lr 3.00e-04 | grad 5.25 | tok/s 59615
step    370 | loss 1.5499 | lr 3.00e-04 | grad 1.90 | tok/s 61012
step    380 | loss 1.9635 | lr 3.00e-04 | grad 2.23 | tok/s 62044
step    390 | loss 1.6887 | lr 3.00e-04 | grad 1.75 | tok/s 60016
step    400 | loss 1.7519 | lr 3.00e-04 | grad 4.41 | tok/s 61167
step    410 | loss 1.5112 | lr 3.00e-04 | grad 4.16 | tok/s 59466
step    420 | loss 1.6505 | lr 3.00e-04 | grad 2.20 | tok/s 59428
step    430 | loss 1.7735 | lr 3.00e-04 | grad 2.83 | tok/s 59137
step    440 | loss 1.7724 | lr 3.00e-04 | grad 1.88 | tok/s 61794
step    450 | loss 1.6818 | lr 3.00e-04 | grad 1.98 | tok/s 59654
step    460 | loss 1.6710 | lr 3.00e-04 | grad 1.80 | tok/s 60275
step    470 | loss 1.6322 | lr 3.00e-04 | grad 2.44 | tok/s 60495
step    480 | loss 1.5655 | lr 3.00e-04 | grad 1.84 | tok/s 58859
step    490 | loss 1.5568 | lr 3.00e-04 | grad 1.63 | tok/s 59657
step    500 | loss 2.1107 | lr 3.00e-04 | grad 2.84 | tok/s 61410
step    510 | loss 1.5201 | lr 3.00e-04 | grad 1.70 | tok/s 60191
step    520 | loss 1.4923 | lr 3.00e-04 | grad 1.81 | tok/s 62133
step    530 | loss 2.0616 | lr 3.00e-04 | grad 1.96 | tok/s 60510
step    540 | loss 1.4953 | lr 3.00e-04 | grad 2.42 | tok/s 60744
step    550 | loss 1.4395 | lr 3.00e-04 | grad 1.71 | tok/s 62467
step    560 | loss 1.3460 | lr 3.00e-04 | grad 1.79 | tok/s 63191
step    570 | loss 1.5768 | lr 3.00e-04 | grad 3.88 | tok/s 62127
step    580 | loss 1.8029 | lr 3.00e-04 | grad 1.96 | tok/s 60923
step    590 | loss 2.0091 | lr 3.00e-04 | grad 2.23 | tok/s 59788
step    600 | loss 1.5837 | lr 3.00e-04 | grad 3.05 | tok/s 60189
step    610 | loss 1.5481 | lr 3.00e-04 | grad 1.85 | tok/s 62773
step    620 | loss 1.5478 | lr 3.00e-04 | grad 1.96 | tok/s 59654
step    630 | loss 1.4679 | lr 3.00e-04 | grad 1.99 | tok/s 61208
step    640 | loss 1.6889 | lr 3.00e-04 | grad 1.96 | tok/s 61709
step    650 | loss 1.5509 | lr 3.00e-04 | grad 2.59 | tok/s 60403
step    660 | loss 1.7599 | lr 3.00e-04 | grad 7.41 | tok/s 59497
step    670 | loss 1.6867 | lr 3.00e-04 | grad 2.95 | tok/s 61865
step    680 | loss 1.6369 | lr 3.00e-04 | grad 2.64 | tok/s 59650
step    690 | loss 1.6448 | lr 3.00e-04 | grad 2.88 | tok/s 60141
step    700 | loss 1.7306 | lr 3.00e-04 | grad 3.31 | tok/s 60539
step    710 | loss 1.6149 | lr 3.00e-04 | grad 2.14 | tok/s 60889
step    720 | loss 1.5685 | lr 3.00e-04 | grad 6.19 | tok/s 60487
step    730 | loss 1.7446 | lr 3.00e-04 | grad 3.02 | tok/s 61007
step    740 | loss 1.6922 | lr 3.00e-04 | grad 4.25 | tok/s 60529
step    750 | loss 1.4938 | lr 3.00e-04 | grad 2.67 | tok/s 59858
step    760 | loss 1.7913 | lr 3.00e-04 | grad 1.91 | tok/s 60677
step    770 | loss 1.5060 | lr 3.00e-04 | grad 2.55 | tok/s 60473
step    780 | loss 1.5892 | lr 3.00e-04 | grad 1.62 | tok/s 60936
step    790 | loss 1.4722 | lr 3.00e-04 | grad 1.59 | tok/s 61249
step    800 | loss 1.4595 | lr 3.00e-04 | grad 2.22 | tok/s 61203
step    810 | loss 1.5321 | lr 3.00e-04 | grad 9.06 | tok/s 61060
step    820 | loss 2.2201 | lr 3.00e-04 | grad 3.31 | tok/s 62568
step    830 | loss 1.8740 | lr 3.00e-04 | grad 2.69 | tok/s 63305
step    840 | loss 1.6322 | lr 3.00e-04 | grad 2.30 | tok/s 63601
step    850 | loss 1.6678 | lr 3.00e-04 | grad 2.19 | tok/s 60433
step    860 | loss 1.5261 | lr 3.00e-04 | grad 1.88 | tok/s 59459
step    870 | loss 1.4967 | lr 3.00e-04 | grad 2.11 | tok/s 61495
step    880 | loss 1.5650 | lr 3.00e-04 | grad 2.31 | tok/s 61066
step    890 | loss 1.4702 | lr 3.00e-04 | grad 1.94 | tok/s 60425
step    900 | loss 1.8660 | lr 3.00e-04 | grad 2.11 | tok/s 58965
step    910 | loss 1.4742 | lr 3.00e-04 | grad 2.02 | tok/s 59672
step    920 | loss 1.5563 | lr 3.00e-04 | grad 1.98 | tok/s 60095
step    930 | loss 1.6195 | lr 3.00e-04 | grad 3.20 | tok/s 59909
step    940 | loss 1.5449 | lr 3.00e-04 | grad 4.06 | tok/s 59404
step    950 | loss 1.6076 | lr 3.00e-04 | grad 2.44 | tok/s 60850
step    960 | loss 1.3980 | lr 3.00e-04 | grad 1.72 | tok/s 63458
step    970 | loss 1.3233 | lr 3.00e-04 | grad 1.75 | tok/s 63869
step    980 | loss 1.4308 | lr 3.00e-04 | grad 2.78 | tok/s 61811
step    990 | loss 1.6341 | lr 3.00e-04 | grad 1.77 | tok/s 59972
step   1000 | loss 1.5961 | lr 3.00e-04 | grad 1.60 | tok/s 58950
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5961.pt
step   1010 | loss 1.7020 | lr 3.00e-04 | grad 1.69 | tok/s 46910
step   1020 | loss 1.3931 | lr 3.00e-04 | grad 1.91 | tok/s 59841
step   1030 | loss 1.7111 | lr 3.00e-04 | grad 1.99 | tok/s 58909
step   1040 | loss 1.5066 | lr 3.00e-04 | grad 2.58 | tok/s 60704
step   1050 | loss 1.5162 | lr 3.00e-04 | grad 2.02 | tok/s 60513
step   1060 | loss 1.6049 | lr 3.00e-04 | grad 3.14 | tok/s 60606
step   1070 | loss 1.7142 | lr 3.00e-04 | grad 2.31 | tok/s 60951
step   1080 | loss 2.0734 | lr 3.00e-04 | grad 2.70 | tok/s 60001
step   1090 | loss 1.8328 | lr 3.00e-04 | grad 2.61 | tok/s 60766
step   1100 | loss 1.5730 | lr 3.00e-04 | grad 2.28 | tok/s 60021
step   1110 | loss 1.4881 | lr 3.00e-04 | grad 2.38 | tok/s 61098
step   1120 | loss 1.5592 | lr 3.00e-04 | grad 1.88 | tok/s 61927
step   1130 | loss 1.6276 | lr 3.00e-04 | grad 1.97 | tok/s 59035
step   1140 | loss 1.4752 | lr 3.00e-04 | grad 2.03 | tok/s 59940
step   1150 | loss 1.7806 | lr 3.00e-04 | grad 3.00 | tok/s 60350
step   1160 | loss 1.4564 | lr 3.00e-04 | grad 1.70 | tok/s 59604
step   1170 | loss 1.6398 | lr 3.00e-04 | grad 2.19 | tok/s 60203
step   1180 | loss 1.5116 | lr 3.00e-04 | grad 1.84 | tok/s 63673
step   1190 | loss 1.4092 | lr 3.00e-04 | grad 1.90 | tok/s 63706
step   1200 | loss 1.3456 | lr 3.00e-04 | grad 1.85 | tok/s 63578
step   1210 | loss 1.3279 | lr 3.00e-04 | grad 1.87 | tok/s 63396
step   1220 | loss 1.3449 | lr 3.00e-04 | grad 2.06 | tok/s 63055
step   1230 | loss 1.4057 | lr 3.00e-04 | grad 1.84 | tok/s 60953
step   1240 | loss 1.5080 | lr 3.00e-04 | grad 1.83 | tok/s 59858
step   1250 | loss 1.6040 | lr 3.00e-04 | grad 9.19 | tok/s 61654
step   1260 | loss 1.6347 | lr 3.00e-04 | grad 5.25 | tok/s 61491
step   1270 | loss 1.6887 | lr 3.00e-04 | grad 2.81 | tok/s 60789
step   1280 | loss 1.5511 | lr 3.00e-04 | grad 2.36 | tok/s 60131
step   1290 | loss 1.5137 | lr 3.00e-04 | grad 1.76 | tok/s 59830
step   1300 | loss 1.5869 | lr 3.00e-04 | grad 2.00 | tok/s 59854
step   1310 | loss 1.6372 | lr 3.00e-04 | grad 1.89 | tok/s 59470
step   1320 | loss 1.6116 | lr 3.00e-04 | grad 2.66 | tok/s 60450
step   1330 | loss 1.4952 | lr 3.00e-04 | grad 2.03 | tok/s 60733
step   1340 | loss 1.4224 | lr 3.00e-04 | grad 1.87 | tok/s 60666
step   1350 | loss 1.4355 | lr 3.00e-04 | grad 2.94 | tok/s 62405
step   1360 | loss 1.4552 | lr 3.00e-04 | grad 2.06 | tok/s 59499
step   1370 | loss 1.5566 | lr 3.00e-04 | grad 1.87 | tok/s 60140
step   1380 | loss 1.6105 | lr 3.00e-04 | grad 2.31 | tok/s 60445
step   1390 | loss 1.5538 | lr 3.00e-04 | grad 3.73 | tok/s 59305
step   1400 | loss 1.4800 | lr 3.00e-04 | grad 6.12 | tok/s 61247
step   1410 | loss 1.5154 | lr 3.00e-04 | grad 2.08 | tok/s 62124
step   1420 | loss 1.5501 | lr 3.00e-04 | grad 1.91 | tok/s 37733
step   1430 | loss 1.4531 | lr 3.00e-04 | grad 2.05 | tok/s 58094
step   1440 | loss 1.3639 | lr 3.00e-04 | grad 1.61 | tok/s 61217
step   1450 | loss 1.3736 | lr 3.00e-04 | grad 3.20 | tok/s 62490
step   1460 | loss 1.5029 | lr 3.00e-04 | grad 2.02 | tok/s 58918
step   1470 | loss 1.5712 | lr 3.00e-04 | grad 4.03 | tok/s 60650
step   1480 | loss 1.4529 | lr 3.00e-04 | grad 4.84 | tok/s 62303
step   1490 | loss 1.5851 | lr 3.00e-04 | grad 4.31 | tok/s 61347
step   1500 | loss 1.7000 | lr 3.00e-04 | grad 5.34 | tok/s 59848
step   1510 | loss 1.5546 | lr 3.00e-04 | grad 2.53 | tok/s 62151
step   1520 | loss 1.5424 | lr 3.00e-04 | grad 2.27 | tok/s 61984
step   1530 | loss 1.5129 | lr 3.00e-04 | grad 1.78 | tok/s 61681
step   1540 | loss 1.4651 | lr 3.00e-04 | grad 1.77 | tok/s 60300
step   1550 | loss 1.4308 | lr 3.00e-04 | grad 6.69 | tok/s 62477
step   1560 | loss 1.9409 | lr 3.00e-04 | grad 2.58 | tok/s 60945
step   1570 | loss 1.5023 | lr 3.00e-04 | grad 2.31 | tok/s 59839
step   1580 | loss 1.5807 | lr 3.00e-04 | grad 2.47 | tok/s 61788
step   1590 | loss 1.4137 | lr 3.00e-04 | grad 1.80 | tok/s 60248
step   1600 | loss 1.5457 | lr 3.00e-04 | grad 1.62 | tok/s 59277
step   1610 | loss 1.3557 | lr 3.00e-04 | grad 1.73 | tok/s 62314
step   1620 | loss 1.5381 | lr 3.00e-04 | grad 1.89 | tok/s 61526
step   1630 | loss 1.5043 | lr 3.00e-04 | grad 1.58 | tok/s 61954
step   1640 | loss 1.4568 | lr 3.00e-04 | grad 1.79 | tok/s 59836
step   1650 | loss 1.5033 | lr 3.00e-04 | grad 3.56 | tok/s 59364
step   1660 | loss 1.5190 | lr 3.00e-04 | grad 2.03 | tok/s 59504
step   1670 | loss 1.5010 | lr 3.00e-04 | grad 3.05 | tok/s 61654
step   1680 | loss 1.8366 | lr 3.00e-04 | grad 1.72 | tok/s 62071
step   1690 | loss 1.4749 | lr 3.00e-04 | grad 2.19 | tok/s 60576
step   1700 | loss 1.5634 | lr 3.00e-04 | grad 1.79 | tok/s 62264
step   1710 | loss 1.5734 | lr 3.00e-04 | grad 3.06 | tok/s 60167
step   1720 | loss 1.4680 | lr 3.00e-04 | grad 2.08 | tok/s 60264
step   1730 | loss 1.6703 | lr 3.00e-04 | grad 2.41 | tok/s 60489
step   1740 | loss 1.4965 | lr 3.00e-04 | grad 1.49 | tok/s 61161
step   1750 | loss 1.4956 | lr 3.00e-04 | grad 2.02 | tok/s 59078
step   1760 | loss 1.6412 | lr 3.00e-04 | grad 1.94 | tok/s 60147
step   1770 | loss 1.5832 | lr 3.00e-04 | grad 1.71 | tok/s 61212
step   1780 | loss 1.5101 | lr 3.00e-04 | grad 2.52 | tok/s 59126
step   1790 | loss 1.6632 | lr 3.00e-04 | grad 2.11 | tok/s 60075
step   1800 | loss 1.4260 | lr 3.00e-04 | grad 1.59 | tok/s 61073
step   1810 | loss 1.5081 | lr 3.00e-04 | grad 1.94 | tok/s 61018
step   1820 | loss 1.3946 | lr 3.00e-04 | grad 1.55 | tok/s 60016
step   1830 | loss 1.4786 | lr 3.00e-04 | grad 2.00 | tok/s 60199
step   1840 | loss 1.4514 | lr 3.00e-04 | grad 1.89 | tok/s 59610
step   1850 | loss 1.7025 | lr 3.00e-04 | grad 2.53 | tok/s 60402
step   1860 | loss 1.4577 | lr 3.00e-04 | grad 1.73 | tok/s 55260
step   1870 | loss 1.4308 | lr 3.00e-04 | grad 2.55 | tok/s 60753
step   1880 | loss 1.4500 | lr 3.00e-04 | grad 1.67 | tok/s 61224
step   1890 | loss 1.5743 | lr 3.00e-04 | grad 1.74 | tok/s 59874
step   1900 | loss 1.5081 | lr 3.00e-04 | grad 1.80 | tok/s 60679
step   1910 | loss 1.6059 | lr 3.00e-04 | grad 3.84 | tok/s 59662
step   1920 | loss 1.3948 | lr 3.00e-04 | grad 1.67 | tok/s 61889
step   1930 | loss 1.4276 | lr 3.00e-04 | grad 1.94 | tok/s 60998
step   1940 | loss 1.4361 | lr 3.00e-04 | grad 2.22 | tok/s 62285
step   1950 | loss 1.5115 | lr 3.00e-04 | grad 1.84 | tok/s 60849
step   1960 | loss 1.6632 | lr 3.00e-04 | grad 4.62 | tok/s 62196
step   1970 | loss 1.3611 | lr 3.00e-04 | grad 1.98 | tok/s 60054
step   1980 | loss 1.5025 | lr 3.00e-04 | grad 3.72 | tok/s 59493
step   1990 | loss 1.5444 | lr 3.00e-04 | grad 2.56 | tok/s 61340
step   2000 | loss 1.4637 | lr 3.00e-04 | grad 2.25 | tok/s 61566
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4637.pt
step   2010 | loss 1.2911 | lr 3.00e-04 | grad 1.62 | tok/s 47646
step   2020 | loss 1.1913 | lr 3.00e-04 | grad 1.45 | tok/s 63434
step   2030 | loss 1.4699 | lr 3.00e-04 | grad 1.63 | tok/s 62672
step   2040 | loss 1.3115 | lr 3.00e-04 | grad 1.73 | tok/s 63232
step   2050 | loss 1.2926 | lr 3.00e-04 | grad 2.23 | tok/s 62944
step   2060 | loss 1.5621 | lr 3.00e-04 | grad 1.79 | tok/s 59728
step   2070 | loss 1.4941 | lr 3.00e-04 | grad 2.03 | tok/s 62630
step   2080 | loss 1.5270 | lr 3.00e-04 | grad 3.56 | tok/s 59141
step   2090 | loss 1.4774 | lr 3.00e-04 | grad 1.83 | tok/s 62162
step   2100 | loss 1.4985 | lr 3.00e-04 | grad 2.02 | tok/s 60275
step   2110 | loss 1.2287 | lr 3.00e-04 | grad 1.67 | tok/s 62372
step   2120 | loss 1.3182 | lr 3.00e-04 | grad 1.98 | tok/s 61441
step   2130 | loss 1.4731 | lr 3.00e-04 | grad 4.72 | tok/s 59376
step   2140 | loss 1.5394 | lr 3.00e-04 | grad 4.28 | tok/s 60020
step   2150 | loss 1.5298 | lr 3.00e-04 | grad 1.72 | tok/s 60627
step   2160 | loss 1.5270 | lr 3.00e-04 | grad 1.68 | tok/s 59914
step   2170 | loss 1.6172 | lr 3.00e-04 | grad 2.09 | tok/s 61136
step   2180 | loss 1.4537 | lr 3.00e-04 | grad 1.91 | tok/s 62147
step   2190 | loss 1.6616 | lr 3.00e-04 | grad 2.00 | tok/s 61722
step   2200 | loss 1.2426 | lr 3.00e-04 | grad 1.48 | tok/s 64080
step   2210 | loss 1.2634 | lr 3.00e-04 | grad 1.66 | tok/s 63828
step   2220 | loss 1.2408 | lr 3.00e-04 | grad 1.77 | tok/s 64189
step   2230 | loss 1.4158 | lr 3.00e-04 | grad 1.96 | tok/s 61719
step   2240 | loss 1.5257 | lr 3.00e-04 | grad 3.14 | tok/s 63174
step   2250 | loss 1.4787 | lr 3.00e-04 | grad 2.16 | tok/s 62550
step   2260 | loss 1.5734 | lr 3.00e-04 | grad 2.28 | tok/s 61704
step   2270 | loss 1.4183 | lr 3.00e-04 | grad 2.73 | tok/s 60288
step   2280 | loss 1.5916 | lr 3.00e-04 | grad 1.71 | tok/s 60430
step   2290 | loss 1.4385 | lr 3.00e-04 | grad 1.97 | tok/s 60927

Training complete! Final step: 2294
