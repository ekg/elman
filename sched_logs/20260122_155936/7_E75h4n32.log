Using device: cuda
Output directory: benchmark_results/mega_100m_20260122_155533/E75h4n32/levelE75h4n32_100m_20260122_155943
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75h4n32, 98,838,400 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.2019 | lr 3.00e-04 | grad 14.25 | tok/s 23344
step     20 | loss 3.8459 | lr 3.00e-04 | grad 15.44 | tok/s 65049
step     30 | loss 4.5398 | lr 3.00e-04 | grad 4.06 | tok/s 68785
step     40 | loss 3.8855 | lr 3.00e-04 | grad 6.69 | tok/s 68640
step     50 | loss 3.5461 | lr 3.00e-04 | grad 3.81 | tok/s 68537
step     60 | loss 3.3664 | lr 3.00e-04 | grad 3.27 | tok/s 67103
step     70 | loss 2.7575 | lr 3.00e-04 | grad 2.08 | tok/s 64753
step     80 | loss 3.2426 | lr 3.00e-04 | grad 6.25 | tok/s 67244
step     90 | loss 2.8336 | lr 3.00e-04 | grad 5.03 | tok/s 64896
step    100 | loss 2.5582 | lr 3.00e-04 | grad 3.33 | tok/s 65542
step    110 | loss 2.4824 | lr 3.00e-04 | grad 3.05 | tok/s 64616
step    120 | loss 2.7452 | lr 3.00e-04 | grad 1.75 | tok/s 63652
step    130 | loss 2.5341 | lr 3.00e-04 | grad 1.55 | tok/s 65020
step    140 | loss 2.3349 | lr 3.00e-04 | grad 1.81 | tok/s 65243
step    150 | loss 2.2367 | lr 3.00e-04 | grad 2.27 | tok/s 62153
step    160 | loss 2.1634 | lr 3.00e-04 | grad 2.47 | tok/s 62758
step    170 | loss 2.3398 | lr 3.00e-04 | grad 5.12 | tok/s 64831
step    180 | loss 2.3786 | lr 3.00e-04 | grad 1.73 | tok/s 65009
step    190 | loss 2.0861 | lr 3.00e-04 | grad 2.03 | tok/s 65964
step    200 | loss 1.8841 | lr 3.00e-04 | grad 1.62 | tok/s 67491
step    210 | loss 2.2444 | lr 3.00e-04 | grad 2.09 | tok/s 64584
step    220 | loss 2.1847 | lr 3.00e-04 | grad 2.08 | tok/s 66749
step    230 | loss 2.0612 | lr 3.00e-04 | grad 2.25 | tok/s 64456
step    240 | loss 2.0965 | lr 3.00e-04 | grad 2.34 | tok/s 65734
step    250 | loss 2.0442 | lr 3.00e-04 | grad 1.88 | tok/s 64834
step    260 | loss 2.0878 | lr 3.00e-04 | grad 1.16 | tok/s 62193
step    270 | loss 1.9887 | lr 3.00e-04 | grad 1.45 | tok/s 64402
step    280 | loss 1.8904 | lr 3.00e-04 | grad 2.89 | tok/s 64394
step    290 | loss 1.8010 | lr 3.00e-04 | grad 1.68 | tok/s 67807
step    300 | loss 1.7293 | lr 3.00e-04 | grad 1.64 | tok/s 67787
step    310 | loss 1.6778 | lr 3.00e-04 | grad 1.51 | tok/s 67775
step    320 | loss 1.8688 | lr 3.00e-04 | grad 8.38 | tok/s 65276
step    330 | loss 1.9724 | lr 3.00e-04 | grad 1.62 | tok/s 63708
step    340 | loss 1.9675 | lr 3.00e-04 | grad 3.16 | tok/s 65096
step    350 | loss 1.9857 | lr 3.00e-04 | grad 1.53 | tok/s 63073
step    360 | loss 1.9551 | lr 3.00e-04 | grad 3.75 | tok/s 63997
step    370 | loss 1.7835 | lr 3.00e-04 | grad 1.49 | tok/s 65173
step    380 | loss 2.2335 | lr 3.00e-04 | grad 1.77 | tok/s 66533
step    390 | loss 1.8430 | lr 3.00e-04 | grad 1.45 | tok/s 64115
step    400 | loss 1.9633 | lr 3.00e-04 | grad 3.66 | tok/s 65894
step    410 | loss 1.6973 | lr 3.00e-04 | grad 2.91 | tok/s 64156
step    420 | loss 1.8327 | lr 3.00e-04 | grad 1.85 | tok/s 63796
step    430 | loss 1.9706 | lr 3.00e-04 | grad 1.92 | tok/s 63539
step    440 | loss 2.0297 | lr 3.00e-04 | grad 1.43 | tok/s 66114
step    450 | loss 1.8203 | lr 3.00e-04 | grad 1.52 | tok/s 64249
step    460 | loss 1.8332 | lr 3.00e-04 | grad 1.38 | tok/s 64448
step    470 | loss 1.8052 | lr 3.00e-04 | grad 1.70 | tok/s 65248
step    480 | loss 1.7234 | lr 3.00e-04 | grad 1.51 | tok/s 62984
step    490 | loss 1.6898 | lr 3.00e-04 | grad 1.34 | tok/s 64193
step    500 | loss 2.3953 | lr 3.00e-04 | grad 2.47 | tok/s 66117
step    510 | loss 1.6895 | lr 3.00e-04 | grad 1.37 | tok/s 64694
step    520 | loss 1.6867 | lr 3.00e-04 | grad 1.48 | tok/s 66746
step    530 | loss 2.1806 | lr 3.00e-04 | grad 1.26 | tok/s 65321
step    540 | loss 1.6451 | lr 3.00e-04 | grad 2.09 | tok/s 65255
step    550 | loss 1.5686 | lr 3.00e-04 | grad 1.53 | tok/s 67062
step    560 | loss 1.4807 | lr 3.00e-04 | grad 1.29 | tok/s 67950
step    570 | loss 1.7445 | lr 3.00e-04 | grad 3.34 | tok/s 66378
step    580 | loss 1.9945 | lr 3.00e-04 | grad 1.70 | tok/s 65549
step    590 | loss 2.2154 | lr 3.00e-04 | grad 2.11 | tok/s 64268
step    600 | loss 1.7312 | lr 3.00e-04 | grad 2.45 | tok/s 64427
step    610 | loss 1.7775 | lr 3.00e-04 | grad 2.33 | tok/s 67524
step    620 | loss 1.6787 | lr 3.00e-04 | grad 1.51 | tok/s 64058
step    630 | loss 1.6346 | lr 3.00e-04 | grad 1.56 | tok/s 66106
step    640 | loss 1.9485 | lr 3.00e-04 | grad 1.42 | tok/s 66167
step    650 | loss 1.6752 | lr 3.00e-04 | grad 2.11 | tok/s 64854
step    660 | loss 1.9191 | lr 3.00e-04 | grad 5.97 | tok/s 63984
step    670 | loss 1.8305 | lr 3.00e-04 | grad 3.03 | tok/s 66245
step    680 | loss 1.8090 | lr 3.00e-04 | grad 2.17 | tok/s 63922
step    690 | loss 1.7884 | lr 3.00e-04 | grad 2.33 | tok/s 64443
step    700 | loss 1.8863 | lr 3.00e-04 | grad 2.52 | tok/s 64796
step    710 | loss 1.7925 | lr 3.00e-04 | grad 1.79 | tok/s 65203
step    720 | loss 1.7646 | lr 3.00e-04 | grad 4.38 | tok/s 64858
step    730 | loss 1.8861 | lr 3.00e-04 | grad 2.20 | tok/s 65545
step    740 | loss 1.8269 | lr 3.00e-04 | grad 3.44 | tok/s 64633
step    750 | loss 1.6236 | lr 3.00e-04 | grad 2.20 | tok/s 64168
step    760 | loss 1.9479 | lr 3.00e-04 | grad 1.34 | tok/s 64956
step    770 | loss 1.6734 | lr 3.00e-04 | grad 2.08 | tok/s 64633
step    780 | loss 1.7186 | lr 3.00e-04 | grad 1.45 | tok/s 65317
step    790 | loss 1.6301 | lr 3.00e-04 | grad 1.56 | tok/s 65860
step    800 | loss 1.6091 | lr 3.00e-04 | grad 1.73 | tok/s 65924
step    810 | loss 1.6392 | lr 3.00e-04 | grad 3.67 | tok/s 65026
step    820 | loss 2.3616 | lr 3.00e-04 | grad 2.69 | tok/s 66718
step    830 | loss 2.1219 | lr 3.00e-04 | grad 2.20 | tok/s 67736
step    840 | loss 1.8969 | lr 3.00e-04 | grad 1.61 | tok/s 67752
step    850 | loss 1.8299 | lr 3.00e-04 | grad 1.77 | tok/s 64436
step    860 | loss 1.6231 | lr 3.00e-04 | grad 1.55 | tok/s 63158
step    870 | loss 1.6052 | lr 3.00e-04 | grad 1.62 | tok/s 65058
step    880 | loss 1.7008 | lr 3.00e-04 | grad 2.05 | tok/s 64847
step    890 | loss 1.5909 | lr 3.00e-04 | grad 1.65 | tok/s 64832
step    900 | loss 1.9994 | lr 3.00e-04 | grad 1.84 | tok/s 63279
step    910 | loss 1.6309 | lr 3.00e-04 | grad 1.48 | tok/s 64454
step    920 | loss 1.6497 | lr 3.00e-04 | grad 1.48 | tok/s 64211
step    930 | loss 1.7287 | lr 3.00e-04 | grad 2.47 | tok/s 64069
step    940 | loss 1.6405 | lr 3.00e-04 | grad 2.77 | tok/s 63403
step    950 | loss 1.7221 | lr 3.00e-04 | grad 2.17 | tok/s 64919
step    960 | loss 1.5053 | lr 3.00e-04 | grad 1.38 | tok/s 67964
step    970 | loss 1.4272 | lr 3.00e-04 | grad 1.50 | tok/s 68009
step    980 | loss 1.5442 | lr 3.00e-04 | grad 2.66 | tok/s 66115
step    990 | loss 1.7585 | lr 3.00e-04 | grad 1.42 | tok/s 64299
step   1000 | loss 1.6861 | lr 3.00e-04 | grad 1.31 | tok/s 62601
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6861.pt
step   1010 | loss 1.8759 | lr 3.00e-04 | grad 1.62 | tok/s 49657
step   1020 | loss 1.5497 | lr 3.00e-04 | grad 1.40 | tok/s 64551
step   1030 | loss 1.9305 | lr 3.00e-04 | grad 1.43 | tok/s 63386
step   1040 | loss 1.5972 | lr 3.00e-04 | grad 2.34 | tok/s 64844
step   1050 | loss 1.6271 | lr 3.00e-04 | grad 1.61 | tok/s 65046
step   1060 | loss 1.7887 | lr 3.00e-04 | grad 3.16 | tok/s 65174
step   1070 | loss 1.8890 | lr 3.00e-04 | grad 1.71 | tok/s 65357
step   1080 | loss 2.2017 | lr 3.00e-04 | grad 2.14 | tok/s 64508
step   1090 | loss 1.9456 | lr 3.00e-04 | grad 2.19 | tok/s 64956
step   1100 | loss 1.6654 | lr 3.00e-04 | grad 1.85 | tok/s 64649
step   1110 | loss 1.6236 | lr 3.00e-04 | grad 1.85 | tok/s 65752
step   1120 | loss 1.7572 | lr 3.00e-04 | grad 1.62 | tok/s 66474
step   1130 | loss 1.7008 | lr 3.00e-04 | grad 1.53 | tok/s 63062
step   1140 | loss 1.5626 | lr 3.00e-04 | grad 1.65 | tok/s 64890
step   1150 | loss 1.8607 | lr 3.00e-04 | grad 2.31 | tok/s 64749
step   1160 | loss 1.5387 | lr 3.00e-04 | grad 1.34 | tok/s 64026
step   1170 | loss 1.8340 | lr 3.00e-04 | grad 1.73 | tok/s 64801
step   1180 | loss 1.5960 | lr 3.00e-04 | grad 1.64 | tok/s 67912
step   1190 | loss 1.5133 | lr 3.00e-04 | grad 1.46 | tok/s 68085
step   1200 | loss 1.4432 | lr 3.00e-04 | grad 1.43 | tok/s 68019
step   1210 | loss 1.4273 | lr 3.00e-04 | grad 1.55 | tok/s 67996
step   1220 | loss 1.4376 | lr 3.00e-04 | grad 1.62 | tok/s 67383
step   1230 | loss 1.4790 | lr 3.00e-04 | grad 1.41 | tok/s 65164
step   1240 | loss 1.5960 | lr 3.00e-04 | grad 1.49 | tok/s 63968
step   1250 | loss 1.6974 | lr 3.00e-04 | grad 5.38 | tok/s 65887
step   1260 | loss 1.7418 | lr 3.00e-04 | grad 3.73 | tok/s 65803
step   1270 | loss 1.8108 | lr 3.00e-04 | grad 2.22 | tok/s 64997
step   1280 | loss 1.6578 | lr 3.00e-04 | grad 1.60 | tok/s 64235
step   1290 | loss 1.6226 | lr 3.00e-04 | grad 1.45 | tok/s 63998
step   1300 | loss 1.6820 | lr 3.00e-04 | grad 1.53 | tok/s 63610
step   1310 | loss 1.7238 | lr 3.00e-04 | grad 1.41 | tok/s 63565
step   1320 | loss 1.7214 | lr 3.00e-04 | grad 2.16 | tok/s 64681
step   1330 | loss 1.6076 | lr 3.00e-04 | grad 1.70 | tok/s 64792
step   1340 | loss 1.5442 | lr 3.00e-04 | grad 1.66 | tok/s 64804
step   1350 | loss 1.5769 | lr 3.00e-04 | grad 2.59 | tok/s 66394
step   1360 | loss 1.5444 | lr 3.00e-04 | grad 1.43 | tok/s 63260
step   1370 | loss 1.6451 | lr 3.00e-04 | grad 1.39 | tok/s 64129
step   1380 | loss 1.7223 | lr 3.00e-04 | grad 1.85 | tok/s 64724
step   1390 | loss 1.6307 | lr 3.00e-04 | grad 3.02 | tok/s 63166
step   1400 | loss 1.6033 | lr 3.00e-04 | grad 5.19 | tok/s 65721
step   1410 | loss 1.6067 | lr 3.00e-04 | grad 2.00 | tok/s 66287
step   1420 | loss 1.6533 | lr 3.00e-04 | grad 1.41 | tok/s 63136
step   1430 | loss 1.5361 | lr 3.00e-04 | grad 1.50 | tok/s 61717
step   1440 | loss 1.4524 | lr 3.00e-04 | grad 1.31 | tok/s 65128
step   1450 | loss 1.4886 | lr 3.00e-04 | grad 3.06 | tok/s 66505
step   1460 | loss 1.5720 | lr 3.00e-04 | grad 1.83 | tok/s 62022
step   1470 | loss 1.6803 | lr 3.00e-04 | grad 3.23 | tok/s 64273
step   1480 | loss 1.5697 | lr 3.00e-04 | grad 3.58 | tok/s 64934
step   1490 | loss 1.6908 | lr 3.00e-04 | grad 3.92 | tok/s 64846
step   1500 | loss 1.7996 | lr 3.00e-04 | grad 7.72 | tok/s 63138
step   1510 | loss 1.6692 | lr 3.00e-04 | grad 3.19 | tok/s 66423
step   1520 | loss 1.6491 | lr 3.00e-04 | grad 1.82 | tok/s 65853
step   1530 | loss 1.6033 | lr 3.00e-04 | grad 1.38 | tok/s 65331
step   1540 | loss 1.5721 | lr 3.00e-04 | grad 1.34 | tok/s 64057
step   1550 | loss 1.5544 | lr 3.00e-04 | grad 14.44 | tok/s 66626
step   1560 | loss 2.1230 | lr 3.00e-04 | grad 2.19 | tok/s 64923
step   1570 | loss 1.5890 | lr 3.00e-04 | grad 2.73 | tok/s 63726
step   1580 | loss 1.7236 | lr 3.00e-04 | grad 1.95 | tok/s 65810
step   1590 | loss 1.4958 | lr 3.00e-04 | grad 1.38 | tok/s 64279
step   1600 | loss 1.6079 | lr 3.00e-04 | grad 1.23 | tok/s 63026
step   1610 | loss 1.4459 | lr 3.00e-04 | grad 1.37 | tok/s 66699
step   1620 | loss 1.6170 | lr 3.00e-04 | grad 1.52 | tok/s 65737
step   1630 | loss 1.6271 | lr 3.00e-04 | grad 1.59 | tok/s 66232
step   1640 | loss 1.5438 | lr 3.00e-04 | grad 1.43 | tok/s 63984
step   1650 | loss 1.5937 | lr 3.00e-04 | grad 3.53 | tok/s 63004
step   1660 | loss 1.5936 | lr 3.00e-04 | grad 1.55 | tok/s 63393
step   1670 | loss 1.6076 | lr 3.00e-04 | grad 2.83 | tok/s 66135
step   1680 | loss 1.9672 | lr 3.00e-04 | grad 1.32 | tok/s 66133
step   1690 | loss 1.5547 | lr 3.00e-04 | grad 1.63 | tok/s 64357
step   1700 | loss 1.7318 | lr 3.00e-04 | grad 1.33 | tok/s 65786
step   1710 | loss 1.6563 | lr 3.00e-04 | grad 2.22 | tok/s 64065
step   1720 | loss 1.5977 | lr 3.00e-04 | grad 1.72 | tok/s 64416
step   1730 | loss 1.7550 | lr 3.00e-04 | grad 1.88 | tok/s 64358
step   1740 | loss 1.6097 | lr 3.00e-04 | grad 1.15 | tok/s 65314
step   1750 | loss 1.5668 | lr 3.00e-04 | grad 1.38 | tok/s 63070
step   1760 | loss 1.7661 | lr 3.00e-04 | grad 1.54 | tok/s 63949
step   1770 | loss 1.6870 | lr 3.00e-04 | grad 1.21 | tok/s 65370
step   1780 | loss 1.5807 | lr 3.00e-04 | grad 1.91 | tok/s 62890
step   1790 | loss 1.7735 | lr 3.00e-04 | grad 1.53 | tok/s 64127
step   1800 | loss 1.4899 | lr 3.00e-04 | grad 1.24 | tok/s 65267
step   1810 | loss 1.5957 | lr 3.00e-04 | grad 1.51 | tok/s 64906
step   1820 | loss 1.4808 | lr 3.00e-04 | grad 1.64 | tok/s 64132
step   1830 | loss 1.5620 | lr 3.00e-04 | grad 1.63 | tok/s 63988
step   1840 | loss 1.5316 | lr 3.00e-04 | grad 1.55 | tok/s 63455
step   1850 | loss 1.7674 | lr 3.00e-04 | grad 2.06 | tok/s 63991
step   1860 | loss 1.5356 | lr 3.00e-04 | grad 1.41 | tok/s 63898
step   1870 | loss 1.5460 | lr 3.00e-04 | grad 2.16 | tok/s 65347
step   1880 | loss 1.5299 | lr 3.00e-04 | grad 1.34 | tok/s 65423
step   1890 | loss 1.6602 | lr 3.00e-04 | grad 1.36 | tok/s 64292
step   1900 | loss 1.6172 | lr 3.00e-04 | grad 1.24 | tok/s 64941
step   1910 | loss 1.6870 | lr 3.00e-04 | grad 2.39 | tok/s 64146
step   1920 | loss 1.5101 | lr 3.00e-04 | grad 1.47 | tok/s 66105
step   1930 | loss 1.5286 | lr 3.00e-04 | grad 1.63 | tok/s 65536
step   1940 | loss 1.4885 | lr 3.00e-04 | grad 1.81 | tok/s 66756
step   1950 | loss 1.5724 | lr 3.00e-04 | grad 1.33 | tok/s 65064
step   1960 | loss 1.8595 | lr 3.00e-04 | grad 8.75 | tok/s 66439
step   1970 | loss 1.5174 | lr 3.00e-04 | grad 1.97 | tok/s 64126
step   1980 | loss 1.5845 | lr 3.00e-04 | grad 3.48 | tok/s 64067
step   1990 | loss 1.6724 | lr 3.00e-04 | grad 1.95 | tok/s 65494
step   2000 | loss 1.5704 | lr 3.00e-04 | grad 2.05 | tok/s 66118
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5704.pt
step   2010 | loss 1.3625 | lr 3.00e-04 | grad 1.27 | tok/s 50643
step   2020 | loss 1.2553 | lr 3.00e-04 | grad 1.36 | tok/s 68160
step   2030 | loss 1.5527 | lr 3.00e-04 | grad 1.32 | tok/s 67412
step   2040 | loss 1.3785 | lr 3.00e-04 | grad 1.28 | tok/s 68170
step   2050 | loss 1.3636 | lr 3.00e-04 | grad 1.84 | tok/s 67141
step   2060 | loss 1.6260 | lr 3.00e-04 | grad 1.56 | tok/s 64192
step   2070 | loss 1.5575 | lr 3.00e-04 | grad 1.88 | tok/s 67338
step   2080 | loss 1.6433 | lr 3.00e-04 | grad 3.45 | tok/s 63659
step   2090 | loss 1.5789 | lr 3.00e-04 | grad 1.60 | tok/s 65903
step   2100 | loss 1.5714 | lr 3.00e-04 | grad 1.59 | tok/s 64024
step   2110 | loss 1.3461 | lr 3.00e-04 | grad 1.26 | tok/s 66256
step   2120 | loss 1.4198 | lr 3.00e-04 | grad 1.80 | tok/s 65622
step   2130 | loss 1.5502 | lr 3.00e-04 | grad 4.84 | tok/s 63686
step   2140 | loss 1.6129 | lr 3.00e-04 | grad 3.83 | tok/s 63700
step   2150 | loss 1.6161 | lr 3.00e-04 | grad 1.39 | tok/s 64584
step   2160 | loss 1.6052 | lr 3.00e-04 | grad 1.28 | tok/s 63853
step   2170 | loss 1.7017 | lr 3.00e-04 | grad 1.62 | tok/s 64600
step   2180 | loss 1.5405 | lr 3.00e-04 | grad 1.62 | tok/s 65885
step   2190 | loss 1.7850 | lr 3.00e-04 | grad 1.58 | tok/s 65785
step   2200 | loss 1.3163 | lr 3.00e-04 | grad 1.33 | tok/s 68072
step   2210 | loss 1.3334 | lr 3.00e-04 | grad 1.49 | tok/s 68018
step   2220 | loss 1.3165 | lr 3.00e-04 | grad 1.32 | tok/s 68023
step   2230 | loss 1.5107 | lr 3.00e-04 | grad 1.56 | tok/s 65509
step   2240 | loss 1.6632 | lr 3.00e-04 | grad 3.03 | tok/s 67187
step   2250 | loss 1.6363 | lr 3.00e-04 | grad 1.86 | tok/s 66664
step   2260 | loss 1.6610 | lr 3.00e-04 | grad 1.94 | tok/s 65723
step   2270 | loss 1.5114 | lr 3.00e-04 | grad 2.41 | tok/s 64283
step   2280 | loss 1.6619 | lr 3.00e-04 | grad 1.23 | tok/s 64226
step   2290 | loss 1.5147 | lr 3.00e-04 | grad 1.67 | tok/s 65033
step   2300 | loss 1.5147 | lr 3.00e-04 | grad 1.24 | tok/s 62922
step   2310 | loss 1.5274 | lr 3.00e-04 | grad 1.41 | tok/s 66973
step   2320 | loss 1.5046 | lr 3.00e-04 | grad 1.50 | tok/s 68007
step   2330 | loss 1.4304 | lr 3.00e-04 | grad 1.23 | tok/s 67980
step   2340 | loss 1.5057 | lr 3.00e-04 | grad 1.74 | tok/s 66507
step   2350 | loss 1.6227 | lr 3.00e-04 | grad 1.44 | tok/s 64470
step   2360 | loss 2.0164 | lr 3.00e-04 | grad 2.61 | tok/s 65645
step   2370 | loss 1.5819 | lr 3.00e-04 | grad 1.44 | tok/s 65324
step   2380 | loss 1.4769 | lr 3.00e-04 | grad 1.79 | tok/s 64568
step   2390 | loss 1.4910 | lr 3.00e-04 | grad 1.71 | tok/s 64440
step   2400 | loss 1.6379 | lr 3.00e-04 | grad 1.79 | tok/s 63749
step   2410 | loss 1.5380 | lr 3.00e-04 | grad 1.65 | tok/s 64248
step   2420 | loss 1.7179 | lr 3.00e-04 | grad 1.66 | tok/s 64752
step   2430 | loss 1.5645 | lr 3.00e-04 | grad 2.38 | tok/s 63985
step   2440 | loss 1.6332 | lr 3.00e-04 | grad 1.44 | tok/s 65705
step   2450 | loss 1.4363 | lr 3.00e-04 | grad 2.97 | tok/s 64796

Training complete! Final step: 2458
