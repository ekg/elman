# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 896 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e88_100m_v2/mamba2
# Started: 2026-01-20T14:57:20.094293
============================================================

Using device: cuda
Output directory: benchmark_results/e88_100m_v2/mamba2/levelmamba2_100m_20260120_145726
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4576 | lr 2.70e-05 | grad 5.00 | tok/s 9669
step     20 | loss 4.3084 | lr 5.70e-05 | grad 6.47 | tok/s 65931
step     30 | loss 4.5682 | lr 8.70e-05 | grad 3.27 | tok/s 69382
step     40 | loss 3.1656 | lr 1.17e-04 | grad 1.87 | tok/s 69055
step     50 | loss 2.3871 | lr 1.47e-04 | grad 1.48 | tok/s 68731
step     60 | loss 2.7604 | lr 1.77e-04 | grad 3.28 | tok/s 67085
step     70 | loss 2.4723 | lr 2.07e-04 | grad 1.88 | tok/s 64599
step     80 | loss 2.8039 | lr 2.37e-04 | grad 3.19 | tok/s 66755
step     90 | loss 2.7423 | lr 2.67e-04 | grad 4.06 | tok/s 64115
step    100 | loss 2.2444 | lr 2.97e-04 | grad 1.20 | tok/s 64390
step    110 | loss 2.2792 | lr 6.94e-06 | grad 1.96 | tok/s 61735
step    120 | loss 2.4874 | lr 2.69e-05 | grad 1.52 | tok/s 61644
step    130 | loss 2.3558 | lr 5.89e-05 | grad 1.16 | tok/s 62803
step    140 | loss 2.0981 | lr 9.99e-05 | grad 1.00 | tok/s 62707
step    150 | loss 1.9594 | lr 1.46e-04 | grad 1.89 | tok/s 59578
step    160 | loss 1.8689 | lr 1.92e-04 | grad 1.62 | tok/s 59885
step    170 | loss 2.0566 | lr 2.35e-04 | grad 3.31 | tok/s 61823
step    180 | loss 2.0395 | lr 2.69e-04 | grad 1.27 | tok/s 61830
step    190 | loss 1.7566 | lr 2.91e-04 | grad 1.27 | tok/s 62450
step    200 | loss 1.3555 | lr 3.00e-04 | grad 1.05 | tok/s 63739
step    210 | loss 2.1288 | lr 2.94e-04 | grad 1.72 | tok/s 60834
step    220 | loss 1.9236 | lr 2.74e-04 | grad 1.27 | tok/s 62576
step    230 | loss 1.8166 | lr 2.42e-04 | grad 1.48 | tok/s 60379
step    240 | loss 1.7892 | lr 2.01e-04 | grad 1.49 | tok/s 61405
step    250 | loss 1.7932 | lr 1.55e-04 | grad 1.30 | tok/s 60274
step    260 | loss 1.8799 | lr 1.09e-04 | grad 0.85 | tok/s 57750
step    270 | loss 1.7254 | lr 6.65e-05 | grad 0.96 | tok/s 59919
step    280 | loss 1.6080 | lr 3.24e-05 | grad 1.73 | tok/s 59836
step    290 | loss 1.6132 | lr 9.84e-06 | grad 0.76 | tok/s 62620
step    300 | loss 1.5961 | lr 1.07e-06 | grad 0.86 | tok/s 62600
step    310 | loss 1.5797 | lr 6.94e-06 | grad 0.68 | tok/s 62616
step    320 | loss 1.6809 | lr 2.69e-05 | grad 1.65 | tok/s 60111
step    330 | loss 1.7108 | lr 5.89e-05 | grad 0.79 | tok/s 58693
step    340 | loss 1.7114 | lr 9.99e-05 | grad 2.16 | tok/s 59952
step    350 | loss 1.7272 | lr 1.46e-04 | grad 0.92 | tok/s 58071
step    360 | loss 1.6745 | lr 1.92e-04 | grad 2.33 | tok/s 58751
step    370 | loss 1.5138 | lr 2.35e-04 | grad 1.00 | tok/s 59750
step    380 | loss 1.9891 | lr 2.69e-04 | grad 1.06 | tok/s 61198
step    390 | loss 1.7116 | lr 2.91e-04 | grad 1.46 | tok/s 59020
step    400 | loss 1.8054 | lr 3.00e-04 | grad 2.38 | tok/s 60322
step    410 | loss 1.5792 | lr 2.94e-04 | grad 2.36 | tok/s 58610
step    420 | loss 1.7143 | lr 2.74e-04 | grad 1.22 | tok/s 57845
step    430 | loss 1.8056 | lr 2.42e-04 | grad 1.24 | tok/s 57917
step    440 | loss 1.8222 | lr 2.01e-04 | grad 0.89 | tok/s 60163
step    450 | loss 1.6825 | lr 1.55e-04 | grad 0.75 | tok/s 58438
step    460 | loss 1.6630 | lr 1.09e-04 | grad 0.67 | tok/s 58632
step    470 | loss 1.6174 | lr 6.65e-05 | grad 1.05 | tok/s 59240
step    480 | loss 1.5207 | lr 3.24e-05 | grad 0.61 | tok/s 57183
step    490 | loss 1.5158 | lr 9.84e-06 | grad 0.64 | tok/s 58185
step    500 | loss 2.4340 | lr 1.07e-06 | grad 0.98 | tok/s 59810
step    510 | loss 1.5493 | lr 6.94e-06 | grad 0.71 | tok/s 58801
step    520 | loss 1.6112 | lr 2.69e-05 | grad 0.62 | tok/s 60535
step    530 | loss 2.1025 | lr 5.89e-05 | grad 0.66 | tok/s 59111
step    540 | loss 1.4979 | lr 9.99e-05 | grad 1.06 | tok/s 59276
step    550 | loss 1.4283 | lr 1.46e-04 | grad 0.59 | tok/s 60709
step    560 | loss 1.3019 | lr 1.92e-04 | grad 0.66 | tok/s 61510
step    570 | loss 1.5554 | lr 2.35e-04 | grad 1.95 | tok/s 59817
step    580 | loss 1.8485 | lr 2.69e-04 | grad 0.88 | tok/s 59274
step    590 | loss 2.1071 | lr 2.91e-04 | grad 1.02 | tok/s 58017
step    600 | loss 1.6760 | lr 3.00e-04 | grad 1.38 | tok/s 58164
step    610 | loss 1.6294 | lr 2.94e-04 | grad 1.34 | tok/s 60949
step    620 | loss 1.6225 | lr 2.74e-04 | grad 0.88 | tok/s 57808
step    630 | loss 1.5272 | lr 2.42e-04 | grad 0.85 | tok/s 59721
step    640 | loss 1.7604 | lr 2.01e-04 | grad 0.96 | tok/s 59675
step    650 | loss 1.5474 | lr 1.55e-04 | grad 0.89 | tok/s 40346
step    660 | loss 1.8270 | lr 1.09e-04 | grad 5.84 | tok/s 57940
step    670 | loss 1.6853 | lr 6.65e-05 | grad 1.84 | tok/s 59880
step    680 | loss 1.6232 | lr 3.24e-05 | grad 0.99 | tok/s 57845
step    690 | loss 1.6456 | lr 9.84e-06 | grad 1.46 | tok/s 58222
step    700 | loss 1.7641 | lr 1.07e-06 | grad 1.54 | tok/s 58654
step    710 | loss 1.6804 | lr 6.94e-06 | grad 1.09 | tok/s 58947
step    720 | loss 1.7476 | lr 2.68e-05 | grad 1.57 | tok/s 58680
step    730 | loss 1.7262 | lr 5.89e-05 | grad 1.19 | tok/s 59157
step    740 | loss 1.6579 | lr 9.99e-05 | grad 1.70 | tok/s 40532
step    750 | loss 1.4798 | lr 1.46e-04 | grad 1.24 | tok/s 58254
step    760 | loss 1.7506 | lr 1.92e-04 | grad 0.68 | tok/s 58868
step    770 | loss 1.5312 | lr 2.35e-04 | grad 0.98 | tok/s 58499
step    780 | loss 1.5801 | lr 2.69e-04 | grad 0.81 | tok/s 59076
step    790 | loss 1.4878 | lr 2.91e-04 | grad 0.57 | tok/s 59580
step    800 | loss 1.4821 | lr 3.00e-04 | grad 0.96 | tok/s 59389
step    810 | loss 1.5810 | lr 2.94e-04 | grad 1.71 | tok/s 59034
step    820 | loss 2.3205 | lr 2.74e-04 | grad 1.55 | tok/s 60373
step    830 | loss 1.7813 | lr 2.42e-04 | grad 0.66 | tok/s 61507
step    840 | loss 1.4285 | lr 2.01e-04 | grad 0.63 | tok/s 61438
step    850 | loss 1.8646 | lr 1.55e-04 | grad 1.11 | tok/s 58313
step    860 | loss 1.6266 | lr 1.09e-04 | grad 0.87 | tok/s 57385
step    870 | loss 1.5436 | lr 6.65e-05 | grad 0.89 | tok/s 46659
step    880 | loss 1.6047 | lr 3.24e-05 | grad 1.09 | tok/s 49120
step    890 | loss 1.5493 | lr 9.84e-06 | grad 0.90 | tok/s 58780
step    900 | loss 1.9484 | lr 1.07e-06 | grad 0.85 | tok/s 57323
step    910 | loss 1.5631 | lr 6.94e-06 | grad 0.73 | tok/s 58285
step    920 | loss 1.5792 | lr 2.68e-05 | grad 0.78 | tok/s 58019
step    930 | loss 1.6671 | lr 5.89e-05 | grad 1.51 | tok/s 57964
step    940 | loss 1.5803 | lr 9.99e-05 | grad 1.60 | tok/s 57358
step    950 | loss 1.6207 | lr 1.46e-04 | grad 1.01 | tok/s 58617
step    960 | loss 1.4063 | lr 1.92e-04 | grad 0.63 | tok/s 61410
step    970 | loss 1.2511 | lr 2.35e-04 | grad 0.44 | tok/s 61248
step    980 | loss 1.3984 | lr 2.69e-04 | grad 2.52 | tok/s 59704
step    990 | loss 1.6770 | lr 2.91e-04 | grad 0.73 | tok/s 46995
step   1000 | loss 1.5916 | lr 3.00e-04 | grad 0.62 | tok/s 32645
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5916.pt
step   1010 | loss 1.7150 | lr 2.94e-04 | grad 0.90 | tok/s 33838
step   1020 | loss 1.4156 | lr 2.74e-04 | grad 0.81 | tok/s 45622
step   1030 | loss 1.8828 | lr 2.42e-04 | grad 0.81 | tok/s 57476
step   1040 | loss 1.5027 | lr 2.01e-04 | grad 1.06 | tok/s 58630
step   1050 | loss 1.5180 | lr 1.55e-04 | grad 0.66 | tok/s 59105
step   1060 | loss 1.6257 | lr 1.09e-04 | grad 1.40 | tok/s 58797
step   1070 | loss 1.7679 | lr 6.65e-05 | grad 0.84 | tok/s 59261
step   1080 | loss 2.1851 | lr 3.24e-05 | grad 1.00 | tok/s 58491
step   1090 | loss 1.8880 | lr 9.84e-06 | grad 0.85 | tok/s 58873
step   1100 | loss 1.5557 | lr 1.07e-06 | grad 0.67 | tok/s 58518
step   1110 | loss 1.4981 | lr 6.93e-06 | grad 0.80 | tok/s 59432
step   1120 | loss 1.7256 | lr 2.68e-05 | grad 0.76 | tok/s 60078
step   1130 | loss 1.5762 | lr 5.89e-05 | grad 0.67 | tok/s 57289
step   1140 | loss 1.4462 | lr 9.99e-05 | grad 0.59 | tok/s 58683
step   1150 | loss 1.6893 | lr 1.46e-04 | grad 1.06 | tok/s 58560
step   1160 | loss 1.3982 | lr 1.92e-04 | grad 0.54 | tok/s 57697
step   1170 | loss 1.7284 | lr 2.35e-04 | grad 0.80 | tok/s 58663
step   1180 | loss 1.4566 | lr 2.69e-04 | grad 0.67 | tok/s 61637
step   1190 | loss 1.3057 | lr 2.91e-04 | grad 0.54 | tok/s 61516
step   1200 | loss 1.2302 | lr 3.00e-04 | grad 0.54 | tok/s 61520
step   1210 | loss 1.2083 | lr 2.94e-04 | grad 0.64 | tok/s 61508
step   1220 | loss 1.2529 | lr 2.74e-04 | grad 0.87 | tok/s 60992
step   1230 | loss 1.4489 | lr 2.42e-04 | grad 0.72 | tok/s 58975
step   1240 | loss 1.5076 | lr 2.01e-04 | grad 0.69 | tok/s 57686
step   1250 | loss 1.5951 | lr 1.55e-04 | grad 3.08 | tok/s 59450
step   1260 | loss 1.6300 | lr 1.09e-04 | grad 2.31 | tok/s 59401
step   1270 | loss 1.6914 | lr 6.65e-05 | grad 1.19 | tok/s 58573
step   1280 | loss 1.5393 | lr 3.24e-05 | grad 0.78 | tok/s 57868
step   1290 | loss 1.4940 | lr 9.84e-06 | grad 0.70 | tok/s 57634
step   1300 | loss 1.5401 | lr 1.07e-06 | grad 0.68 | tok/s 57221
step   1310 | loss 1.6144 | lr 6.93e-06 | grad 0.68 | tok/s 57268
step   1320 | loss 1.6019 | lr 2.68e-05 | grad 1.05 | tok/s 58285
step   1330 | loss 1.5100 | lr 5.89e-05 | grad 0.55 | tok/s 58475
step   1340 | loss 1.4305 | lr 9.99e-05 | grad 0.84 | tok/s 58438
step   1350 | loss 1.4395 | lr 1.46e-04 | grad 1.31 | tok/s 59950
step   1360 | loss 1.4269 | lr 1.92e-04 | grad 0.66 | tok/s 56903
step   1370 | loss 1.5156 | lr 2.35e-04 | grad 0.68 | tok/s 57873
step   1380 | loss 1.5842 | lr 2.69e-04 | grad 0.75 | tok/s 58241
step   1390 | loss 1.5182 | lr 2.91e-04 | grad 1.30 | tok/s 56873
step   1400 | loss 1.5299 | lr 3.00e-04 | grad 7.59 | tok/s 59262
step   1410 | loss 1.5204 | lr 2.94e-04 | grad 0.88 | tok/s 59688
step   1420 | loss 1.5771 | lr 2.74e-04 | grad 0.72 | tok/s 56797
step   1430 | loss 1.4213 | lr 2.42e-04 | grad 0.77 | tok/s 55574
step   1440 | loss 1.3541 | lr 2.01e-04 | grad 0.57 | tok/s 58442
step   1450 | loss 1.3517 | lr 1.55e-04 | grad 1.51 | tok/s 59799
step   1460 | loss 1.4713 | lr 1.09e-04 | grad 0.53 | tok/s 55756
step   1470 | loss 1.5565 | lr 6.65e-05 | grad 2.02 | tok/s 57816
step   1480 | loss 1.4314 | lr 3.24e-05 | grad 1.59 | tok/s 58301
step   1490 | loss 1.5769 | lr 9.84e-06 | grad 2.03 | tok/s 58196
step   1500 | loss 1.6791 | lr 1.07e-06 | grad 1.93 | tok/s 46051
step   1510 | loss 1.5598 | lr 6.93e-06 | grad 1.02 | tok/s 31174
step   1520 | loss 1.5065 | lr 2.68e-05 | grad 1.07 | tok/s 31415
step   1530 | loss 1.4819 | lr 5.89e-05 | grad 0.52 | tok/s 31550
step   1540 | loss 1.4488 | lr 9.99e-05 | grad 0.48 | tok/s 30781
step   1550 | loss 1.4397 | lr 1.46e-04 | grad 2.52 | tok/s 32431
step   1560 | loss 1.8872 | lr 1.92e-04 | grad 1.04 | tok/s 29985
step   1570 | loss 1.4602 | lr 2.35e-04 | grad 0.92 | tok/s 29075
step   1580 | loss 1.6013 | lr 2.69e-04 | grad 0.99 | tok/s 30022
step   1590 | loss 1.4090 | lr 2.91e-04 | grad 0.57 | tok/s 29138
step   1600 | loss 1.5538 | lr 3.00e-04 | grad 0.71 | tok/s 28839
step   1610 | loss 1.3563 | lr 2.94e-04 | grad 0.71 | tok/s 30387
step   1620 | loss 1.5287 | lr 2.74e-04 | grad 0.55 | tok/s 29922
step   1630 | loss 1.4836 | lr 2.42e-04 | grad 0.70 | tok/s 30088
step   1640 | loss 1.4425 | lr 2.01e-04 | grad 0.61 | tok/s 29021
step   1650 | loss 1.4603 | lr 1.55e-04 | grad 0.88 | tok/s 28635
step   1660 | loss 1.4602 | lr 1.09e-04 | grad 0.56 | tok/s 28936
step   1670 | loss 1.4849 | lr 6.65e-05 | grad 1.70 | tok/s 30374
step   1680 | loss 2.0155 | lr 3.24e-05 | grad 0.51 | tok/s 30549
step   1690 | loss 1.4440 | lr 9.84e-06 | grad 0.88 | tok/s 29803
step   1700 | loss 1.6815 | lr 1.07e-06 | grad 0.51 | tok/s 30644
step   1710 | loss 1.4854 | lr 6.93e-06 | grad 0.89 | tok/s 29668
step   1720 | loss 1.4488 | lr 2.68e-05 | grad 0.62 | tok/s 29919
step   1730 | loss 1.5744 | lr 5.89e-05 | grad 0.81 | tok/s 29686
step   1740 | loss 1.4858 | lr 9.99e-05 | grad 0.58 | tok/s 30237
step   1750 | loss 1.4167 | lr 1.46e-04 | grad 0.51 | tok/s 29157
step   1760 | loss 1.5900 | lr 1.92e-04 | grad 0.61 | tok/s 29511
step   1770 | loss 1.5551 | lr 2.35e-04 | grad 0.52 | tok/s 30403
step   1780 | loss 1.4492 | lr 2.69e-04 | grad 0.92 | tok/s 29029
step   1790 | loss 1.6046 | lr 2.91e-04 | grad 0.68 | tok/s 29453
step   1800 | loss 1.3994 | lr 3.00e-04 | grad 0.60 | tok/s 30081
step   1810 | loss 1.4937 | lr 2.94e-04 | grad 0.82 | tok/s 30018
step   1820 | loss 1.4070 | lr 2.74e-04 | grad 0.63 | tok/s 29828
step   1830 | loss 1.4499 | lr 2.42e-04 | grad 0.62 | tok/s 29458
step   1840 | loss 1.4571 | lr 2.01e-04 | grad 0.75 | tok/s 29285

Training complete! Final step: 1849
