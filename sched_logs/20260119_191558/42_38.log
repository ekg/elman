# Job 42: 38
# GPU: 7
# Command: python train.py --level 38 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/elman48_20260119_191441/38
# Started: 2026-01-19T20:06:45.160604
============================================================

Using device: cuda
Output directory: benchmark_results/elman48_20260119_191441/38/level38_100m_20260119_200650
Auto r_h_mode: none (level 38 has bounded/no W_h)
Model: Level 38, 65,738,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.3566 | lr 2.70e-05 | grad 7.72 | tok/s 20979
step     20 | loss 4.1556 | lr 5.70e-05 | grad 5.91 | tok/s 52098
step     30 | loss 4.6930 | lr 8.70e-05 | grad 3.55 | tok/s 54679
step     40 | loss 3.6811 | lr 1.17e-04 | grad 2.50 | tok/s 55064
step     50 | loss 2.9070 | lr 1.47e-04 | grad 2.05 | tok/s 52085
step     60 | loss 2.9514 | lr 1.77e-04 | grad 2.27 | tok/s 44245
step     70 | loss 2.5648 | lr 2.07e-04 | grad 1.84 | tok/s 47501
step     80 | loss 2.9265 | lr 2.37e-04 | grad 2.77 | tok/s 39133
step     90 | loss 2.8611 | lr 2.67e-04 | grad 3.55 | tok/s 50427
step    100 | loss 2.4392 | lr 2.97e-04 | grad 1.12 | tok/s 55046
step    110 | loss 2.4176 | lr 6.94e-06 | grad 1.76 | tok/s 51185
step    120 | loss 2.6459 | lr 2.69e-05 | grad 1.14 | tok/s 51757
step    130 | loss 2.4773 | lr 5.89e-05 | grad 1.01 | tok/s 51309
step    140 | loss 2.2659 | lr 9.99e-05 | grad 0.77 | tok/s 52925
step    150 | loss 2.1736 | lr 1.46e-04 | grad 1.85 | tok/s 50355
step    160 | loss 2.0721 | lr 1.92e-04 | grad 1.37 | tok/s 50414
step    170 | loss 2.2540 | lr 2.35e-04 | grad 3.23 | tok/s 52784
step    180 | loss 2.2464 | lr 2.69e-04 | grad 1.31 | tok/s 53121
step    190 | loss 2.0253 | lr 2.91e-04 | grad 1.30 | tok/s 53709
step    200 | loss 1.6680 | lr 3.00e-04 | grad 1.00 | tok/s 54671
step    210 | loss 2.2681 | lr 2.94e-04 | grad 1.54 | tok/s 52817
step    220 | loss 2.1285 | lr 2.74e-04 | grad 1.22 | tok/s 54249
step    230 | loss 1.9949 | lr 2.42e-04 | grad 1.45 | tok/s 52673
step    240 | loss 1.9847 | lr 2.01e-04 | grad 1.55 | tok/s 53138
step    250 | loss 1.9731 | lr 1.55e-04 | grad 1.16 | tok/s 52837
step    260 | loss 2.0425 | lr 1.09e-04 | grad 0.70 | tok/s 50735
step    270 | loss 1.8945 | lr 6.65e-05 | grad 0.90 | tok/s 52272
step    280 | loss 1.7773 | lr 3.24e-05 | grad 1.52 | tok/s 52721
step    290 | loss 1.7818 | lr 9.84e-06 | grad 0.75 | tok/s 54914
step    300 | loss 1.7512 | lr 1.07e-06 | grad 0.75 | tok/s 55373
step    310 | loss 1.7448 | lr 6.94e-06 | grad 0.66 | tok/s 55400
step    320 | loss 1.8382 | lr 2.69e-05 | grad 1.41 | tok/s 52933
step    330 | loss 1.8662 | lr 5.89e-05 | grad 0.70 | tok/s 50998
step    340 | loss 1.8793 | lr 9.99e-05 | grad 1.80 | tok/s 53342
step    350 | loss 1.8833 | lr 1.46e-04 | grad 0.95 | tok/s 51306
step    360 | loss 1.8642 | lr 1.92e-04 | grad 2.09 | tok/s 52101
step    370 | loss 1.7048 | lr 2.35e-04 | grad 1.12 | tok/s 53346
step    380 | loss 2.2177 | lr 2.69e-04 | grad 1.20 | tok/s 54241
step    390 | loss 1.8774 | lr 2.91e-04 | grad 1.38 | tok/s 52564
step    400 | loss 1.9699 | lr 3.00e-04 | grad 2.34 | tok/s 53274
step    410 | loss 1.7461 | lr 2.94e-04 | grad 1.92 | tok/s 52319
step    420 | loss 1.8785 | lr 2.74e-04 | grad 1.12 | tok/s 51845
step    430 | loss 1.9929 | lr 2.42e-04 | grad 1.39 | tok/s 51742
step    440 | loss 2.0257 | lr 2.01e-04 | grad 1.23 | tok/s 53400
step    450 | loss 1.8359 | lr 1.55e-04 | grad 0.68 | tok/s 52427
step    460 | loss 1.7822 | lr 1.09e-04 | grad 0.74 | tok/s 51916
step    470 | loss 1.7652 | lr 6.65e-05 | grad 1.05 | tok/s 52857
step    480 | loss 1.6708 | lr 3.24e-05 | grad 0.57 | tok/s 51242
step    490 | loss 1.6481 | lr 9.84e-06 | grad 0.55 | tok/s 51732
step    500 | loss 2.5715 | lr 1.07e-06 | grad 0.93 | tok/s 54109
step    510 | loss 1.6569 | lr 6.94e-06 | grad 0.62 | tok/s 52891
step    520 | loss 1.7286 | lr 2.69e-05 | grad 0.50 | tok/s 54610
step    530 | loss 2.2384 | lr 5.89e-05 | grad 0.62 | tok/s 53396
step    540 | loss 1.6574 | lr 9.99e-05 | grad 0.91 | tok/s 52906
step    550 | loss 1.5672 | lr 1.46e-04 | grad 0.72 | tok/s 54563
step    560 | loss 1.4341 | lr 1.92e-04 | grad 0.72 | tok/s 55474
step    570 | loss 1.6939 | lr 2.35e-04 | grad 1.77 | tok/s 54211
step    580 | loss 2.0741 | lr 2.69e-04 | grad 1.17 | tok/s 53541
step    590 | loss 2.3468 | lr 2.91e-04 | grad 2.19 | tok/s 52668
step    600 | loss 1.8526 | lr 3.00e-04 | grad 1.45 | tok/s 52681
step    610 | loss 1.7930 | lr 2.94e-04 | grad 1.10 | tok/s 55061
step    620 | loss 1.7575 | lr 2.74e-04 | grad 0.90 | tok/s 51640
step    630 | loss 1.6595 | lr 2.42e-04 | grad 0.78 | tok/s 51925
step    640 | loss 1.9418 | lr 2.01e-04 | grad 1.06 | tok/s 52918
step    650 | loss 1.7195 | lr 1.55e-04 | grad 0.96 | tok/s 51979
step    660 | loss 2.0025 | lr 1.09e-04 | grad 5.66 | tok/s 51047
step    670 | loss 1.8340 | lr 6.65e-05 | grad 1.55 | tok/s 52655
step    680 | loss 1.7804 | lr 3.24e-05 | grad 0.97 | tok/s 50506
step    690 | loss 1.7936 | lr 9.84e-06 | grad 1.23 | tok/s 51394
step    700 | loss 1.8879 | lr 1.07e-06 | grad 1.24 | tok/s 51883
step    710 | loss 1.8011 | lr 6.94e-06 | grad 0.97 | tok/s 52220
step    720 | loss 1.8996 | lr 2.68e-05 | grad 1.38 | tok/s 51978
step    730 | loss 1.8676 | lr 5.89e-05 | grad 1.13 | tok/s 52360
step    740 | loss 1.8124 | lr 9.99e-05 | grad 1.78 | tok/s 51769
step    750 | loss 1.6124 | lr 1.46e-04 | grad 1.26 | tok/s 51431
step    760 | loss 1.9700 | lr 1.92e-04 | grad 0.75 | tok/s 51245
step    770 | loss 1.6954 | lr 2.35e-04 | grad 1.28 | tok/s 51630
step    780 | loss 1.7209 | lr 2.69e-04 | grad 0.92 | tok/s 52204
step    790 | loss 1.6454 | lr 2.91e-04 | grad 0.75 | tok/s 52415
step    800 | loss 1.6112 | lr 3.00e-04 | grad 0.93 | tok/s 52379
step    810 | loss 1.7316 | lr 2.94e-04 | grad 1.56 | tok/s 51848
step    820 | loss 2.4264 | lr 2.74e-04 | grad 1.23 | tok/s 53452
step    830 | loss 1.8872 | lr 2.42e-04 | grad 0.75 | tok/s 53672
step    840 | loss 1.5727 | lr 2.01e-04 | grad 0.59 | tok/s 54440
step    850 | loss 1.9722 | lr 1.55e-04 | grad 1.25 | tok/s 51768
step    860 | loss 1.7332 | lr 1.09e-04 | grad 0.95 | tok/s 50584
step    870 | loss 1.6658 | lr 6.65e-05 | grad 0.85 | tok/s 51856
step    880 | loss 1.7195 | lr 3.24e-05 | grad 1.05 | tok/s 51957
step    890 | loss 1.6475 | lr 9.84e-06 | grad 0.81 | tok/s 51767
step    900 | loss 2.0743 | lr 1.07e-06 | grad 0.87 | tok/s 50523
step    910 | loss 1.6897 | lr 6.94e-06 | grad 0.65 | tok/s 51518
step    920 | loss 1.6820 | lr 2.68e-05 | grad 0.70 | tok/s 51364
step    930 | loss 1.7690 | lr 5.89e-05 | grad 1.41 | tok/s 50702
step    940 | loss 1.6892 | lr 9.99e-05 | grad 1.38 | tok/s 50054
step    950 | loss 1.7679 | lr 1.46e-04 | grad 1.27 | tok/s 51932
step    960 | loss 1.5385 | lr 1.92e-04 | grad 0.60 | tok/s 54014
step    970 | loss 1.3614 | lr 2.35e-04 | grad 0.49 | tok/s 54347
step    980 | loss 1.5144 | lr 2.69e-04 | grad 1.92 | tok/s 52797
step    990 | loss 1.8684 | lr 2.91e-04 | grad 1.01 | tok/s 51224
step   1000 | loss 1.7478 | lr 3.00e-04 | grad 0.73 | tok/s 50138
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7478.pt
step   1010 | loss 1.9304 | lr 2.94e-04 | grad 1.63 | tok/s 44795
step   1020 | loss 1.6103 | lr 2.74e-04 | grad 0.84 | tok/s 50587
step   1030 | loss 1.9974 | lr 2.42e-04 | grad 0.84 | tok/s 50623
step   1040 | loss 1.6347 | lr 2.01e-04 | grad 1.18 | tok/s 51984
step   1050 | loss 1.6418 | lr 1.55e-04 | grad 0.79 | tok/s 52867
step   1060 | loss 1.8110 | lr 1.09e-04 | grad 1.17 | tok/s 52801
step   1070 | loss 1.9371 | lr 6.65e-05 | grad 0.84 | tok/s 52726
step   1080 | loss 2.3527 | lr 3.24e-05 | grad 1.11 | tok/s 52414
step   1090 | loss 2.0251 | lr 9.84e-06 | grad 0.90 | tok/s 51987
step   1100 | loss 1.6929 | lr 1.07e-06 | grad 0.74 | tok/s 52121
step   1110 | loss 1.6603 | lr 6.93e-06 | grad 0.80 | tok/s 52921
step   1120 | loss 1.8706 | lr 2.68e-05 | grad 0.74 | tok/s 54070
step   1130 | loss 1.6891 | lr 5.89e-05 | grad 0.75 | tok/s 51232
step   1140 | loss 1.5563 | lr 9.99e-05 | grad 0.64 | tok/s 52316
step   1150 | loss 1.8361 | lr 1.46e-04 | grad 1.05 | tok/s 52643
step   1160 | loss 1.5249 | lr 1.92e-04 | grad 0.57 | tok/s 51810
step   1170 | loss 1.8419 | lr 2.35e-04 | grad 1.02 | tok/s 52046
step   1180 | loss 1.5723 | lr 2.69e-04 | grad 0.77 | tok/s 55217
step   1190 | loss 1.4239 | lr 2.91e-04 | grad 0.66 | tok/s 55295
step   1200 | loss 1.3372 | lr 3.00e-04 | grad 0.59 | tok/s 55437
step   1210 | loss 1.3064 | lr 2.94e-04 | grad 0.63 | tok/s 54502
step   1220 | loss 1.3552 | lr 2.74e-04 | grad 0.98 | tok/s 54992
step   1230 | loss 1.5848 | lr 2.42e-04 | grad 0.86 | tok/s 52847
step   1240 | loss 1.6601 | lr 2.01e-04 | grad 0.79 | tok/s 52027
step   1250 | loss 1.7246 | lr 1.55e-04 | grad 3.25 | tok/s 53635
step   1260 | loss 1.7911 | lr 1.09e-04 | grad 2.28 | tok/s 53516
step   1270 | loss 1.8254 | lr 6.65e-05 | grad 1.21 | tok/s 52925
step   1280 | loss 1.6579 | lr 3.24e-05 | grad 0.76 | tok/s 52241
step   1290 | loss 1.6141 | lr 9.84e-06 | grad 0.80 | tok/s 51372
step   1300 | loss 1.6696 | lr 1.07e-06 | grad 0.72 | tok/s 51111
step   1310 | loss 1.7527 | lr 6.93e-06 | grad 0.69 | tok/s 51772
step   1320 | loss 1.7164 | lr 2.68e-05 | grad 1.08 | tok/s 52739
step   1330 | loss 1.6383 | lr 5.89e-05 | grad 0.57 | tok/s 52617
step   1340 | loss 1.5552 | lr 9.99e-05 | grad 0.92 | tok/s 52566
step   1350 | loss 1.5911 | lr 1.46e-04 | grad 1.42 | tok/s 53921
step   1360 | loss 1.5647 | lr 1.92e-04 | grad 0.74 | tok/s 51279
step   1370 | loss 1.6644 | lr 2.35e-04 | grad 0.81 | tok/s 52316
step   1380 | loss 1.7566 | lr 2.69e-04 | grad 0.90 | tok/s 52048
step   1390 | loss 1.6717 | lr 2.91e-04 | grad 1.49 | tok/s 51470
step   1400 | loss 1.6766 | lr 3.00e-04 | grad 5.31 | tok/s 53391
step   1410 | loss 1.6827 | lr 2.94e-04 | grad 1.36 | tok/s 53754
step   1420 | loss 1.7377 | lr 2.74e-04 | grad 1.05 | tok/s 51530
step   1430 | loss 1.5575 | lr 2.42e-04 | grad 0.77 | tok/s 50097
step   1440 | loss 1.4693 | lr 2.01e-04 | grad 0.68 | tok/s 52993
step   1450 | loss 1.4939 | lr 1.55e-04 | grad 1.69 | tok/s 53620
step   1460 | loss 1.5710 | lr 1.09e-04 | grad 0.59 | tok/s 50322
step   1470 | loss 1.6813 | lr 6.65e-05 | grad 1.87 | tok/s 52234
step   1480 | loss 1.5541 | lr 3.24e-05 | grad 1.48 | tok/s 52643
step   1490 | loss 1.6865 | lr 9.84e-06 | grad 2.12 | tok/s 52946
step   1500 | loss 1.7951 | lr 1.07e-06 | grad 1.98 | tok/s 51213
step   1510 | loss 1.6833 | lr 6.93e-06 | grad 0.95 | tok/s 54105
step   1520 | loss 1.6284 | lr 2.68e-05 | grad 1.08 | tok/s 53688
step   1530 | loss 1.5943 | lr 5.89e-05 | grad 0.55 | tok/s 53126
step   1540 | loss 1.5684 | lr 9.99e-05 | grad 0.59 | tok/s 52220
step   1550 | loss 1.5404 | lr 1.46e-04 | grad 2.44 | tok/s 53468
step   1560 | loss 2.1533 | lr 1.92e-04 | grad 1.62 | tok/s 52734
step   1570 | loss 1.6117 | lr 2.35e-04 | grad 0.99 | tok/s 51924
step   1580 | loss 1.7642 | lr 2.69e-04 | grad 1.16 | tok/s 53678
step   1590 | loss 1.5800 | lr 2.91e-04 | grad 1.05 | tok/s 52321
step   1600 | loss 1.6860 | lr 3.00e-04 | grad 0.87 | tok/s 51148
step   1610 | loss 1.5143 | lr 2.94e-04 | grad 0.72 | tok/s 54271
step   1620 | loss 1.6730 | lr 2.74e-04 | grad 0.74 | tok/s 52417
step   1630 | loss 1.6597 | lr 2.42e-04 | grad 0.90 | tok/s 53666
step   1640 | loss 1.5951 | lr 2.01e-04 | grad 0.80 | tok/s 52059
step   1650 | loss 1.6022 | lr 1.55e-04 | grad 1.22 | tok/s 51389
step   1660 | loss 1.5990 | lr 1.09e-04 | grad 0.70 | tok/s 51625
step   1670 | loss 1.6373 | lr 6.65e-05 | grad 1.70 | tok/s 53678
step   1680 | loss 2.0778 | lr 3.24e-05 | grad 0.61 | tok/s 53859
step   1690 | loss 1.5586 | lr 9.84e-06 | grad 0.84 | tok/s 51746
step   1700 | loss 1.8770 | lr 1.07e-06 | grad 0.55 | tok/s 53438
step   1710 | loss 1.6150 | lr 6.93e-06 | grad 0.91 | tok/s 52079
step   1720 | loss 1.5968 | lr 2.68e-05 | grad 0.70 | tok/s 52332
step   1730 | loss 1.7139 | lr 5.89e-05 | grad 0.77 | tok/s 52136
step   1740 | loss 1.6152 | lr 9.99e-05 | grad 0.57 | tok/s 53225
step   1750 | loss 1.5312 | lr 1.46e-04 | grad 0.59 | tok/s 51134
step   1760 | loss 1.7824 | lr 1.92e-04 | grad 0.75 | tok/s 51875
step   1770 | loss 1.7003 | lr 2.35e-04 | grad 0.69 | tok/s 52522
step   1780 | loss 1.6117 | lr 2.69e-04 | grad 1.03 | tok/s 50498
step   1790 | loss 1.7917 | lr 2.91e-04 | grad 0.86 | tok/s 52102
step   1800 | loss 1.5313 | lr 3.00e-04 | grad 0.67 | tok/s 52883
step   1810 | loss 1.6236 | lr 2.94e-04 | grad 0.84 | tok/s 52802
step   1820 | loss 1.5479 | lr 2.74e-04 | grad 0.84 | tok/s 51994
step   1830 | loss 1.5984 | lr 2.42e-04 | grad 0.77 | tok/s 51932
step   1840 | loss 1.5982 | lr 2.01e-04 | grad 1.51 | tok/s 51133
step   1850 | loss 1.8096 | lr 1.55e-04 | grad 1.00 | tok/s 52070
step   1860 | loss 1.5519 | lr 1.09e-04 | grad 0.57 | tok/s 52089
step   1870 | loss 1.5911 | lr 6.65e-05 | grad 1.12 | tok/s 53090
step   1880 | loss 1.5492 | lr 3.24e-05 | grad 0.60 | tok/s 53323
step   1890 | loss 1.6408 | lr 9.84e-06 | grad 0.55 | tok/s 52156
step   1900 | loss 1.6762 | lr 1.07e-06 | grad 1.07 | tok/s 53216
step   1910 | loss 1.6589 | lr 6.93e-06 | grad 1.09 | tok/s 51627
step   1920 | loss 1.5543 | lr 2.68e-05 | grad 0.84 | tok/s 53714
step   1930 | loss 1.5290 | lr 5.89e-05 | grad 0.80 | tok/s 53426
step   1940 | loss 1.4749 | lr 9.99e-05 | grad 0.72 | tok/s 53845
step   1950 | loss 1.5314 | lr 1.46e-04 | grad 0.81 | tok/s 53002
step   1960 | loss 1.8889 | lr 1.92e-04 | grad 3.44 | tok/s 53891
step   1970 | loss 1.5760 | lr 2.35e-04 | grad 1.17 | tok/s 51495
step   1980 | loss 1.6522 | lr 2.69e-04 | grad 1.79 | tok/s 52112

Training complete! Final step: 1987
