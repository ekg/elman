# Job 18: 63
# GPU: 3
# Command: python train.py --level 63 --dim 512 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/elman48_20260119_191441/63
# Started: 2026-01-19T19:36:18.151538
============================================================

Using device: cuda
Output directory: benchmark_results/elman48_20260119_191441/63/level63_100m_20260119_193623
Auto r_h_mode: none (level 63 has bounded/no W_h)
Model: Level 63, 84,068,864 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4566 | lr 2.70e-05 | grad 9.81 | tok/s 20464
step     20 | loss 4.4547 | lr 5.70e-05 | grad 7.25 | tok/s 47859
step     30 | loss 4.7046 | lr 8.70e-05 | grad 3.20 | tok/s 50730
step     40 | loss 3.9199 | lr 1.17e-04 | grad 1.60 | tok/s 50268
step     50 | loss 3.5891 | lr 1.47e-04 | grad 2.72 | tok/s 51034
step     60 | loss 3.6557 | lr 1.77e-04 | grad 1.66 | tok/s 50235
step     70 | loss 3.2799 | lr 2.07e-04 | grad 1.10 | tok/s 47934
step     80 | loss 3.5264 | lr 2.37e-04 | grad 1.85 | tok/s 49916
step     90 | loss 3.4594 | lr 2.67e-04 | grad 2.83 | tok/s 48295
step    100 | loss 3.2718 | lr 2.97e-04 | grad 0.79 | tok/s 48632
step    110 | loss 3.2387 | lr 6.94e-06 | grad 1.45 | tok/s 45266
step    120 | loss 3.6097 | lr 2.69e-05 | grad 1.00 | tok/s 44962
step    130 | loss 3.4106 | lr 5.89e-05 | grad 0.95 | tok/s 46263
step    140 | loss 3.2024 | lr 9.99e-05 | grad 0.62 | tok/s 46373
step    150 | loss 3.1010 | lr 1.46e-04 | grad 1.68 | tok/s 44130
step    160 | loss 3.0656 | lr 1.92e-04 | grad 0.94 | tok/s 44528
step    170 | loss 3.2362 | lr 2.35e-04 | grad 2.09 | tok/s 46126
step    180 | loss 3.3452 | lr 2.69e-04 | grad 1.00 | tok/s 46341
step    190 | loss 3.1676 | lr 2.91e-04 | grad 1.16 | tok/s 46896
step    200 | loss 3.1267 | lr 3.00e-04 | grad 3.83 | tok/s 48026
step    210 | loss 3.1717 | lr 2.94e-04 | grad 1.37 | tok/s 45975
step    220 | loss 3.1291 | lr 2.74e-04 | grad 0.80 | tok/s 47716
step    230 | loss 2.8235 | lr 2.42e-04 | grad 1.49 | tok/s 45926
step    240 | loss 2.8050 | lr 2.01e-04 | grad 1.48 | tok/s 46800
step    250 | loss 2.6598 | lr 1.55e-04 | grad 1.30 | tok/s 45979
step    260 | loss 2.6124 | lr 1.09e-04 | grad 0.82 | tok/s 44404
step    270 | loss 2.5254 | lr 6.65e-05 | grad 0.83 | tok/s 46186
step    280 | loss 2.4123 | lr 3.24e-05 | grad 1.70 | tok/s 45805
step    290 | loss 2.4890 | lr 9.84e-06 | grad 0.70 | tok/s 48398
step    300 | loss 2.4789 | lr 1.07e-06 | grad 0.75 | tok/s 48523
step    310 | loss 2.4723 | lr 6.94e-06 | grad 0.59 | tok/s 48468
step    320 | loss 2.4520 | lr 2.69e-05 | grad 1.32 | tok/s 46368
step    330 | loss 2.5212 | lr 5.89e-05 | grad 0.83 | tok/s 45550
step    340 | loss 2.5361 | lr 9.99e-05 | grad 1.27 | tok/s 46597
step    350 | loss 2.5291 | lr 1.46e-04 | grad 1.49 | tok/s 45098
step    360 | loss 2.5120 | lr 1.92e-04 | grad 2.27 | tok/s 45603
step    370 | loss 2.4170 | lr 2.35e-04 | grad 1.69 | tok/s 46308
step    380 | loss 2.7970 | lr 2.69e-04 | grad 2.86 | tok/s 47489
step    390 | loss 2.4149 | lr 2.91e-04 | grad 1.73 | tok/s 47324
step    400 | loss 2.4937 | lr 3.00e-04 | grad 1.70 | tok/s 46990
step    410 | loss 2.2204 | lr 2.94e-04 | grad 2.30 | tok/s 45773
step    420 | loss 2.3267 | lr 2.74e-04 | grad 1.20 | tok/s 45501
step    430 | loss 2.4168 | lr 2.42e-04 | grad 1.31 | tok/s 45257
step    440 | loss 2.5491 | lr 2.01e-04 | grad 1.58 | tok/s 49706
step    450 | loss 2.2416 | lr 1.55e-04 | grad 0.82 | tok/s 50286
step    460 | loss 2.1979 | lr 1.09e-04 | grad 0.80 | tok/s 47442
step    470 | loss 2.2230 | lr 6.65e-05 | grad 1.66 | tok/s 47472
step    480 | loss 2.1222 | lr 3.24e-05 | grad 0.67 | tok/s 47073
step    490 | loss 2.0724 | lr 9.84e-06 | grad 0.55 | tok/s 47509
step    500 | loss 2.9411 | lr 1.07e-06 | grad 0.83 | tok/s 51735
step    510 | loss 2.0577 | lr 6.94e-06 | grad 0.62 | tok/s 50566
step    520 | loss 2.1527 | lr 2.69e-05 | grad 0.54 | tok/s 52158
step    530 | loss 2.5511 | lr 5.89e-05 | grad 0.98 | tok/s 51048
step    540 | loss 2.0776 | lr 9.99e-05 | grad 1.04 | tok/s 50984
step    550 | loss 2.0521 | lr 1.46e-04 | grad 1.09 | tok/s 48099
step    560 | loss 1.9330 | lr 1.92e-04 | grad 1.02 | tok/s 49152
step    570 | loss 2.1521 | lr 2.35e-04 | grad 3.84 | tok/s 48119
step    580 | loss 2.4166 | lr 2.69e-04 | grad 1.07 | tok/s 47232
step    590 | loss 2.6044 | lr 2.91e-04 | grad 2.27 | tok/s 47233
step    600 | loss 2.2168 | lr 3.00e-04 | grad 1.32 | tok/s 46656
step    610 | loss 2.2183 | lr 2.94e-04 | grad 1.41 | tok/s 48085
step    620 | loss 2.1266 | lr 2.74e-04 | grad 0.98 | tok/s 45620
step    630 | loss 2.0751 | lr 2.42e-04 | grad 1.12 | tok/s 47639
step    640 | loss 2.3602 | lr 2.01e-04 | grad 1.41 | tok/s 47550
step    650 | loss 2.0720 | lr 1.55e-04 | grad 1.23 | tok/s 46890
step    660 | loss 2.3295 | lr 1.09e-04 | grad 3.97 | tok/s 46096
step    670 | loss 2.1627 | lr 6.65e-05 | grad 2.25 | tok/s 47735
step    680 | loss 2.0968 | lr 3.24e-05 | grad 0.98 | tok/s 46015
step    690 | loss 2.1249 | lr 9.84e-06 | grad 1.12 | tok/s 46444
step    700 | loss 2.2073 | lr 1.07e-06 | grad 1.07 | tok/s 46716
step    710 | loss 2.1337 | lr 6.94e-06 | grad 0.97 | tok/s 47014
step    720 | loss 2.2393 | lr 2.68e-05 | grad 1.36 | tok/s 46646
step    730 | loss 2.1739 | lr 5.89e-05 | grad 1.20 | tok/s 47244
step    740 | loss 2.1026 | lr 9.99e-05 | grad 1.95 | tok/s 46819
step    750 | loss 1.9462 | lr 1.46e-04 | grad 1.59 | tok/s 46203
step    760 | loss 2.3031 | lr 1.92e-04 | grad 1.09 | tok/s 46744
step    770 | loss 1.9824 | lr 2.35e-04 | grad 1.31 | tok/s 46362
step    780 | loss 2.0314 | lr 2.69e-04 | grad 1.59 | tok/s 47099
step    790 | loss 2.0273 | lr 2.91e-04 | grad 1.18 | tok/s 47324
step    800 | loss 1.9845 | lr 3.00e-04 | grad 1.46 | tok/s 47373
step    810 | loss 1.9864 | lr 2.94e-04 | grad 2.14 | tok/s 46914
step    820 | loss 2.5711 | lr 2.74e-04 | grad 1.32 | tok/s 48241
step    830 | loss 2.1963 | lr 2.42e-04 | grad 0.91 | tok/s 48866
step    840 | loss 1.9293 | lr 2.01e-04 | grad 0.96 | tok/s 48926
step    850 | loss 2.2343 | lr 1.55e-04 | grad 1.41 | tok/s 46555
step    860 | loss 2.0302 | lr 1.09e-04 | grad 1.25 | tok/s 45736
step    870 | loss 1.9831 | lr 6.65e-05 | grad 0.91 | tok/s 47025
step    880 | loss 2.0454 | lr 3.24e-05 | grad 1.23 | tok/s 46703
step    890 | loss 1.9262 | lr 9.84e-06 | grad 0.85 | tok/s 46851
step    900 | loss 2.3438 | lr 1.07e-06 | grad 0.80 | tok/s 45541
step    910 | loss 1.9863 | lr 6.94e-06 | grad 0.77 | tok/s 46507
step    920 | loss 1.9569 | lr 2.68e-05 | grad 0.73 | tok/s 46255
step    930 | loss 2.0775 | lr 5.89e-05 | grad 1.41 | tok/s 45615
step    940 | loss 1.9734 | lr 9.99e-05 | grad 1.34 | tok/s 48880
step    950 | loss 2.0306 | lr 1.46e-04 | grad 1.52 | tok/s 49408
step    960 | loss 1.8339 | lr 1.92e-04 | grad 0.98 | tok/s 52720
step    970 | loss 1.6540 | lr 2.35e-04 | grad 0.94 | tok/s 51086
step    980 | loss 1.7831 | lr 2.69e-04 | grad 2.58 | tok/s 48058
step    990 | loss 2.1499 | lr 2.91e-04 | grad 1.39 | tok/s 46932
step   1000 | loss 1.9829 | lr 3.00e-04 | grad 0.86 | tok/s 45824
  >>> saved checkpoint: checkpoint_step_001000_loss_1.9829.pt
step   1010 | loss 2.2132 | lr 2.94e-04 | grad 1.41 | tok/s 38669
step   1020 | loss 1.8443 | lr 2.74e-04 | grad 1.23 | tok/s 46246
step   1030 | loss 2.1560 | lr 2.42e-04 | grad 1.07 | tok/s 45691
step   1040 | loss 1.8605 | lr 2.01e-04 | grad 1.66 | tok/s 44021
step   1050 | loss 1.8703 | lr 1.55e-04 | grad 0.98 | tok/s 46148
step   1060 | loss 2.1228 | lr 1.09e-04 | grad 1.34 | tok/s 46665
step   1070 | loss 2.2043 | lr 6.65e-05 | grad 1.02 | tok/s 46879
step   1080 | loss 2.4377 | lr 3.24e-05 | grad 1.05 | tok/s 46042
step   1090 | loss 2.1695 | lr 9.84e-06 | grad 1.04 | tok/s 45017
step   1100 | loss 1.8542 | lr 1.07e-06 | grad 0.89 | tok/s 42776
step   1110 | loss 1.9110 | lr 6.93e-06 | grad 0.88 | tok/s 44387
step   1120 | loss 2.1720 | lr 2.68e-05 | grad 0.85 | tok/s 45238
step   1130 | loss 1.8687 | lr 5.89e-05 | grad 0.76 | tok/s 41798
step   1140 | loss 1.7686 | lr 9.99e-05 | grad 0.91 | tok/s 43092
step   1150 | loss 2.0514 | lr 1.46e-04 | grad 1.17 | tok/s 43713
step   1160 | loss 1.7129 | lr 1.92e-04 | grad 0.82 | tok/s 43550
step   1170 | loss 1.9867 | lr 2.35e-04 | grad 1.16 | tok/s 43720
step   1180 | loss 1.7499 | lr 2.69e-04 | grad 0.89 | tok/s 45877
step   1190 | loss 1.6299 | lr 2.91e-04 | grad 0.99 | tok/s 46123
step   1200 | loss 1.5433 | lr 3.00e-04 | grad 0.89 | tok/s 46547
step   1210 | loss 1.4890 | lr 2.94e-04 | grad 0.98 | tok/s 45304
step   1220 | loss 1.5314 | lr 2.74e-04 | grad 1.27 | tok/s 44932
step   1230 | loss 1.7378 | lr 2.42e-04 | grad 1.15 | tok/s 43453
step   1240 | loss 1.8190 | lr 2.01e-04 | grad 1.09 | tok/s 45208
step   1250 | loss 1.9235 | lr 1.55e-04 | grad 3.58 | tok/s 44536
step   1260 | loss 1.9704 | lr 1.09e-04 | grad 2.23 | tok/s 44105
step   1270 | loss 1.9915 | lr 6.65e-05 | grad 1.38 | tok/s 43451
step   1280 | loss 1.8282 | lr 3.24e-05 | grad 1.00 | tok/s 42996
step   1290 | loss 1.7725 | lr 9.84e-06 | grad 1.10 | tok/s 42813
step   1300 | loss 1.8222 | lr 1.07e-06 | grad 0.89 | tok/s 44386
step   1310 | loss 1.9224 | lr 6.93e-06 | grad 0.88 | tok/s 42637
step   1320 | loss 1.8855 | lr 2.68e-05 | grad 1.30 | tok/s 43269
step   1330 | loss 1.8096 | lr 5.89e-05 | grad 0.83 | tok/s 43198
step   1340 | loss 1.7463 | lr 9.99e-05 | grad 1.09 | tok/s 43367
step   1350 | loss 1.8179 | lr 1.46e-04 | grad 1.98 | tok/s 44557
step   1360 | loss 1.7417 | lr 1.92e-04 | grad 1.09 | tok/s 42427
step   1370 | loss 1.8333 | lr 2.35e-04 | grad 1.12 | tok/s 42781
step   1380 | loss 1.9474 | lr 2.69e-04 | grad 1.73 | tok/s 43442
step   1390 | loss 1.8351 | lr 2.91e-04 | grad 1.57 | tok/s 42290
step   1400 | loss 1.8695 | lr 3.00e-04 | grad 4.53 | tok/s 43823
step   1410 | loss 1.9046 | lr 2.94e-04 | grad 1.43 | tok/s 44297
step   1420 | loss 1.9422 | lr 2.74e-04 | grad 1.45 | tok/s 42074
step   1430 | loss 1.7325 | lr 2.42e-04 | grad 1.09 | tok/s 41156
step   1440 | loss 1.6210 | lr 2.01e-04 | grad 0.95 | tok/s 43458
step   1450 | loss 1.6758 | lr 1.55e-04 | grad 2.05 | tok/s 44335
step   1460 | loss 1.7137 | lr 1.09e-04 | grad 0.97 | tok/s 41352
step   1470 | loss 1.8276 | lr 6.65e-05 | grad 1.89 | tok/s 42850
step   1480 | loss 1.7009 | lr 3.24e-05 | grad 1.70 | tok/s 43562
step   1490 | loss 1.8336 | lr 9.84e-06 | grad 2.45 | tok/s 43899
step   1500 | loss 1.9236 | lr 1.07e-06 | grad 2.22 | tok/s 42519
step   1510 | loss 1.8222 | lr 6.93e-06 | grad 1.12 | tok/s 44506
step   1520 | loss 1.7960 | lr 2.68e-05 | grad 1.33 | tok/s 44493
step   1530 | loss 1.7391 | lr 5.89e-05 | grad 0.66 | tok/s 44127
step   1540 | loss 1.7225 | lr 9.99e-05 | grad 0.76 | tok/s 43249
step   1550 | loss 1.7041 | lr 1.46e-04 | grad 2.16 | tok/s 45100
step   1560 | loss 2.2131 | lr 1.92e-04 | grad 1.43 | tok/s 43703
step   1570 | loss 1.7358 | lr 2.35e-04 | grad 1.29 | tok/s 42922
step   1580 | loss 1.9300 | lr 2.69e-04 | grad 1.31 | tok/s 44317
step   1590 | loss 1.7004 | lr 2.91e-04 | grad 1.20 | tok/s 43118
step   1600 | loss 1.7742 | lr 3.00e-04 | grad 1.13 | tok/s 42530
step   1610 | loss 1.6362 | lr 2.94e-04 | grad 1.05 | tok/s 45316
step   1620 | loss 1.7896 | lr 2.74e-04 | grad 1.06 | tok/s 44616
step   1630 | loss 1.8096 | lr 2.42e-04 | grad 1.35 | tok/s 44919
step   1640 | loss 1.6963 | lr 2.01e-04 | grad 0.76 | tok/s 43151
step   1650 | loss 1.7091 | lr 1.55e-04 | grad 1.50 | tok/s 43010
step   1660 | loss 1.7056 | lr 1.09e-04 | grad 0.85 | tok/s 43161
step   1670 | loss 1.7690 | lr 6.65e-05 | grad 2.27 | tok/s 44858
step   1680 | loss 2.2205 | lr 3.24e-05 | grad 0.71 | tok/s 45076
step   1690 | loss 1.6826 | lr 9.84e-06 | grad 1.04 | tok/s 44075
step   1700 | loss 2.0507 | lr 1.07e-06 | grad 0.68 | tok/s 44880
step   1710 | loss 1.7182 | lr 6.93e-06 | grad 0.95 | tok/s 43995
step   1720 | loss 1.7266 | lr 2.68e-05 | grad 0.95 | tok/s 46914
step   1730 | loss 1.8326 | lr 5.89e-05 | grad 0.88 | tok/s 44610
step   1740 | loss 1.7325 | lr 9.99e-05 | grad 0.68 | tok/s 45626

Training complete! Final step: 1740
