# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 896  --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/elman48_20260119_191441/mamba2
# Started: 2026-01-19T19:15:58.747061
============================================================

Using device: cuda
Output directory: benchmark_results/elman48_20260119_191441/mamba2/levelmamba2_100m_20260119_191605
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4576 | lr 2.70e-05 | grad 5.00 | tok/s 9699
step     20 | loss 4.3086 | lr 5.70e-05 | grad 6.47 | tok/s 66286
step     30 | loss 4.5682 | lr 8.70e-05 | grad 3.27 | tok/s 69741
step     40 | loss 3.1654 | lr 1.17e-04 | grad 1.87 | tok/s 69414
step     50 | loss 2.3870 | lr 1.47e-04 | grad 1.48 | tok/s 69186
step     60 | loss 2.7602 | lr 1.77e-04 | grad 3.28 | tok/s 67395
step     70 | loss 2.4722 | lr 2.07e-04 | grad 1.88 | tok/s 64848
step     80 | loss 2.8032 | lr 2.37e-04 | grad 3.17 | tok/s 67067
step     90 | loss 2.7416 | lr 2.67e-04 | grad 4.06 | tok/s 64444
step    100 | loss 2.2440 | lr 2.97e-04 | grad 1.20 | tok/s 64816
step    110 | loss 2.2792 | lr 6.94e-06 | grad 1.96 | tok/s 60857
step    120 | loss 2.4867 | lr 2.69e-05 | grad 1.52 | tok/s 61197
step    130 | loss 2.3557 | lr 5.89e-05 | grad 1.16 | tok/s 62434
step    140 | loss 2.0978 | lr 9.99e-05 | grad 1.00 | tok/s 62285
step    150 | loss 1.9592 | lr 1.46e-04 | grad 1.89 | tok/s 59187
step    160 | loss 1.8689 | lr 1.92e-04 | grad 1.62 | tok/s 59443
step    170 | loss 2.0567 | lr 2.35e-04 | grad 3.31 | tok/s 61561
step    180 | loss 2.0396 | lr 2.69e-04 | grad 1.27 | tok/s 61197
step    190 | loss 1.7565 | lr 2.91e-04 | grad 1.27 | tok/s 61810
step    200 | loss 1.3552 | lr 3.00e-04 | grad 1.05 | tok/s 63085
step    210 | loss 2.1291 | lr 2.94e-04 | grad 1.72 | tok/s 60378
step    220 | loss 1.9242 | lr 2.74e-04 | grad 1.27 | tok/s 61999
step    230 | loss 1.8166 | lr 2.42e-04 | grad 1.48 | tok/s 59217
step    240 | loss 1.7894 | lr 2.01e-04 | grad 1.50 | tok/s 61097
step    250 | loss 1.7929 | lr 1.55e-04 | grad 1.30 | tok/s 59890
step    260 | loss 1.8800 | lr 1.09e-04 | grad 0.84 | tok/s 57713
step    270 | loss 1.7251 | lr 6.65e-05 | grad 0.96 | tok/s 59699
step    280 | loss 1.6084 | lr 3.24e-05 | grad 1.73 | tok/s 59406
step    290 | loss 1.6134 | lr 9.84e-06 | grad 0.76 | tok/s 62333
step    300 | loss 1.5963 | lr 1.07e-06 | grad 0.86 | tok/s 62016
step    310 | loss 1.5800 | lr 6.94e-06 | grad 0.68 | tok/s 61934
step    320 | loss 1.6812 | lr 2.69e-05 | grad 1.66 | tok/s 59496
step    330 | loss 1.7111 | lr 5.89e-05 | grad 0.79 | tok/s 58030
step    340 | loss 1.7114 | lr 9.99e-05 | grad 2.14 | tok/s 59112
step    350 | loss 1.7273 | lr 1.46e-04 | grad 0.92 | tok/s 57430
step    360 | loss 1.6747 | lr 1.92e-04 | grad 2.33 | tok/s 58140
step    370 | loss 1.5135 | lr 2.35e-04 | grad 1.00 | tok/s 59076
step    380 | loss 1.9876 | lr 2.69e-04 | grad 1.06 | tok/s 60572
step    390 | loss 1.7123 | lr 2.91e-04 | grad 1.48 | tok/s 58259
step    400 | loss 1.8045 | lr 3.00e-04 | grad 2.38 | tok/s 59611
step    410 | loss 1.5785 | lr 2.94e-04 | grad 2.39 | tok/s 57688
step    420 | loss 1.7148 | lr 2.74e-04 | grad 1.21 | tok/s 57150
step    430 | loss 1.8066 | lr 2.42e-04 | grad 1.24 | tok/s 56930
step    440 | loss 1.8262 | lr 2.01e-04 | grad 0.89 | tok/s 59193
step    450 | loss 1.6837 | lr 1.55e-04 | grad 0.77 | tok/s 57371
step    460 | loss 1.6699 | lr 1.09e-04 | grad 0.68 | tok/s 57761
step    470 | loss 1.6153 | lr 6.65e-05 | grad 1.03 | tok/s 58354
step    480 | loss 1.5201 | lr 3.24e-05 | grad 0.61 | tok/s 56277
step    490 | loss 1.5155 | lr 9.84e-06 | grad 0.64 | tok/s 57252
step    500 | loss 2.4395 | lr 1.07e-06 | grad 0.98 | tok/s 59077
step    510 | loss 1.5675 | lr 6.94e-06 | grad 0.70 | tok/s 57818
step    520 | loss 1.6068 | lr 2.69e-05 | grad 0.63 | tok/s 59722
step    530 | loss 2.1005 | lr 5.89e-05 | grad 0.66 | tok/s 58230
step    540 | loss 1.4963 | lr 9.99e-05 | grad 1.06 | tok/s 58273
step    550 | loss 1.4280 | lr 1.46e-04 | grad 0.59 | tok/s 59828
step    560 | loss 1.3013 | lr 1.92e-04 | grad 0.66 | tok/s 60591
step    570 | loss 1.5560 | lr 2.35e-04 | grad 1.96 | tok/s 59262
step    580 | loss 1.8468 | lr 2.69e-04 | grad 0.87 | tok/s 58374
step    590 | loss 2.1133 | lr 2.91e-04 | grad 1.02 | tok/s 57126
step    600 | loss 1.6511 | lr 3.00e-04 | grad 1.39 | tok/s 57364
step    610 | loss 1.6273 | lr 2.94e-04 | grad 1.30 | tok/s 60141
step    620 | loss 1.6221 | lr 2.74e-04 | grad 0.89 | tok/s 57019
step    630 | loss 1.5232 | lr 2.42e-04 | grad 0.85 | tok/s 58844
step    640 | loss 1.7578 | lr 2.01e-04 | grad 0.96 | tok/s 58863
step    650 | loss 1.5470 | lr 1.55e-04 | grad 0.89 | tok/s 57649
step    660 | loss 1.8285 | lr 1.09e-04 | grad 5.81 | tok/s 56998
step    670 | loss 1.6871 | lr 6.65e-05 | grad 1.84 | tok/s 58936
step    680 | loss 1.6238 | lr 3.24e-05 | grad 0.98 | tok/s 56911
step    690 | loss 1.6499 | lr 9.84e-06 | grad 1.39 | tok/s 57306
step    700 | loss 1.7740 | lr 1.07e-06 | grad 1.64 | tok/s 57574
step    710 | loss 1.6759 | lr 6.94e-06 | grad 1.09 | tok/s 58024
step    720 | loss 1.7580 | lr 2.68e-05 | grad 1.59 | tok/s 57777
step    730 | loss 1.7259 | lr 5.89e-05 | grad 1.17 | tok/s 58226
step    740 | loss 1.6620 | lr 9.99e-05 | grad 1.68 | tok/s 57728
step    750 | loss 1.4800 | lr 1.46e-04 | grad 1.20 | tok/s 57236
step    760 | loss 1.7594 | lr 1.92e-04 | grad 0.68 | tok/s 57872
step    770 | loss 1.5322 | lr 2.35e-04 | grad 0.97 | tok/s 57441
step    780 | loss 1.5798 | lr 2.69e-04 | grad 0.80 | tok/s 58049
step    790 | loss 1.4870 | lr 2.91e-04 | grad 0.56 | tok/s 58476
step    800 | loss 1.4869 | lr 3.00e-04 | grad 0.97 | tok/s 58660
step    810 | loss 1.5949 | lr 2.94e-04 | grad 1.77 | tok/s 58048
step    820 | loss 2.3117 | lr 2.74e-04 | grad 1.45 | tok/s 59438
step    830 | loss 1.7696 | lr 2.42e-04 | grad 0.68 | tok/s 60505
step    840 | loss 1.4282 | lr 2.01e-04 | grad 0.64 | tok/s 60411
step    850 | loss 1.8444 | lr 1.55e-04 | grad 1.06 | tok/s 57548
step    860 | loss 1.6231 | lr 1.09e-04 | grad 0.86 | tok/s 56359
step    870 | loss 1.5419 | lr 6.65e-05 | grad 0.89 | tok/s 57990
step    880 | loss 1.6014 | lr 3.24e-05 | grad 1.10 | tok/s 57672
step    890 | loss 1.5489 | lr 9.84e-06 | grad 0.92 | tok/s 57694
step    900 | loss 1.9416 | lr 1.07e-06 | grad 0.87 | tok/s 56281
step    910 | loss 1.5636 | lr 6.94e-06 | grad 0.73 | tok/s 57161
step    920 | loss 1.5766 | lr 2.68e-05 | grad 0.80 | tok/s 56976
step    930 | loss 1.6648 | lr 5.89e-05 | grad 1.50 | tok/s 56903
step    940 | loss 1.5777 | lr 9.99e-05 | grad 1.66 | tok/s 56280
step    950 | loss 1.6212 | lr 1.46e-04 | grad 1.01 | tok/s 57636
step    960 | loss 1.4047 | lr 1.92e-04 | grad 0.62 | tok/s 60340
step    970 | loss 1.2516 | lr 2.35e-04 | grad 0.45 | tok/s 60421
step    980 | loss 1.4016 | lr 2.69e-04 | grad 2.98 | tok/s 58713
step    990 | loss 1.6535 | lr 2.91e-04 | grad 0.71 | tok/s 57090
step   1000 | loss 1.5924 | lr 3.00e-04 | grad 0.61 | tok/s 55774
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5924.pt
step   1010 | loss 1.7144 | lr 2.94e-04 | grad 1.03 | tok/s 46275
step   1020 | loss 1.4158 | lr 2.74e-04 | grad 0.79 | tok/s 57444
step   1030 | loss 1.8981 | lr 2.42e-04 | grad 0.85 | tok/s 56371
step   1040 | loss 1.5009 | lr 2.01e-04 | grad 1.06 | tok/s 57615
step   1050 | loss 1.5222 | lr 1.55e-04 | grad 0.67 | tok/s 57657
step   1060 | loss 1.6271 | lr 1.09e-04 | grad 1.37 | tok/s 57854
step   1070 | loss 1.7608 | lr 6.65e-05 | grad 0.84 | tok/s 57751
step   1080 | loss 2.1874 | lr 3.24e-05 | grad 0.98 | tok/s 57505
step   1090 | loss 1.8888 | lr 9.84e-06 | grad 0.84 | tok/s 57528
step   1100 | loss 1.5512 | lr 1.07e-06 | grad 0.65 | tok/s 57315
step   1110 | loss 1.5316 | lr 6.93e-06 | grad 0.81 | tok/s 58252
step   1120 | loss 1.7200 | lr 2.68e-05 | grad 0.78 | tok/s 58967
step   1130 | loss 1.5789 | lr 5.89e-05 | grad 0.67 | tok/s 56096
step   1140 | loss 1.4487 | lr 9.99e-05 | grad 0.58 | tok/s 57451
step   1150 | loss 1.6893 | lr 1.46e-04 | grad 1.07 | tok/s 57435
step   1160 | loss 1.3977 | lr 1.92e-04 | grad 0.57 | tok/s 56985
step   1170 | loss 1.7141 | lr 2.35e-04 | grad 0.80 | tok/s 57457
step   1180 | loss 1.4576 | lr 2.69e-04 | grad 0.68 | tok/s 60279
step   1190 | loss 1.3066 | lr 2.91e-04 | grad 0.55 | tok/s 60193
step   1200 | loss 1.2306 | lr 3.00e-04 | grad 0.55 | tok/s 60321
step   1210 | loss 1.2078 | lr 2.94e-04 | grad 0.62 | tok/s 60028
step   1220 | loss 1.2556 | lr 2.74e-04 | grad 0.82 | tok/s 59791
step   1230 | loss 1.4458 | lr 2.42e-04 | grad 0.72 | tok/s 57993
step   1240 | loss 1.5077 | lr 2.01e-04 | grad 0.69 | tok/s 56814
step   1250 | loss 1.6013 | lr 1.55e-04 | grad 3.02 | tok/s 58471
step   1260 | loss 1.6221 | lr 1.09e-04 | grad 2.27 | tok/s 58384
step   1270 | loss 1.6897 | lr 6.65e-05 | grad 1.21 | tok/s 57058
step   1280 | loss 1.5361 | lr 3.24e-05 | grad 0.77 | tok/s 56940
step   1290 | loss 1.4949 | lr 9.84e-06 | grad 0.76 | tok/s 56597
step   1300 | loss 1.5404 | lr 1.07e-06 | grad 0.69 | tok/s 56290
step   1310 | loss 1.6107 | lr 6.93e-06 | grad 0.67 | tok/s 56289
step   1320 | loss 1.6016 | lr 2.68e-05 | grad 1.05 | tok/s 57047
step   1330 | loss 1.5112 | lr 5.89e-05 | grad 0.55 | tok/s 57366
step   1340 | loss 1.4146 | lr 9.99e-05 | grad 0.84 | tok/s 57608
step   1350 | loss 1.4400 | lr 1.46e-04 | grad 1.34 | tok/s 59075
step   1360 | loss 1.4270 | lr 1.92e-04 | grad 0.66 | tok/s 56123
step   1370 | loss 1.5165 | lr 2.35e-04 | grad 0.66 | tok/s 56937
step   1380 | loss 1.5848 | lr 2.69e-04 | grad 0.76 | tok/s 57327
step   1390 | loss 1.5135 | lr 2.91e-04 | grad 1.31 | tok/s 55981
step   1400 | loss 1.5268 | lr 3.00e-04 | grad 7.47 | tok/s 58138
step   1410 | loss 1.5179 | lr 2.94e-04 | grad 0.85 | tok/s 58748
step   1420 | loss 1.5912 | lr 2.74e-04 | grad 0.71 | tok/s 55846
step   1430 | loss 1.4237 | lr 2.42e-04 | grad 0.96 | tok/s 54612
step   1440 | loss 1.3559 | lr 2.01e-04 | grad 0.58 | tok/s 57660
step   1450 | loss 1.3555 | lr 1.55e-04 | grad 1.60 | tok/s 58882
step   1460 | loss 1.4733 | lr 1.09e-04 | grad 0.54 | tok/s 55082
step   1470 | loss 1.5633 | lr 6.65e-05 | grad 2.08 | tok/s 57047
step   1480 | loss 1.4282 | lr 3.24e-05 | grad 1.53 | tok/s 57514
step   1490 | loss 1.5780 | lr 9.84e-06 | grad 1.97 | tok/s 57665
step   1500 | loss 1.6797 | lr 1.07e-06 | grad 1.93 | tok/s 55708
step   1510 | loss 1.5568 | lr 6.93e-06 | grad 1.01 | tok/s 59011
step   1520 | loss 1.5057 | lr 2.68e-05 | grad 1.11 | tok/s 58464
step   1530 | loss 1.4838 | lr 5.89e-05 | grad 0.53 | tok/s 58072
step   1540 | loss 1.4518 | lr 9.99e-05 | grad 0.48 | tok/s 56780
step   1550 | loss 1.4344 | lr 1.46e-04 | grad 2.64 | tok/s 59072
step   1560 | loss 1.8812 | lr 1.92e-04 | grad 0.98 | tok/s 57359
step   1570 | loss 1.4638 | lr 2.35e-04 | grad 0.95 | tok/s 56550
step   1580 | loss 1.6011 | lr 2.69e-04 | grad 1.05 | tok/s 58472
step   1590 | loss 1.4108 | lr 2.91e-04 | grad 0.59 | tok/s 57054
step   1600 | loss 1.5595 | lr 3.00e-04 | grad 0.71 | tok/s 55998
step   1610 | loss 1.3621 | lr 2.94e-04 | grad 0.73 | tok/s 59217
step   1620 | loss 1.5363 | lr 2.74e-04 | grad 0.55 | tok/s 58163
step   1630 | loss 1.4860 | lr 2.42e-04 | grad 0.74 | tok/s 58655
step   1640 | loss 1.4412 | lr 2.01e-04 | grad 0.59 | tok/s 56669
step   1650 | loss 1.4613 | lr 1.55e-04 | grad 0.87 | tok/s 55876
step   1660 | loss 1.4588 | lr 1.09e-04 | grad 0.56 | tok/s 56013
step   1670 | loss 1.4854 | lr 6.65e-05 | grad 1.69 | tok/s 58601
step   1680 | loss 1.9145 | lr 3.24e-05 | grad 0.54 | tok/s 58552
step   1690 | loss 1.4503 | lr 9.84e-06 | grad 0.86 | tok/s 57270
step   1700 | loss 1.6985 | lr 1.07e-06 | grad 0.50 | tok/s 58556
step   1710 | loss 1.4880 | lr 6.93e-06 | grad 0.89 | tok/s 56909
step   1720 | loss 1.4495 | lr 2.68e-05 | grad 0.62 | tok/s 57103
step   1730 | loss 1.5657 | lr 5.89e-05 | grad 0.85 | tok/s 57171
step   1740 | loss 1.4910 | lr 9.99e-05 | grad 0.58 | tok/s 57949
step   1750 | loss 1.4144 | lr 1.46e-04 | grad 0.51 | tok/s 55930
step   1760 | loss 1.5937 | lr 1.92e-04 | grad 0.63 | tok/s 56631
step   1770 | loss 1.5555 | lr 2.35e-04 | grad 0.51 | tok/s 57805
step   1780 | loss 1.4474 | lr 2.69e-04 | grad 0.93 | tok/s 55710
step   1790 | loss 1.6094 | lr 2.91e-04 | grad 0.68 | tok/s 56804
step   1800 | loss 1.4002 | lr 3.00e-04 | grad 0.61 | tok/s 57802
step   1810 | loss 1.4929 | lr 2.94e-04 | grad 0.80 | tok/s 57283
step   1820 | loss 1.4141 | lr 2.74e-04 | grad 0.59 | tok/s 56908
step   1830 | loss 1.4468 | lr 2.42e-04 | grad 0.62 | tok/s 56605
step   1840 | loss 1.4554 | lr 2.01e-04 | grad 0.75 | tok/s 56243
step   1850 | loss 1.6539 | lr 1.55e-04 | grad 0.91 | tok/s 56748
step   1860 | loss 1.4194 | lr 1.09e-04 | grad 0.51 | tok/s 56576
step   1870 | loss 1.4499 | lr 6.65e-05 | grad 1.00 | tok/s 58076
step   1880 | loss 1.4318 | lr 3.24e-05 | grad 0.55 | tok/s 58041
step   1890 | loss 1.5036 | lr 9.84e-06 | grad 0.51 | tok/s 57082
step   1900 | loss 1.4762 | lr 1.07e-06 | grad 0.68 | tok/s 57669
step   1910 | loss 1.5466 | lr 6.93e-06 | grad 1.51 | tok/s 56868
step   1920 | loss 1.4280 | lr 2.68e-05 | grad 0.79 | tok/s 58700
step   1930 | loss 1.4086 | lr 5.89e-05 | grad 0.86 | tok/s 58262
step   1940 | loss 1.3887 | lr 9.99e-05 | grad 0.59 | tok/s 59381
step   1950 | loss 1.4264 | lr 1.46e-04 | grad 0.71 | tok/s 57548
step   1960 | loss 1.7277 | lr 1.92e-04 | grad 3.47 | tok/s 58938
step   1970 | loss 1.3798 | lr 2.35e-04 | grad 1.02 | tok/s 57038
step   1980 | loss 1.4953 | lr 2.69e-04 | grad 1.99 | tok/s 56879
step   1990 | loss 1.5352 | lr 2.91e-04 | grad 0.90 | tok/s 58180
step   2000 | loss 1.4448 | lr 3.00e-04 | grad 0.87 | tok/s 58620
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4448.pt
step   2010 | loss 1.2784 | lr 2.94e-04 | grad 0.57 | tok/s 47346
step   2020 | loss 1.1558 | lr 2.74e-04 | grad 0.48 | tok/s 60249
step   2030 | loss 1.4101 | lr 2.42e-04 | grad 0.71 | tok/s 59839
step   2040 | loss 1.2958 | lr 2.01e-04 | grad 0.48 | tok/s 60559
step   2050 | loss 1.2388 | lr 1.55e-04 | grad 0.75 | tok/s 59571
step   2060 | loss 1.5597 | lr 1.09e-04 | grad 0.59 | tok/s 56938
step   2070 | loss 1.5191 | lr 6.65e-05 | grad 1.00 | tok/s 59823
step   2080 | loss 1.5496 | lr 3.24e-05 | grad 2.62 | tok/s 56591
step   2090 | loss 1.5319 | lr 9.84e-06 | grad 0.75 | tok/s 58434
step   2100 | loss 1.4786 | lr 1.07e-06 | grad 0.83 | tok/s 56802
step   2110 | loss 1.3615 | lr 6.93e-06 | grad 0.57 | tok/s 58800
step   2120 | loss 1.3426 | lr 2.68e-05 | grad 1.06 | tok/s 58214
step   2130 | loss 1.4649 | lr 5.89e-05 | grad 1.34 | tok/s 56492
step   2140 | loss 1.5181 | lr 9.99e-05 | grad 2.50 | tok/s 56429
step   2150 | loss 1.5072 | lr 1.46e-04 | grad 0.47 | tok/s 57225
step   2160 | loss 1.4790 | lr 1.92e-04 | grad 0.53 | tok/s 56585
step   2170 | loss 1.5498 | lr 2.35e-04 | grad 0.66 | tok/s 57282

Training complete! Final step: 2179
