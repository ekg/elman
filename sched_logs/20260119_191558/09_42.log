# Job 9: 42
# GPU: 1
# Command: python train.py --level 42 --dim 768 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/elman48_20260119_191441/42
# Started: 2026-01-19T19:26:08.509468
============================================================

Using device: cuda
Output directory: benchmark_results/elman48_20260119_191441/42/level42_100m_20260119_192613
Auto r_h_mode: spectral_norm (level 42 has full W_h)
Model: Level 42, 94,615,296 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.1966 | lr 2.70e-05 | grad 18.12 | tok/s 15647
step     20 | loss 3.3847 | lr 5.70e-05 | grad 25.62 | tok/s 43345
step     30 | loss 3.9630 | lr 8.70e-05 | grad 26.12 | tok/s 47318
step     40 | loss 2.8882 | lr 1.17e-04 | grad 6.28 | tok/s 47419
step     50 | loss 2.4705 | lr 1.47e-04 | grad 10.06 | tok/s 47570
step     60 | loss 2.7954 | lr 1.77e-04 | grad 7.88 | tok/s 46688
step     70 | loss 2.4643 | lr 2.07e-04 | grad 2.78 | tok/s 45412
step     80 | loss 2.8608 | lr 2.37e-04 | grad 3.30 | tok/s 47152
step     90 | loss 2.8260 | lr 2.67e-04 | grad 4.44 | tok/s 45948
step    100 | loss 2.4034 | lr 2.97e-04 | grad 1.55 | tok/s 45353
step    110 | loss 2.4012 | lr 6.94e-06 | grad 1.94 | tok/s 43440
step    120 | loss 2.7539 | lr 2.69e-05 | grad 1.29 | tok/s 42167
step    130 | loss 2.4070 | lr 5.89e-05 | grad 1.33 | tok/s 40328
step    140 | loss 2.2154 | lr 9.99e-05 | grad 1.34 | tok/s 40396
step    150 | loss 2.1196 | lr 1.46e-04 | grad 2.05 | tok/s 38550
step    160 | loss 2.0353 | lr 1.92e-04 | grad 1.59 | tok/s 39120
step    170 | loss 2.2432 | lr 2.35e-04 | grad 4.53 | tok/s 40434
step    180 | loss 2.2536 | lr 2.69e-04 | grad 1.54 | tok/s 40224
step    190 | loss 1.9787 | lr 2.91e-04 | grad 1.48 | tok/s 41296
step    200 | loss 1.6031 | lr 3.00e-04 | grad 1.20 | tok/s 42065
step    210 | loss 2.2783 | lr 2.94e-04 | grad 1.75 | tok/s 40084
step    220 | loss 2.1152 | lr 2.74e-04 | grad 1.09 | tok/s 41625
step    230 | loss 1.9864 | lr 2.42e-04 | grad 1.47 | tok/s 39903
step    240 | loss 1.9711 | lr 2.01e-04 | grad 1.64 | tok/s 40950
step    250 | loss 1.9610 | lr 1.55e-04 | grad 1.21 | tok/s 40359
step    260 | loss 2.0420 | lr 1.09e-04 | grad 0.75 | tok/s 38894
step    270 | loss 1.8843 | lr 6.65e-05 | grad 0.98 | tok/s 40141
step    280 | loss 1.7617 | lr 3.24e-05 | grad 1.54 | tok/s 40283
step    290 | loss 1.7473 | lr 9.84e-06 | grad 0.77 | tok/s 42530
step    300 | loss 1.7124 | lr 1.07e-06 | grad 0.81 | tok/s 42274
step    310 | loss 1.7052 | lr 6.94e-06 | grad 0.68 | tok/s 43035
step    320 | loss 1.8133 | lr 2.69e-05 | grad 1.51 | tok/s 41720
step    330 | loss 1.8463 | lr 5.89e-05 | grad 0.80 | tok/s 40713
step    340 | loss 1.8548 | lr 9.99e-05 | grad 2.12 | tok/s 41300
step    350 | loss 1.8671 | lr 1.46e-04 | grad 1.04 | tok/s 40414
step    360 | loss 1.8482 | lr 1.92e-04 | grad 2.47 | tok/s 40851
step    370 | loss 1.6850 | lr 2.35e-04 | grad 1.14 | tok/s 41257
step    380 | loss 2.2026 | lr 2.69e-04 | grad 1.05 | tok/s 42356
step    390 | loss 1.8515 | lr 2.91e-04 | grad 1.41 | tok/s 40926
step    400 | loss 1.9766 | lr 3.00e-04 | grad 2.58 | tok/s 42376
step    410 | loss 1.7361 | lr 2.94e-04 | grad 1.87 | tok/s 40942
step    420 | loss 1.8655 | lr 2.74e-04 | grad 1.16 | tok/s 40803
step    430 | loss 2.0152 | lr 2.42e-04 | grad 1.21 | tok/s 40817
step    440 | loss 2.0241 | lr 2.01e-04 | grad 0.99 | tok/s 42262
step    450 | loss 1.8123 | lr 1.55e-04 | grad 0.64 | tok/s 41002
step    460 | loss 1.7778 | lr 1.09e-04 | grad 0.68 | tok/s 41056
step    470 | loss 1.7568 | lr 6.65e-05 | grad 1.07 | tok/s 41752
step    480 | loss 1.6557 | lr 3.24e-05 | grad 0.60 | tok/s 40483
step    490 | loss 1.6293 | lr 9.84e-06 | grad 0.54 | tok/s 40985
step    500 | loss 2.5761 | lr 1.07e-06 | grad 0.92 | tok/s 42095
step    510 | loss 1.6496 | lr 6.94e-06 | grad 0.63 | tok/s 41520
step    520 | loss 1.7162 | lr 2.69e-05 | grad 0.53 | tok/s 42529
step    530 | loss 2.2162 | lr 5.89e-05 | grad 0.69 | tok/s 41327
step    540 | loss 1.6428 | lr 9.99e-05 | grad 1.09 | tok/s 40581
step    550 | loss 1.5484 | lr 1.46e-04 | grad 0.71 | tok/s 43849
step    560 | loss 1.4132 | lr 1.92e-04 | grad 0.71 | tok/s 44785
step    570 | loss 1.7016 | lr 2.35e-04 | grad 1.84 | tok/s 43683
step    580 | loss 2.0263 | lr 2.69e-04 | grad 0.98 | tok/s 43301
step    590 | loss 2.3215 | lr 2.91e-04 | grad 1.55 | tok/s 42114
step    600 | loss 1.8276 | lr 3.00e-04 | grad 1.29 | tok/s 42595
step    610 | loss 1.8007 | lr 2.94e-04 | grad 1.26 | tok/s 44676
step    620 | loss 1.7577 | lr 2.74e-04 | grad 0.81 | tok/s 42125
step    630 | loss 1.6476 | lr 2.42e-04 | grad 0.79 | tok/s 43514
step    640 | loss 1.9752 | lr 2.01e-04 | grad 0.83 | tok/s 43517
step    650 | loss 1.7040 | lr 1.55e-04 | grad 1.01 | tok/s 42818
step    660 | loss 2.0001 | lr 1.09e-04 | grad 4.75 | tok/s 42003
step    670 | loss 1.8354 | lr 6.65e-05 | grad 1.49 | tok/s 44275
step    680 | loss 1.7752 | lr 3.24e-05 | grad 0.95 | tok/s 41990
step    690 | loss 1.7773 | lr 9.84e-06 | grad 1.16 | tok/s 42361
step    700 | loss 1.8659 | lr 1.07e-06 | grad 1.12 | tok/s 43727
step    710 | loss 1.8017 | lr 6.94e-06 | grad 0.96 | tok/s 44252
step    720 | loss 1.8939 | lr 2.68e-05 | grad 1.26 | tok/s 44216
step    730 | loss 1.8587 | lr 5.89e-05 | grad 1.10 | tok/s 44245
step    740 | loss 1.7997 | lr 9.99e-05 | grad 1.66 | tok/s 44260
step    750 | loss 1.6068 | lr 1.46e-04 | grad 1.24 | tok/s 43702
step    760 | loss 1.9566 | lr 1.92e-04 | grad 0.66 | tok/s 44542
step    770 | loss 1.6749 | lr 2.35e-04 | grad 1.00 | tok/s 44033
step    780 | loss 1.7236 | lr 2.69e-04 | grad 0.87 | tok/s 43859
step    790 | loss 1.6513 | lr 2.91e-04 | grad 0.78 | tok/s 43325
step    800 | loss 1.6277 | lr 3.00e-04 | grad 0.90 | tok/s 42153
step    810 | loss 1.7549 | lr 2.94e-04 | grad 1.56 | tok/s 41606
step    820 | loss 2.4398 | lr 2.74e-04 | grad 1.63 | tok/s 43087
step    830 | loss 1.9708 | lr 2.42e-04 | grad 0.75 | tok/s 43432
step    840 | loss 1.6513 | lr 2.01e-04 | grad 0.54 | tok/s 43355
step    850 | loss 2.0469 | lr 1.55e-04 | grad 1.10 | tok/s 41423
step    860 | loss 1.7656 | lr 1.09e-04 | grad 0.88 | tok/s 40660
step    870 | loss 1.6790 | lr 6.65e-05 | grad 0.78 | tok/s 41905
step    880 | loss 1.7301 | lr 3.24e-05 | grad 0.99 | tok/s 41763
step    890 | loss 1.6524 | lr 9.84e-06 | grad 0.75 | tok/s 41566
step    900 | loss 2.1081 | lr 1.07e-06 | grad 0.78 | tok/s 40287
step    910 | loss 1.7006 | lr 6.94e-06 | grad 0.59 | tok/s 41026
step    920 | loss 1.6882 | lr 2.68e-05 | grad 0.63 | tok/s 40741
step    930 | loss 1.7780 | lr 5.89e-05 | grad 1.26 | tok/s 40931
step    940 | loss 1.6910 | lr 9.99e-05 | grad 1.32 | tok/s 40499
step    950 | loss 1.7636 | lr 1.46e-04 | grad 1.14 | tok/s 41471
step    960 | loss 1.5287 | lr 1.92e-04 | grad 0.55 | tok/s 43560
step    970 | loss 1.3613 | lr 2.35e-04 | grad 0.47 | tok/s 43617
step    980 | loss 1.5242 | lr 2.69e-04 | grad 1.55 | tok/s 42166
step    990 | loss 1.8615 | lr 2.91e-04 | grad 0.66 | tok/s 41095
step   1000 | loss 1.7291 | lr 3.00e-04 | grad 0.57 | tok/s 40666
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7291.pt
step   1010 | loss 1.9445 | lr 2.94e-04 | grad 1.43 | tok/s 35176
step   1020 | loss 1.6064 | lr 2.74e-04 | grad 0.78 | tok/s 41418
step   1030 | loss 2.0447 | lr 2.42e-04 | grad 0.76 | tok/s 40672
step   1040 | loss 1.6452 | lr 2.01e-04 | grad 1.16 | tok/s 41911
step   1050 | loss 1.6539 | lr 1.55e-04 | grad 0.72 | tok/s 42813
step   1060 | loss 1.8352 | lr 1.09e-04 | grad 1.21 | tok/s 42610
step   1070 | loss 1.9416 | lr 6.65e-05 | grad 0.74 | tok/s 42122
step   1080 | loss 2.3466 | lr 3.24e-05 | grad 0.94 | tok/s 41449
step   1090 | loss 2.0293 | lr 9.84e-06 | grad 0.81 | tok/s 41733
step   1100 | loss 1.7128 | lr 1.07e-06 | grad 0.72 | tok/s 41681
step   1110 | loss 1.6901 | lr 6.93e-06 | grad 0.70 | tok/s 42396
step   1120 | loss 1.8753 | lr 2.68e-05 | grad 0.72 | tok/s 43004
step   1130 | loss 1.6927 | lr 5.89e-05 | grad 0.61 | tok/s 40710
step   1140 | loss 1.5593 | lr 9.99e-05 | grad 0.63 | tok/s 41854
step   1150 | loss 1.8469 | lr 1.46e-04 | grad 1.00 | tok/s 41686
step   1160 | loss 1.5361 | lr 1.92e-04 | grad 0.62 | tok/s 41349
step   1170 | loss 1.8369 | lr 2.35e-04 | grad 0.70 | tok/s 42344
step   1180 | loss 1.5799 | lr 2.69e-04 | grad 0.66 | tok/s 44336
step   1190 | loss 1.4285 | lr 2.91e-04 | grad 0.58 | tok/s 43163
step   1200 | loss 1.3466 | lr 3.00e-04 | grad 0.57 | tok/s 43805
step   1210 | loss 1.3188 | lr 2.94e-04 | grad 0.59 | tok/s 43189
step   1220 | loss 1.3635 | lr 2.74e-04 | grad 0.79 | tok/s 43188
step   1230 | loss 1.5811 | lr 2.42e-04 | grad 0.79 | tok/s 42415
step   1240 | loss 1.6558 | lr 2.01e-04 | grad 0.75 | tok/s 42959
step   1250 | loss 1.7372 | lr 1.55e-04 | grad 2.45 | tok/s 44436
step   1260 | loss 1.7997 | lr 1.09e-04 | grad 2.20 | tok/s 44134
step   1270 | loss 1.8469 | lr 6.65e-05 | grad 1.06 | tok/s 43798
step   1280 | loss 1.6743 | lr 3.24e-05 | grad 0.70 | tok/s 43150
step   1290 | loss 1.6211 | lr 9.84e-06 | grad 0.74 | tok/s 43099
step   1300 | loss 1.6731 | lr 1.07e-06 | grad 0.65 | tok/s 42908
step   1310 | loss 1.7601 | lr 6.93e-06 | grad 0.64 | tok/s 43130
step   1320 | loss 1.7250 | lr 2.68e-05 | grad 0.98 | tok/s 43573
step   1330 | loss 1.6496 | lr 5.89e-05 | grad 0.60 | tok/s 43717
step   1340 | loss 1.5593 | lr 9.99e-05 | grad 0.84 | tok/s 43718
step   1350 | loss 1.6024 | lr 1.46e-04 | grad 1.52 | tok/s 45150
step   1360 | loss 1.5637 | lr 1.92e-04 | grad 0.62 | tok/s 42653
step   1370 | loss 1.6683 | lr 2.35e-04 | grad 0.72 | tok/s 43591
step   1380 | loss 1.7516 | lr 2.69e-04 | grad 0.84 | tok/s 43367
step   1390 | loss 1.6778 | lr 2.91e-04 | grad 1.96 | tok/s 42597
step   1400 | loss 1.6718 | lr 3.00e-04 | grad 2.75 | tok/s 44279
step   1410 | loss 1.7019 | lr 2.94e-04 | grad 1.27 | tok/s 44948
step   1420 | loss 1.7678 | lr 2.74e-04 | grad 0.80 | tok/s 43066
step   1430 | loss 1.5618 | lr 2.42e-04 | grad 0.76 | tok/s 41786
step   1440 | loss 1.4810 | lr 2.01e-04 | grad 0.61 | tok/s 43919
step   1450 | loss 1.5141 | lr 1.55e-04 | grad 1.66 | tok/s 44709
step   1460 | loss 1.5906 | lr 1.09e-04 | grad 0.48 | tok/s 41606
step   1470 | loss 1.6987 | lr 6.65e-05 | grad 1.65 | tok/s 43415
step   1480 | loss 1.5762 | lr 3.24e-05 | grad 1.51 | tok/s 43687
step   1490 | loss 1.7031 | lr 9.84e-06 | grad 1.95 | tok/s 43679
step   1500 | loss 1.8107 | lr 1.07e-06 | grad 1.57 | tok/s 42540
step   1510 | loss 1.7023 | lr 6.93e-06 | grad 0.90 | tok/s 44736
step   1520 | loss 1.6447 | lr 2.68e-05 | grad 1.00 | tok/s 44271
step   1530 | loss 1.5999 | lr 5.89e-05 | grad 0.51 | tok/s 43808
step   1540 | loss 1.5837 | lr 9.99e-05 | grad 0.47 | tok/s 43207
step   1550 | loss 1.5586 | lr 1.46e-04 | grad 2.08 | tok/s 44886
step   1560 | loss 2.1675 | lr 1.92e-04 | grad 1.23 | tok/s 43638
step   1570 | loss 1.6016 | lr 2.35e-04 | grad 0.92 | tok/s 42836
step   1580 | loss 1.7799 | lr 2.69e-04 | grad 1.00 | tok/s 44274
step   1590 | loss 1.5826 | lr 2.91e-04 | grad 0.70 | tok/s 43225
step   1600 | loss 1.6829 | lr 3.00e-04 | grad 0.84 | tok/s 42339

Training complete! Final step: 1606
