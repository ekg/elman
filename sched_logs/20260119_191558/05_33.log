# Job 5: 33
# GPU: 5
# Command: python train.py --level 33 --dim 640 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/elman48_20260119_191441/33
# Started: 2026-01-19T19:15:58.747804
============================================================

Using device: cuda
Output directory: benchmark_results/elman48_20260119_191441/33/level33_100m_20260119_191605
Auto r_h_mode: spectral_norm (level 33 has full W_h)
Model: Level 33, 98,506,880 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 4.8169 | lr 2.70e-05 | grad 4.94 | tok/s 20967
step     20 | loss 3.6755 | lr 5.70e-05 | grad 8.62 | tok/s 52374
step     30 | loss 4.4034 | lr 8.70e-05 | grad 4.41 | tok/s 55321
step     40 | loss 3.2372 | lr 1.17e-04 | grad 3.03 | tok/s 55318
step     50 | loss 2.5787 | lr 1.47e-04 | grad 3.06 | tok/s 55037
step     60 | loss 2.7955 | lr 1.77e-04 | grad 3.80 | tok/s 54077
step     70 | loss 2.5306 | lr 2.07e-04 | grad 2.48 | tok/s 52311
step     80 | loss 2.6377 | lr 2.37e-04 | grad 2.98 | tok/s 49320
step     90 | loss 2.7993 | lr 2.67e-04 | grad 3.59 | tok/s 42285
step    100 | loss 2.3896 | lr 2.97e-04 | grad 1.66 | tok/s 42167
step    110 | loss 2.3987 | lr 6.94e-06 | grad 1.98 | tok/s 39052
step    120 | loss 2.6437 | lr 2.69e-05 | grad 1.18 | tok/s 42778
step    130 | loss 2.4333 | lr 5.89e-05 | grad 1.32 | tok/s 47602
step    140 | loss 2.2112 | lr 9.99e-05 | grad 1.06 | tok/s 48580
step    150 | loss 2.1059 | lr 1.46e-04 | grad 2.17 | tok/s 46400
step    160 | loss 2.0024 | lr 1.92e-04 | grad 1.62 | tok/s 48981
step    170 | loss 2.1980 | lr 2.35e-04 | grad 3.81 | tok/s 50809
step    180 | loss 2.1835 | lr 2.69e-04 | grad 1.50 | tok/s 51081
step    190 | loss 1.9342 | lr 2.91e-04 | grad 1.53 | tok/s 52213
step    200 | loss 1.5179 | lr 3.00e-04 | grad 1.22 | tok/s 54520
step    210 | loss 2.2516 | lr 2.94e-04 | grad 1.88 | tok/s 52241
step    220 | loss 2.0804 | lr 2.74e-04 | grad 1.44 | tok/s 53803
step    230 | loss 1.9499 | lr 2.42e-04 | grad 1.81 | tok/s 51546
step    240 | loss 1.9251 | lr 2.01e-04 | grad 1.73 | tok/s 52368
step    250 | loss 1.9212 | lr 1.55e-04 | grad 1.27 | tok/s 50729
step    260 | loss 1.9932 | lr 1.09e-04 | grad 0.88 | tok/s 49329
step    270 | loss 1.8493 | lr 6.65e-05 | grad 1.02 | tok/s 51128
step    280 | loss 1.7281 | lr 3.24e-05 | grad 1.55 | tok/s 51068
step    290 | loss 1.7118 | lr 9.84e-06 | grad 0.86 | tok/s 53861
step    300 | loss 1.6776 | lr 1.07e-06 | grad 0.87 | tok/s 53800
step    310 | loss 1.6687 | lr 6.94e-06 | grad 0.73 | tok/s 53842
step    320 | loss 1.7788 | lr 2.69e-05 | grad 1.62 | tok/s 51801
step    330 | loss 1.8119 | lr 5.89e-05 | grad 0.88 | tok/s 50526
step    340 | loss 1.8198 | lr 9.99e-05 | grad 2.38 | tok/s 51639
step    350 | loss 1.8349 | lr 1.46e-04 | grad 1.03 | tok/s 50191
step    360 | loss 1.8156 | lr 1.92e-04 | grad 2.36 | tok/s 50775
step    370 | loss 1.6535 | lr 2.35e-04 | grad 1.23 | tok/s 51855
step    380 | loss 2.1702 | lr 2.69e-04 | grad 1.39 | tok/s 53051
step    390 | loss 1.8347 | lr 2.91e-04 | grad 1.61 | tok/s 51125
step    400 | loss 1.9282 | lr 3.00e-04 | grad 2.72 | tok/s 52453
step    410 | loss 1.7030 | lr 2.94e-04 | grad 2.19 | tok/s 50904
step    420 | loss 1.8419 | lr 2.74e-04 | grad 1.27 | tok/s 50611
step    430 | loss 1.9647 | lr 2.42e-04 | grad 1.51 | tok/s 50418
step    440 | loss 1.9836 | lr 2.01e-04 | grad 1.33 | tok/s 52299
step    450 | loss 1.8134 | lr 1.55e-04 | grad 0.77 | tok/s 51045
step    460 | loss 1.7602 | lr 1.09e-04 | grad 0.84 | tok/s 51346
step    470 | loss 1.7413 | lr 6.65e-05 | grad 1.20 | tok/s 51797
step    480 | loss 1.6421 | lr 3.24e-05 | grad 0.64 | tok/s 50156
step    490 | loss 1.6188 | lr 9.84e-06 | grad 0.62 | tok/s 50939
step    500 | loss 2.5644 | lr 1.07e-06 | grad 1.05 | tok/s 52430
step    510 | loss 1.6288 | lr 6.94e-06 | grad 0.69 | tok/s 51238
step    520 | loss 1.7026 | lr 2.69e-05 | grad 0.57 | tok/s 52925
step    530 | loss 2.1997 | lr 5.89e-05 | grad 0.81 | tok/s 51601
step    540 | loss 1.6195 | lr 9.99e-05 | grad 1.02 | tok/s 51424
step    550 | loss 1.5293 | lr 1.46e-04 | grad 0.71 | tok/s 52916
step    560 | loss 1.3921 | lr 1.92e-04 | grad 0.73 | tok/s 53633
step    570 | loss 1.6786 | lr 2.35e-04 | grad 2.08 | tok/s 52405
step    580 | loss 2.0693 | lr 2.69e-04 | grad 1.18 | tok/s 51707
step    590 | loss 2.3353 | lr 2.91e-04 | grad 2.22 | tok/s 50687
step    600 | loss 1.8302 | lr 3.00e-04 | grad 1.52 | tok/s 50792
step    610 | loss 1.7288 | lr 2.94e-04 | grad 1.27 | tok/s 53146
step    620 | loss 1.7582 | lr 2.74e-04 | grad 0.97 | tok/s 50406
step    630 | loss 1.6412 | lr 2.42e-04 | grad 0.86 | tok/s 52132
step    640 | loss 1.9278 | lr 2.01e-04 | grad 0.98 | tok/s 52089
step    650 | loss 1.7107 | lr 1.55e-04 | grad 1.17 | tok/s 51124
step    660 | loss 1.9940 | lr 1.09e-04 | grad 5.34 | tok/s 50397
step    670 | loss 1.8327 | lr 6.65e-05 | grad 1.78 | tok/s 52200
step    680 | loss 1.7772 | lr 3.24e-05 | grad 1.05 | tok/s 50305
step    690 | loss 1.7880 | lr 9.84e-06 | grad 1.32 | tok/s 50754
step    700 | loss 1.8817 | lr 1.07e-06 | grad 1.44 | tok/s 50818
step    710 | loss 1.8026 | lr 6.94e-06 | grad 1.10 | tok/s 51197
step    720 | loss 1.8870 | lr 2.68e-05 | grad 1.45 | tok/s 51083
step    730 | loss 1.8617 | lr 5.89e-05 | grad 1.23 | tok/s 51609
step    740 | loss 1.8003 | lr 9.99e-05 | grad 1.90 | tok/s 51167
step    750 | loss 1.6024 | lr 1.46e-04 | grad 1.30 | tok/s 50591
step    760 | loss 2.0151 | lr 1.92e-04 | grad 0.76 | tok/s 51200
step    770 | loss 1.6879 | lr 2.35e-04 | grad 1.31 | tok/s 50894
step    780 | loss 1.7309 | lr 2.69e-04 | grad 0.91 | tok/s 51485
step    790 | loss 1.6395 | lr 2.91e-04 | grad 0.72 | tok/s 51897
step    800 | loss 1.6110 | lr 3.00e-04 | grad 0.97 | tok/s 51859
step    810 | loss 1.7305 | lr 2.94e-04 | grad 1.69 | tok/s 51384
step    820 | loss 2.3714 | lr 2.74e-04 | grad 1.55 | tok/s 52755
step    830 | loss 1.8227 | lr 2.42e-04 | grad 0.95 | tok/s 53576
step    840 | loss 1.5354 | lr 2.01e-04 | grad 0.71 | tok/s 53617
step    850 | loss 1.9625 | lr 1.55e-04 | grad 1.34 | tok/s 50928
step    860 | loss 1.7133 | lr 1.09e-04 | grad 0.95 | tok/s 49996
step    870 | loss 1.6486 | lr 6.65e-05 | grad 0.88 | tok/s 51441
step    880 | loss 1.7080 | lr 3.24e-05 | grad 1.06 | tok/s 51158
step    890 | loss 1.6320 | lr 9.84e-06 | grad 0.82 | tok/s 51074
step    900 | loss 2.0787 | lr 1.07e-06 | grad 0.87 | tok/s 49739
step    910 | loss 1.6948 | lr 6.94e-06 | grad 0.66 | tok/s 50707
step    920 | loss 1.6726 | lr 2.68e-05 | grad 0.71 | tok/s 50459
step    930 | loss 1.7625 | lr 5.89e-05 | grad 1.54 | tok/s 50514
step    940 | loss 1.6713 | lr 9.99e-05 | grad 1.52 | tok/s 49984
step    950 | loss 1.7631 | lr 1.46e-04 | grad 1.25 | tok/s 51137
step    960 | loss 1.5096 | lr 1.92e-04 | grad 0.58 | tok/s 53660
step    970 | loss 1.3389 | lr 2.35e-04 | grad 0.49 | tok/s 53658
step    980 | loss 1.5169 | lr 2.69e-04 | grad 1.96 | tok/s 52133
step    990 | loss 1.9035 | lr 2.91e-04 | grad 1.05 | tok/s 50676
step   1000 | loss 1.7516 | lr 3.00e-04 | grad 0.78 | tok/s 49620
  >>> saved checkpoint: checkpoint_step_001000_loss_1.7516.pt
step   1010 | loss 1.9192 | lr 2.94e-04 | grad 1.41 | tok/s 43145
step   1020 | loss 1.6071 | lr 2.74e-04 | grad 0.83 | tok/s 50879
step   1030 | loss 1.9797 | lr 2.42e-04 | grad 0.82 | tok/s 50238
step   1040 | loss 1.6460 | lr 2.01e-04 | grad 1.40 | tok/s 51269
step   1050 | loss 1.6617 | lr 1.55e-04 | grad 0.82 | tok/s 51104
step   1060 | loss 1.8329 | lr 1.09e-04 | grad 1.36 | tok/s 51478
step   1070 | loss 1.9546 | lr 6.65e-05 | grad 0.89 | tok/s 51232
step   1080 | loss 2.3625 | lr 3.24e-05 | grad 1.07 | tok/s 49999
step   1090 | loss 2.0471 | lr 9.84e-06 | grad 0.95 | tok/s 50496
step   1100 | loss 1.7240 | lr 1.07e-06 | grad 0.75 | tok/s 50146
step   1110 | loss 1.7019 | lr 6.93e-06 | grad 0.79 | tok/s 50490
step   1120 | loss 1.8977 | lr 2.68e-05 | grad 0.78 | tok/s 51176
step   1130 | loss 1.6979 | lr 5.89e-05 | grad 0.72 | tok/s 49834
step   1140 | loss 1.5637 | lr 9.99e-05 | grad 0.69 | tok/s 51158
step   1150 | loss 1.8607 | lr 1.46e-04 | grad 1.06 | tok/s 51167
step   1160 | loss 1.5390 | lr 1.92e-04 | grad 0.75 | tok/s 50628
step   1170 | loss 1.8433 | lr 2.35e-04 | grad 0.86 | tok/s 50879
step   1180 | loss 1.5610 | lr 2.69e-04 | grad 0.69 | tok/s 53683
step   1190 | loss 1.4108 | lr 2.91e-04 | grad 0.64 | tok/s 53652
step   1200 | loss 1.3301 | lr 3.00e-04 | grad 0.52 | tok/s 53814
step   1210 | loss 1.3054 | lr 2.94e-04 | grad 0.64 | tok/s 53682
step   1220 | loss 1.3606 | lr 2.74e-04 | grad 1.03 | tok/s 52951
step   1230 | loss 1.5933 | lr 2.42e-04 | grad 0.88 | tok/s 51484
step   1240 | loss 1.6538 | lr 2.01e-04 | grad 0.72 | tok/s 50462
step   1250 | loss 1.7383 | lr 1.55e-04 | grad 2.98 | tok/s 52013
step   1260 | loss 1.7913 | lr 1.09e-04 | grad 2.28 | tok/s 51927
step   1270 | loss 1.8426 | lr 6.65e-05 | grad 1.19 | tok/s 51287
step   1280 | loss 1.6734 | lr 3.24e-05 | grad 0.75 | tok/s 50755
step   1290 | loss 1.6245 | lr 9.84e-06 | grad 0.83 | tok/s 50488
step   1300 | loss 1.6727 | lr 1.07e-06 | grad 0.70 | tok/s 50190
step   1310 | loss 1.7667 | lr 6.93e-06 | grad 0.68 | tok/s 49997
step   1320 | loss 1.7249 | lr 2.68e-05 | grad 1.05 | tok/s 50993
step   1330 | loss 1.6471 | lr 5.89e-05 | grad 0.61 | tok/s 51055
step   1340 | loss 1.5652 | lr 9.99e-05 | grad 0.94 | tok/s 51178
step   1350 | loss 1.5966 | lr 1.46e-04 | grad 1.51 | tok/s 52431
step   1360 | loss 1.5720 | lr 1.92e-04 | grad 0.71 | tok/s 50631
step   1370 | loss 1.6720 | lr 2.35e-04 | grad 0.88 | tok/s 51360
step   1380 | loss 1.7619 | lr 2.69e-04 | grad 0.96 | tok/s 51749
step   1390 | loss 1.6842 | lr 2.91e-04 | grad 1.72 | tok/s 50705
step   1400 | loss 1.6910 | lr 3.00e-04 | grad 5.09 | tok/s 52590
step   1410 | loss 1.6915 | lr 2.94e-04 | grad 1.24 | tok/s 53101
step   1420 | loss 1.7802 | lr 2.74e-04 | grad 1.08 | tok/s 50540
step   1430 | loss 1.5733 | lr 2.42e-04 | grad 0.82 | tok/s 49421
step   1440 | loss 1.4916 | lr 2.01e-04 | grad 0.61 | tok/s 52102
step   1450 | loss 1.5155 | lr 1.55e-04 | grad 1.91 | tok/s 53174
step   1460 | loss 1.5957 | lr 1.09e-04 | grad 0.54 | tok/s 49613
step   1470 | loss 1.7029 | lr 6.65e-05 | grad 1.76 | tok/s 51412
step   1480 | loss 1.5725 | lr 3.24e-05 | grad 1.73 | tok/s 51993
step   1490 | loss 1.7099 | lr 9.84e-06 | grad 2.20 | tok/s 51972
step   1500 | loss 1.8134 | lr 1.07e-06 | grad 1.81 | tok/s 50515
step   1510 | loss 1.7058 | lr 6.93e-06 | grad 0.92 | tok/s 53124
step   1520 | loss 1.6496 | lr 2.68e-05 | grad 1.11 | tok/s 52694
step   1530 | loss 1.6085 | lr 5.89e-05 | grad 0.54 | tok/s 52280
step   1540 | loss 1.5798 | lr 9.99e-05 | grad 0.54 | tok/s 51269
step   1550 | loss 1.5494 | lr 1.46e-04 | grad 2.05 | tok/s 53303
step   1560 | loss 2.1597 | lr 1.92e-04 | grad 1.64 | tok/s 51965
step   1570 | loss 1.6342 | lr 2.35e-04 | grad 0.90 | tok/s 51118
step   1580 | loss 1.7854 | lr 2.69e-04 | grad 1.26 | tok/s 52748
step   1590 | loss 1.6165 | lr 2.91e-04 | grad 0.84 | tok/s 51494
step   1600 | loss 1.7085 | lr 3.00e-04 | grad 1.05 | tok/s 50450
step   1610 | loss 1.5305 | lr 2.94e-04 | grad 0.71 | tok/s 53375
step   1620 | loss 1.6829 | lr 2.74e-04 | grad 0.61 | tok/s 52696
step   1630 | loss 1.6629 | lr 2.42e-04 | grad 0.76 | tok/s 53098
step   1640 | loss 1.6031 | lr 2.01e-04 | grad 0.72 | tok/s 51263
step   1650 | loss 1.6153 | lr 1.55e-04 | grad 1.21 | tok/s 50521
step   1660 | loss 1.6116 | lr 1.09e-04 | grad 0.67 | tok/s 50827
step   1670 | loss 1.6693 | lr 6.65e-05 | grad 1.99 | tok/s 52937
step   1680 | loss 2.0208 | lr 3.24e-05 | grad 0.69 | tok/s 52919
step   1690 | loss 1.5881 | lr 9.84e-06 | grad 0.84 | tok/s 51623
step   1700 | loss 1.9127 | lr 1.07e-06 | grad 0.57 | tok/s 52864
step   1710 | loss 1.6367 | lr 6.93e-06 | grad 0.90 | tok/s 51368
step   1720 | loss 1.6306 | lr 2.68e-05 | grad 0.76 | tok/s 51518
step   1730 | loss 1.7356 | lr 5.89e-05 | grad 0.74 | tok/s 51568
step   1740 | loss 1.6336 | lr 9.99e-05 | grad 0.55 | tok/s 52357
step   1750 | loss 1.5501 | lr 1.46e-04 | grad 0.55 | tok/s 49764
step   1760 | loss 1.8067 | lr 1.92e-04 | grad 0.89 | tok/s 51196
step   1770 | loss 1.7144 | lr 2.35e-04 | grad 0.73 | tok/s 52286
step   1780 | loss 1.6251 | lr 2.69e-04 | grad 1.00 | tok/s 50336
step   1790 | loss 1.8164 | lr 2.91e-04 | grad 0.86 | tok/s 51155
step   1800 | loss 1.5480 | lr 3.00e-04 | grad 0.60 | tok/s 51948
step   1810 | loss 1.6393 | lr 2.94e-04 | grad 0.80 | tok/s 51717
step   1820 | loss 1.5810 | lr 2.74e-04 | grad 0.75 | tok/s 51211
step   1830 | loss 1.6161 | lr 2.42e-04 | grad 0.79 | tok/s 51108
step   1840 | loss 1.6105 | lr 2.01e-04 | grad 1.21 | tok/s 50365
step   1850 | loss 1.8322 | lr 1.55e-04 | grad 0.95 | tok/s 51149
step   1860 | loss 1.5849 | lr 1.09e-04 | grad 0.52 | tok/s 50921
step   1870 | loss 1.6146 | lr 6.65e-05 | grad 1.05 | tok/s 52293
step   1880 | loss 1.5628 | lr 3.24e-05 | grad 0.55 | tok/s 52542
step   1890 | loss 1.6730 | lr 9.84e-06 | grad 0.54 | tok/s 51640
step   1900 | loss 1.7038 | lr 1.07e-06 | grad 1.06 | tok/s 52127
step   1910 | loss 1.6814 | lr 6.93e-06 | grad 1.44 | tok/s 51477
step   1920 | loss 1.5749 | lr 2.68e-05 | grad 0.84 | tok/s 53032
step   1930 | loss 1.5484 | lr 5.89e-05 | grad 0.76 | tok/s 52625
step   1940 | loss 1.4883 | lr 9.99e-05 | grad 0.71 | tok/s 53668

Training complete! Final step: 1946
