# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 896 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e88_100m_balanced/mamba2
# Started: 2026-01-20T16:39:11.909120
============================================================

Using device: cuda
Output directory: benchmark_results/e88_100m_balanced/mamba2/levelmamba2_100m_20260120_163916
Model: Level mamba2, 101,936,528 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.4576 | lr 2.70e-05 | grad 5.00 | tok/s 9756
step     20 | loss 4.3084 | lr 5.70e-05 | grad 6.47 | tok/s 64990
step     30 | loss 4.5681 | lr 8.70e-05 | grad 3.27 | tok/s 68405
step     40 | loss 3.1653 | lr 1.17e-04 | grad 1.87 | tok/s 67987
step     50 | loss 2.3871 | lr 1.47e-04 | grad 1.48 | tok/s 67676
step     60 | loss 2.7602 | lr 1.77e-04 | grad 3.27 | tok/s 65845
step     70 | loss 2.4718 | lr 2.07e-04 | grad 1.88 | tok/s 63378
step     80 | loss 2.8034 | lr 2.37e-04 | grad 3.17 | tok/s 65462
step     90 | loss 2.7418 | lr 2.67e-04 | grad 4.06 | tok/s 62880
step    100 | loss 2.2444 | lr 2.97e-04 | grad 1.20 | tok/s 63061
step    110 | loss 2.2790 | lr 6.94e-06 | grad 1.96 | tok/s 60536
step    120 | loss 2.4862 | lr 2.69e-05 | grad 1.52 | tok/s 60148
step    130 | loss 2.3558 | lr 5.89e-05 | grad 1.16 | tok/s 61218
step    140 | loss 2.0979 | lr 9.99e-05 | grad 1.00 | tok/s 61221
step    150 | loss 1.9594 | lr 1.46e-04 | grad 1.89 | tok/s 58321
step    160 | loss 1.8689 | lr 1.92e-04 | grad 1.62 | tok/s 58562
step    170 | loss 2.0566 | lr 2.35e-04 | grad 3.31 | tok/s 60067
step    180 | loss 2.0397 | lr 2.69e-04 | grad 1.27 | tok/s 60216
step    190 | loss 1.7566 | lr 2.91e-04 | grad 1.27 | tok/s 61181
step    200 | loss 1.3556 | lr 3.00e-04 | grad 1.05 | tok/s 62601
step    210 | loss 2.1305 | lr 2.94e-04 | grad 1.70 | tok/s 59830
step    220 | loss 1.9252 | lr 2.74e-04 | grad 1.30 | tok/s 61427
step    230 | loss 1.8177 | lr 2.42e-04 | grad 1.48 | tok/s 59219
step    240 | loss 1.7893 | lr 2.01e-04 | grad 1.50 | tok/s 60501
step    250 | loss 1.7935 | lr 1.55e-04 | grad 1.31 | tok/s 59421
step    260 | loss 1.8799 | lr 1.09e-04 | grad 0.85 | tok/s 56943
step    270 | loss 1.7249 | lr 6.65e-05 | grad 0.95 | tok/s 59028
step    280 | loss 1.6074 | lr 3.24e-05 | grad 1.73 | tok/s 58981
step    290 | loss 1.6134 | lr 9.84e-06 | grad 0.76 | tok/s 62062
step    300 | loss 1.5959 | lr 1.07e-06 | grad 0.86 | tok/s 61830
step    310 | loss 1.5800 | lr 6.94e-06 | grad 0.68 | tok/s 61595
step    320 | loss 1.6812 | lr 2.69e-05 | grad 1.66 | tok/s 59378
step    330 | loss 1.7106 | lr 5.89e-05 | grad 0.79 | tok/s 57672
step    340 | loss 1.7108 | lr 9.99e-05 | grad 2.14 | tok/s 58971
step    350 | loss 1.7274 | lr 1.46e-04 | grad 0.92 | tok/s 57237
step    360 | loss 1.6735 | lr 1.92e-04 | grad 2.34 | tok/s 57997
step    370 | loss 1.5137 | lr 2.35e-04 | grad 1.00 | tok/s 59101
step    380 | loss 1.9893 | lr 2.69e-04 | grad 1.07 | tok/s 60483
step    390 | loss 1.7125 | lr 2.91e-04 | grad 1.48 | tok/s 58283
step    400 | loss 1.8051 | lr 3.00e-04 | grad 2.34 | tok/s 59804
step    410 | loss 1.5779 | lr 2.94e-04 | grad 2.38 | tok/s 58073
step    420 | loss 1.7137 | lr 2.74e-04 | grad 1.22 | tok/s 57516
step    430 | loss 1.8065 | lr 2.42e-04 | grad 1.23 | tok/s 57018
step    440 | loss 1.8281 | lr 2.01e-04 | grad 0.89 | tok/s 59506
step    450 | loss 1.6847 | lr 1.55e-04 | grad 0.78 | tok/s 57547
step    460 | loss 1.6713 | lr 1.09e-04 | grad 0.68 | tok/s 57904
step    470 | loss 1.6167 | lr 6.65e-05 | grad 1.02 | tok/s 58309
step    480 | loss 1.5200 | lr 3.24e-05 | grad 0.61 | tok/s 56530
step    490 | loss 1.5158 | lr 9.84e-06 | grad 0.64 | tok/s 57469
step    500 | loss 2.4375 | lr 1.07e-06 | grad 0.98 | tok/s 59357
step    510 | loss 1.5766 | lr 6.94e-06 | grad 0.71 | tok/s 58114
step    520 | loss 1.6102 | lr 2.69e-05 | grad 0.63 | tok/s 59814
step    530 | loss 2.1004 | lr 5.89e-05 | grad 0.67 | tok/s 58403
step    540 | loss 1.4969 | lr 9.99e-05 | grad 1.06 | tok/s 58492
step    550 | loss 1.4282 | lr 1.46e-04 | grad 0.59 | tok/s 60028
step    560 | loss 1.3018 | lr 1.92e-04 | grad 0.66 | tok/s 60804
step    570 | loss 1.5565 | lr 2.35e-04 | grad 1.97 | tok/s 59421
step    580 | loss 1.8485 | lr 2.69e-04 | grad 0.88 | tok/s 58592
step    590 | loss 2.1231 | lr 2.91e-04 | grad 1.03 | tok/s 57437
step    600 | loss 1.6528 | lr 3.00e-04 | grad 1.42 | tok/s 57607
step    610 | loss 1.6257 | lr 2.94e-04 | grad 1.32 | tok/s 60342
step    620 | loss 1.6189 | lr 2.74e-04 | grad 0.91 | tok/s 57262
step    630 | loss 1.5251 | lr 2.42e-04 | grad 0.86 | tok/s 59138
step    640 | loss 1.7622 | lr 2.01e-04 | grad 0.96 | tok/s 59022
step    650 | loss 1.5496 | lr 1.55e-04 | grad 0.90 | tok/s 58081
step    660 | loss 1.8368 | lr 1.09e-04 | grad 5.66 | tok/s 57170
step    670 | loss 1.6889 | lr 6.65e-05 | grad 1.78 | tok/s 59260
step    680 | loss 1.6227 | lr 3.24e-05 | grad 0.98 | tok/s 57248
step    690 | loss 1.6454 | lr 9.84e-06 | grad 1.42 | tok/s 57565
step    700 | loss 1.7687 | lr 1.07e-06 | grad 1.52 | tok/s 57911
step    710 | loss 1.6769 | lr 6.94e-06 | grad 1.09 | tok/s 58243
step    720 | loss 1.7599 | lr 2.68e-05 | grad 1.56 | tok/s 57875
step    730 | loss 1.7258 | lr 5.89e-05 | grad 1.16 | tok/s 58571
step    740 | loss 1.6615 | lr 9.99e-05 | grad 1.68 | tok/s 57976
step    750 | loss 1.4827 | lr 1.46e-04 | grad 1.20 | tok/s 57409
step    760 | loss 1.7688 | lr 1.92e-04 | grad 0.68 | tok/s 58032
step    770 | loss 1.5344 | lr 2.35e-04 | grad 0.97 | tok/s 57736
step    780 | loss 1.5823 | lr 2.69e-04 | grad 0.82 | tok/s 58416
step    790 | loss 1.4866 | lr 2.91e-04 | grad 0.56 | tok/s 58822
step    800 | loss 1.4886 | lr 3.00e-04 | grad 0.96 | tok/s 58984
step    810 | loss 1.5804 | lr 2.94e-04 | grad 1.79 | tok/s 58298
step    820 | loss 2.3185 | lr 2.74e-04 | grad 1.47 | tok/s 59866
step    830 | loss 1.7783 | lr 2.42e-04 | grad 0.66 | tok/s 60806
step    840 | loss 1.4299 | lr 2.01e-04 | grad 0.64 | tok/s 60903
step    850 | loss 1.8684 | lr 1.55e-04 | grad 1.06 | tok/s 57766
step    860 | loss 1.6281 | lr 1.09e-04 | grad 0.87 | tok/s 56808
step    870 | loss 1.5454 | lr 6.65e-05 | grad 0.90 | tok/s 58518
step    880 | loss 1.6086 | lr 3.24e-05 | grad 1.09 | tok/s 58064
step    890 | loss 1.5519 | lr 9.84e-06 | grad 0.93 | tok/s 58066
step    900 | loss 1.9433 | lr 1.07e-06 | grad 0.87 | tok/s 56670
step    910 | loss 1.5673 | lr 6.94e-06 | grad 0.72 | tok/s 57558
step    920 | loss 1.5813 | lr 2.68e-05 | grad 0.80 | tok/s 57450
step    930 | loss 1.6700 | lr 5.89e-05 | grad 1.54 | tok/s 57413
step    940 | loss 1.5849 | lr 9.99e-05 | grad 1.63 | tok/s 56723
step    950 | loss 1.6278 | lr 1.46e-04 | grad 1.02 | tok/s 57986
step    960 | loss 1.4077 | lr 1.92e-04 | grad 0.63 | tok/s 60955
step    970 | loss 1.2530 | lr 2.35e-04 | grad 0.45 | tok/s 60680
step    980 | loss 1.4022 | lr 2.69e-04 | grad 2.73 | tok/s 59131
step    990 | loss 1.6649 | lr 2.91e-04 | grad 0.74 | tok/s 57639
step   1000 | loss 1.5901 | lr 3.00e-04 | grad 0.62 | tok/s 56177
  >>> saved checkpoint: checkpoint_step_001000_loss_1.5901.pt
step   1010 | loss 1.7138 | lr 2.94e-04 | grad 0.94 | tok/s 48243
step   1020 | loss 1.4153 | lr 2.74e-04 | grad 0.83 | tok/s 57802
step   1030 | loss 1.9014 | lr 2.42e-04 | grad 0.82 | tok/s 56799
step   1040 | loss 1.5038 | lr 2.01e-04 | grad 1.12 | tok/s 57750
step   1050 | loss 1.5170 | lr 1.55e-04 | grad 0.72 | tok/s 58272
step   1060 | loss 1.6222 | lr 1.09e-04 | grad 1.34 | tok/s 58309
step   1070 | loss 1.7657 | lr 6.65e-05 | grad 0.84 | tok/s 58409
step   1080 | loss 2.1863 | lr 3.24e-05 | grad 1.00 | tok/s 57773
step   1090 | loss 1.8867 | lr 9.84e-06 | grad 0.88 | tok/s 58171
step   1100 | loss 1.5561 | lr 1.07e-06 | grad 0.66 | tok/s 57697
step   1110 | loss 1.5271 | lr 6.93e-06 | grad 0.81 | tok/s 58722
step   1120 | loss 1.7228 | lr 2.68e-05 | grad 0.78 | tok/s 59580
step   1130 | loss 1.5817 | lr 5.89e-05 | grad 0.68 | tok/s 56721
step   1140 | loss 1.4508 | lr 9.99e-05 | grad 0.59 | tok/s 57948
step   1150 | loss 1.6944 | lr 1.46e-04 | grad 1.09 | tok/s 57923
step   1160 | loss 1.3994 | lr 1.92e-04 | grad 0.55 | tok/s 57251
step   1170 | loss 1.7335 | lr 2.35e-04 | grad 0.80 | tok/s 57927
step   1180 | loss 1.4579 | lr 2.69e-04 | grad 0.68 | tok/s 60841
step   1190 | loss 1.3072 | lr 2.91e-04 | grad 0.56 | tok/s 60813
step   1200 | loss 1.2318 | lr 3.00e-04 | grad 0.53 | tok/s 60813
step   1210 | loss 1.2090 | lr 2.94e-04 | grad 0.64 | tok/s 60864
step   1220 | loss 1.2529 | lr 2.74e-04 | grad 0.86 | tok/s 60282
step   1230 | loss 1.4785 | lr 2.42e-04 | grad 0.73 | tok/s 58253
step   1240 | loss 1.5163 | lr 2.01e-04 | grad 0.69 | tok/s 57179
step   1250 | loss 1.5872 | lr 1.55e-04 | grad 2.91 | tok/s 58877
step   1260 | loss 1.6340 | lr 1.09e-04 | grad 2.30 | tok/s 58919
step   1270 | loss 1.6947 | lr 6.65e-05 | grad 1.16 | tok/s 58069
step   1280 | loss 1.5380 | lr 3.24e-05 | grad 0.76 | tok/s 57511
step   1290 | loss 1.4936 | lr 9.84e-06 | grad 0.73 | tok/s 57108
step   1300 | loss 1.5430 | lr 1.07e-06 | grad 0.69 | tok/s 56893
step   1310 | loss 1.6190 | lr 6.93e-06 | grad 0.67 | tok/s 56788
step   1320 | loss 1.6041 | lr 2.68e-05 | grad 1.05 | tok/s 57935
step   1330 | loss 1.5131 | lr 5.89e-05 | grad 0.56 | tok/s 57919
step   1340 | loss 1.4292 | lr 9.99e-05 | grad 0.86 | tok/s 58070
step   1350 | loss 1.4385 | lr 1.46e-04 | grad 1.32 | tok/s 59593
step   1360 | loss 1.4278 | lr 1.92e-04 | grad 0.66 | tok/s 56503
step   1370 | loss 1.5238 | lr 2.35e-04 | grad 0.66 | tok/s 57297
step   1380 | loss 1.5877 | lr 2.69e-04 | grad 0.75 | tok/s 57856
step   1390 | loss 1.5137 | lr 2.91e-04 | grad 1.27 | tok/s 56587
step   1400 | loss 1.5164 | lr 3.00e-04 | grad 5.50 | tok/s 58746
step   1410 | loss 1.5199 | lr 2.94e-04 | grad 0.96 | tok/s 59263
step   1420 | loss 1.5918 | lr 2.74e-04 | grad 0.70 | tok/s 56458
step   1430 | loss 1.4221 | lr 2.42e-04 | grad 0.72 | tok/s 55129
step   1440 | loss 1.3562 | lr 2.01e-04 | grad 0.57 | tok/s 58225
step   1450 | loss 1.3505 | lr 1.55e-04 | grad 1.46 | tok/s 59295
step   1460 | loss 1.4662 | lr 1.09e-04 | grad 0.52 | tok/s 55435
step   1470 | loss 1.5565 | lr 6.65e-05 | grad 2.00 | tok/s 57446
step   1480 | loss 1.4273 | lr 3.24e-05 | grad 1.51 | tok/s 58171
step   1490 | loss 1.5723 | lr 9.84e-06 | grad 2.02 | tok/s 57993
step   1500 | loss 1.6781 | lr 1.07e-06 | grad 1.76 | tok/s 56514
step   1510 | loss 1.5565 | lr 6.93e-06 | grad 0.99 | tok/s 59379
step   1520 | loss 1.5079 | lr 2.68e-05 | grad 1.10 | tok/s 58693
step   1530 | loss 1.4834 | lr 5.89e-05 | grad 0.52 | tok/s 58354
step   1540 | loss 1.4502 | lr 9.99e-05 | grad 0.49 | tok/s 57154
step   1550 | loss 1.4414 | lr 1.46e-04 | grad 2.95 | tok/s 59609
step   1560 | loss 1.8811 | lr 1.92e-04 | grad 0.98 | tok/s 58273
step   1570 | loss 1.4589 | lr 2.35e-04 | grad 0.90 | tok/s 57044
step   1580 | loss 1.5990 | lr 2.69e-04 | grad 1.02 | tok/s 59086
step   1590 | loss 1.3944 | lr 2.91e-04 | grad 0.56 | tok/s 57609
step   1600 | loss 1.5560 | lr 3.00e-04 | grad 0.70 | tok/s 56465
step   1610 | loss 1.3579 | lr 2.94e-04 | grad 0.64 | tok/s 59814
step   1620 | loss 1.5338 | lr 2.74e-04 | grad 0.57 | tok/s 58891
step   1630 | loss 1.4832 | lr 2.42e-04 | grad 0.73 | tok/s 59504
step   1640 | loss 1.4442 | lr 2.01e-04 | grad 0.61 | tok/s 57440
step   1650 | loss 1.4657 | lr 1.55e-04 | grad 0.83 | tok/s 56514
step   1660 | loss 1.4585 | lr 1.09e-04 | grad 0.56 | tok/s 56881
step   1670 | loss 1.4894 | lr 6.65e-05 | grad 1.70 | tok/s 59167
step   1680 | loss 1.8608 | lr 3.24e-05 | grad 0.53 | tok/s 59361
step   1690 | loss 1.4455 | lr 9.84e-06 | grad 0.80 | tok/s 58005
step   1700 | loss 1.6714 | lr 1.07e-06 | grad 0.50 | tok/s 59147
step   1710 | loss 1.4890 | lr 6.93e-06 | grad 0.93 | tok/s 57545
step   1720 | loss 1.4483 | lr 2.68e-05 | grad 0.62 | tok/s 57753
step   1730 | loss 1.5735 | lr 5.89e-05 | grad 0.77 | tok/s 57550
step   1740 | loss 1.4896 | lr 9.99e-05 | grad 0.58 | tok/s 58433
step   1750 | loss 1.4156 | lr 1.46e-04 | grad 0.52 | tok/s 56441
step   1760 | loss 1.5920 | lr 1.92e-04 | grad 0.62 | tok/s 57265
step   1770 | loss 1.5555 | lr 2.35e-04 | grad 0.53 | tok/s 58682
step   1780 | loss 1.4506 | lr 2.69e-04 | grad 0.93 | tok/s 56416
step   1790 | loss 1.6018 | lr 2.91e-04 | grad 0.70 | tok/s 57433
step   1800 | loss 1.4032 | lr 3.00e-04 | grad 0.60 | tok/s 58594
step   1810 | loss 1.4929 | lr 2.94e-04 | grad 0.81 | tok/s 58243
step   1820 | loss 1.4055 | lr 2.74e-04 | grad 0.61 | tok/s 57624
step   1830 | loss 1.4501 | lr 2.42e-04 | grad 0.63 | tok/s 57492
step   1840 | loss 1.4538 | lr 2.01e-04 | grad 0.76 | tok/s 56968
step   1850 | loss 1.6474 | lr 1.55e-04 | grad 0.93 | tok/s 57604
step   1860 | loss 1.4280 | lr 1.09e-04 | grad 0.50 | tok/s 57361
step   1870 | loss 1.4540 | lr 6.65e-05 | grad 1.11 | tok/s 58812
step   1880 | loss 1.4362 | lr 3.24e-05 | grad 0.55 | tok/s 58807
step   1890 | loss 1.5055 | lr 9.84e-06 | grad 0.50 | tok/s 57888
step   1900 | loss 1.4790 | lr 1.07e-06 | grad 0.67 | tok/s 58469
step   1910 | loss 1.5415 | lr 6.93e-06 | grad 1.58 | tok/s 57788
step   1920 | loss 1.4252 | lr 2.68e-05 | grad 0.78 | tok/s 59597
step   1930 | loss 1.4086 | lr 5.89e-05 | grad 0.87 | tok/s 58926
step   1940 | loss 1.3909 | lr 9.99e-05 | grad 0.59 | tok/s 60039
step   1950 | loss 1.4522 | lr 1.46e-04 | grad 0.70 | tok/s 58537
step   1960 | loss 1.7183 | lr 1.92e-04 | grad 3.41 | tok/s 59579
step   1970 | loss 1.3830 | lr 2.35e-04 | grad 1.04 | tok/s 57752
step   1980 | loss 1.4901 | lr 2.69e-04 | grad 1.87 | tok/s 57579
step   1990 | loss 1.5350 | lr 2.91e-04 | grad 0.92 | tok/s 58901
step   2000 | loss 1.4509 | lr 3.00e-04 | grad 0.91 | tok/s 59433
  >>> saved checkpoint: checkpoint_step_002000_loss_1.4509.pt
step   2010 | loss 1.2792 | lr 2.94e-04 | grad 0.56 | tok/s 48038
step   2020 | loss 1.1555 | lr 2.74e-04 | grad 0.51 | tok/s 61308
step   2030 | loss 1.4111 | lr 2.42e-04 | grad 0.73 | tok/s 60564
step   2040 | loss 1.2946 | lr 2.01e-04 | grad 0.47 | tok/s 61208
step   2050 | loss 1.2401 | lr 1.55e-04 | grad 0.75 | tok/s 60339
step   2060 | loss 1.5606 | lr 1.09e-04 | grad 0.59 | tok/s 57679
step   2070 | loss 1.5155 | lr 6.65e-05 | grad 1.05 | tok/s 60556
step   2080 | loss 1.5426 | lr 3.24e-05 | grad 2.14 | tok/s 57029
step   2090 | loss 1.5221 | lr 9.84e-06 | grad 0.75 | tok/s 59246
step   2100 | loss 1.4758 | lr 1.07e-06 | grad 0.84 | tok/s 57581
step   2110 | loss 1.3630 | lr 6.93e-06 | grad 0.59 | tok/s 59659
step   2120 | loss 1.3411 | lr 2.68e-05 | grad 1.04 | tok/s 59035
step   2130 | loss 1.4643 | lr 5.89e-05 | grad 1.30 | tok/s 57346
step   2140 | loss 1.5218 | lr 9.99e-05 | grad 2.44 | tok/s 57387
step   2150 | loss 1.5088 | lr 1.46e-04 | grad 0.49 | tok/s 58052
step   2160 | loss 1.4790 | lr 1.92e-04 | grad 0.53 | tok/s 57404
step   2170 | loss 1.5421 | lr 2.35e-04 | grad 0.78 | tok/s 58073
step   2180 | loss 1.4312 | lr 2.69e-04 | grad 0.66 | tok/s 59254
step   2190 | loss 1.6838 | lr 2.91e-04 | grad 0.67 | tok/s 59185

Training complete! Final step: 2191
