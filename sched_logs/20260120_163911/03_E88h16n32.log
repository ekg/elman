# Job 3: E88h16n32
# GPU: 3
# Command: python train.py --level E88h16 --dim 1152 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e88_100m_balanced/E88h16n32
# Started: 2026-01-20T16:39:11.910788
============================================================

Using device: cuda
Output directory: benchmark_results/e88_100m_balanced/E88h16n32/levelE88h16_100m_20260120_163917
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88h16, 95,225,344 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.7294 | lr 2.70e-05 | grad 129.00 | tok/s 14621
step     20 | loss 5.2777 | lr 5.70e-05 | grad 61.00 | tok/s 26287
step     30 | loss 5.2160 | lr 8.70e-05 | grad 48.25 | tok/s 27687
step     40 | loss 4.7108 | lr 1.17e-04 | grad 34.50 | tok/s 27558
step     50 | loss 4.4312 | lr 1.47e-04 | grad 22.88 | tok/s 27366
step     60 | loss 4.2637 | lr 1.77e-04 | grad 54.50 | tok/s 26737
step     70 | loss 3.3717 | lr 2.07e-04 | grad 3.94 | tok/s 25658
step     80 | loss 3.4941 | lr 2.37e-04 | grad 3.78 | tok/s 26543
step     90 | loss 3.4507 | lr 2.67e-04 | grad 4.34 | tok/s 25515
step    100 | loss 3.2733 | lr 2.97e-04 | grad 1.57 | tok/s 25670
step    110 | loss 3.2017 | lr 6.94e-06 | grad 2.19 | tok/s 24292
step    120 | loss 3.5866 | lr 2.69e-05 | grad 1.73 | tok/s 24018
step    130 | loss 3.3771 | lr 5.89e-05 | grad 1.61 | tok/s 24573
step    140 | loss 3.1419 | lr 9.99e-05 | grad 0.98 | tok/s 24614
step    150 | loss 3.0370 | lr 1.46e-04 | grad 3.03 | tok/s 23437
step    160 | loss 2.8987 | lr 1.92e-04 | grad 1.99 | tok/s 23600
step    170 | loss 2.9579 | lr 2.35e-04 | grad 5.66 | tok/s 24462
step    180 | loss 2.9004 | lr 2.69e-04 | grad 2.66 | tok/s 24476
step    190 | loss 2.5513 | lr 2.91e-04 | grad 1.76 | tok/s 24849
step    200 | loss 2.1861 | lr 3.00e-04 | grad 1.48 | tok/s 25450
step    210 | loss 2.6085 | lr 2.94e-04 | grad 2.38 | tok/s 24374
step    220 | loss 2.4794 | lr 2.74e-04 | grad 1.32 | tok/s 25184
step    230 | loss 2.2953 | lr 2.42e-04 | grad 2.28 | tok/s 24334
step    240 | loss 2.3065 | lr 2.01e-04 | grad 1.93 | tok/s 24868
step    250 | loss 2.2424 | lr 1.55e-04 | grad 1.44 | tok/s 24508
step    260 | loss 2.2780 | lr 1.09e-04 | grad 0.89 | tok/s 23501
step    270 | loss 2.1275 | lr 6.65e-05 | grad 1.16 | tok/s 24389
step    280 | loss 2.0323 | lr 3.24e-05 | grad 2.09 | tok/s 24357
step    290 | loss 2.0452 | lr 9.84e-06 | grad 1.07 | tok/s 25713
step    300 | loss 2.0169 | lr 1.07e-06 | grad 1.05 | tok/s 25713
step    310 | loss 2.0185 | lr 6.94e-06 | grad 0.91 | tok/s 25684
step    320 | loss 2.1129 | lr 2.69e-05 | grad 2.17 | tok/s 24775
step    330 | loss 2.1564 | lr 5.89e-05 | grad 0.97 | tok/s 24006
step    340 | loss 2.1631 | lr 9.99e-05 | grad 2.34 | tok/s 24670
step    350 | loss 2.1844 | lr 1.46e-04 | grad 1.32 | tok/s 23980
step    360 | loss 2.1192 | lr 1.92e-04 | grad 2.83 | tok/s 24258
step    370 | loss 1.9644 | lr 2.35e-04 | grad 1.79 | tok/s 24753
step    380 | loss 2.4674 | lr 2.69e-04 | grad 1.88 | tok/s 25361
step    390 | loss 2.0752 | lr 2.91e-04 | grad 1.87 | tok/s 24372
step    400 | loss 2.1473 | lr 3.00e-04 | grad 2.98 | tok/s 25109
step    410 | loss 1.8658 | lr 2.94e-04 | grad 2.53 | tok/s 24312
step    420 | loss 2.0432 | lr 2.74e-04 | grad 1.68 | tok/s 24119
step    430 | loss 2.1685 | lr 2.42e-04 | grad 1.95 | tok/s 23994
step    440 | loss 2.2048 | lr 2.01e-04 | grad 1.75 | tok/s 24957
step    450 | loss 1.9636 | lr 1.55e-04 | grad 0.96 | tok/s 24305
step    460 | loss 1.9385 | lr 1.09e-04 | grad 1.04 | tok/s 24355
step    470 | loss 1.8986 | lr 6.65e-05 | grad 1.52 | tok/s 24616
step    480 | loss 1.8241 | lr 3.24e-05 | grad 0.84 | tok/s 23771
step    490 | loss 1.7768 | lr 9.84e-06 | grad 0.77 | tok/s 24191
step    500 | loss 2.7931 | lr 1.07e-06 | grad 1.38 | tok/s 24977
step    510 | loss 1.8571 | lr 6.94e-06 | grad 0.86 | tok/s 24469
step    520 | loss 1.8668 | lr 2.69e-05 | grad 0.80 | tok/s 25313
step    530 | loss 2.3550 | lr 5.89e-05 | grad 0.93 | tok/s 24708
step    540 | loss 1.8064 | lr 9.99e-05 | grad 1.29 | tok/s 24629
step    550 | loss 1.6992 | lr 1.46e-04 | grad 0.96 | tok/s 25310
step    560 | loss 1.5593 | lr 1.92e-04 | grad 1.02 | tok/s 25676
step    570 | loss 1.8401 | lr 2.35e-04 | grad 2.61 | tok/s 25089
step    580 | loss 2.1737 | lr 2.69e-04 | grad 1.41 | tok/s 24775
step    590 | loss 2.4648 | lr 2.91e-04 | grad 2.08 | tok/s 24288
step    600 | loss 1.9430 | lr 3.00e-04 | grad 1.86 | tok/s 24341
step    610 | loss 1.9208 | lr 2.94e-04 | grad 1.46 | tok/s 25480
step    620 | loss 1.8466 | lr 2.74e-04 | grad 1.25 | tok/s 24171
step    630 | loss 1.7865 | lr 2.42e-04 | grad 1.38 | tok/s 24971
step    640 | loss 2.0439 | lr 2.01e-04 | grad 1.25 | tok/s 24925
step    650 | loss 1.8074 | lr 1.55e-04 | grad 1.41 | tok/s 24494
step    660 | loss 2.1190 | lr 1.09e-04 | grad 6.03 | tok/s 24141
step    670 | loss 1.9169 | lr 6.65e-05 | grad 2.23 | tok/s 25060
step    680 | loss 1.8603 | lr 3.24e-05 | grad 1.26 | tok/s 24229
step    690 | loss 1.9155 | lr 9.84e-06 | grad 1.73 | tok/s 24405
step    700 | loss 1.9881 | lr 1.07e-06 | grad 1.55 | tok/s 24611
step    710 | loss 1.9158 | lr 6.94e-06 | grad 1.33 | tok/s 24713
step    720 | loss 2.0466 | lr 2.68e-05 | grad 1.89 | tok/s 24653
step    730 | loss 1.9752 | lr 5.89e-05 | grad 1.59 | tok/s 24920
step    740 | loss 1.8940 | lr 9.99e-05 | grad 2.33 | tok/s 24633
step    750 | loss 1.7086 | lr 1.46e-04 | grad 1.85 | tok/s 24317
step    760 | loss 2.0995 | lr 1.92e-04 | grad 1.12 | tok/s 24622
step    770 | loss 1.7857 | lr 2.35e-04 | grad 1.56 | tok/s 24454
step    780 | loss 1.8192 | lr 2.69e-04 | grad 1.42 | tok/s 24787
step    790 | loss 1.7407 | lr 2.91e-04 | grad 1.19 | tok/s 24987
step    800 | loss 1.7276 | lr 3.00e-04 | grad 1.50 | tok/s 24991
step    810 | loss 1.8444 | lr 2.94e-04 | grad 2.36 | tok/s 24736
step    820 | loss 2.5781 | lr 2.74e-04 | grad 1.77 | tok/s 25409
step    830 | loss 2.0755 | lr 2.42e-04 | grad 0.96 | tok/s 25849
step    840 | loss 1.7378 | lr 2.01e-04 | grad 0.75 | tok/s 25828
step    850 | loss 2.1392 | lr 1.55e-04 | grad 1.62 | tok/s 24623
step    860 | loss 1.9156 | lr 1.09e-04 | grad 1.34 | tok/s 24079
step    870 | loss 1.8178 | lr 6.65e-05 | grad 1.11 | tok/s 24796
step    880 | loss 1.8827 | lr 3.24e-05 | grad 1.38 | tok/s 24641
step    890 | loss 1.7890 | lr 9.84e-06 | grad 1.09 | tok/s 24586
step    900 | loss 2.1981 | lr 1.07e-06 | grad 1.07 | tok/s 24076
step    910 | loss 1.8263 | lr 6.94e-06 | grad 0.88 | tok/s 24513
step    920 | loss 1.8143 | lr 2.68e-05 | grad 0.90 | tok/s 24308
step    930 | loss 1.9239 | lr 5.89e-05 | grad 1.84 | tok/s 24302
step    940 | loss 1.8373 | lr 9.99e-05 | grad 1.66 | tok/s 24130

Training complete! Final step: 940
