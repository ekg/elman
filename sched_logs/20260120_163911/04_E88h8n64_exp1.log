# Job 4: E88h8n64_exp1
# GPU: 4
# Command: python train.py --level E88h8 --dim 1792 --expansion 1.0 --n_state 64 --data data/pile.txt --depth 20 --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/e88_100m_balanced/E88h8n64_exp1
# Started: 2026-01-20T16:39:11.910883
============================================================

Using device: cuda
Output directory: benchmark_results/e88_100m_balanced/E88h8n64_exp1/levelE88h8_100m_20260120_163917
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88h8, 92,657,984 parameters

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 5.7860 | lr 2.70e-05 | grad 159.00 | tok/s 14911
step     20 | loss 5.2339 | lr 5.70e-05 | grad 51.00 | tok/s 27706
step     30 | loss 5.3432 | lr 8.70e-05 | grad 41.25 | tok/s 29302
step     40 | loss 4.6675 | lr 1.17e-04 | grad 25.88 | tok/s 29126
step     50 | loss 4.1639 | lr 1.47e-04 | grad 11.75 | tok/s 28974
step     60 | loss 3.8927 | lr 1.77e-04 | grad 10.19 | tok/s 28319
step     70 | loss 3.2497 | lr 2.07e-04 | grad 2.30 | tok/s 27244
step     80 | loss 3.4491 | lr 2.37e-04 | grad 4.47 | tok/s 28258
step     90 | loss 3.3774 | lr 2.67e-04 | grad 5.25 | tok/s 27167
step    100 | loss 3.0839 | lr 2.97e-04 | grad 2.50 | tok/s 27350
step    110 | loss 2.9990 | lr 6.94e-06 | grad 2.92 | tok/s 25519
step    120 | loss 3.3696 | lr 2.69e-05 | grad 2.56 | tok/s 25268
step    130 | loss 3.1213 | lr 5.89e-05 | grad 2.17 | tok/s 25982
step    140 | loss 2.8758 | lr 9.99e-05 | grad 1.67 | tok/s 26154
step    150 | loss 2.6913 | lr 1.46e-04 | grad 3.02 | tok/s 24914
step    160 | loss 2.5476 | lr 1.92e-04 | grad 2.50 | tok/s 25153
step    170 | loss 2.6699 | lr 2.35e-04 | grad 6.16 | tok/s 26011
step    180 | loss 2.6509 | lr 2.69e-04 | grad 2.45 | tok/s 26125
step    190 | loss 2.3671 | lr 2.91e-04 | grad 2.17 | tok/s 26522
step    200 | loss 1.9547 | lr 3.00e-04 | grad 1.93 | tok/s 27157
step    210 | loss 2.5240 | lr 2.94e-04 | grad 3.20 | tok/s 25873
step    220 | loss 2.3652 | lr 2.74e-04 | grad 1.76 | tok/s 26799
step    230 | loss 2.2041 | lr 2.42e-04 | grad 2.78 | tok/s 25858
step    240 | loss 2.2131 | lr 2.01e-04 | grad 2.77 | tok/s 26326
step    250 | loss 2.1626 | lr 1.55e-04 | grad 1.85 | tok/s 26054
step    260 | loss 2.1941 | lr 1.09e-04 | grad 1.29 | tok/s 25028
step    270 | loss 2.0426 | lr 6.65e-05 | grad 1.43 | tok/s 25907
step    280 | loss 1.9531 | lr 3.24e-05 | grad 2.67 | tok/s 25866
step    290 | loss 1.9574 | lr 9.84e-06 | grad 1.27 | tok/s 27209
step    300 | loss 1.9319 | lr 1.07e-06 | grad 1.27 | tok/s 27301
step    310 | loss 1.9357 | lr 6.94e-06 | grad 1.08 | tok/s 27225
step    320 | loss 2.0370 | lr 2.69e-05 | grad 2.50 | tok/s 26252
step    330 | loss 2.0565 | lr 5.89e-05 | grad 1.09 | tok/s 25630
step    340 | loss 2.0635 | lr 9.99e-05 | grad 2.91 | tok/s 26218
step    350 | loss 2.0886 | lr 1.46e-04 | grad 1.78 | tok/s 25385
step    360 | loss 2.0324 | lr 1.92e-04 | grad 3.31 | tok/s 25672
step    370 | loss 1.8563 | lr 2.35e-04 | grad 2.12 | tok/s 26282
step    380 | loss 2.3829 | lr 2.69e-04 | grad 2.05 | tok/s 26888
step    390 | loss 2.0119 | lr 2.91e-04 | grad 2.31 | tok/s 25898
step    400 | loss 2.1136 | lr 3.00e-04 | grad 4.03 | tok/s 26690
step    410 | loss 1.8216 | lr 2.94e-04 | grad 3.36 | tok/s 25809
step    420 | loss 1.9968 | lr 2.74e-04 | grad 2.16 | tok/s 25645
step    430 | loss 2.1234 | lr 2.42e-04 | grad 2.55 | tok/s 25597
step    440 | loss 2.1325 | lr 2.01e-04 | grad 1.75 | tok/s 26469
step    450 | loss 1.9090 | lr 1.55e-04 | grad 1.08 | tok/s 25836
step    460 | loss 1.8935 | lr 1.09e-04 | grad 1.16 | tok/s 25901
step    470 | loss 1.8395 | lr 6.65e-05 | grad 1.73 | tok/s 26055
step    480 | loss 1.7534 | lr 3.24e-05 | grad 0.92 | tok/s 25208
step    490 | loss 1.7225 | lr 9.84e-06 | grad 0.82 | tok/s 25675
step    500 | loss 2.7399 | lr 1.07e-06 | grad 1.44 | tok/s 26471
step    510 | loss 1.8043 | lr 6.94e-06 | grad 0.95 | tok/s 25864
step    520 | loss 1.8124 | lr 2.69e-05 | grad 0.82 | tok/s 26764
step    530 | loss 2.3010 | lr 5.89e-05 | grad 1.02 | tok/s 26227
step    540 | loss 1.7451 | lr 9.99e-05 | grad 1.50 | tok/s 26092
step    550 | loss 1.6392 | lr 1.46e-04 | grad 1.02 | tok/s 26813
step    560 | loss 1.4995 | lr 1.92e-04 | grad 1.24 | tok/s 27191
step    570 | loss 1.7912 | lr 2.35e-04 | grad 3.17 | tok/s 26593
step    580 | loss 2.1145 | lr 2.69e-04 | grad 1.55 | tok/s 26214
step    590 | loss 2.4452 | lr 2.91e-04 | grad 2.73 | tok/s 25794
step    600 | loss 1.9182 | lr 3.00e-04 | grad 2.03 | tok/s 25756
step    610 | loss 1.9330 | lr 2.94e-04 | grad 2.03 | tok/s 27021
step    620 | loss 1.8236 | lr 2.74e-04 | grad 1.55 | tok/s 25739
step    630 | loss 1.7274 | lr 2.42e-04 | grad 1.45 | tok/s 26585
step    640 | loss 2.0158 | lr 2.01e-04 | grad 1.29 | tok/s 26629
step    650 | loss 1.7772 | lr 1.55e-04 | grad 1.56 | tok/s 26074
step    660 | loss 2.0775 | lr 1.09e-04 | grad 6.84 | tok/s 25746
step    670 | loss 1.8838 | lr 6.65e-05 | grad 2.52 | tok/s 26680
step    680 | loss 1.8214 | lr 3.24e-05 | grad 1.48 | tok/s 25691
step    690 | loss 1.8655 | lr 9.84e-06 | grad 1.83 | tok/s 25850
step    700 | loss 1.9464 | lr 1.07e-06 | grad 1.75 | tok/s 26000
step    710 | loss 1.8929 | lr 6.94e-06 | grad 1.64 | tok/s 26218
step    720 | loss 2.0033 | lr 2.68e-05 | grad 2.22 | tok/s 26113
step    730 | loss 1.9246 | lr 5.89e-05 | grad 1.69 | tok/s 26326
step    740 | loss 1.8578 | lr 9.99e-05 | grad 2.48 | tok/s 26124
step    750 | loss 1.6659 | lr 1.46e-04 | grad 1.93 | tok/s 25835
step    760 | loss 2.0330 | lr 1.92e-04 | grad 1.18 | tok/s 26121
step    770 | loss 1.7477 | lr 2.35e-04 | grad 1.95 | tok/s 25993
step    780 | loss 1.7794 | lr 2.69e-04 | grad 1.67 | tok/s 26342
step    790 | loss 1.6924 | lr 2.91e-04 | grad 1.18 | tok/s 26363
step    800 | loss 1.7140 | lr 3.00e-04 | grad 1.56 | tok/s 26268
step    810 | loss 1.8140 | lr 2.94e-04 | grad 2.78 | tok/s 26064
step    820 | loss 2.5638 | lr 2.74e-04 | grad 2.44 | tok/s 26716
step    830 | loss 2.0699 | lr 2.42e-04 | grad 1.10 | tok/s 27296
step    840 | loss 1.7076 | lr 2.01e-04 | grad 0.90 | tok/s 27287
step    850 | loss 2.1376 | lr 1.55e-04 | grad 2.11 | tok/s 26027
step    860 | loss 1.9086 | lr 1.09e-04 | grad 1.62 | tok/s 25482
step    870 | loss 1.7985 | lr 6.65e-05 | grad 1.21 | tok/s 26280
step    880 | loss 1.8537 | lr 3.24e-05 | grad 1.63 | tok/s 26121
step    890 | loss 1.7686 | lr 9.84e-06 | grad 1.21 | tok/s 26110
step    900 | loss 2.1821 | lr 1.07e-06 | grad 1.18 | tok/s 25428
step    910 | loss 1.8053 | lr 6.94e-06 | grad 1.05 | tok/s 25890
step    920 | loss 1.7949 | lr 2.68e-05 | grad 1.06 | tok/s 25801
step    930 | loss 1.8998 | lr 5.89e-05 | grad 2.06 | tok/s 25711
step    940 | loss 1.8246 | lr 9.99e-05 | grad 1.95 | tok/s 25340
step    950 | loss 1.8424 | lr 1.46e-04 | grad 1.71 | tok/s 25922
step    960 | loss 1.6470 | lr 1.92e-04 | grad 1.00 | tok/s 27266
step    970 | loss 1.4398 | lr 2.35e-04 | grad 0.86 | tok/s 27089
step    980 | loss 1.5696 | lr 2.69e-04 | grad 2.17 | tok/s 26501
step    990 | loss 1.9105 | lr 2.91e-04 | grad 1.23 | tok/s 25744

Training complete! Final step: 996
