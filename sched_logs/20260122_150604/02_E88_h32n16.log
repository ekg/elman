# Job 2: E88_h32n16
# GPU: 2
# Command: python train.py --level E88f_h32n16 --dim 1536 --depth 20 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10.0 --output benchmark_results/e88_nstate_20260122_150556/E88_h32n16
# Started: 2026-01-22T15:06:04.906244
============================================================

Using device: cuda
Output directory: benchmark_results/e88_nstate_20260122_150556/E88_h32n16/levelE88f_h32n16_100m_20260122_150610
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E88f_h32n16, 64,324,672 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 3.8763 | lr 3.00e-04 | grad 6.34 | tok/s 24011
step     20 | loss 3.3054 | lr 3.00e-04 | grad 14.06 | tok/s 77541
step     30 | loss 4.7170 | lr 3.00e-04 | grad 9.00 | tok/s 81635
step     40 | loss 3.5418 | lr 3.00e-04 | grad 3.69 | tok/s 81758
step     50 | loss 2.7704 | lr 3.00e-04 | grad 2.52 | tok/s 81719
step     60 | loss 2.9227 | lr 3.00e-04 | grad 3.03 | tok/s 79008
step     70 | loss 2.5999 | lr 3.00e-04 | grad 2.25 | tok/s 76648
step     80 | loss 2.6933 | lr 3.00e-04 | grad 2.50 | tok/s 78879
step     90 | loss 2.6890 | lr 3.00e-04 | grad 8.62 | tok/s 76804
step    100 | loss 2.4016 | lr 3.00e-04 | grad 2.05 | tok/s 76598
step    110 | loss 2.3552 | lr 3.00e-04 | grad 2.25 | tok/s 73688
step    120 | loss 2.3599 | lr 3.00e-04 | grad 1.63 | tok/s 71847
step    130 | loss 2.3915 | lr 3.00e-04 | grad 1.66 | tok/s 74201
step    140 | loss 2.1921 | lr 3.00e-04 | grad 1.79 | tok/s 74430
step    150 | loss 2.0956 | lr 3.00e-04 | grad 2.75 | tok/s 70772
step    160 | loss 2.0196 | lr 3.00e-04 | grad 2.22 | tok/s 71269
step    170 | loss 2.2013 | lr 3.00e-04 | grad 9.00 | tok/s 73664
step    180 | loss 2.2039 | lr 3.00e-04 | grad 1.44 | tok/s 73473
step    190 | loss 1.9298 | lr 3.00e-04 | grad 2.16 | tok/s 77197
step    200 | loss 1.6765 | lr 3.00e-04 | grad 1.73 | tok/s 77533
step    210 | loss 2.1078 | lr 3.00e-04 | grad 2.81 | tok/s 74642
step    220 | loss 2.0381 | lr 3.00e-04 | grad 1.80 | tok/s 77411
step    230 | loss 1.9319 | lr 3.00e-04 | grad 3.30 | tok/s 75117
step    240 | loss 1.9550 | lr 3.00e-04 | grad 2.86 | tok/s 76570
step    250 | loss 1.9237 | lr 3.00e-04 | grad 2.20 | tok/s 75682
step    260 | loss 1.9828 | lr 3.00e-04 | grad 1.63 | tok/s 71901
step    270 | loss 1.8709 | lr 3.00e-04 | grad 1.95 | tok/s 74426
step    280 | loss 1.7755 | lr 3.00e-04 | grad 3.05 | tok/s 74397
step    290 | loss 1.6819 | lr 3.00e-04 | grad 1.91 | tok/s 78778
step    300 | loss 1.6094 | lr 3.00e-04 | grad 1.81 | tok/s 75882
step    310 | loss 1.5613 | lr 3.00e-04 | grad 1.63 | tok/s 76303
step    320 | loss 1.7486 | lr 3.00e-04 | grad 3.69 | tok/s 75261
step    330 | loss 1.8389 | lr 3.00e-04 | grad 2.02 | tok/s 73805
step    340 | loss 1.8474 | lr 3.00e-04 | grad 4.44 | tok/s 75280
step    350 | loss 1.8656 | lr 3.00e-04 | grad 1.93 | tok/s 73160
step    360 | loss 1.8267 | lr 3.00e-04 | grad 4.91 | tok/s 73517
step    370 | loss 1.6452 | lr 3.00e-04 | grad 1.78 | tok/s 75621
step    380 | loss 2.1061 | lr 3.00e-04 | grad 2.12 | tok/s 76804
step    390 | loss 1.7680 | lr 3.00e-04 | grad 1.72 | tok/s 74733
step    400 | loss 1.8435 | lr 3.00e-04 | grad 4.31 | tok/s 75693
step    410 | loss 1.5997 | lr 3.00e-04 | grad 3.58 | tok/s 74253
step    420 | loss 1.7414 | lr 3.00e-04 | grad 2.42 | tok/s 73007
step    430 | loss 1.8492 | lr 3.00e-04 | grad 2.58 | tok/s 73464
step    440 | loss 1.8798 | lr 3.00e-04 | grad 1.72 | tok/s 75920
step    450 | loss 1.7416 | lr 3.00e-04 | grad 1.80 | tok/s 74152
step    460 | loss 1.7363 | lr 3.00e-04 | grad 1.68 | tok/s 74039
step    470 | loss 1.7027 | lr 3.00e-04 | grad 2.17 | tok/s 75207
step    480 | loss 1.6361 | lr 3.00e-04 | grad 1.81 | tok/s 72001
step    490 | loss 1.6176 | lr 3.00e-04 | grad 1.60 | tok/s 73891
step    500 | loss 2.2806 | lr 3.00e-04 | grad 2.62 | tok/s 75373
step    510 | loss 1.5757 | lr 3.00e-04 | grad 1.66 | tok/s 74288
step    520 | loss 1.5910 | lr 3.00e-04 | grad 1.67 | tok/s 76293
step    530 | loss 2.1035 | lr 3.00e-04 | grad 1.77 | tok/s 75092
step    540 | loss 1.5584 | lr 3.00e-04 | grad 2.27 | tok/s 74423
step    550 | loss 1.5012 | lr 3.00e-04 | grad 1.79 | tok/s 77085
step    560 | loss 1.4074 | lr 3.00e-04 | grad 1.92 | tok/s 77415
step    570 | loss 1.6534 | lr 3.00e-04 | grad 3.64 | tok/s 76027
step    580 | loss 1.8869 | lr 3.00e-04 | grad 1.75 | tok/s 74817
step    590 | loss 2.0837 | lr 3.00e-04 | grad 2.34 | tok/s 73600
step    600 | loss 1.6453 | lr 3.00e-04 | grad 2.97 | tok/s 73694
step    610 | loss 1.6584 | lr 3.00e-04 | grad 1.87 | tok/s 77306
step    620 | loss 1.6007 | lr 3.00e-04 | grad 1.89 | tok/s 72963
step    630 | loss 1.5403 | lr 3.00e-04 | grad 1.79 | tok/s 75488
step    640 | loss 1.8109 | lr 3.00e-04 | grad 1.82 | tok/s 75543
step    650 | loss 1.6128 | lr 3.00e-04 | grad 2.42 | tok/s 74399
step    660 | loss 1.8438 | lr 3.00e-04 | grad 8.25 | tok/s 73234
step    670 | loss 1.7481 | lr 3.00e-04 | grad 3.03 | tok/s 76050
step    680 | loss 1.7037 | lr 3.00e-04 | grad 2.59 | tok/s 73111
step    690 | loss 1.7247 | lr 3.00e-04 | grad 3.06 | tok/s 73729
step    700 | loss 1.8103 | lr 3.00e-04 | grad 3.09 | tok/s 74232
step    710 | loss 1.6849 | lr 3.00e-04 | grad 2.16 | tok/s 72711
step    720 | loss 1.6745 | lr 3.00e-04 | grad 4.38 | tok/s 71974
step    730 | loss 1.7956 | lr 3.00e-04 | grad 2.86 | tok/s 74886
step    740 | loss 1.7495 | lr 3.00e-04 | grad 3.62 | tok/s 74280
step    750 | loss 1.5524 | lr 3.00e-04 | grad 2.94 | tok/s 73416
step    760 | loss 1.8339 | lr 3.00e-04 | grad 1.88 | tok/s 74554
step    770 | loss 1.5744 | lr 3.00e-04 | grad 2.59 | tok/s 73748
step    780 | loss 1.6380 | lr 3.00e-04 | grad 1.65 | tok/s 74797
step    790 | loss 1.5478 | lr 3.00e-04 | grad 1.73 | tok/s 75096
step    800 | loss 1.5236 | lr 3.00e-04 | grad 2.22 | tok/s 75445
step    810 | loss 1.5884 | lr 3.00e-04 | grad 7.81 | tok/s 74021
step    820 | loss 2.3183 | lr 3.00e-04 | grad 3.14 | tok/s 76591
step    830 | loss 2.0151 | lr 3.00e-04 | grad 2.33 | tok/s 77622
step    840 | loss 1.7603 | lr 3.00e-04 | grad 2.00 | tok/s 77898
step    850 | loss 1.7480 | lr 3.00e-04 | grad 2.12 | tok/s 73814
step    860 | loss 1.5801 | lr 3.00e-04 | grad 1.98 | tok/s 72745
step    870 | loss 1.5554 | lr 3.00e-04 | grad 2.08 | tok/s 74703
step    880 | loss 1.6277 | lr 3.00e-04 | grad 2.45 | tok/s 74276
step    890 | loss 1.5379 | lr 3.00e-04 | grad 1.99 | tok/s 74248
step    900 | loss 1.9277 | lr 3.00e-04 | grad 1.98 | tok/s 72448
step    910 | loss 1.5345 | lr 3.00e-04 | grad 1.91 | tok/s 73465
step    920 | loss 1.5963 | lr 3.00e-04 | grad 1.94 | tok/s 73554
step    930 | loss 1.6778 | lr 3.00e-04 | grad 3.27 | tok/s 73213
step    940 | loss 1.6118 | lr 3.00e-04 | grad 4.09 | tok/s 72878
step    950 | loss 1.6605 | lr 3.00e-04 | grad 2.44 | tok/s 73851
step    960 | loss 1.4634 | lr 3.00e-04 | grad 1.85 | tok/s 77981
step    970 | loss 1.3802 | lr 3.00e-04 | grad 1.55 | tok/s 77637
step    980 | loss 1.4885 | lr 3.00e-04 | grad 2.70 | tok/s 75726
step    990 | loss 1.6954 | lr 3.00e-04 | grad 1.79 | tok/s 73460
step   1000 | loss 1.6460 | lr 3.00e-04 | grad 1.66 | tok/s 72049
  >>> saved checkpoint: checkpoint_step_001000_loss_1.6460.pt
step   1010 | loss 1.7864 | lr 3.00e-04 | grad 1.85 | tok/s 61267
step   1020 | loss 1.4781 | lr 3.00e-04 | grad 1.88 | tok/s 73980
step   1030 | loss 1.7851 | lr 3.00e-04 | grad 1.90 | tok/s 72613
step   1040 | loss 1.5509 | lr 3.00e-04 | grad 2.58 | tok/s 74139
step   1050 | loss 1.5686 | lr 3.00e-04 | grad 2.08 | tok/s 74464
step   1060 | loss 1.7012 | lr 3.00e-04 | grad 2.70 | tok/s 74534
step   1070 | loss 1.8009 | lr 3.00e-04 | grad 2.28 | tok/s 75156
step   1080 | loss 2.1505 | lr 3.00e-04 | grad 2.55 | tok/s 73778
step   1090 | loss 1.9100 | lr 3.00e-04 | grad 2.75 | tok/s 74729
step   1100 | loss 1.6109 | lr 3.00e-04 | grad 2.30 | tok/s 73735
step   1110 | loss 1.5461 | lr 3.00e-04 | grad 2.36 | tok/s 75132
step   1120 | loss 1.6564 | lr 3.00e-04 | grad 1.81 | tok/s 75776
step   1130 | loss 1.6631 | lr 3.00e-04 | grad 2.02 | tok/s 72608
step   1140 | loss 1.5222 | lr 3.00e-04 | grad 2.03 | tok/s 74006
step   1150 | loss 1.8287 | lr 3.00e-04 | grad 2.75 | tok/s 74236
step   1160 | loss 1.4957 | lr 3.00e-04 | grad 1.71 | tok/s 73018
step   1170 | loss 1.6627 | lr 3.00e-04 | grad 2.19 | tok/s 74298
step   1180 | loss 1.5660 | lr 3.00e-04 | grad 1.77 | tok/s 77879
step   1190 | loss 1.4742 | lr 3.00e-04 | grad 1.70 | tok/s 78027
step   1200 | loss 1.4054 | lr 3.00e-04 | grad 1.76 | tok/s 77700
step   1210 | loss 1.3883 | lr 3.00e-04 | grad 1.92 | tok/s 77846
step   1220 | loss 1.4033 | lr 3.00e-04 | grad 2.11 | tok/s 77564
step   1230 | loss 1.4403 | lr 3.00e-04 | grad 1.79 | tok/s 74734
step   1240 | loss 1.5495 | lr 3.00e-04 | grad 1.86 | tok/s 73200
step   1250 | loss 1.6498 | lr 3.00e-04 | grad 9.25 | tok/s 75277
step   1260 | loss 1.7009 | lr 3.00e-04 | grad 5.25 | tok/s 75106
step   1270 | loss 1.7508 | lr 3.00e-04 | grad 2.95 | tok/s 74503
step   1280 | loss 1.6097 | lr 3.00e-04 | grad 2.45 | tok/s 73082
step   1290 | loss 1.5628 | lr 3.00e-04 | grad 1.97 | tok/s 73414
step   1300 | loss 1.6320 | lr 3.00e-04 | grad 2.06 | tok/s 72357
step   1310 | loss 1.6963 | lr 3.00e-04 | grad 1.95 | tok/s 73184
step   1320 | loss 1.6712 | lr 3.00e-04 | grad 2.73 | tok/s 73941
step   1330 | loss 1.5600 | lr 3.00e-04 | grad 2.12 | tok/s 74502
step   1340 | loss 1.4728 | lr 3.00e-04 | grad 1.97 | tok/s 74497
step   1350 | loss 1.5262 | lr 3.00e-04 | grad 3.41 | tok/s 76177
step   1360 | loss 1.5073 | lr 3.00e-04 | grad 2.03 | tok/s 72501
step   1370 | loss 1.6056 | lr 3.00e-04 | grad 1.80 | tok/s 73845
step   1380 | loss 1.6649 | lr 3.00e-04 | grad 2.27 | tok/s 74404
step   1390 | loss 1.6045 | lr 3.00e-04 | grad 3.81 | tok/s 73000
step   1400 | loss 1.5520 | lr 3.00e-04 | grad 6.06 | tok/s 75500
step   1410 | loss 1.5568 | lr 3.00e-04 | grad 2.09 | tok/s 76253
step   1420 | loss 1.6042 | lr 3.00e-04 | grad 1.91 | tok/s 72552
step   1430 | loss 1.4994 | lr 3.00e-04 | grad 1.90 | tok/s 70615
step   1440 | loss 1.4120 | lr 3.00e-04 | grad 1.67 | tok/s 71719
step   1450 | loss 1.4320 | lr 3.00e-04 | grad 3.12 | tok/s 76163
step   1460 | loss 1.5408 | lr 3.00e-04 | grad 2.05 | tok/s 71017
step   1470 | loss 1.6312 | lr 3.00e-04 | grad 4.19 | tok/s 73885
step   1480 | loss 1.5096 | lr 3.00e-04 | grad 4.66 | tok/s 74643
step   1490 | loss 1.6436 | lr 3.00e-04 | grad 4.50 | tok/s 74523
step   1500 | loss 1.7553 | lr 3.00e-04 | grad 5.38 | tok/s 72345
step   1510 | loss 1.6241 | lr 3.00e-04 | grad 2.44 | tok/s 76089
step   1520 | loss 1.5968 | lr 3.00e-04 | grad 2.38 | tok/s 75440
step   1530 | loss 1.5637 | lr 3.00e-04 | grad 1.86 | tok/s 75127
step   1540 | loss 1.5320 | lr 3.00e-04 | grad 1.79 | tok/s 73706
step   1550 | loss 1.4969 | lr 3.00e-04 | grad 6.59 | tok/s 76484
step   1560 | loss 2.0202 | lr 3.00e-04 | grad 2.64 | tok/s 74520
step   1570 | loss 1.5494 | lr 3.00e-04 | grad 2.27 | tok/s 73026
step   1580 | loss 1.6678 | lr 3.00e-04 | grad 2.56 | tok/s 75421
step   1590 | loss 1.4584 | lr 3.00e-04 | grad 1.80 | tok/s 73482
step   1600 | loss 1.5772 | lr 3.00e-04 | grad 1.70 | tok/s 72370
step   1610 | loss 1.3988 | lr 3.00e-04 | grad 1.87 | tok/s 76313
step   1620 | loss 1.5761 | lr 3.00e-04 | grad 1.77 | tok/s 74242
step   1630 | loss 1.5531 | lr 3.00e-04 | grad 1.73 | tok/s 76009
step   1640 | loss 1.5006 | lr 3.00e-04 | grad 1.82 | tok/s 73455
step   1650 | loss 1.5519 | lr 3.00e-04 | grad 3.78 | tok/s 72108
step   1660 | loss 1.5634 | lr 3.00e-04 | grad 2.05 | tok/s 72944
step   1670 | loss 1.5676 | lr 3.00e-04 | grad 3.12 | tok/s 75671
step   1680 | loss 1.9292 | lr 3.00e-04 | grad 1.74 | tok/s 76189
step   1690 | loss 1.5261 | lr 3.00e-04 | grad 2.30 | tok/s 73886
step   1700 | loss 1.6559 | lr 3.00e-04 | grad 1.85 | tok/s 75668
step   1710 | loss 1.6135 | lr 3.00e-04 | grad 3.12 | tok/s 73307
step   1720 | loss 1.5365 | lr 3.00e-04 | grad 2.12 | tok/s 73829
step   1730 | loss 1.7324 | lr 3.00e-04 | grad 2.47 | tok/s 73422
step   1740 | loss 1.5639 | lr 3.00e-04 | grad 1.55 | tok/s 75070
step   1750 | loss 1.5431 | lr 3.00e-04 | grad 1.97 | tok/s 71841
step   1760 | loss 1.7003 | lr 3.00e-04 | grad 1.90 | tok/s 73519
step   1770 | loss 1.6420 | lr 3.00e-04 | grad 1.77 | tok/s 75011
step   1780 | loss 1.5571 | lr 3.00e-04 | grad 2.58 | tok/s 72308
step   1790 | loss 1.7314 | lr 3.00e-04 | grad 2.14 | tok/s 73469
step   1800 | loss 1.4645 | lr 3.00e-04 | grad 1.58 | tok/s 75020
step   1810 | loss 1.5613 | lr 3.00e-04 | grad 1.95 | tok/s 74773
step   1820 | loss 1.4393 | lr 3.00e-04 | grad 1.63 | tok/s 73967
step   1830 | loss 1.5241 | lr 3.00e-04 | grad 2.11 | tok/s 72819
step   1840 | loss 1.5087 | lr 3.00e-04 | grad 2.02 | tok/s 73058
step   1850 | loss 1.7481 | lr 3.00e-04 | grad 2.52 | tok/s 73285
step   1860 | loss 1.5006 | lr 3.00e-04 | grad 1.64 | tok/s 66826
step   1870 | loss 1.4892 | lr 3.00e-04 | grad 2.72 | tok/s 75025
step   1880 | loss 1.4997 | lr 3.00e-04 | grad 1.70 | tok/s 75382
step   1890 | loss 1.6226 | lr 3.00e-04 | grad 1.79 | tok/s 73735
step   1900 | loss 1.5655 | lr 3.00e-04 | grad 1.73 | tok/s 74979
step   1910 | loss 1.6479 | lr 3.00e-04 | grad 3.94 | tok/s 73680
step   1920 | loss 1.4484 | lr 3.00e-04 | grad 1.73 | tok/s 76243
step   1930 | loss 1.4806 | lr 3.00e-04 | grad 2.05 | tok/s 75201
step   1940 | loss 1.4706 | lr 3.00e-04 | grad 2.30 | tok/s 76800
step   1950 | loss 1.5441 | lr 3.00e-04 | grad 1.89 | tok/s 74518
step   1960 | loss 1.7714 | lr 3.00e-04 | grad 6.06 | tok/s 76383
step   1970 | loss 1.4255 | lr 3.00e-04 | grad 2.06 | tok/s 73541
step   1980 | loss 1.5488 | lr 3.00e-04 | grad 3.83 | tok/s 73662
step   1990 | loss 1.6206 | lr 3.00e-04 | grad 2.58 | tok/s 75677
step   2000 | loss 1.5199 | lr 3.00e-04 | grad 2.41 | tok/s 76047
  >>> saved checkpoint: checkpoint_step_002000_loss_1.5199.pt
step   2010 | loss 1.3382 | lr 3.00e-04 | grad 1.65 | tok/s 61788
step   2020 | loss 1.2385 | lr 3.00e-04 | grad 1.54 | tok/s 78096
step   2030 | loss 1.5064 | lr 3.00e-04 | grad 1.71 | tok/s 77503
step   2040 | loss 1.3584 | lr 3.00e-04 | grad 1.66 | tok/s 78008
step   2050 | loss 1.3426 | lr 3.00e-04 | grad 2.36 | tok/s 77469
step   2060 | loss 1.6027 | lr 3.00e-04 | grad 1.99 | tok/s 73356
step   2070 | loss 1.5374 | lr 3.00e-04 | grad 2.17 | tok/s 77392
step   2080 | loss 1.5882 | lr 3.00e-04 | grad 3.50 | tok/s 73058
step   2090 | loss 1.5313 | lr 3.00e-04 | grad 1.91 | tok/s 75972
step   2100 | loss 1.5396 | lr 3.00e-04 | grad 2.20 | tok/s 73054
step   2110 | loss 1.3029 | lr 3.00e-04 | grad 1.66 | tok/s 76471
step   2120 | loss 1.3725 | lr 3.00e-04 | grad 2.39 | tok/s 74989
step   2130 | loss 1.5183 | lr 3.00e-04 | grad 4.41 | tok/s 73475
step   2140 | loss 1.5827 | lr 3.00e-04 | grad 4.06 | tok/s 72853
step   2150 | loss 1.5800 | lr 3.00e-04 | grad 1.81 | tok/s 74275
step   2160 | loss 1.5755 | lr 3.00e-04 | grad 1.80 | tok/s 73044
step   2170 | loss 1.6699 | lr 3.00e-04 | grad 2.09 | tok/s 74405
step   2180 | loss 1.5112 | lr 3.00e-04 | grad 1.87 | tok/s 74893
step   2190 | loss 1.7525 | lr 3.00e-04 | grad 2.06 | tok/s 76081
step   2200 | loss 1.2885 | lr 3.00e-04 | grad 1.57 | tok/s 77920
step   2210 | loss 1.3102 | lr 3.00e-04 | grad 1.71 | tok/s 77693
step   2220 | loss 1.2887 | lr 3.00e-04 | grad 1.86 | tok/s 77780
step   2230 | loss 1.4593 | lr 3.00e-04 | grad 2.23 | tok/s 75494
step   2240 | loss 1.5823 | lr 3.00e-04 | grad 2.69 | tok/s 77066
step   2250 | loss 1.5447 | lr 3.00e-04 | grad 2.28 | tok/s 76808
step   2260 | loss 1.6292 | lr 3.00e-04 | grad 2.38 | tok/s 74414
step   2270 | loss 1.4758 | lr 3.00e-04 | grad 2.83 | tok/s 73851
step   2280 | loss 1.6425 | lr 3.00e-04 | grad 1.83 | tok/s 73492
step   2290 | loss 1.4787 | lr 3.00e-04 | grad 2.14 | tok/s 74894
step   2300 | loss 1.4734 | lr 3.00e-04 | grad 1.89 | tok/s 71027
step   2310 | loss 1.5068 | lr 3.00e-04 | grad 1.90 | tok/s 74383
step   2320 | loss 1.4854 | lr 3.00e-04 | grad 1.87 | tok/s 75041
step   2330 | loss 1.4146 | lr 3.00e-04 | grad 1.54 | tok/s 75940
step   2340 | loss 1.4723 | lr 3.00e-04 | grad 2.38 | tok/s 74872
step   2350 | loss 1.5821 | lr 3.00e-04 | grad 1.85 | tok/s 72223
step   2360 | loss 1.9672 | lr 3.00e-04 | grad 3.20 | tok/s 73747
step   2370 | loss 1.5384 | lr 3.00e-04 | grad 1.94 | tok/s 72850
step   2380 | loss 1.4484 | lr 3.00e-04 | grad 2.41 | tok/s 72681
step   2390 | loss 1.4791 | lr 3.00e-04 | grad 2.27 | tok/s 72013
step   2400 | loss 1.5941 | lr 3.00e-04 | grad 2.34 | tok/s 71548
step   2410 | loss 1.4997 | lr 3.00e-04 | grad 2.11 | tok/s 72115
step   2420 | loss 1.5979 | lr 3.00e-04 | grad 2.23 | tok/s 72875
step   2430 | loss 1.5172 | lr 3.00e-04 | grad 2.72 | tok/s 71995
step   2440 | loss 1.5884 | lr 3.00e-04 | grad 1.80 | tok/s 74032
step   2450 | loss 1.4088 | lr 3.00e-04 | grad 2.88 | tok/s 72746
step   2460 | loss 1.6086 | lr 3.00e-04 | grad 3.33 | tok/s 74444
step   2470 | loss 1.3980 | lr 3.00e-04 | grad 2.47 | tok/s 76134
step   2480 | loss 1.4407 | lr 3.00e-04 | grad 2.00 | tok/s 74307
step   2490 | loss 1.4182 | lr 3.00e-04 | grad 2.02 | tok/s 71676
step   2500 | loss 1.5270 | lr 3.00e-04 | grad 2.53 | tok/s 73742
step   2510 | loss 1.5539 | lr 3.00e-04 | grad 2.27 | tok/s 73873
step   2520 | loss 1.7027 | lr 3.00e-04 | grad 2.75 | tok/s 71389
step   2530 | loss 1.5940 | lr 3.00e-04 | grad 2.97 | tok/s 71455
step   2540 | loss 1.5436 | lr 3.00e-04 | grad 2.48 | tok/s 71682
step   2550 | loss 1.9656 | lr 3.00e-04 | grad 3.41 | tok/s 72915
step   2560 | loss 1.6035 | lr 3.00e-04 | grad 2.02 | tok/s 74147
step   2570 | loss 1.5496 | lr 3.00e-04 | grad 3.62 | tok/s 71092
step   2580 | loss 1.5916 | lr 3.00e-04 | grad 1.73 | tok/s 73510
step   2590 | loss 1.4115 | lr 3.00e-04 | grad 2.30 | tok/s 72444
step   2600 | loss 1.5553 | lr 3.00e-04 | grad 1.71 | tok/s 73832
step   2610 | loss 1.4903 | lr 3.00e-04 | grad 1.81 | tok/s 74451
step   2620 | loss 1.4089 | lr 3.00e-04 | grad 1.92 | tok/s 76331
step   2630 | loss 1.4598 | lr 3.00e-04 | grad 1.70 | tok/s 73828
step   2640 | loss 1.5402 | lr 3.00e-04 | grad 2.55 | tok/s 72252
step   2650 | loss 1.4801 | lr 3.00e-04 | grad 2.36 | tok/s 69719
step   2660 | loss 1.8549 | lr 3.00e-04 | grad 2.72 | tok/s 75087
step   2670 | loss 1.4094 | lr 3.00e-04 | grad 1.52 | tok/s 74361
step   2680 | loss 1.3888 | lr 3.00e-04 | grad 1.77 | tok/s 76200
step   2690 | loss 1.3388 | lr 3.00e-04 | grad 1.59 | tok/s 76057
step   2700 | loss 1.5180 | lr 3.00e-04 | grad 2.17 | tok/s 73431
step   2710 | loss 1.6130 | lr 3.00e-04 | grad 3.05 | tok/s 70756
step   2720 | loss 1.8433 | lr 3.00e-04 | grad 2.19 | tok/s 75109
step   2730 | loss 1.5634 | lr 3.00e-04 | grad 2.25 | tok/s 72879
step   2740 | loss 1.3745 | lr 3.00e-04 | grad 2.72 | tok/s 72418
step   2750 | loss 1.5768 | lr 3.00e-04 | grad 2.02 | tok/s 73635
step   2760 | loss 1.6752 | lr 3.00e-04 | grad 3.62 | tok/s 70449
step   2770 | loss 1.5180 | lr 3.00e-04 | grad 1.98 | tok/s 73754
step   2780 | loss 1.5045 | lr 3.00e-04 | grad 1.98 | tok/s 70860
step   2790 | loss 1.3750 | lr 3.00e-04 | grad 1.89 | tok/s 73007
step   2800 | loss 1.7865 | lr 3.00e-04 | grad 3.97 | tok/s 72746
step   2810 | loss 1.5423 | lr 3.00e-04 | grad 2.06 | tok/s 74103

Training complete! Final step: 2813
