# Job 1: mamba2
# GPU: 1
# Command: python train.py --level mamba2 --dim 896 --depth 20 --data data/pile.txt --batch_size 32 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10.0 --output benchmark_results/e88_nstate_20260122_150556/mamba2
# Started: 2026-01-22T15:06:04.906628
============================================================

Using device: cuda
Output directory: benchmark_results/e88_nstate_20260122_150556/mamba2/levelmamba2_100m_20260122_150610
Model: Level mamba2, 101,936,528 parameters
Using schedule-free AdamW (lr=0.0003)

Starting training from step 0...
Batch size: 32, Chunk size: 512
Gradient accumulation: 1, Effective batch: 32

Time-based training: 10.0 minutes
step     10 | loss 3.2654 | lr 3.00e-04 | grad 3.05 | tok/s 9768
step     20 | loss 2.7133 | lr 3.00e-04 | grad 6.91 | tok/s 67915
step     30 | loss 3.5857 | lr 3.00e-04 | grad 1.58 | tok/s 71442
step     40 | loss 2.4253 | lr 3.00e-04 | grad 1.29 | tok/s 71267
step     50 | loss 2.0542 | lr 3.00e-04 | grad 1.27 | tok/s 70918
step     60 | loss 2.3645 | lr 3.00e-04 | grad 1.80 | tok/s 69293
step     70 | loss 2.0896 | lr 3.00e-04 | grad 1.73 | tok/s 66650
step     80 | loss 2.4013 | lr 3.00e-04 | grad 2.42 | tok/s 69106
step     90 | loss 2.2560 | lr 3.00e-04 | grad 3.95 | tok/s 66493
step    100 | loss 1.9162 | lr 3.00e-04 | grad 1.14 | tok/s 66940
step    110 | loss 1.8966 | lr 3.00e-04 | grad 1.95 | tok/s 65756
step    120 | loss 1.9426 | lr 3.00e-04 | grad 1.26 | tok/s 64643
step    130 | loss 1.9611 | lr 3.00e-04 | grad 1.12 | tok/s 65953
step    140 | loss 1.8302 | lr 3.00e-04 | grad 1.31 | tok/s 65869
step    150 | loss 1.7284 | lr 3.00e-04 | grad 2.02 | tok/s 62765
step    160 | loss 1.7108 | lr 3.00e-04 | grad 1.77 | tok/s 63218
step    170 | loss 1.8595 | lr 3.00e-04 | grad 4.31 | tok/s 65184
step    180 | loss 1.8508 | lr 3.00e-04 | grad 1.05 | tok/s 65277
step    190 | loss 1.5847 | lr 3.00e-04 | grad 1.45 | tok/s 66188
step    200 | loss 1.3086 | lr 3.00e-04 | grad 1.20 | tok/s 67637
step    210 | loss 1.8379 | lr 3.00e-04 | grad 2.06 | tok/s 64714
step    220 | loss 1.7131 | lr 3.00e-04 | grad 1.30 | tok/s 66819
step    230 | loss 1.6560 | lr 3.00e-04 | grad 2.33 | tok/s 64563
step    240 | loss 1.6566 | lr 3.00e-04 | grad 2.14 | tok/s 65763
step    250 | loss 1.6449 | lr 3.00e-04 | grad 1.49 | tok/s 64832
step    260 | loss 1.7335 | lr 3.00e-04 | grad 1.21 | tok/s 62128
step    270 | loss 1.6332 | lr 3.00e-04 | grad 1.20 | tok/s 64300
step    280 | loss 1.5238 | lr 3.00e-04 | grad 2.00 | tok/s 64270
step    290 | loss 1.4291 | lr 3.00e-04 | grad 1.09 | tok/s 67649
step    300 | loss 1.3789 | lr 3.00e-04 | grad 1.11 | tok/s 67560
step    310 | loss 1.3382 | lr 3.00e-04 | grad 0.94 | tok/s 67454
step    320 | loss 1.5123 | lr 3.00e-04 | grad 1.91 | tok/s 64973
step    330 | loss 1.6212 | lr 3.00e-04 | grad 1.31 | tok/s 63343
step    340 | loss 1.6379 | lr 3.00e-04 | grad 3.06 | tok/s 64718
step    350 | loss 1.6432 | lr 3.00e-04 | grad 1.26 | tok/s 62887
step    360 | loss 1.5925 | lr 3.00e-04 | grad 3.11 | tok/s 63640
step    370 | loss 1.4121 | lr 3.00e-04 | grad 1.15 | tok/s 64811
step    380 | loss 1.8376 | lr 3.00e-04 | grad 1.29 | tok/s 66340
step    390 | loss 1.5353 | lr 3.00e-04 | grad 1.12 | tok/s 63893
step    400 | loss 1.6263 | lr 3.00e-04 | grad 2.86 | tok/s 65480
step    410 | loss 1.3930 | lr 3.00e-04 | grad 2.58 | tok/s 63636
step    420 | loss 1.5280 | lr 3.00e-04 | grad 1.39 | tok/s 63094
step    430 | loss 1.6162 | lr 3.00e-04 | grad 1.76 | tok/s 62941
step    440 | loss 1.6382 | lr 3.00e-04 | grad 1.09 | tok/s 65392
step    450 | loss 1.5576 | lr 3.00e-04 | grad 1.11 | tok/s 63588
step    460 | loss 1.5301 | lr 3.00e-04 | grad 0.99 | tok/s 63786
step    470 | loss 1.5092 | lr 3.00e-04 | grad 1.46 | tok/s 64588
step    480 | loss 1.4493 | lr 3.00e-04 | grad 0.92 | tok/s 62360
step    490 | loss 1.4505 | lr 3.00e-04 | grad 1.02 | tok/s 63475
step    500 | loss 1.9455 | lr 3.00e-04 | grad 1.76 | tok/s 65454
step    510 | loss 1.4008 | lr 3.00e-04 | grad 1.01 | tok/s 64105
step    520 | loss 1.4115 | lr 3.00e-04 | grad 1.09 | tok/s 66056
step    530 | loss 1.9332 | lr 3.00e-04 | grad 1.12 | tok/s 64605
step    540 | loss 1.3635 | lr 3.00e-04 | grad 1.61 | tok/s 64693
step    550 | loss 1.3457 | lr 3.00e-04 | grad 1.08 | tok/s 66402
step    560 | loss 1.2557 | lr 3.00e-04 | grad 0.84 | tok/s 67176
step    570 | loss 1.4693 | lr 3.00e-04 | grad 2.47 | tok/s 65661
step    580 | loss 1.6961 | lr 3.00e-04 | grad 1.19 | tok/s 64865
step    590 | loss 1.8580 | lr 3.00e-04 | grad 1.13 | tok/s 63623
step    600 | loss 1.4630 | lr 3.00e-04 | grad 1.79 | tok/s 63713
step    610 | loss 1.4645 | lr 3.00e-04 | grad 1.20 | tok/s 66812
step    620 | loss 1.4344 | lr 3.00e-04 | grad 1.18 | tok/s 63515
step    630 | loss 1.3758 | lr 3.00e-04 | grad 1.15 | tok/s 65472
step    640 | loss 1.5507 | lr 3.00e-04 | grad 1.16 | tok/s 65491
step    650 | loss 1.4299 | lr 3.00e-04 | grad 1.38 | tok/s 64194
step    660 | loss 1.6398 | lr 3.00e-04 | grad 4.81 | tok/s 63416
step    670 | loss 1.5720 | lr 3.00e-04 | grad 1.92 | tok/s 65502
step    680 | loss 1.5078 | lr 3.00e-04 | grad 1.63 | tok/s 63314
step    690 | loss 1.5128 | lr 3.00e-04 | grad 1.55 | tok/s 63889
step    700 | loss 1.6198 | lr 3.00e-04 | grad 2.58 | tok/s 64146
step    710 | loss 1.4935 | lr 3.00e-04 | grad 1.34 | tok/s 64520
step    720 | loss 1.4620 | lr 3.00e-04 | grad 4.91 | tok/s 64269
step    730 | loss 1.6267 | lr 3.00e-04 | grad 1.96 | tok/s 64845
step    740 | loss 1.5699 | lr 3.00e-04 | grad 2.64 | tok/s 64258
step    750 | loss 1.3886 | lr 3.00e-04 | grad 1.66 | tok/s 63512
step    760 | loss 1.6502 | lr 3.00e-04 | grad 1.12 | tok/s 64306
step    770 | loss 1.4165 | lr 3.00e-04 | grad 1.48 | tok/s 63937
step    780 | loss 1.4903 | lr 3.00e-04 | grad 1.03 | tok/s 64673
step    790 | loss 1.3786 | lr 3.00e-04 | grad 0.96 | tok/s 65225
step    800 | loss 1.3627 | lr 3.00e-04 | grad 1.41 | tok/s 65280
step    810 | loss 1.4100 | lr 3.00e-04 | grad 3.72 | tok/s 64712
step    820 | loss 2.1583 | lr 3.00e-04 | grad 1.62 | tok/s 66285
step    830 | loss 1.8473 | lr 3.00e-04 | grad 1.36 | tok/s 67235
step    840 | loss 1.5684 | lr 3.00e-04 | grad 1.23 | tok/s 67197
step    850 | loss 1.5642 | lr 3.00e-04 | grad 1.35 | tok/s 64062
step    860 | loss 1.4427 | lr 3.00e-04 | grad 1.20 | tok/s 62749
step    870 | loss 1.3999 | lr 3.00e-04 | grad 1.48 | tok/s 64704
step    880 | loss 1.4550 | lr 3.00e-04 | grad 1.48 | tok/s 64329
step    890 | loss 1.3877 | lr 3.00e-04 | grad 1.28 | tok/s 64329
step    900 | loss 1.7242 | lr 3.00e-04 | grad 1.33 | tok/s 62601
step    910 | loss 1.3642 | lr 3.00e-04 | grad 1.31 | tok/s 63790
step    920 | loss 1.4597 | lr 3.00e-04 | grad 1.17 | tok/s 63445
step    930 | loss 1.5094 | lr 3.00e-04 | grad 1.97 | tok/s 63282
step    940 | loss 1.4271 | lr 3.00e-04 | grad 2.48 | tok/s 62692
step    950 | loss 1.4847 | lr 3.00e-04 | grad 1.47 | tok/s 64207
step    960 | loss 1.3134 | lr 3.00e-04 | grad 1.16 | tok/s 67320
step    970 | loss 1.2409 | lr 3.00e-04 | grad 0.79 | tok/s 67397
step    980 | loss 1.3305 | lr 3.00e-04 | grad 1.66 | tok/s 65591
step    990 | loss 1.5200 | lr 3.00e-04 | grad 1.16 | tok/s 63830
step   1000 | loss 1.4823 | lr 3.00e-04 | grad 1.04 | tok/s 62359
  >>> saved checkpoint: checkpoint_step_001000_loss_1.4823.pt
step   1010 | loss 1.5836 | lr 3.00e-04 | grad 1.27 | tok/s 50848
step   1020 | loss 1.3137 | lr 3.00e-04 | grad 1.14 | tok/s 64222
step   1030 | loss 1.6599 | lr 3.00e-04 | grad 1.21 | tok/s 62986
step   1040 | loss 1.4117 | lr 3.00e-04 | grad 1.57 | tok/s 64346
step   1050 | loss 1.4192 | lr 3.00e-04 | grad 1.16 | tok/s 64597
step   1060 | loss 1.4817 | lr 3.00e-04 | grad 2.53 | tok/s 64739
step   1070 | loss 1.6120 | lr 3.00e-04 | grad 1.39 | tok/s 64906
step   1080 | loss 2.0065 | lr 3.00e-04 | grad 1.74 | tok/s 64140
step   1090 | loss 1.7378 | lr 3.00e-04 | grad 1.72 | tok/s 64605
step   1100 | loss 1.4590 | lr 3.00e-04 | grad 1.22 | tok/s 64189
step   1110 | loss 1.3785 | lr 3.00e-04 | grad 1.49 | tok/s 65213
step   1120 | loss 1.4370 | lr 3.00e-04 | grad 1.16 | tok/s 66055
step   1130 | loss 1.5218 | lr 3.00e-04 | grad 1.30 | tok/s 62765
step   1140 | loss 1.3791 | lr 3.00e-04 | grad 1.15 | tok/s 64251
step   1150 | loss 1.6478 | lr 3.00e-04 | grad 1.94 | tok/s 64155
step   1160 | loss 1.3584 | lr 3.00e-04 | grad 1.05 | tok/s 63451
step   1170 | loss 1.5325 | lr 3.00e-04 | grad 1.41 | tok/s 64169
step   1180 | loss 1.4254 | lr 3.00e-04 | grad 1.09 | tok/s 67460
step   1190 | loss 1.3287 | lr 3.00e-04 | grad 1.01 | tok/s 67476
step   1200 | loss 1.2685 | lr 3.00e-04 | grad 1.02 | tok/s 67506
step   1210 | loss 1.2529 | lr 3.00e-04 | grad 1.10 | tok/s 67429
step   1220 | loss 1.2697 | lr 3.00e-04 | grad 1.45 | tok/s 66922
step   1230 | loss 1.3272 | lr 3.00e-04 | grad 1.12 | tok/s 64730
step   1240 | loss 1.4145 | lr 3.00e-04 | grad 1.10 | tok/s 63456
step   1250 | loss 1.5018 | lr 3.00e-04 | grad 5.38 | tok/s 65364
step   1260 | loss 1.5139 | lr 3.00e-04 | grad 3.31 | tok/s 65279
step   1270 | loss 1.5767 | lr 3.00e-04 | grad 1.71 | tok/s 64570
step   1280 | loss 1.4517 | lr 3.00e-04 | grad 1.66 | tok/s 63764
step   1290 | loss 1.4155 | lr 3.00e-04 | grad 1.06 | tok/s 63596
step   1300 | loss 1.4756 | lr 3.00e-04 | grad 1.13 | tok/s 63130
step   1310 | loss 1.5186 | lr 3.00e-04 | grad 1.13 | tok/s 63109
step   1320 | loss 1.5117 | lr 3.00e-04 | grad 1.62 | tok/s 64242
step   1330 | loss 1.3943 | lr 3.00e-04 | grad 1.18 | tok/s 64329
step   1340 | loss 1.3110 | lr 3.00e-04 | grad 1.12 | tok/s 64296
step   1350 | loss 1.3331 | lr 3.00e-04 | grad 1.89 | tok/s 65995
step   1360 | loss 1.3594 | lr 3.00e-04 | grad 1.23 | tok/s 62742
step   1370 | loss 1.4683 | lr 3.00e-04 | grad 1.16 | tok/s 63762
step   1380 | loss 1.5054 | lr 3.00e-04 | grad 1.46 | tok/s 64248
step   1390 | loss 1.4337 | lr 3.00e-04 | grad 2.42 | tok/s 62658
step   1400 | loss 1.3824 | lr 3.00e-04 | grad 4.41 | tok/s 65293
step   1410 | loss 1.3967 | lr 3.00e-04 | grad 1.30 | tok/s 65860
step   1420 | loss 1.4410 | lr 3.00e-04 | grad 1.16 | tok/s 62702
step   1430 | loss 1.3507 | lr 3.00e-04 | grad 1.27 | tok/s 61223
step   1440 | loss 1.2766 | lr 3.00e-04 | grad 1.00 | tok/s 64641
step   1450 | loss 1.2734 | lr 3.00e-04 | grad 1.72 | tok/s 65943
step   1460 | loss 1.4077 | lr 3.00e-04 | grad 1.12 | tok/s 61623
step   1470 | loss 1.4658 | lr 3.00e-04 | grad 2.89 | tok/s 63811
step   1480 | loss 1.3615 | lr 3.00e-04 | grad 3.16 | tok/s 64432
step   1490 | loss 1.4967 | lr 3.00e-04 | grad 2.77 | tok/s 64412
step   1500 | loss 1.6000 | lr 3.00e-04 | grad 2.77 | tok/s 62703
step   1510 | loss 1.4536 | lr 3.00e-04 | grad 1.55 | tok/s 65912
step   1520 | loss 1.4338 | lr 3.00e-04 | grad 1.54 | tok/s 65322
step   1530 | loss 1.4150 | lr 3.00e-04 | grad 1.07 | tok/s 64863
step   1540 | loss 1.3723 | lr 3.00e-04 | grad 0.99 | tok/s 63603
step   1550 | loss 1.3202 | lr 3.00e-04 | grad 3.50 | tok/s 66103
step   1560 | loss 1.8574 | lr 3.00e-04 | grad 1.85 | tok/s 64401
step   1570 | loss 1.4042 | lr 3.00e-04 | grad 1.34 | tok/s 63307
step   1580 | loss 1.4925 | lr 3.00e-04 | grad 1.62 | tok/s 65319
step   1590 | loss 1.3182 | lr 3.00e-04 | grad 1.23 | tok/s 63661
step   1600 | loss 1.4580 | lr 3.00e-04 | grad 0.99 | tok/s 62530
step   1610 | loss 1.2752 | lr 3.00e-04 | grad 1.09 | tok/s 66136
step   1620 | loss 1.4402 | lr 3.00e-04 | grad 1.04 | tok/s 65161
step   1630 | loss 1.4021 | lr 3.00e-04 | grad 0.96 | tok/s 65722
step   1640 | loss 1.3567 | lr 3.00e-04 | grad 1.13 | tok/s 63439
step   1650 | loss 1.4046 | lr 3.00e-04 | grad 1.77 | tok/s 62554
step   1660 | loss 1.4118 | lr 3.00e-04 | grad 1.20 | tok/s 62951
step   1670 | loss 1.3958 | lr 3.00e-04 | grad 2.27 | tok/s 65587
step   1680 | loss 1.6730 | lr 3.00e-04 | grad 1.08 | tok/s 65610
step   1690 | loss 1.3656 | lr 3.00e-04 | grad 1.34 | tok/s 64053
step   1700 | loss 1.4352 | lr 3.00e-04 | grad 1.11 | tok/s 65394
step   1710 | loss 1.4687 | lr 3.00e-04 | grad 2.03 | tok/s 63536
step   1720 | loss 1.3589 | lr 3.00e-04 | grad 1.26 | tok/s 63820
step   1730 | loss 1.5401 | lr 3.00e-04 | grad 1.66 | tok/s 63934
step   1740 | loss 1.4012 | lr 3.00e-04 | grad 0.95 | tok/s 64931
step   1750 | loss 1.3924 | lr 3.00e-04 | grad 1.19 | tok/s 62669
step   1760 | loss 1.5232 | lr 3.00e-04 | grad 1.23 | tok/s 63464
step   1770 | loss 1.4900 | lr 3.00e-04 | grad 0.93 | tok/s 64967
step   1780 | loss 1.4094 | lr 3.00e-04 | grad 1.61 | tok/s 62480
step   1790 | loss 1.5456 | lr 3.00e-04 | grad 1.39 | tok/s 63664
step   1800 | loss 1.3373 | lr 3.00e-04 | grad 0.96 | tok/s 64830
step   1810 | loss 1.4192 | lr 3.00e-04 | grad 1.21 | tok/s 64449
step   1820 | loss 1.3028 | lr 3.00e-04 | grad 0.94 | tok/s 63767
step   1830 | loss 1.3776 | lr 3.00e-04 | grad 1.27 | tok/s 63543
step   1840 | loss 1.3595 | lr 3.00e-04 | grad 1.27 | tok/s 62979
step   1850 | loss 1.5896 | lr 3.00e-04 | grad 1.58 | tok/s 63633
step   1860 | loss 1.3562 | lr 3.00e-04 | grad 1.10 | tok/s 63454
step   1870 | loss 1.3404 | lr 3.00e-04 | grad 1.66 | tok/s 64865
step   1880 | loss 1.3632 | lr 3.00e-04 | grad 1.02 | tok/s 65017
step   1890 | loss 1.4592 | lr 3.00e-04 | grad 1.03 | tok/s 63876
step   1900 | loss 1.3997 | lr 3.00e-04 | grad 0.96 | tok/s 64541
step   1910 | loss 1.5012 | lr 3.00e-04 | grad 2.00 | tok/s 63664
step   1920 | loss 1.3132 | lr 3.00e-04 | grad 1.02 | tok/s 65678
step   1930 | loss 1.3310 | lr 3.00e-04 | grad 1.26 | tok/s 65135
step   1940 | loss 1.3557 | lr 3.00e-04 | grad 1.27 | tok/s 66363
step   1950 | loss 1.3785 | lr 3.00e-04 | grad 1.13 | tok/s 64635
step   1960 | loss 1.5377 | lr 3.00e-04 | grad 3.52 | tok/s 65975
step   1970 | loss 1.2570 | lr 3.00e-04 | grad 1.24 | tok/s 63674
step   1980 | loss 1.4053 | lr 3.00e-04 | grad 2.91 | tok/s 63582
step   1990 | loss 1.4417 | lr 3.00e-04 | grad 1.72 | tok/s 65101
step   2000 | loss 1.3399 | lr 3.00e-04 | grad 1.36 | tok/s 65619
  >>> saved checkpoint: checkpoint_step_002000_loss_1.3399.pt
step   2010 | loss 1.2147 | lr 3.00e-04 | grad 0.91 | tok/s 51309
step   2020 | loss 1.1212 | lr 3.00e-04 | grad 0.85 | tok/s 67721
step   2030 | loss 1.3620 | lr 3.00e-04 | grad 1.02 | tok/s 66815
step   2040 | loss 1.2396 | lr 3.00e-04 | grad 1.09 | tok/s 67656
step   2050 | loss 1.2182 | lr 3.00e-04 | grad 1.49 | tok/s 66667
step   2060 | loss 1.4705 | lr 3.00e-04 | grad 1.13 | tok/s 63699
step   2070 | loss 1.4153 | lr 3.00e-04 | grad 1.33 | tok/s 66833
step   2080 | loss 1.4013 | lr 3.00e-04 | grad 2.19 | tok/s 63219
step   2090 | loss 1.3882 | lr 3.00e-04 | grad 1.18 | tok/s 65485
step   2100 | loss 1.3952 | lr 3.00e-04 | grad 1.30 | tok/s 63572
step   2110 | loss 1.1619 | lr 3.00e-04 | grad 1.03 | tok/s 65853
step   2120 | loss 1.2164 | lr 3.00e-04 | grad 1.25 | tok/s 65220
step   2130 | loss 1.3727 | lr 3.00e-04 | grad 2.45 | tok/s 63349
step   2140 | loss 1.4323 | lr 3.00e-04 | grad 2.80 | tok/s 63260
step   2150 | loss 1.4287 | lr 3.00e-04 | grad 1.08 | tok/s 64227
step   2160 | loss 1.4312 | lr 3.00e-04 | grad 1.05 | tok/s 63495
step   2170 | loss 1.4976 | lr 3.00e-04 | grad 1.31 | tok/s 64260
step   2180 | loss 1.3678 | lr 3.00e-04 | grad 1.19 | tok/s 65496
step   2190 | loss 1.5572 | lr 3.00e-04 | grad 1.17 | tok/s 65318
step   2200 | loss 1.1635 | lr 3.00e-04 | grad 0.91 | tok/s 67565
step   2210 | loss 1.1864 | lr 3.00e-04 | grad 0.96 | tok/s 67518
step   2220 | loss 1.1664 | lr 3.00e-04 | grad 1.13 | tok/s 67520
step   2230 | loss 1.3104 | lr 3.00e-04 | grad 1.27 | tok/s 65218
step   2240 | loss 1.3850 | lr 3.00e-04 | grad 1.77 | tok/s 66676
step   2250 | loss 1.3484 | lr 3.00e-04 | grad 1.35 | tok/s 66111
step   2260 | loss 1.4625 | lr 3.00e-04 | grad 1.58 | tok/s 65235
step   2270 | loss 1.3151 | lr 3.00e-04 | grad 1.62 | tok/s 63780
step   2280 | loss 1.4714 | lr 3.00e-04 | grad 1.03 | tok/s 63730
step   2290 | loss 1.3457 | lr 3.00e-04 | grad 1.11 | tok/s 64426
step   2300 | loss 1.3284 | lr 3.00e-04 | grad 1.10 | tok/s 62398
step   2310 | loss 1.3824 | lr 3.00e-04 | grad 1.01 | tok/s 66442
step   2320 | loss 1.3534 | lr 3.00e-04 | grad 0.98 | tok/s 67538
step   2330 | loss 1.2893 | lr 3.00e-04 | grad 0.93 | tok/s 67625
step   2340 | loss 1.3248 | lr 3.00e-04 | grad 1.37 | tok/s 66023
step   2350 | loss 1.4248 | lr 3.00e-04 | grad 1.13 | tok/s 63948
step   2360 | loss 1.7320 | lr 3.00e-04 | grad 2.12 | tok/s 65195
step   2370 | loss 1.3770 | lr 3.00e-04 | grad 1.22 | tok/s 64789
step   2380 | loss 1.3176 | lr 3.00e-04 | grad 1.28 | tok/s 64131
step   2390 | loss 1.3295 | lr 3.00e-04 | grad 1.30 | tok/s 64006
step   2400 | loss 1.4589 | lr 3.00e-04 | grad 1.37 | tok/s 63257

Training complete! Final step: 2408
