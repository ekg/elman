# Job 14: E75h8n32
# GPU: 2
# Command: python train.py --level E75h8n32 --dim 4352 --expansion 2.0 --n_state 32 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_013342/E75h8n32
# Started: 2026-01-20T01:44:13.988268
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_013342/E75h8n32/levelE75h8n32_100m_20260120_014419
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level E75h8n32, 959,346,944 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.9509 | lr 2.70e-05 | grad 201.00 | tok/s 7140
step     20 | loss 3.8678 | lr 5.70e-05 | grad 22.50 | tok/s 11990
step     30 | loss 3.9167 | lr 8.70e-05 | grad 26.12 | tok/s 11855
step     40 | loss 3.5723 | lr 1.17e-04 | grad 20.25 | tok/s 12233
step     50 | loss 4.8709 | lr 1.47e-04 | grad 18.25 | tok/s 12657
step     60 | loss 4.0309 | lr 1.77e-04 | grad 10.06 | tok/s 12623
step     70 | loss 3.8014 | lr 2.07e-04 | grad 15.62 | tok/s 12564
step     80 | loss 3.5385 | lr 2.37e-04 | grad 9.38 | tok/s 12527
step     90 | loss 3.1412 | lr 2.67e-04 | grad 6.03 | tok/s 12532
step    100 | loss 3.1054 | lr 2.97e-04 | grad 13.19 | tok/s 12531
step    110 | loss 2.8907 | lr 6.94e-06 | grad 8.81 | tok/s 12503
step    120 | loss 4.3051 | lr 2.69e-05 | grad 9.44 | tok/s 12207
step    130 | loss 3.3359 | lr 5.89e-05 | grad 2.47 | tok/s 11935
step    140 | loss 2.9479 | lr 9.99e-05 | grad 5.72 | tok/s 12018
step    150 | loss 2.9926 | lr 1.46e-04 | grad 6.19 | tok/s 12430
step    160 | loss 2.7669 | lr 1.92e-04 | grad 7.34 | tok/s 12466
step    170 | loss 2.9290 | lr 2.35e-04 | grad 8.62 | tok/s 11762
step    180 | loss 2.8478 | lr 2.69e-04 | grad 6.06 | tok/s 12185
step    190 | loss 2.8891 | lr 2.91e-04 | grad 7.69 | tok/s 11707
step    200 | loss 2.3797 | lr 3.00e-04 | grad 3.08 | tok/s 12460
step    210 | loss 2.3154 | lr 2.94e-04 | grad 6.22 | tok/s 12124
step    220 | loss 2.7638 | lr 2.74e-04 | grad 7.22 | tok/s 11708
step    230 | loss 3.4689 | lr 2.42e-04 | grad 3.23 | tok/s 11720
step    240 | loss 2.5408 | lr 2.01e-04 | grad 4.44 | tok/s 11778
step    250 | loss 2.7575 | lr 1.55e-04 | grad 4.38 | tok/s 11819
step    260 | loss 2.2880 | lr 1.09e-04 | grad 2.14 | tok/s 12210
step    270 | loss 2.4227 | lr 6.65e-05 | grad 3.02 | tok/s 12228
step    280 | loss 2.0819 | lr 3.24e-05 | grad 2.42 | tok/s 11886
step    290 | loss 2.1137 | lr 9.84e-06 | grad 3.28 | tok/s 11434
step    300 | loss 2.2211 | lr 1.07e-06 | grad 3.34 | tok/s 11605
step    310 | loss 2.2056 | lr 6.94e-06 | grad 1.75 | tok/s 11867
step    320 | loss 2.0099 | lr 2.69e-05 | grad 3.11 | tok/s 11371
step    330 | loss 2.2908 | lr 5.89e-05 | grad 2.16 | tok/s 11908
step    340 | loss 2.3638 | lr 9.99e-05 | grad 9.19 | tok/s 12145
step    350 | loss 2.3705 | lr 1.46e-04 | grad 4.09 | tok/s 11922
step    360 | loss 2.2968 | lr 1.92e-04 | grad 3.88 | tok/s 12210
step    370 | loss 2.0430 | lr 2.35e-04 | grad 2.19 | tok/s 11979
step    380 | loss 2.1146 | lr 2.69e-04 | grad 2.97 | tok/s 12513
step    390 | loss 1.6809 | lr 2.91e-04 | grad 2.41 | tok/s 12617
step    400 | loss 1.5968 | lr 3.00e-04 | grad 2.27 | tok/s 12456
step    410 | loss 2.5968 | lr 2.94e-04 | grad 3.53 | tok/s 12006
step    420 | loss 2.4166 | lr 2.74e-04 | grad 5.59 | tok/s 12023
step    430 | loss 2.4337 | lr 2.42e-04 | grad 5.16 | tok/s 12590
step    440 | loss 2.1961 | lr 2.01e-04 | grad 4.47 | tok/s 12232
step    450 | loss 2.2824 | lr 1.55e-04 | grad 3.02 | tok/s 12066
step    460 | loss 1.9699 | lr 1.09e-04 | grad 5.09 | tok/s 11960
step    470 | loss 2.0920 | lr 6.65e-05 | grad 3.09 | tok/s 11966
step    480 | loss 2.1421 | lr 3.24e-05 | grad 3.34 | tok/s 12540
step    490 | loss 2.1754 | lr 9.84e-06 | grad 1.83 | tok/s 12109
step    500 | loss 2.0256 | lr 1.07e-06 | grad 2.19 | tok/s 12045
step    510 | loss 2.2816 | lr 6.94e-06 | grad 6.00 | tok/s 11825
step    520 | loss 2.0442 | lr 2.69e-05 | grad 1.89 | tok/s 11345
step    530 | loss 1.9125 | lr 5.89e-05 | grad 2.11 | tok/s 12038
step    540 | loss 2.1300 | lr 9.99e-05 | grad 2.38 | tok/s 12003
step    550 | loss 2.0646 | lr 1.46e-04 | grad 2.78 | tok/s 11741
step    560 | loss 1.8463 | lr 1.92e-04 | grad 3.80 | tok/s 12310
step    570 | loss 1.8391 | lr 2.35e-04 | grad 2.52 | tok/s 12618
step    580 | loss 1.6436 | lr 2.69e-04 | grad 1.71 | tok/s 12615
step    590 | loss 1.5847 | lr 2.91e-04 | grad 2.39 | tok/s 12636
step    600 | loss 1.6886 | lr 3.00e-04 | grad 2.77 | tok/s 12627
step    610 | loss 1.5993 | lr 2.94e-04 | grad 2.59 | tok/s 12628
step    620 | loss 1.6140 | lr 2.74e-04 | grad 1.74 | tok/s 12629
step    630 | loss 1.8169 | lr 2.42e-04 | grad 5.19 | tok/s 12455
step    640 | loss 2.2101 | lr 2.01e-04 | grad 6.56 | tok/s 11943
step    650 | loss 2.1701 | lr 1.55e-04 | grad 4.22 | tok/s 11827
step    660 | loss 2.0474 | lr 1.09e-04 | grad 4.09 | tok/s 11934
step    670 | loss 1.9954 | lr 6.65e-05 | grad 2.84 | tok/s 12325
step    680 | loss 2.0443 | lr 3.24e-05 | grad 3.30 | tok/s 11929
step    690 | loss 2.0406 | lr 9.84e-06 | grad 2.83 | tok/s 11855
step    700 | loss 2.0187 | lr 1.07e-06 | grad 2.08 | tok/s 11730
step    710 | loss 1.9034 | lr 6.94e-06 | grad 2.67 | tok/s 12065
step    720 | loss 2.1295 | lr 2.68e-05 | grad 4.94 | tok/s 11818
step    730 | loss 1.7820 | lr 5.89e-05 | grad 1.88 | tok/s 12336
step    740 | loss 1.9118 | lr 9.99e-05 | grad 3.16 | tok/s 12002
step    750 | loss 2.5208 | lr 1.46e-04 | grad 6.06 | tok/s 12455
step    760 | loss 2.2232 | lr 1.92e-04 | grad 2.78 | tok/s 12464
step    770 | loss 2.0489 | lr 2.35e-04 | grad 5.16 | tok/s 12193
step    780 | loss 2.0638 | lr 2.69e-04 | grad 4.81 | tok/s 11850
step    790 | loss 2.0225 | lr 2.91e-04 | grad 4.34 | tok/s 12147
step    800 | loss 2.3944 | lr 3.00e-04 | grad 8.75 | tok/s 12504
step    810 | loss 2.1740 | lr 2.94e-04 | grad 2.98 | tok/s 12132
step    820 | loss 1.8369 | lr 2.74e-04 | grad 5.72 | tok/s 11864
step    830 | loss 2.0617 | lr 2.42e-04 | grad 3.98 | tok/s 12016
step    840 | loss 2.0597 | lr 2.01e-04 | grad 3.23 | tok/s 11765
step    850 | loss 2.3546 | lr 1.55e-04 | grad 3.27 | tok/s 11797
step    860 | loss 2.1034 | lr 1.09e-04 | grad 2.84 | tok/s 11920
step    870 | loss 2.0999 | lr 6.65e-05 | grad 7.78 | tok/s 12023
step    880 | loss 2.6625 | lr 3.24e-05 | grad 2.97 | tok/s 12593
step    890 | loss 1.9572 | lr 9.84e-06 | grad 2.45 | tok/s 11994
step    900 | loss 1.8563 | lr 1.07e-06 | grad 2.06 | tok/s 11984
step    910 | loss 1.8660 | lr 6.94e-06 | grad 2.19 | tok/s 12127

Training complete! Final step: 914
