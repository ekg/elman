# Job 1: fla-gdn
# GPU: 1
# Command: python train.py --level fla-gdn --dim 2560 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_013342/fla-gdn
# Started: 2026-01-20T01:33:49.010212
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_013342/fla-gdn/levelfla-gdn_100m_20260120_013355
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level fla-gdn, 1,052,158,240 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.8442 | lr 2.70e-05 | grad 163.00 | tok/s 564
step     20 | loss 5.1672 | lr 5.70e-05 | grad 79.50 | tok/s 11653
step     30 | loss 4.4168 | lr 8.70e-05 | grad 55.00 | tok/s 11523
step     40 | loss 3.9462 | lr 1.17e-04 | grad 61.25 | tok/s 11877
step     50 | loss 5.3042 | lr 1.47e-04 | grad 42.25 | tok/s 12255
step     60 | loss 3.9935 | lr 1.77e-04 | grad 31.25 | tok/s 12167
step     70 | loss 3.5135 | lr 2.07e-04 | grad 18.38 | tok/s 12100
step     80 | loss 3.4065 | lr 2.37e-04 | grad 11.75 | tok/s 12048
step     90 | loss 3.2657 | lr 2.67e-04 | grad 10.69 | tok/s 12008
step    100 | loss 3.4738 | lr 2.97e-04 | grad 6.94 | tok/s 11959
step    110 | loss 2.9566 | lr 6.94e-06 | grad 7.94 | tok/s 12018
step    120 | loss 3.6623 | lr 2.69e-05 | grad 5.34 | tok/s 11627
step    130 | loss 2.7134 | lr 5.89e-05 | grad 4.03 | tok/s 11359
step    140 | loss 2.3996 | lr 9.99e-05 | grad 5.09 | tok/s 11389
step    150 | loss 2.2021 | lr 1.46e-04 | grad 11.88 | tok/s 11769
step    160 | loss 2.2114 | lr 1.92e-04 | grad 8.25 | tok/s 11821
step    170 | loss 2.5072 | lr 2.35e-04 | grad 11.69 | tok/s 11162
step    180 | loss 2.1579 | lr 2.69e-04 | grad 4.72 | tok/s 11551
step    190 | loss 2.0362 | lr 2.91e-04 | grad 6.16 | tok/s 11066
step    200 | loss 1.8106 | lr 3.00e-04 | grad 2.61 | tok/s 11810
step    210 | loss 1.6784 | lr 2.94e-04 | grad 4.12 | tok/s 11457
step    220 | loss 2.3293 | lr 2.74e-04 | grad 9.69 | tok/s 11080
step    230 | loss 2.4675 | lr 2.42e-04 | grad 3.56 | tok/s 11075
step    240 | loss 2.0634 | lr 2.01e-04 | grad 3.47 | tok/s 11129
step    250 | loss 2.2162 | lr 1.55e-04 | grad 3.61 | tok/s 11149
step    260 | loss 1.8256 | lr 1.09e-04 | grad 2.91 | tok/s 11509
step    270 | loss 2.0001 | lr 6.65e-05 | grad 3.27 | tok/s 11523
step    280 | loss 1.7281 | lr 3.24e-05 | grad 3.25 | tok/s 11190
step    290 | loss 1.7311 | lr 9.84e-06 | grad 3.28 | tok/s 10770
step    300 | loss 1.7927 | lr 1.07e-06 | grad 3.66 | tok/s 10954
step    310 | loss 1.8268 | lr 6.94e-06 | grad 1.79 | tok/s 11179
step    320 | loss 1.6508 | lr 2.69e-05 | grad 3.05 | tok/s 10706
step    330 | loss 1.8674 | lr 5.89e-05 | grad 2.22 | tok/s 11201
step    340 | loss 1.8845 | lr 9.99e-05 | grad 5.09 | tok/s 11415
step    350 | loss 1.9340 | lr 1.46e-04 | grad 6.31 | tok/s 11210
step    360 | loss 1.7529 | lr 1.92e-04 | grad 3.78 | tok/s 11470
step    370 | loss 1.5295 | lr 2.35e-04 | grad 2.16 | tok/s 11262
step    380 | loss 1.5778 | lr 2.69e-04 | grad 1.91 | tok/s 11759
step    390 | loss 1.1635 | lr 2.91e-04 | grad 2.50 | tok/s 11865
step    400 | loss 1.1145 | lr 3.00e-04 | grad 3.47 | tok/s 11676
step    410 | loss 2.1760 | lr 2.94e-04 | grad 4.53 | tok/s 11291
step    420 | loss 1.9750 | lr 2.74e-04 | grad 3.69 | tok/s 11286
step    430 | loss 1.7646 | lr 2.42e-04 | grad 3.66 | tok/s 11823
step    440 | loss 1.7257 | lr 2.01e-04 | grad 3.61 | tok/s 11470
step    450 | loss 1.8921 | lr 1.55e-04 | grad 1.99 | tok/s 11313
step    460 | loss 1.6218 | lr 1.09e-04 | grad 4.50 | tok/s 11204
step    470 | loss 1.7167 | lr 6.65e-05 | grad 3.38 | tok/s 11238
step    480 | loss 1.7363 | lr 3.24e-05 | grad 3.12 | tok/s 11759
step    490 | loss 1.8521 | lr 9.84e-06 | grad 1.87 | tok/s 11369
step    500 | loss 1.6713 | lr 1.07e-06 | grad 2.05 | tok/s 11295
step    510 | loss 1.9721 | lr 6.94e-06 | grad 8.12 | tok/s 11128
step    520 | loss 1.7343 | lr 2.69e-05 | grad 1.84 | tok/s 10664
step    530 | loss 1.5728 | lr 5.89e-05 | grad 1.79 | tok/s 11299
step    540 | loss 1.7487 | lr 9.99e-05 | grad 2.11 | tok/s 11275
step    550 | loss 1.6514 | lr 1.46e-04 | grad 2.56 | tok/s 11028
step    560 | loss 1.4319 | lr 1.92e-04 | grad 3.27 | tok/s 11563
step    570 | loss 1.4753 | lr 2.35e-04 | grad 2.14 | tok/s 11844
step    580 | loss 1.3149 | lr 2.69e-04 | grad 1.62 | tok/s 11860
step    590 | loss 1.2640 | lr 2.91e-04 | grad 1.60 | tok/s 11849
step    600 | loss 1.3682 | lr 3.00e-04 | grad 1.92 | tok/s 11844
step    610 | loss 1.2730 | lr 2.94e-04 | grad 2.20 | tok/s 11848
step    620 | loss 1.3191 | lr 2.74e-04 | grad 1.91 | tok/s 11844
step    630 | loss 1.4712 | lr 2.42e-04 | grad 8.12 | tok/s 11677
step    640 | loss 1.8765 | lr 2.01e-04 | grad 3.95 | tok/s 11189
step    650 | loss 1.8164 | lr 1.55e-04 | grad 3.39 | tok/s 11102
step    660 | loss 1.6599 | lr 1.09e-04 | grad 2.70 | tok/s 11222
step    670 | loss 1.6581 | lr 6.65e-05 | grad 2.70 | tok/s 11591

Training complete! Final step: 679
