# Job 4: minlstm
# GPU: 4
# Command: python train.py --level minlstm --dim 2560 --expansion 2.0 --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_013342/minlstm
# Started: 2026-01-20T01:33:49.011735
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_013342/minlstm/levelminlstm_100m_20260120_013354
Auto r_h_mode: none (level 0 has bounded/no W_h)
Model: Level minlstm, 1,049,338,880 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 5.0043 | lr 2.70e-05 | grad 19.75 | tok/s 7492
step     20 | loss 3.5211 | lr 5.70e-05 | grad 7.91 | tok/s 7477
step     30 | loss 3.9831 | lr 8.70e-05 | grad 18.12 | tok/s 7429
step     40 | loss 4.0777 | lr 1.17e-04 | grad 16.12 | tok/s 7685
step     50 | loss 5.5925 | lr 1.47e-04 | grad 10.19 | tok/s 7957
step     60 | loss 4.5279 | lr 1.77e-04 | grad 6.75 | tok/s 7926
step     70 | loss 4.1858 | lr 2.07e-04 | grad 7.47 | tok/s 7883
step     80 | loss 4.0189 | lr 2.37e-04 | grad 6.47 | tok/s 7841
step     90 | loss 3.6325 | lr 2.67e-04 | grad 5.66 | tok/s 7808
step    100 | loss 3.4579 | lr 2.97e-04 | grad 6.44 | tok/s 7785
step    110 | loss 3.2005 | lr 6.94e-06 | grad 5.41 | tok/s 7771
step    120 | loss 4.0044 | lr 2.69e-05 | grad 5.19 | tok/s 7534
step    130 | loss 3.1653 | lr 5.89e-05 | grad 2.34 | tok/s 7363
step    140 | loss 2.9187 | lr 9.99e-05 | grad 2.77 | tok/s 7398
step    150 | loss 3.4574 | lr 1.46e-04 | grad 8.56 | tok/s 7653
step    160 | loss 3.4394 | lr 1.92e-04 | grad 5.12 | tok/s 7685
step    170 | loss 3.3078 | lr 2.35e-04 | grad 6.31 | tok/s 7263
step    180 | loss 3.1740 | lr 2.69e-04 | grad 4.19 | tok/s 7507
step    190 | loss 3.1147 | lr 2.91e-04 | grad 4.44 | tok/s 7200
step    200 | loss 2.6580 | lr 3.00e-04 | grad 2.55 | tok/s 7690
step    210 | loss 2.5608 | lr 2.94e-04 | grad 3.16 | tok/s 7465
step    220 | loss 2.9549 | lr 2.74e-04 | grad 6.25 | tok/s 7205
step    230 | loss 3.3227 | lr 2.42e-04 | grad 3.16 | tok/s 7216
step    240 | loss 2.7021 | lr 2.01e-04 | grad 3.72 | tok/s 7250
step    250 | loss 2.9192 | lr 1.55e-04 | grad 3.36 | tok/s 7269
step    260 | loss 2.5296 | lr 1.09e-04 | grad 1.81 | tok/s 7490
step    270 | loss 2.6171 | lr 6.65e-05 | grad 2.28 | tok/s 7515
step    280 | loss 2.3031 | lr 3.24e-05 | grad 1.62 | tok/s 7296
step    290 | loss 2.3544 | lr 9.84e-06 | grad 2.34 | tok/s 7022
step    300 | loss 2.4332 | lr 1.07e-06 | grad 2.03 | tok/s 7129
step    310 | loss 2.4154 | lr 6.94e-06 | grad 1.37 | tok/s 7298
step    320 | loss 2.2420 | lr 2.69e-05 | grad 2.11 | tok/s 6993
step    330 | loss 2.4945 | lr 5.89e-05 | grad 1.74 | tok/s 7318
step    340 | loss 2.5735 | lr 9.99e-05 | grad 4.91 | tok/s 7457
step    350 | loss 2.6007 | lr 1.46e-04 | grad 3.12 | tok/s 7318
step    360 | loss 2.6415 | lr 1.92e-04 | grad 2.09 | tok/s 7503
step    370 | loss 2.3985 | lr 2.35e-04 | grad 2.05 | tok/s 7369
step    380 | loss 2.3487 | lr 2.69e-04 | grad 2.66 | tok/s 7699
step    390 | loss 2.1525 | lr 2.91e-04 | grad 2.30 | tok/s 7765
step    400 | loss 2.0297 | lr 3.00e-04 | grad 1.34 | tok/s 7646
step    410 | loss 2.6418 | lr 2.94e-04 | grad 2.28 | tok/s 7385
step    420 | loss 2.4737 | lr 2.74e-04 | grad 2.89 | tok/s 7384
step    430 | loss 2.5881 | lr 2.42e-04 | grad 4.72 | tok/s 7740
step    440 | loss 2.3770 | lr 2.01e-04 | grad 3.03 | tok/s 7498
step    450 | loss 2.3971 | lr 1.55e-04 | grad 1.83 | tok/s 7403
step    460 | loss 2.1373 | lr 1.09e-04 | grad 3.03 | tok/s 7337
step    470 | loss 2.2953 | lr 6.65e-05 | grad 1.95 | tok/s 7343
step    480 | loss 2.3605 | lr 3.24e-05 | grad 1.71 | tok/s 7694
step    490 | loss 2.3351 | lr 9.84e-06 | grad 1.11 | tok/s 7439
step    500 | loss 2.2238 | lr 1.07e-06 | grad 1.62 | tok/s 7386
step    510 | loss 2.4251 | lr 6.94e-06 | grad 3.48 | tok/s 7275
step    520 | loss 2.1862 | lr 2.69e-05 | grad 1.14 | tok/s 6961
step    530 | loss 2.1266 | lr 5.89e-05 | grad 1.41 | tok/s 7387
step    540 | loss 2.2957 | lr 9.99e-05 | grad 2.02 | tok/s 7374
step    550 | loss 2.2590 | lr 1.46e-04 | grad 2.16 | tok/s 7198
step    560 | loss 1.9907 | lr 1.92e-04 | grad 2.88 | tok/s 7543

Training complete! Final step: 568
