# Job 0: mamba2
# GPU: 0
# Command: python train.py --level mamba2 --dim 2944  --data data/pile.txt --depth 20 --batch_size 16 --chunk_size 512 --lr 3e-4 --warmup_steps 100 --seed 42 --bf16 --train_minutes 10 --output benchmark_results/1b_10min_20260120_013342/mamba2
# Started: 2026-01-20T01:33:49.010060
============================================================

Using device: cuda
Output directory: benchmark_results/1b_10min_20260120_013342/mamba2/levelmamba2_100m_20260120_013354
Model: Level mamba2, 1,062,161,552 parameters

Starting training from step 0...
Batch size: 16, Chunk size: 512
Gradient accumulation: 1, Effective batch: 16

Time-based training: 10.0 minutes
step     10 | loss 4.9856 | lr 2.70e-05 | grad 13.50 | tok/s 3703
step     20 | loss 2.8035 | lr 5.70e-05 | grad 7.72 | tok/s 9980
step     30 | loss 2.7979 | lr 8.70e-05 | grad 11.12 | tok/s 9820
step     40 | loss 2.9035 | lr 1.17e-04 | grad 15.94 | tok/s 10042
step     50 | loss 4.4207 | lr 1.47e-04 | grad 9.31 | tok/s 10295
step     60 | loss 3.8622 | lr 1.77e-04 | grad 11.06 | tok/s 10169
step     70 | loss 3.1797 | lr 2.07e-04 | grad 14.19 | tok/s 10063
step     80 | loss 3.0245 | lr 2.37e-04 | grad 10.06 | tok/s 9989
step     90 | loss 2.6192 | lr 2.67e-04 | grad 7.75 | tok/s 9905
step    100 | loss 2.7075 | lr 2.97e-04 | grad 11.75 | tok/s 9832
step    110 | loss 2.5255 | lr 6.94e-06 | grad 9.69 | tok/s 9761
step    120 | loss 3.6781 | lr 2.69e-05 | grad 4.56 | tok/s 9431
step    130 | loss 2.4414 | lr 5.89e-05 | grad 2.70 | tok/s 9213
step    140 | loss 2.1222 | lr 9.99e-05 | grad 4.53 | tok/s 9222
step    150 | loss 2.1627 | lr 1.46e-04 | grad 3.44 | tok/s 9521
step    160 | loss 2.0287 | lr 1.92e-04 | grad 5.91 | tok/s 9573
step    170 | loss 2.4888 | lr 2.35e-04 | grad 7.19 | tok/s 9038
step    180 | loss 2.1342 | lr 2.69e-04 | grad 4.31 | tok/s 9336
step    190 | loss 2.1110 | lr 2.91e-04 | grad 4.22 | tok/s 8944
step    200 | loss 1.8132 | lr 3.00e-04 | grad 2.77 | tok/s 9560
step    210 | loss 1.7062 | lr 2.94e-04 | grad 2.83 | tok/s 9263
step    220 | loss 2.3090 | lr 2.74e-04 | grad 5.34 | tok/s 8935
step    230 | loss 2.7926 | lr 2.42e-04 | grad 2.72 | tok/s 8949
step    240 | loss 2.0984 | lr 2.01e-04 | grad 2.78 | tok/s 8991
step    250 | loss 2.2714 | lr 1.55e-04 | grad 4.84 | tok/s 9001
step    260 | loss 1.8919 | lr 1.09e-04 | grad 1.89 | tok/s 9300
step    270 | loss 2.0214 | lr 6.65e-05 | grad 2.73 | tok/s 9295
step    280 | loss 1.7405 | lr 3.24e-05 | grad 1.95 | tok/s 9009
step    290 | loss 1.6865 | lr 9.84e-06 | grad 2.41 | tok/s 8661
step    300 | loss 1.7927 | lr 1.07e-06 | grad 2.33 | tok/s 8777
step    310 | loss 1.8234 | lr 6.94e-06 | grad 1.43 | tok/s 8990
step    320 | loss 1.6635 | lr 2.69e-05 | grad 2.44 | tok/s 8609
step    330 | loss 1.8719 | lr 5.89e-05 | grad 1.69 | tok/s 9008
step    340 | loss 1.9125 | lr 9.99e-05 | grad 6.72 | tok/s 9178
step    350 | loss 1.9478 | lr 1.46e-04 | grad 3.52 | tok/s 9008
step    360 | loss 1.8116 | lr 1.92e-04 | grad 2.09 | tok/s 9231
step    370 | loss 1.6173 | lr 2.35e-04 | grad 1.73 | tok/s 9064
step    380 | loss 1.6556 | lr 2.69e-04 | grad 3.03 | tok/s 9463
step    390 | loss 1.2759 | lr 2.91e-04 | grad 2.03 | tok/s 9547
step    400 | loss 1.1744 | lr 3.00e-04 | grad 2.94 | tok/s 9389
step    410 | loss 2.2318 | lr 2.94e-04 | grad 3.05 | tok/s 9071
step    420 | loss 2.0432 | lr 2.74e-04 | grad 3.58 | tok/s 9075
step    430 | loss 1.8753 | lr 2.42e-04 | grad 3.45 | tok/s 9521
step    440 | loss 1.8130 | lr 2.01e-04 | grad 3.08 | tok/s 9226
step    450 | loss 1.9265 | lr 1.55e-04 | grad 1.84 | tok/s 9094
step    460 | loss 1.6471 | lr 1.09e-04 | grad 3.41 | tok/s 9016
step    470 | loss 1.7233 | lr 6.65e-05 | grad 2.33 | tok/s 9018
step    480 | loss 1.7489 | lr 3.24e-05 | grad 2.39 | tok/s 9448
step    490 | loss 1.8614 | lr 9.84e-06 | grad 1.34 | tok/s 9129
step    500 | loss 1.6918 | lr 1.07e-06 | grad 1.55 | tok/s 9068
step    510 | loss 1.9874 | lr 6.94e-06 | grad 5.28 | tok/s 8921
step    520 | loss 1.7583 | lr 2.69e-05 | grad 1.34 | tok/s 8557
step    530 | loss 1.6030 | lr 5.89e-05 | grad 1.38 | tok/s 9078
step    540 | loss 1.7733 | lr 9.99e-05 | grad 1.74 | tok/s 9060
step    550 | loss 1.6842 | lr 1.46e-04 | grad 2.20 | tok/s 8848
step    560 | loss 1.4357 | lr 1.92e-04 | grad 2.62 | tok/s 9262
step    570 | loss 1.5133 | lr 2.35e-04 | grad 1.37 | tok/s 9515
step    580 | loss 1.3577 | lr 2.69e-04 | grad 1.62 | tok/s 9516
step    590 | loss 1.3065 | lr 2.91e-04 | grad 1.64 | tok/s 9519
step    600 | loss 1.4126 | lr 3.00e-04 | grad 1.96 | tok/s 9522
step    610 | loss 1.3120 | lr 2.94e-04 | grad 1.79 | tok/s 9525
step    620 | loss 1.3549 | lr 2.74e-04 | grad 1.45 | tok/s 9534
step    630 | loss 1.5015 | lr 2.42e-04 | grad 5.19 | tok/s 9394
step    640 | loss 1.9181 | lr 2.01e-04 | grad 5.94 | tok/s 8985
step    650 | loss 1.8348 | lr 1.55e-04 | grad 1.93 | tok/s 8920
step    660 | loss 1.6764 | lr 1.09e-04 | grad 2.05 | tok/s 9004
step    670 | loss 1.6901 | lr 6.65e-05 | grad 2.27 | tok/s 9312
step    680 | loss 1.7077 | lr 3.24e-05 | grad 2.06 | tok/s 8980

Training complete! Final step: 689
