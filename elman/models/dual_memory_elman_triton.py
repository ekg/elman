"""
E23 Dual-Memory Elman - Triton kernels for forward and backward pass.

Key operations per timestep:
1. Read: h_work queries tape via attention â†’ read value
2. Update: h_work_new = tanh(W_h @ h_work + W_x @ x + read + b)
3. Write: h_tape_new = (1-attn)*h_tape + attn*write_value

Triton strategy:
- Fuse attention (dot product + softmax) with reads/writes
- Fuse tanh computation with updates
- Batch process where possible
"""

import math
import torch
import triton
import triton.language as tl


@triton.jit
def e23_forward_kernel(
    # Input tensors
    x_ptr,          # [B, T, D]
    h_tape_ptr,     # [B, N, D] - input tape state
    h_work_ptr,     # [B, D] - input working memory
    # Weight tensors (all [D, D] except b which is [D])
    W_h_ptr, W_x_ptr, b_h_ptr, W_write_ptr,
    # Output tensors
    h_work_out_ptr,     # [B, T, D] - working memory at each step
    h_tape_out_ptr,     # [B, T+1, N, D] - tape states (including initial)
    read_attn_out_ptr,  # [B, T, N] - read attention weights
    write_attn_out_ptr, # [B, T, N] - write attention weights
    # Dimensions
    B: tl.constexpr, T: tl.constexpr, N: tl.constexpr, D: tl.constexpr,
    # Strides
    stride_xb, stride_xt, stride_xd,
    stride_tape_b, stride_tape_n, stride_tape_d,
    stride_work_b, stride_work_d,
    stride_W,  # Stride for D x D matrices
    # Scale
    scale,
    # Block sizes
    BLOCK_D: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    E23 forward kernel - one block per batch element.

    Each block processes all T timesteps sequentially for one batch element.
    """
    b = tl.program_id(0)
    if b >= B:
        return

    # Thread-local accumulators
    d_off = tl.arange(0, BLOCK_D)
    n_off = tl.arange(0, BLOCK_N)

    # Load initial h_work for this batch
    h_work = tl.load(h_work_ptr + b * stride_work_b + d_off * stride_work_d,
                     mask=d_off < D, other=0.0)

    # Load initial h_tape for this batch and store to output
    # h_tape is [N, D] for this batch element
    h_tape = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)
    for n in range(N):
        if n < BLOCK_N:
            tape_vals = tl.load(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                               mask=d_off < D, other=0.0)
            # Store initial tape to output position 0
            tl.store(h_tape_out_ptr + b * (T + 1) * N * D + 0 * N * D + n * D + d_off,
                    tape_vals, mask=d_off < D)

    # Process each timestep
    for t in range(T):
        # ============================================
        # STEP 1: READ FROM TAPE
        # ============================================
        # read_scores[n] = sum_d h_tape[n, d] * h_work[d]
        read_scores = tl.zeros((BLOCK_N,), dtype=tl.float32)
        for n in tl.static_range(BLOCK_N):
            tape_row = tl.load(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                              mask=d_off < D, other=0.0)
            score = tl.sum(tape_row * h_work, axis=0)
            read_scores = tl.where(n_off == n, score * scale, read_scores)

        # Softmax
        max_score = tl.max(read_scores, axis=0)
        read_scores = tl.exp(read_scores - max_score)
        read_attn = read_scores / tl.sum(read_scores, axis=0)

        # Store read attention
        tl.store(read_attn_out_ptr + b * T * N + t * N + n_off, read_attn, mask=n_off < N)

        # Weighted sum: read = sum_n read_attn[n] * h_tape[n, :]
        read = tl.zeros((BLOCK_D,), dtype=tl.float32)
        for n in tl.static_range(BLOCK_N):
            tape_row = tl.load(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                              mask=d_off < D, other=0.0)
            attn_n = tl.sum(tl.where(n_off == n, read_attn, 0.0), axis=0)
            read = read + attn_n * tape_row

        # ============================================
        # STEP 2: UPDATE WORKING MEMORY
        # ============================================
        # Load x for this timestep
        x = tl.load(x_ptr + b * stride_xb + t * stride_xt + d_off * stride_xd,
                   mask=d_off < D, other=0.0)

        # pre_act = W_h @ h_work + W_x @ x + read + b_h
        # We need to compute matrix-vector products
        # This is expensive in Triton - better to use cuBLAS externally
        # For now, use simple loop (will be slow)
        pre_act = tl.zeros((BLOCK_D,), dtype=tl.float32)

        # Load bias
        b_h = tl.load(b_h_ptr + d_off, mask=d_off < D, other=0.0)
        pre_act = pre_act + b_h + read

        # W_h @ h_work and W_x @ x (row by row)
        for i in tl.static_range(BLOCK_D):
            # W_h[i, :] @ h_work
            w_h_row = tl.load(W_h_ptr + i * stride_W + d_off, mask=d_off < D, other=0.0)
            wh_contrib = tl.sum(w_h_row * h_work, axis=0)

            # W_x[i, :] @ x
            w_x_row = tl.load(W_x_ptr + i * stride_W + d_off, mask=d_off < D, other=0.0)
            wx_contrib = tl.sum(w_x_row * x, axis=0)

            pre_act = tl.where(d_off == i, pre_act + wh_contrib + wx_contrib, pre_act)

        # tanh
        h_work_new = tl.libdevice.tanh(pre_act)

        # Store h_work output
        tl.store(h_work_out_ptr + b * T * D + t * D + d_off, h_work_new, mask=d_off < D)

        # ============================================
        # STEP 3: WRITE TO TAPE
        # ============================================
        # write_value = h_work_new @ W_write.T
        write_value = tl.zeros((BLOCK_D,), dtype=tl.float32)
        for i in tl.static_range(BLOCK_D):
            w_write_row = tl.load(W_write_ptr + i * stride_W + d_off, mask=d_off < D, other=0.0)
            ww_contrib = tl.sum(w_write_row * h_work_new, axis=0)
            write_value = tl.where(d_off == i, ww_contrib, write_value)

        # Write attention scores
        write_scores = tl.zeros((BLOCK_N,), dtype=tl.float32)
        for n in tl.static_range(BLOCK_N):
            tape_row = tl.load(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                              mask=d_off < D, other=0.0)
            score = tl.sum(tape_row * h_work_new, axis=0)
            write_scores = tl.where(n_off == n, score * scale, write_scores)

        # Softmax
        max_score = tl.max(write_scores, axis=0)
        write_scores = tl.exp(write_scores - max_score)
        write_attn = write_scores / tl.sum(write_scores, axis=0)

        # Store write attention
        tl.store(write_attn_out_ptr + b * T * N + t * N + n_off, write_attn, mask=n_off < N)

        # Update tape: h_tape = (1 - attn) * h_tape + attn * write_value
        for n in tl.static_range(BLOCK_N):
            tape_row = tl.load(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                              mask=d_off < D, other=0.0)
            attn_n = tl.sum(tl.where(n_off == n, write_attn, 0.0), axis=0)
            new_tape_row = (1 - attn_n) * tape_row + attn_n * write_value

            # Store updated tape
            tl.store(h_tape_ptr + b * stride_tape_b + n * stride_tape_n + d_off * stride_tape_d,
                    new_tape_row, mask=d_off < D)
            # Also store to output history
            tl.store(h_tape_out_ptr + b * (T + 1) * N * D + (t + 1) * N * D + n * D + d_off,
                    new_tape_row, mask=d_off < D)

        # Update h_work for next iteration
        h_work = h_work_new


def dual_memory_elman_forward_triton(
    x_seq: torch.Tensor,       # [B, T, D]
    h_tape_init: torch.Tensor, # [B, N, D]
    h_work_init: torch.Tensor, # [B, D]
    W_h: torch.Tensor,         # [D, D]
    W_x: torch.Tensor,         # [D, D]
    b_h: torch.Tensor,         # [D]
    W_write: torch.Tensor,     # [D, D]
):
    """
    Triton implementation of E23 forward pass.

    Note: This is a reference implementation. For maximum performance,
    use the CUDA kernel with cuBLAS for matrix ops.
    """
    B, T, D = x_seq.shape
    N = h_tape_init.shape[1]
    scale = 1.0 / math.sqrt(D)

    # Convert to float32 for Triton (better precision)
    x_seq_f32 = x_seq.float().contiguous()
    h_tape_f32 = h_tape_init.float().contiguous()
    h_work_f32 = h_work_init.float().contiguous()
    W_h_f32 = W_h.float().contiguous()
    W_x_f32 = W_x.float().contiguous()
    b_h_f32 = b_h.float().contiguous()
    W_write_f32 = W_write.float().contiguous()

    # Allocate outputs
    h_work_out = torch.empty(B, T, D, device=x_seq.device, dtype=torch.float32)
    h_tape_out = torch.empty(B, T + 1, N, D, device=x_seq.device, dtype=torch.float32)
    read_attn_out = torch.empty(B, T, N, device=x_seq.device, dtype=torch.float32)
    write_attn_out = torch.empty(B, T, N, device=x_seq.device, dtype=torch.float32)

    # Block sizes
    BLOCK_D = triton.next_power_of_2(D)
    BLOCK_N = triton.next_power_of_2(N)

    # Launch kernel
    grid = (B,)
    e23_forward_kernel[grid](
        x_seq_f32, h_tape_f32, h_work_f32,
        W_h_f32, W_x_f32, b_h_f32, W_write_f32,
        h_work_out, h_tape_out, read_attn_out, write_attn_out,
        B, T, N, D,
        x_seq_f32.stride(0), x_seq_f32.stride(1), x_seq_f32.stride(2),
        h_tape_f32.stride(0), h_tape_f32.stride(1), h_tape_f32.stride(2),
        h_work_f32.stride(0), h_work_f32.stride(1),
        D,  # stride_W
        scale,
        BLOCK_D=BLOCK_D,
        BLOCK_N=BLOCK_N,
    )

    # Convert back to original dtype
    return (
        h_work_out.to(x_seq.dtype),
        h_tape_out.to(x_seq.dtype),
        read_attn_out.to(x_seq.dtype),
        write_attn_out.to(x_seq.dtype),
    )


def dual_memory_elman_backward_triton(
    x_seq, h_work_all, h_tape_all, read_attn_all, write_attn_all,
    W_h, W_x, b_h, W_write, d_h_work_all, d_h_tape_final
):
    """
    Triton backward pass for E23.

    For now, falls back to Python implementation.
    TODO: Implement optimized Triton backward kernel.
    """
    # Import Python backward
    from .dual_memory_elman import e23_backward_python

    B, T, D = x_seq.shape
    N = h_tape_all.shape[2]
    scale = 1.0 / math.sqrt(D)

    # Get h_work_init from first timestep of h_tape_all (need to pass separately in real impl)
    h_work_init = torch.zeros(B, D, device=x_seq.device, dtype=x_seq.dtype)

    return e23_backward_python(
        x_seq, h_work_all, h_tape_all, read_attn_all, write_attn_all,
        W_h, W_x, b_h, W_write, h_work_init, d_h_work_all, d_h_tape_final, scale
    )


# =============================================================================
# Optimized Triton kernel using cuBLAS for matrix ops
# =============================================================================

def dual_memory_elman_forward_hybrid(
    x_seq: torch.Tensor,       # [B, T, D]
    h_tape_init: torch.Tensor, # [B, N, D]
    h_work_init: torch.Tensor, # [B, D]
    W_h: torch.Tensor,         # [D, D]
    W_x: torch.Tensor,         # [D, D]
    b_h: torch.Tensor,         # [D]
    W_write: torch.Tensor,     # [D, D]
):
    """
    Hybrid implementation: Use PyTorch (cuBLAS) for GEMMs, Triton for fused ops.

    This is more practical than pure Triton for matrix operations.
    """
    B, T, D = x_seq.shape
    N = h_tape_init.shape[1]
    scale = 1.0 / math.sqrt(D)

    # Pre-compute all input projections (batch GEMM)
    # x_proj[t] = x[t] @ W_x.T for all t
    x_proj = x_seq @ W_x.T  # [B, T, D]

    # Initialize states
    h_tape = h_tape_init.clone()
    h_work = h_work_init.clone()

    # Allocate outputs
    h_work_list = []
    h_tape_list = [h_tape.clone()]
    read_attn_list = []
    write_attn_list = []

    for t in range(T):
        # === STEP 1: READ FROM TAPE ===
        # read_scores = (h_tape * h_work[:, None, :]).sum(dim=-1) * scale
        read_scores = torch.einsum('bnd,bd->bn', h_tape, h_work) * scale
        read_attn = torch.softmax(read_scores, dim=-1)  # [B, N]
        read = torch.einsum('bn,bnd->bd', read_attn, h_tape)  # [B, D]

        # === STEP 2: UPDATE WORKING MEMORY ===
        # pre_act = W_h @ h_work + W_x @ x + read + b
        pre_act = h_work @ W_h.T + x_proj[:, t] + read + b_h
        h_work_new = torch.tanh(pre_act)

        # === STEP 3: WRITE TO TAPE ===
        write_value = h_work_new @ W_write.T
        write_scores = torch.einsum('bnd,bd->bn', h_tape, h_work_new) * scale
        write_attn = torch.softmax(write_scores, dim=-1)

        # Replacement write
        h_tape = (1 - write_attn[:, :, None]) * h_tape + write_attn[:, :, None] * write_value[:, None, :]

        # Store
        h_work_list.append(h_work_new)
        h_tape_list.append(h_tape.clone())
        read_attn_list.append(read_attn)
        write_attn_list.append(write_attn)

        h_work = h_work_new

    return (
        torch.stack(h_work_list, dim=1),  # [B, T, D]
        torch.stack(h_tape_list, dim=1),  # [B, T+1, N, D]
        torch.stack(read_attn_list, dim=1),  # [B, T, N]
        torch.stack(write_attn_list, dim=1),  # [B, T, N]
    )


if __name__ == "__main__":
    print("Testing E23 Triton kernels...")
    print("=" * 60)

    device = 'cuda'
    dtype = torch.bfloat16

    # Test dimensions
    B, T, D, N = 2, 16, 64, 8

    # Create inputs
    torch.manual_seed(42)
    x_seq = torch.randn(B, T, D, device=device, dtype=dtype)
    h_tape_init = torch.zeros(B, N, D, device=device, dtype=dtype)
    h_work_init = torch.zeros(B, D, device=device, dtype=dtype)

    W_h = torch.randn(D, D, device=device, dtype=dtype) * 0.1
    W_x = torch.randn(D, D, device=device, dtype=dtype) * 0.1
    b_h = torch.zeros(D, device=device, dtype=dtype)
    W_write = torch.randn(D, D, device=device, dtype=dtype) * 0.1

    # Test hybrid implementation
    print("1. Testing hybrid (PyTorch + einsum) implementation...")
    h_work_out, h_tape_out, read_attn, write_attn = dual_memory_elman_forward_hybrid(
        x_seq, h_tape_init, h_work_init,
        W_h, W_x, b_h, W_write
    )
    print(f"   h_work_out: {h_work_out.shape}")
    print(f"   h_tape_out: {h_tape_out.shape}")
    print(f"   read_attn: {read_attn.shape}")
    print(f"   write_attn: {write_attn.shape}")

    # Verify attention sums to 1
    print(f"   read_attn sum: {read_attn[0, 0].sum().item():.4f}")
    print(f"   write_attn sum: {write_attn[0, 0].sum().item():.4f}")

    # Compare with Python reference
    print("\n2. Comparing with Python reference...")
    from dual_memory_elman import e23_sequence_python

    h_work_ref, h_tape_ref, _ = e23_sequence_python(
        x_seq, h_tape_init, h_work_init,
        W_h, W_x, b_h, W_write
    )

    diff = (h_work_out - h_work_ref).abs().max().item()
    print(f"   Max diff (h_work): {diff:.6f}")
    print(f"   Match: {'PASSED' if diff < 0.01 else 'FAILED'}")

    print("\n" + "=" * 60)
    print("E23 Triton tests completed!")
