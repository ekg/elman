======================================================================
Elman Ablation Ladder Training
======================================================================
  Level 0: Stock Elman (e0) ✓
  Level 1: Mamba-Gated Elman (e1) ✓
  Level 2: Slot Elman (e2) ✓ <-- TRAINING
  Level 3: Low-Rank Slot Elman (e3) ✓
======================================================================
Tokenizer: <elman.data.tokenizers.ByteTokenizer object at 0x723ae20a5a00> (vocab_size=256)
Device: cuda, dtype: torch.bfloat16, world_size: 1
Created Level 2 model: dim=512, depth=21, params=49,715,112

Model Parameters: 49,715,112 (49.72M)
  Embedding: 131,072
  Layers: 21 x 512d
  Layer 0: 2,360,072 params

Loading data from data/pile.txt...
Using AdamWScheduleFree optimizer
Logging to: output/comparison_50m/e2/steps.jsonl

Starting training for 2000 steps...
Batch size per GPU: 32, World size: 1
Effective batch size: 32
Tokens per step: 16,384

Step    100 | Loss 2.9102 | PPL 18.4 | LR 3.00e-04 | Grad 1.24 | Tok/s 20,720 | Mem 4357MB | Elapsed 79.1s
Step    200 | Loss 2.0973 | PPL 8.1 | LR 3.00e-04 | Grad 1.94 | Tok/s 20,771 | Mem 4357MB | Elapsed 157.8s
Step    300 | Loss 1.8835 | PPL 6.6 | LR 3.00e-04 | Grad 1.84 | Tok/s 20,781 | Mem 4357MB | Elapsed 236.5s
Step    400 | Loss 1.8435 | PPL 6.3 | LR 3.00e-04 | Grad 4.62 | Tok/s 20,784 | Mem 4357MB | Elapsed 315.3s
Step    500 | Loss 1.8615 | PPL 6.4 | LR 3.00e-04 | Grad 3.03 | Tok/s 20,789 | Mem 4357MB | Elapsed 394.1s
Step    600 | Loss 1.7723 | PPL 5.9 | LR 3.00e-04 | Grad 3.39 | Tok/s 20,782 | Mem 4357MB | Elapsed 473.0s
Step    700 | Loss 1.7914 | PPL 6.0 | LR 3.00e-04 | Grad 4.81 | Tok/s 20,765 | Mem 4357MB | Elapsed 552.3s
Step    800 | Loss 1.7449 | PPL 5.7 | LR 3.00e-04 | Grad 2.81 | Tok/s 20,751 | Mem 4357MB | Elapsed 631.6s
Step    900 | Loss 1.8625 | PPL 6.4 | LR 3.00e-04 | Grad 2.45 | Tok/s 20,741 | Mem 4357MB | Elapsed 710.9s
Step   1000 | Loss 1.6733 | PPL 5.3 | LR 3.00e-04 | Grad 2.42 | Tok/s 20,749 | Mem 4357MB | Elapsed 789.6s
  >>> Saved: output/comparison_50m/e2/level2_step001000_loss1.6733.pt
Step   1100 | Loss 1.8565 | PPL 6.4 | LR 3.00e-04 | Grad 2.25 | Tok/s 20,724 | Mem 4357MB | Elapsed 869.6s
Step   1200 | Loss 1.6676 | PPL 5.3 | LR 3.00e-04 | Grad 2.00 | Tok/s 20,696 | Mem 4357MB | Elapsed 950.0s
Step   1300 | Loss 1.6671 | PPL 5.3 | LR 3.00e-04 | Grad 2.55 | Tok/s 20,653 | Mem 4357MB | Elapsed 1031.3s
Step   1400 | Loss 1.7033 | PPL 5.5 | LR 3.00e-04 | Grad 6.41 | Tok/s 20,613 | Mem 4357MB | Elapsed 1112.8s
Step   1500 | Loss 1.6888 | PPL 5.4 | LR 3.00e-04 | Grad 4.06 | Tok/s 20,523 | Mem 4357MB | Elapsed 1197.5s
Step   1600 | Loss 1.7248 | PPL 5.6 | LR 3.00e-04 | Grad 1.99 | Tok/s 20,456 | Mem 4357MB | Elapsed 1281.5s
Step   1700 | Loss 1.6902 | PPL 5.4 | LR 3.00e-04 | Grad 2.45 | Tok/s 20,396 | Mem 4357MB | Elapsed 1365.6s
Step   1800 | Loss 1.7447 | PPL 5.7 | LR 3.00e-04 | Grad 1.95 | Tok/s 20,342 | Mem 4357MB | Elapsed 1449.7s
Step   1900 | Loss 1.6730 | PPL 5.3 | LR 3.00e-04 | Grad 1.67 | Tok/s 20,299 | Mem 4357MB | Elapsed 1533.6s
Step   2000 | Loss 1.6677 | PPL 5.3 | LR 3.00e-04 | Grad 2.88 | Tok/s 20,286 | Mem 4357MB | Elapsed 1615.3s
  >>> Saved: output/comparison_50m/e2/level2_step002000_loss1.6677.pt

======================================================================
Training Complete!
======================================================================
Final loss: 1.6677
Total tokens: 32,768,000
Total time: 1615.6s
Parameters: 49,715,112 (49.72M)
