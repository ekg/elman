======================================================================
Elman Ablation Ladder Training
======================================================================
  Level 0: Stock Elman (e0) ✓
  Level 1: Mamba-Gated Elman (e1) ✓ <-- TRAINING
  Level 2: Slot Elman (e2) ✓
  Level 3: Low-Rank Slot Elman (e3) ✓
  Level 4: Low-Rank Elman (e4) ✓
  Level 5: Pure Low-Rank Elman (e5) ✓
  Level 6: Diagonal Elman (e6) ✓
======================================================================
Tokenizer: <elman.data.tokenizers.ByteTokenizer object at 0x793ea4f69940> (vocab_size=256)
Device: cuda, dtype: torch.bfloat16, world_size: 1
Created Level 1 model: dim=512, depth=4, params=9,576,448

Model Parameters: 9,576,448 (9.58M)
  Embedding: 131,072
  Layers: 4 x 512d
  Layer 0: 2,360,064 params

Loading data from data/pile.txt...
Using AdamWScheduleFree optimizer
Logging to: output/e5_parallel_20260105/e1_10m/steps.jsonl

Starting training for 1000 steps...
Batch size per GPU: 32, World size: 1
Effective batch size: 32
Tokens per step: 16,384

Step    100 | Loss 2.9820 | PPL 19.7 | LR 3.00e-04 | Grad 1.15 | Tok/s 157,593 | Mem 177MB | Elapsed 10.4s
Step    200 | Loss 2.2132 | PPL 9.1 | LR 3.00e-04 | Grad 1.45 | Tok/s 149,131 | Mem 177MB | Elapsed 22.0s
Step    300 | Loss 1.9952 | PPL 7.4 | LR 3.00e-04 | Grad 1.56 | Tok/s 143,108 | Mem 177MB | Elapsed 34.3s
Step    400 | Loss 1.9406 | PPL 7.0 | LR 3.00e-04 | Grad 3.83 | Tok/s 145,679 | Mem 177MB | Elapsed 45.0s
Step    500 | Loss 1.9462 | PPL 7.0 | LR 3.00e-04 | Grad 2.50 | Tok/s 153,744 | Mem 177MB | Elapsed 53.3s
Step    600 | Loss 1.8440 | PPL 6.3 | LR 3.00e-04 | Grad 2.92 | Tok/s 160,910 | Mem 177MB | Elapsed 61.1s
Step    700 | Loss 1.8648 | PPL 6.5 | LR 3.00e-04 | Grad 2.88 | Tok/s 166,468 | Mem 177MB | Elapsed 68.9s
Step    800 | Loss 1.8148 | PPL 6.1 | LR 3.00e-04 | Grad 2.11 | Tok/s 171,010 | Mem 177MB | Elapsed 76.6s
Step    900 | Loss 1.9441 | PPL 7.0 | LR 3.00e-04 | Grad 2.03 | Tok/s 174,673 | Mem 177MB | Elapsed 84.4s
Step   1000 | Loss 1.7337 | PPL 5.7 | LR 3.00e-04 | Grad 1.95 | Tok/s 177,740 | Mem 177MB | Elapsed 92.2s
  >>> Saved: output/e5_parallel_20260105/e1_10m/level1_step001000_loss1.7337.pt

======================================================================
Training Complete!
======================================================================
Final loss: 1.7337
Total tokens: 16,384,000
Total time: 92.3s
Parameters: 9,576,448 (9.58M)
